{"cells":[{"cell_type":"markdown","metadata":{},"source":["# SET UP"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: arcgis in c:\\program files\\arcgis\\pro\\bin\\python\\envs\\arcgispro-py3\\lib\\site-packages (2.2.0.1)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install arcgis"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["#Before processing Stream Habitat Condition you must process SEZ data\n","#Stream Habitat Condition is calculated based on Riverine Indicators and IPI.. or is it just IPI and CSCI\n","\n","import arcpy\n","from datetime import datetime\n","from functools import reduce\n","import os\n","from sqlalchemy.engine import URL\n","from arcgis.features import FeatureSet, GeoAccessor, GeoSeriesAccessor, FeatureLayer\n","import pandas as pd\n"]},{"cell_type":"markdown","metadata":{},"source":["## Look up Dictionaries- SEZ_ID"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Large Polygons or only polygon shapes lookup dictionary for Assessment Units with lerger values of acreage\n","\n","# Step 1: Read the Excel file into a DataFrame\n","excel_data = pd.read_csv(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Large_Polygon_Lookup.csv\")  \n","\n","#Define Empty look up dataframe\n","lookup_dict = {}\n","\n","for index, row in excel_data.iterrows():\n","    lookup_dict[row['Assessment_Unit_Name']] = row['SEZ_ID']\n","\n","# See dictionary where keys are Assessment Unit Names and values are SEZ IDs\n","print(lookup_dict)\n","\n","#Small Polygon if there are two acres for an SEZ\n","# Step 1: Read the Excel file into a DataFrame\n","excel_data = pd.read_csv(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Small_Polygon_Lookup.csv\")  \n","\n","#Define Empty look up dataframe\n","lookup_riverine = {}\n","\n","for index, row in excel_data.iterrows():\n","    lookup_riverine[row['Assessment_Unit_Name']] = row['SEZ_ID']\n","\n","# See dictionary where keys are Assessment Unit Names and values are SEZ IDs\n","print(lookup_riverine)\n","\n","#All Polygons\n","# Step 1: Read the Excel file into a DataFrame\n","excel_data = pd.read_csv(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\All_SEZID_Lookup.csv\")  \n","\n","#Define Empty look up dataframe\n","lookup_all = {}\n","\n","for index, row in excel_data.iterrows():\n","    lookup_all[row['SEZ_ID']] = {'SEZ_Type': row['SEZ_Type']}\n","\n","# See dictionary where keys are Assessment Unit Names and values are SEZ IDs\n","print(lookup_all)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Import Data and Create DataFrames"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["   OBJECTID     Acres                Assessment              SEZ_Type  \\\n","0         1  1.935925  Big Meadow Creek - upper  Riverine (Perennial)   \n","1         2   6.85073              UTR - middle  Riverine (Perennial)   \n","2         3  4.089876  UTR - Christmas Valley 3  Riverine (Perennial)   \n","3         4  5.695386  Saxon Creek - headwaters  Riverine (Perennial)   \n","4         5   6.32458       Trout Creek - upper  Riverine (Perennial)   \n","\n","  Ownership_ Final_Rati                                GlobalID created_us  \\\n","0       USFS          A  {C2A1C096-7BDE-4365-8C86-26BE5B92DF3E}              \n","1       USFS          A  {65A0A4D1-8DF3-408A-80F2-5EF4BEC8C0B3}              \n","2       USFS          A  {CA6A0D77-BB25-48AC-A76B-8292EEA6BEC3}              \n","3       USFS          A  {285EC42B-CFC3-487E-8B0B-1B871EEE1974}              \n","4       USFS          A  {22B839DC-D548-470A-BB7C-5E5734ACAC0A}              \n","\n","  created_da last_edite last_edi_1  SEZ_ID  \\\n","0 1899-12-30            1899-12-30     237   \n","1 1899-12-30            1899-12-30     238   \n","2 1899-12-30            1899-12-30     239   \n","3 1899-12-30            1899-12-30     241   \n","4 1899-12-30            1899-12-30     242   \n","\n","                                               SHAPE  \n","0  {\"rings\": [[[762418.6545000002, 4295905.1272],...  \n","1  {\"rings\": [[[758307.4699999997, 4296962.810000...  \n","2  {\"rings\": [[[758641.4299999997, 4301673.67], [...  \n","3  {\"rings\": [[[761229.2355000004, 4304824.2784],...  \n","4  {\"rings\": [[[763248.3300000001, 4306888.689999...  \n"]}],"source":["#SETUP\n","def get_fs_data(fs_url):\n","    feature_layer = FeatureLayer(fs_url)\n","    query_result = feature_layer.query()\n","    feature_list = query_result.features\n","    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n","    return all_data\n","\n","def get_fs_data_spatial(fs_url):\n","    feature_layer = FeatureLayer(fs_url)\n","    query_result = feature_layer.query().sdf\n","    query_result.spatial.sr = 26910\n","    return query_result\n","\n","# Get Stream Location data so we can do a spatial join on stream miles or riverine units\n","stream_url = \"https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/8\"\n","\n","streamsdf = get_fs_data_spatial(stream_url)# Create DataFrame\n","\n","#Get SEZ spatially enabled dataframe from REST Service\n","SEZ_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/0\"\n","\n","dfSEZ = get_fs_data_spatial(SEZ_url)\n","\n","#spatial reference stuff\n","streamsdf.spatial.sr = dfSEZ.spatial.sr\n","\n","#import Riverine Indicators\n","\n","#REST SERVICES\n","bank_stability_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/4\"\n","biotic_integrity_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/5\"\n","incision_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/10\"\n","headcuts_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/9\"\n","AOP_url= \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/3\"\n","Hab_Frag_url = 'https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/8'\n","\n","#Get feature service data inot dataframe\n","dfbanks = get_fs_data(bank_stability_url)\n","dfbiotic = get_fs_data(biotic_integrity_url)\n","dfincision = get_fs_data(incision_url)\n","dfheadcuts = get_fs_data(headcuts_url)\n","dfAOP = get_fs_data(AOP_url)\n","dfhabitat = get_fs_data(Hab_Frag_url)\n","\n","#Stream Habitat Condition ASsessment fc that has SEZ_ID These are a copy ofsde.Fisheries.sde.Stream_Assessment_2020\n","\n","def get_fc_data_spatial(fc_path, spatial_reference=26910):\n","   \n","    # Load the feature class into a spatially enabled DataFrame\n","    sdf = pd.DataFrame.spatial.from_featureclass(fc_path)\n","    \n","    # Set the spatial reference\n","    sdf.spatial.sr = spatial_reference\n","    \n","    return sdf\n","\n","# Example usage\n","# Path to your feature class within a geodatabase\n","Streammiles_path = \"F:\\GIS\\PROJECTS\\ResearchAnalysis\\Stream Habitat Condition\\Stream_Habitat_Condition.gdb\\Stream_Habitat_Condition\"\n","\n","# Get the spatially enabled DataFrame\n","#Add Threshold Year to this data so that we can stack this data like SEZ Assessment Units\n","Stream_Miles_sdf = get_fc_data_spatial(Streammiles_path)\n","\n","# Display the first few rows\n","print(Stream_Miles_sdf.head())\n","\n","#Import IPI Data \n","IPIfolder = \"F:\\Research and Analysis\\Fisheries\\Streams\\Bioassessment\"\n","IPI22 = os.path.join(IPIfolder, \"2022\", \"IPI_22.csv\")\n","IPI20 = os.path.join(IPIfolder, \"2020\", \"IPI_20.csv\")\n","\n","#Create IPI Dataframes\n","IPI22df = pd.read_csv(IPI22)\n","IPI20df = pd.read_csv(IPI20)\n","\n","IPI22df['IPIYear']= '2022'\n","IPI20df['IPIYear']= '2020'\n","\n","#Merge dataframes into one \n","concatIPI_df = pd.concat([IPI22df, IPI20df], axis=0, ignore_index=True)\n","#perform spatial join of sde.stream and sez units\n","#thesdf = SEZsdf.spatial.join(streamsdf, how='inner')\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Grading Indicators"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Grading \n","    \n","#Scoring based off of grading - check this\n","def score_indicator(Rating):\n","    if pd.isna(Rating):\n","        return np.nan\n","    elif  Rating == 'A':\n","        return '12'\n","    elif Rating == 'B':\n","        return '9'\n","    elif Rating == 'C':\n","        return '6'\n","    else:\n","        return '3'\n","\n","    \n","#define rating SEZ Rating\n","def rate_SEZ(percent):\n","    if 0 <= percent < .70:\n","        return 'D'\n","    elif .7 <= percent < .80:\n","        return 'C'\n","    elif .80 <= percent < .90:\n","        return 'B'\n","    else:\n","        return 'A'\n","    \n","#Define Grade for IPI Score - Used only for Stream HAbitat Condition\n","def categorize_phab(IPI):\n","     if   IPI >= 0.94:\n","        return 'A'\n","     elif 0.83 < IPI < 0.94:\n","        return 'B'\n","     elif 0.7 < IPI <= 0.83:\n","        return 'C'\n","     else:\n","        return 'D'\n","     \n","\n","#Define Grade for Bioassessment Score\n","def categorize_csci(biotic_integrity):\n","     if pd.isna(biotic_integrity):\n","        return np.nan\n","     elif   biotic_integrity > 0.92:\n","        return 'A'\n","     elif 0.79 < biotic_integrity <= 0.92:\n","        return 'B'\n","     elif 0.62 < biotic_integrity <= 0.79:\n","        return 'C'\n","     else:\n","        return 'D'\n","\n","     \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Prep Riverine Indicator Data"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["         Assessment_Unit_Name  Year  Biotic_Integrity_CSCI  \\\n","0    Angora Creek - tributary  2013               0.996000   \n","1        Angora Creek - upper  2013               0.996000   \n","2          Angora meadows - 1  2019               0.690000   \n","3          Angora meadows - 2  2019               0.820000   \n","4          Angora meadows - 3  2017               0.940000   \n","..                        ...   ...                    ...   \n","226       Woods Creek - lower  2018               0.874000   \n","227      Woods Creek - middle  2015               1.014000   \n","228       Woods Creek - upper  2010               1.101000   \n","229            small meadow 1  2018               1.010000   \n","230           small meadow 57  2022               0.564053   \n","\n","    Biotic_Integrity_Data_Source Biotic_Integrity_Rating  \\\n","0          TRPA, 634S13217, 2013                       A   \n","1          TRPA, 634S13217, 2013                       A   \n","2          TRPA, 634S19606, 2019                       C   \n","3          TRPA, 634S19498, 2019                       B   \n","4          TRPA, 634S17345, 2017                       A   \n","..                           ...                     ...   \n","226        TRPA, 634TPB112, 2018                       B   \n","227        TRPA, 634S15348, 2015                       A   \n","228        TRPA, 634S10092, 2010                       A   \n","229        TRPA, 634REFNLH, 2018                       A   \n","230         TRPA, 634HVC-1, 2022                       D   \n","\n","     Biotic_Integrity_Score  \n","0                        12  \n","1                        12  \n","2                         6  \n","3                         9  \n","4                        12  \n","..                      ...  \n","226                       9  \n","227                      12  \n","228                      12  \n","229                      12  \n","230                       3  \n","\n","[231 rows x 6 columns]\n","     OBJECTID  SEZ_ID      Assessment_Unit_Name  Year  Number_of_Headcuts  \\\n","0         449   519.0  Angora Creek - tributary  2020                   0   \n","1         450   519.0  Angora Creek - tributary  2022                   0   \n","2         451   519.0  Angora Creek - tributary  2023                   4   \n","3         452   519.0  Angora Creek - tributary  2023                   0   \n","4         453    87.0        Angora meadows - 1  2019                   0   \n","..        ...     ...                       ...   ...                 ...   \n","377      2631    44.0           small meadow 96  2023                   0   \n","378      2632    62.0           small meadow 98  2019                   0   \n","379      2633    75.0           small meadow 99  2020                   0   \n","380      2634    75.0           small meadow 99  2022                   0   \n","381      2635     NaN            unnamed meadow  2019                   0   \n","\n","    Headcuts_Data_Source  Headcuts_Score Headcuts_Rating  \\\n","0                   TRPA            12.0               A   \n","1                   TRPA            12.0               A   \n","2                   TRPA             9.0               B   \n","3                   TRPA            12.0               A   \n","4                   TRPA            12.0               A   \n","..                   ...             ...             ...   \n","377                 TRPA            12.0               A   \n","378                 TRPA            12.0               A   \n","379                 TRPA            12.0               A   \n","380                 TRPA            12.0               A   \n","381                 TRPA            12.0               A   \n","\n","                                   GlobalID created_user   created_date  \\\n","0    {B791D11F-A207-4BC2-BA77-6A76EAD19115}     SNEWSOME  1720715810000   \n","1    {B21B47EF-207C-4AA5-A8E7-9AB5251E460F}     SNEWSOME  1720715810000   \n","2    {98D76551-DBB4-467C-92B8-A6EADEC0CB24}     SNEWSOME  1720715810000   \n","3    {7184D8C1-428A-479D-B08E-4CD4AFB8B7D0}     SNEWSOME  1720715810000   \n","4    {EFBCE2A2-3F62-47A7-8CD3-6536E5CC18AA}     SNEWSOME  1720715810000   \n","..                                      ...          ...            ...   \n","377  {01430875-7EB8-44F1-9B8D-EE0E2711E682}     SNEWSOME  1720715810000   \n","378  {1894CDC9-5519-4A84-8D4D-2B1BA17A7594}     SNEWSOME  1720715810000   \n","379  {39EEB79D-A118-4E62-B3BB-3F9CF2A05277}     SNEWSOME  1720715810000   \n","380  {A1A5D8F6-D4A5-4C23-9D20-A794F5CDC601}     SNEWSOME  1720715810000   \n","381  {4C00C623-EF20-47E3-9B31-03A8BC96F80E}     SNEWSOME  1720715810000   \n","\n","    last_edited_user  last_edited_date  \n","0           SNEWSOME     1720715810000  \n","1           SNEWSOME     1720715810000  \n","2           SNEWSOME     1720715810000  \n","3           SNEWSOME     1720715810000  \n","4           SNEWSOME     1720715810000  \n","..               ...               ...  \n","377         SNEWSOME     1720715810000  \n","378         SNEWSOME     1720715810000  \n","379         SNEWSOME     1720715810000  \n","380         SNEWSOME     1720715810000  \n","381         SNEWSOME     1720715810000  \n","\n","[382 rows x 13 columns]\n"]}],"source":["#------------------#\n","#Biotic Integrity\n","#------------------#\n","#Prep data- Add any scores and find average oif there are two stream sites for one sez. Also rename data source so it includes are streams that were averaged\n","# Function to average scores and concatenate data sources for each Year and Assessment_Unit_Name\n","def average_biotic_scores(dfbiotic, unit_col='Assessment_Unit_Name', year_col='Year', score='Biotic_Integrity_CSCI', source_col='Biotic_Integrity_Data_Source'):\n","    # Group by Assessment Unit and Year\n","    group = dfbiotic.groupby([unit_col, year_col])\n","    \n","    # Calculate the mean of the scores\n","    averaged_scores = group[score].mean().reset_index()\n","    \n","    # Concatenate the data sources with specific formatting\n","    def concatenate_sources(x, year):\n","        formatted_sources = []\n","        for entry in x:\n","            parts = entry.split(\",\")\n","            if len(parts) >= 3:\n","                formatted_sources.append(f'TRPA, {parts[1].strip()}, {parts[-1].strip()}')  # Extract station code and year\n","        if formatted_sources:\n","            return '/ '.join(formatted_sources)\n","        else:\n","            return None  # Return None if all entries are invalid\n","    \n","    # Apply concatenate_sources to each group\n","    concatenated_sources = group.apply(lambda grp: concatenate_sources(grp[source_col], grp[year_col])).reset_index(name=source_col)\n","    \n","    # Merge the averaged scores with concatenated sources\n","    averaged_df = pd.merge(averaged_scores, concatenated_sources, on=[unit_col, year_col], how='left')\n","    \n","    return averaged_df\n","\n","\n","# Drop duplicates based on 'Assessment_Unit_Name' and 'Year'\n","dfbiotic = dfbiotic.drop_duplicates(subset=['Assessment_Unit_Name', 'Year', 'Biotic_Integrity_CSCI'])\n","\n","# Apply the function to dfbiotic\n","averaged_biotic_df = average_biotic_scores(dfbiotic)\n","\n","# Apply the rating function to the averaged biotic integrity scores\n","averaged_biotic_df['Biotic_Integrity_Rating'] = averaged_biotic_df['Biotic_Integrity_CSCI'].apply(categorize_csci)\n","\n","# Calculate the biotic score for each SEZ\n","averaged_biotic_df['Biotic_Integrity_Score'] = averaged_biotic_df['Biotic_Integrity_Rating'].apply(score_indicator)\n","\n","averaged_biotic_df['Biotic_Integrity_Score']=averaged_biotic_df['Biotic_Integrity_Score'].astype(int)\n","\n","# Output the resulting DataFrame\n","print(averaged_biotic_df)\n","\n","#-------------------\n","# Headcuts \n","#------------------\n","#Reorganize dfHeadcuts to drop small medium large headcut columns\n","# Drop the columns 'small', 'medium', and 'large'\n","dfheadcuts = dfheadcuts.drop(columns=['small', 'medium', 'large'])\n","\n","# Print the DataFrame to see the changes\n","print(dfheadcuts)\n","\n","#---------------\n","#add year to data source so we can drop the year column later (Dont double run this)\n","#---------------\n","#Create Dictionary of Dataframes to adjust year to be in datashource column and not its own column\n","yeartodatasource = {\n","    'dfbanks': dfbanks,\n","    'dfheadcuts': dfheadcuts,\n","    'dfincision': dfincision\n","}\n","\n","# Iterate over each DataFrame in meadowdata\n","for name, df in yeartodatasource.items():\n","    # Iterate over columns in the DataFrame\n","    for col in df.columns:\n","        # Check if the column name contains 'Data'\n","        if 'Data_' in col:\n","            # Add Year to the column if it contains 'Data'\n","            df[col] = df[col] + ', ' + df['Year'].astype(str)\n","#------------#\n","#not sure we need this\n","#Prep SEZ Baseline Data for assessment unit...will need to rethink if acreage changes.. or just manually change in sde\n","#keep_columns = ['SHAPE', 'SEZ_ID', 'Feature_Type', 'SEZ_Type', 'Ownership_Primary', 'Ownership_Secondary', 'Ownership_Secondary_2', 'Ownership_Secondary_3', 'Acres', 'Comments']\n","#dfSEZ is assessment unit information from SDE?\n","#dfSEZinfo=dfSEZ.loc[:,keep_columns].copy()\n","\n","#dfSEZinfo['SEZ_ID']= dfSEZinfo['SEZ_ID'].astype(int)"]},{"cell_type":"markdown","metadata":{},"source":["# Prep/Process IPI Scores"]},{"cell_type":"markdown","metadata":{},"source":["## Score IPI"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["   StationCode   IPI IPI_Rating IPI_Score IPIYear         IPI_DataSource\n","0    634EDG001  0.89          B         9    2022  TRPA, 634EDG001, 2022\n","1    634REFBMW  0.96          A        12    2022  TRPA, 634REFBMW, 2022\n","2    634REFSAX  0.84          B         9    2022  TRPA, 634REFSAX, 2022\n","3    634REFTRT  0.94          A        12    2022  TRPA, 634REFTRT, 2022\n","4    634REFUTR  0.88          B         9    2022  TRPA, 634REFUTR, 2022\n","..         ...   ...        ...       ...     ...                    ...\n","85   634TPB151  0.99          A        12    2020  TRPA, 634TPB151, 2020\n","86   634TPB155  1.09          A        12    2020  TRPA, 634TPB155, 2020\n","87   634TRT003  0.75          C         6    2020  TRPA, 634TRT003, 2020\n","88   634UTR002  0.99          A        12    2020  TRPA, 634UTR002, 2020\n","89   634UTR006  1.07          A        12    2020  TRPA, 634UTR006, 2020\n","\n","[90 rows x 6 columns]\n"]}],"source":["#Import IPI Data in set up block\n","#IPIfolder = \"F:\\Research and Analysis\\Fisheries\\Streams\\Bioassessment\"\n","#IPI22 = os.path.join(IPIfolder, \"2022\", \"IPI_22.csv\")\n","#IPI20 = os.path.join(IPIfolder, \"2020\", \"IPI_20.csv\")\n","\n","#Create IPI Dataframes\n","#IPI22df = pd.read_csv(IPI22)\n","#IPI20df = pd.read_csv(IPI20)\n","\n","#IPI22df['IPIYear']= '2022'\n","#IPI20df['IPIYear']= '2020'\n","\n","#Merge dataframes into one \n","#concatIPI_df = pd.concat([IPI22df, IPI20df], axis=0, ignore_index=True)\n","\n","#Calculate Scores in IPI\n","#Code for Grading IPI\n","#Define Grade for TRPA's IPI Score - Used only for Stream HAbitat Condition\n","#def categorize_phab(IPI):\n"," #    if   IPI >= 0.94:\n","  #      return 'A'\n","   #  elif 0.83 < IPI < 0.94:\n","    #    return 'B'\n","#     elif 0.7 < IPI <= 0.83:\n"," #       return 'C'\n","  #   else:\n","   #     return 'D'\n","\n","concatIPI_df['IPI_Rating']=concatIPI_df['IPI'].apply(categorize_phab)\n","concatIPI_df['IPI_Score']= concatIPI_df['IPI_Rating'].apply(score_indicator)\n","\n","\n","\n","concatIPI_df.head()\n","\n","columns_to_keep = ['StationCode', 'IPI', 'IPI_Rating', 'IPI_Score', 'IPIYear']\n","\n","concatIPI_df = concatIPI_df[columns_to_keep]\n","concatIPI_df['IPIYear']= concatIPI_df['IPIYear'].astype(str)\n","concatIPI_df['IPI_DataSource']= 'TRPA, ' + concatIPI_df['StationCode'] +', ' + concatIPI_df['IPIYear']\n","\n","#Tune Up DATA Source Column to include year and station code\n","\n","print(concatIPI_df)\n","\n","#Join on station code(StationCode) to streamsdf(SITE_NAME)\n"]},{"cell_type":"markdown","metadata":{},"source":["## IPI site locations"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["   StationCode   IPI IPI_Rating IPI_Score IPIYear         IPI_DataSource  \\\n","0    634REFUTR  0.88          B         9    2022  TRPA, 634REFUTR, 2022   \n","7    634REFUTR  0.88          B         9    2020  TRPA, 634REFUTR, 2020   \n","14   634TPB136  1.02          A        12    2022  TRPA, 634TPB136, 2022   \n","19   634TPB136  1.02          A        12    2020  TRPA, 634TPB136, 2020   \n","24   634S22534  0.90          B         9    2022  TRPA, 634S22534, 2022   \n","25   634S22534  0.90          B         9    2020  TRPA, 634S22534, 2020   \n","26   634TPB108  0.86          B         9    2022  TRPA, 634TPB108, 2022   \n","30   634TPB108  0.86          B         9    2020  TRPA, 634TPB108, 2020   \n","34   634S22514  1.09          A        12    2022  TRPA, 634S22514, 2022   \n","35   634S22514  1.09          A        12    2020  TRPA, 634S22514, 2020   \n","36   634S11162  0.91          B         9    2022  TRPA, 634S11162, 2022   \n","38   634S11162  0.91          B         9    2020  TRPA, 634S11162, 2020   \n","40   634TPB141  1.07          A        12    2020  TRPA, 634TPB141, 2020   \n","44   634TPB141  1.07          A        12    2022  TRPA, 634TPB141, 2022   \n","50   634TRT003  0.75          C         6    2022  TRPA, 634TRT003, 2022   \n","52   634TRT003  0.75          C         6    2020  TRPA, 634TRT003, 2020   \n","\n","                                                SHAPE  SEZ_ID  \\\n","0   {\"rings\": [[[758307.4699999997, 4296962.810000...     238   \n","7   {\"rings\": [[[758307.4699999997, 4296962.810000...     238   \n","14  {\"rings\": [[[749957.0899999999, 4345614.25], [...     259   \n","19  {\"rings\": [[[749957.0899999999, 4345614.25], [...     259   \n","24  {\"rings\": [[[764069.9391000001, 4310205.329299...     276   \n","25  {\"rings\": [[[764069.9391000001, 4310205.329299...     276   \n","26  {\"rings\": [[[763699.5778000001, 4350516.2985],...     284   \n","30  {\"rings\": [[[763699.5778000001, 4350516.2985],...     284   \n","34  {\"rings\": [[[758532.9122000001, 4303153.091499...     347   \n","35  {\"rings\": [[[758532.9122000001, 4303153.091499...     347   \n","36  {\"rings\": [[[760878.9175000004, 4312483.1335],...     396   \n","38  {\"rings\": [[[760878.9175000004, 4312483.1335],...     396   \n","40  {\"rings\": [[[760594.4546999997, 4308065.613500...     397   \n","44  {\"rings\": [[[760594.4546999997, 4308065.613500...     397   \n","50  {\"rings\": [[[762393.5003000004, 4311674.07], [...     425   \n","52  {\"rings\": [[[762393.5003000004, 4311674.07], [...     425   \n","\n","                      Assessment  \n","0                   UTR - middle  \n","7                   UTR - middle  \n","14                  Watson Creek  \n","19                  Watson Creek  \n","24           Cold Creek - middle  \n","25           Cold Creek - middle  \n","26         Deer Creek - middle 2  \n","30         Deer Creek - middle 2  \n","34      UTR - Christmas Valley 1  \n","35      UTR - Christmas Valley 1  \n","36          UTR Marsh - UTR side  \n","38          UTR Marsh - UTR side  \n","40                 UTR - Reach 5  \n","44                 UTR - Reach 5  \n","50  Trout Creek above Black Bart  \n","52  Trout Creek above Black Bart  \n"]}],"source":["#Collect IPI SEZ location information of based on stationcode\n","#  Join on ipi (StationCode) to streamsdf(SITE_NAME) so we have a location for the sites\n","merged_df = pd.merge(concatIPI_df, streamsdf, left_on='StationCode', right_on='SITE_NAME', how='inner')\n","\n","#Clean up physical habitat dataframe\n","# Keep only phab data and spatial data from Stream data do i need latitude and longitude.. i don't think so... (, 'LATITUDE', 'LONGITUDE')\n","phab_columns = ['StationCode', 'IPI', 'IPI_Rating', 'IPI_Score', 'IPIYear', 'IPI_DataSource','SHAPE']\n","\n","Phabsdf = merged_df[phab_columns]\n","\n","#Spatial join to assessment units \n","PHABSEZsdf= Stream_Miles_sdf.spatial.join(Phabsdf, how='inner')\n","\n","\n","\n","# Clean up duplicates drop duplicates directly if that's more appropriate for your case:\n","cleaned_sdf = PHABSEZsdf.drop_duplicates(subset=['SEZ_ID', 'IPIYear', 'IPI_Score'])\n","\n","IPI_columns = ['StationCode', 'IPI', 'IPI_Rating', 'IPI_Score', 'IPIYear', 'IPI_DataSource','SHAPE', 'SEZ_ID', 'Assessment']\n","\n","IPI_sdf = cleaned_sdf[IPI_columns]\n","print(IPI_sdf)\n","#Get most recent"]},{"cell_type":"markdown","metadata":{},"source":["# Prep Riverine Indicators"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#RiverineIndicators = ['AOP_Score', 'Bank_Stability_Score', 'Biotic_Integrity_Score', 'Habitat_Frag_Score', 'Incision_Score', 'Headcuts_Score']\n","#SEZID's aren't correct in score tables so I need to do all of these in order to get the correct SEZ IDS?\n","# Same for meadow(large polygon) and riverine(small polygon) data drop these columns because not needed in final merge, will assign SEZ ID later\n","columns_to_drop = {'Year', 'SEZ_ID', 'GlobalID', 'last_edited_user', 'created_date', 'OBJECTID', 'created_user', 'last_edited_date'}\n","\n","#Name dataframes so we can reference later\n","largepolygondata= {'dfbanks': dfbanks, \n","             'dfaveraged_biotic':averaged_biotic_df,\n","                'dfincision': dfincision,\n","                'dfhabitat': dfhabitat,\n","                'dfheadcuts': dfheadcuts,\n","                'dfAOP': dfAOP\n","}\n","\n","\n","#Staging Tables Riverine/ small polygons\n","smallpolygondata = {'dfbanks': dfbanks, \n","                'dfaveraged_biotic':averaged_biotic_df,\n","                'dfincision': dfincision,\n","                'dfhabitat': dfhabitat,\n","                'dfheadcuts': dfheadcuts,\n","                'dfAOP': dfAOP\n","}\n","\n","#Get most recent year of data for each Assessment Unit NAme\n","# Function to get the most recent year of data\n","# Function to get the most recent year of data\n","def get_most_recent_scores(df, groupfield):\n","    return df.loc[df.groupby(groupfield)['Year'].idxmax()]\n","\n","#most_recent_small = get_most_recent_scores(smallpolygondata, 'Assessment_Unit_Name')\n","#mosrecent_large = get_most_recent_scores(largepolygondata, 'Assessment_Unit_Name')\n","\n","# Function to drop unnecessary columns from DataFrames\n","def drop_columns(df, columns_to_drop):\n","    return df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n","\n","\n","# Function to assign SEZ_ID to each DataFrame using the provided lookup dictionary\n","def assign_sez_ids(df, sezid_dict):\n","    df['SEZ_ID'] = df['Assessment_Unit_Name'].map(sezid_dict)\n","    df = df.dropna(subset=['SEZ_ID'])\n","    \n","    # Use .loc to modify SEZ_ID safely\n","    df.loc[:, 'SEZ_ID'] = df['SEZ_ID'].astype(int)\n","    \n","    return df\n","\n","# Process data for large and small polygons\n","def process_data(data_dict, sezid_dict, columns_to_drop):\n","    processed_data = {}\n","    for key, df in data_dict.items():\n","        # Step 1: Get most recent scores\n","        df_most_recent = get_most_recent_scores(df, 'Assessment_Unit_Name')\n","        \n","        # Step 2: Drop unnecessary columns\n","        df_cleaned = drop_columns(df_most_recent, columns_to_drop)\n","        \n","        # Step 3: Assign SEZ_ID\n","        df_with_sez_id = assign_sez_ids(df_cleaned, sezid_dict)\n","        \n","        # Store the processed DataFrame\n","        processed_data[key] = df_with_sez_id\n","    return processed_data\n","\n","# Process large polygon (meadow) and small polygon (riverine) data\n","processed_largepolygon_data = process_data(largepolygondata, lookup_dict, columns_to_drop)\n","processed_smallpolygon_data = process_data(smallpolygondata, lookup_riverine, columns_to_drop)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to merge all DataFrames on multiple keys\n","def merge_dataframes(data_dict, keys):\n","    return reduce(lambda left, right: pd.merge(left, right, on=keys, how='outer'), data_dict.values())\n","\n","# Merge small polygon DataFrames\n","smallpolygon_df = merge_dataframes(processed_smallpolygon_data, ['SEZ_ID', 'Assessment_Unit_Name'])\n","\n","# Merge large polygon DataFrames\n","largepolygon_df = merge_dataframes(processed_largepolygon_data, ['SEZ_ID', 'Assessment_Unit_Name'])\n","\n","# Append smallpolygon_df to largepolygon_df\n","final_combined_df = pd.concat([largepolygon_df, smallpolygon_df], ignore_index=True)\n","\n","\n","# Print the final combined DataFrame to check\n","print(\"Final Combined DataFrame:\")\n","print(final_combined_df)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":2}
