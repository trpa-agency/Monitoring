{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Raw CSV's created in R to update SDE.Stream with CSCI scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureSet, GeoAccessor, GeoSeriesAccessor, FeatureLayer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "gis = GIS()\n",
    "# # Set Pandas display options to show all rows and columns\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# in memory output file path\n",
    "wk_memory = \"memory\" + \"\\\\\"\n",
    "\n",
    "# set workspace and sde connections \n",
    "working_folder = r\"F:/Research and Analysis/Fisheries/Streams/Bioassessment/California Stream Condition Index/California Stream Condition Index\"\n",
    "workspace      = r\"F:/Research and Analysis/Workspace/Evelyn/Scratch.gdb\"\n",
    "arcpy.env.workspace = r\"F:/Research and Analysis/Workspace/Evelyn/Scratch.gdb\"\n",
    "\n",
    "# network path to connection files\n",
    "filePath = r\"F:\\GIS\\DB_CONNECT\"\n",
    "\n",
    "# database file path \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "\n",
    "# local variables\n",
    "fdata = os.path.join(sdeBase, \"sde.SDE.Monitoring\")\n",
    "## Final feature class to append to in Enterprise Geodatabase\n",
    "sdeStreams = os.path.join(sdeBase, \"sde.SDE.Monitoring\\sde.SDE.Stream\")\n",
    "## orginal CSVs that come from preprocessing or R tools created by State\n",
    "#originalcsv22 = os.path.join(working_folder,\"2022_CSCI\",\"19-20NV-22allcore.csv\")\n",
    "#locationcsv22 = os.path.join(working_folder, \"2022_CSCI\",\"Stations19_22.csv\")\n",
    "#originalcsv20 = os.path.join(working_folder,\"2020_CSCI\",\"core.csv\")\n",
    "#locationcsv20 = os.path.join(working_folder,\"2020_CSCI\",\"Stations_20.csv\")\n",
    "#originalcsv23 = os.path.join(working_folder,\"2023_CSCI\",\"CSCI_Report_2023.csv\")\n",
    "#locationcsv23 = os.path.join(working_folder,\"2023_CSCI\",\"stations_23.csv\")\n",
    "\n",
    "originalcsv24 = os.path.join(working_folder,\"2024_CSCI\",\"CSCI_Report_2024.csv\")\n",
    "locationcsv24 = os.path.join(working_folder,\"2024_CSCI\",\"Stations_2024.csv\")\n",
    "\n",
    "working_folder = os.path.join(working_folder,\"2024_CSCI\")\n",
    "\n",
    "originalcsv = originalcsv24\n",
    "locationcsv = locationcsv24\n",
    "\n",
    "if not os.path.exists(originalcsv):\n",
    "    print(f\"Error: File not found at {originalcsv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do I need this? -->\n",
    "#df = get_fs_data('https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assign Station type and lat long and LTinfo website to Trend Sites\n",
    "\n",
    "\n",
    "#Calculate Rating for CSCI value\n",
    "#Define a function to categorize values based on ranges\n",
    "def categorize_value(value):\n",
    "    if 0 <= value < 0.6:\n",
    "        return 'poor'\n",
    "    elif 0.6 <= value < 0.8:\n",
    "        return 'marginal'\n",
    "    elif 0.8 <= value < 1.0:\n",
    "        return 'good'\n",
    "    else:\n",
    "        return 'excellent'\n",
    "    \n",
    "def get_fs_data(service_url):\n",
    "    \n",
    "    feature_layer = FeatureLayer(service_url)\n",
    "    query_result = feature_layer.query()\n",
    "    # Convert the query result to a list of dictionaries\n",
    "    feature_list = query_result.features\n",
    "\n",
    "    # Create a pandas DataFrame from the list of dictionaries\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dictionary Using Rest Service data\n",
    "# setup\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureLayer\n",
    "\n",
    "# Connect to TRPA Enterprise GIS Portal *if it's a service only shared with org\n",
    "# portal_user = \"TRPA_PORTAL_ADMIN\"\n",
    "# portal_pwd = str(os.environ.get('Password'))\n",
    "# portal_url = \"https://maps.trpa.org/portal/\"\n",
    "\n",
    "# setup connection to GIS server this can be GIS() with a public service\n",
    "gis = GIS()\n",
    "\n",
    "\n",
    "# get Stream data as a Spatially Enabled Dataframe\n",
    "service_url = 'https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/8'\n",
    "feature_layer = FeatureLayer(service_url)\n",
    "query_result = feature_layer.query()\n",
    "\n",
    "# Convert the query result to a Spatially Enabled Dataframe\n",
    "sdfStreamHab = query_result.sdf\n",
    "\n",
    "sdfStreamHab.info()\n",
    "columnstokeep = ['SITE_NAME','STATION_TYPE', 'LATITUDE', 'LONGITUDE', 'LTINFO']\n",
    "sdfStreamHab = sdfStreamHab.loc[:, columnstokeep]\n",
    "unique_values = sdfStreamHab.drop_duplicates()\n",
    "\n",
    "# Select specific columns for look up\n",
    "selected_columns = ['STATION_TYPE', 'LATITUDE', 'LONGITUDE', 'LTINFO']\n",
    "\n",
    "# Convert selected columns to dictionary\n",
    "lookup_dict = unique_values.set_index('SITE_NAME')[selected_columns].to_dict(orient='index')\n",
    "\n",
    "# Display the dictionary\n",
    "print(lookup_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform CSCI Scores to Point feature class in Enterprise Geodatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames from CSV files\n",
    "#For 2024 data add locations of new data and label in similar way\n",
    "dfCSCI = pd.read_csv(originalcsv)\n",
    "dflocations = pd.read_csv(locationcsv)\n",
    "\n",
    "# # Append DataFrames from additional CSV files\n",
    "# dfCSCI = dfCSCI.append(pd.read_csv(originalcsv), ignore_index=True)\n",
    "# dflocations = dflocations.append(pd.read_csv(locationcsv), ignore_index=True)\n",
    "\n",
    "# merge CSCI scores and location data\n",
    "RawData_df = pd.merge(dfCSCI, dflocations, how='inner', on='StationCode')\n",
    "\n",
    "try:\n",
    "    dfCSCI = pd.read_csv(originalcsv)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please check the file path.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"The CSV file is empty.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error parsing the CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get year from sample id\n",
    "RawData_df['Year']=RawData_df.SampleID.str.split(\"_\").str[-1]\n",
    "\n",
    "#Calculate Station Type \n",
    "RawData_df['STATION_TYPE'] = RawData_df['StationCode'].map(lambda x: lookup_dict[x]['STATION_TYPE'] if x in lookup_dict else 'Status')\n",
    "\n",
    "#Calculate LATITUDE\n",
    "RawData_df['LATITUDE'] = RawData_df['StationCode'].map(lambda x: lookup_dict[x]['LATITUDE']if x in lookup_dict else None)\n",
    "RawData_df['LATITUDE'] = RawData_df['LATITUDE'].fillna(RawData_df['New_Lat'])\n",
    "#Calculate LONGITUDE\n",
    "RawData_df['LONGITUDE'] = RawData_df['StationCode'].map(lambda x: lookup_dict[x]['LONGITUDE']if x in lookup_dict else None)\n",
    "RawData_df['LONGITUDE'] = RawData_df['LONGITUDE'].fillna(RawData_df['New_Long'])\n",
    "#Caculate LTINFO\n",
    "RawData_df['LTINFO'] = RawData_df['StationCode'].map(lambda x: lookup_dict[x]['LTINFO'] if x in lookup_dict else None)\n",
    "\n",
    "#Use only for threshold?\n",
    "# Apply the categorization function to create the new field\n",
    "#RawData_df['Rating'] = RawData_df['CSCI'].apply(categorize_value)\n",
    "\n",
    "\n",
    "Field_Mapping={\n",
    "    'StationCode': 'SITE_NAME',\n",
    "    'Year': 'YEAR_OF_COUNT',\n",
    "    'LATITUDE': 'LATITUDE',\n",
    "    'LONGITUDE': 'LONGITUDE',\n",
    "    'CSCI': 'COUNT_VALUE',\n",
    "    'STATION_TYPE': 'STATION_TYPE',\n",
    "    'LTINFO': 'LTINFO'\n",
    "    \n",
    "}\n",
    "# rename fields based on field mappings\n",
    "df_final = RawData_df.rename(columns=Field_Mapping).drop(columns=[col for col in RawData_df.columns if col not in Field_Mapping])\n",
    "\n",
    "# establish duration field\n",
    "def assign_duration(stationtype):\n",
    "    if stationtype == 'Status' :\n",
    "        return 'One-time'\n",
    "    else:\n",
    "        return 'Long-term'\n",
    "df_final['DURATION']= df_final['STATION_TYPE'].apply(assign_duration)\n",
    "\n",
    "# station code is site name, site name is station code.\n",
    "df_final['STATION_CODE']=df_final['SITE_NAME']\n",
    "\n",
    "# export to csv for QA\n",
    "df_final.to_csv(os.path.join(working_folder,\"StreamCSCI_proccesed.csv\"), index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert CSV to point feature class\n",
    "arcpy.management.XYTableToPoint(os.path.join(working_folder,\"StreamCSCI_proccesed.csv\"), \n",
    "                                \"NewCSCI_points\", \n",
    "                                \"LONGITUDE\", \"LATITUDE\",\n",
    "                                coordinate_system= arcpy.SpatialReference(4269)) #NAD 1983\n",
    "\n",
    "# project to UTM Zone 10N\n",
    "arcpy.Project_management(\"NewCSCI_points\", \"NewStream_CSCI_Projected\", 26910)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS IS HITTING SDE - BE CAREFUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputfc= \"NewStream_CSCI_Projected\"\n",
    "\n",
    "# disconnect all users\n",
    "print(\"\\nDisconnecting all users...\")\n",
    "arcpy.DisconnectUser(sdeBase, \"ALL\")\n",
    " \n",
    "# unregister the sde feature class as versioned\n",
    "print (\"\\nUnregistering feature dataset as versioned...\")\n",
    "arcpy.UnregisterAsVersioned_management(fdata,\"NO_KEEP_EDIT\",\"COMPRESS_DEFAULT\")\n",
    "print (\"\\nFinished unregistering feature dataset as versioned.\")\n",
    "\n",
    "arcpy.management.Append(inputfc, sdeStreams,\"NO_TEST\")\n",
    "\n",
    "# disconnect all users\n",
    "print(\"\\nDisconnecting all users...\")\n",
    "arcpy.DisconnectUser(sdeBase, \"ALL\")\n",
    "\n",
    "# register SDE feature class as versioned\n",
    "arcpy.RegisterAsVersioned_management(fdata, \"NO_EDITS_TO_BASE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data from SDE.Index and SDE.Monitoring to get one place for all stream data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This next bit of code was used to add status sites to SDE.Stream so we can consolidate all stream data into one place. This code has been used and SDE.Stream is now the master data set for all stream data.\n",
    "#I would also like to incorporate IPI scores into SDE.Stream so we have a single place for all stream data. IPI scores are used to calcluate Stream Habitat Condition. \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from arcgis.features import FeatureLayer\n",
    "from arcgis import GIS\n",
    "import arcpy\n",
    "from arcgis.features import GeoAccessor\n",
    "import os\n",
    "\n",
    "# Set up the GIS connection\n",
    "gis = GIS()\n",
    "\n",
    "# set workspace and sde connections \n",
    "working_folder = r\"F:\\Research and Analysis\\Fisheries\\Streams\\Bioassessment\\California Stream Condition Index\\California Stream Condition Index\"\n",
    "workspace      = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb\"\n",
    "arcpy.env.workspace = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\"\n",
    "\n",
    "# network path to connection files\n",
    "filePath = r\"F:\\GIS\\DB_CONNECT\"\n",
    "# network path to connection files\n",
    "#filePath = r'F:\\Research and Analysis\\Workspace\\Sarah'\n",
    "\n",
    "# database file path \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "# database file path \n",
    "#sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "#sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "\n",
    "arcpy.env.workspace = workspace\n",
    "\n",
    "\n",
    "# Paths to feature classes\n",
    "CSCIfromIndex = os.path.join(sdeBase, \"SDE.Index\", \"SDE.StreamHabitat_CSCI\")\n",
    "\n",
    "\n",
    "# Create a DataFrame from SDE.StreamHabitat_CSCI\n",
    "sdfCSCIfromIndex = pd.DataFrame.spatial.from_featureclass(CSCIfromIndex)\n",
    "\n",
    "# Define columns for the new DataFrame\n",
    "columns_for_new_df = ['StationCod', 'F2000_score', 'F2007_score', 'F2008_score', 'F2009_score', 'F2010_score',\n",
    "                       'F2011_score', 'F2012_score', 'F2013_score', 'F2014_score', 'F2015_score', 'F2016_score',\n",
    "                       'F2017_score', 'F2018_score', 'F2019_score', 'LATITUDE', 'LONGITUDE']\n",
    "\n",
    "# Create a new DataFrame using the defined columns\n",
    "wide_df = pd.DataFrame(sdfCSCIfromIndex, columns=columns_for_new_df)\n",
    "\n",
    "# Melt the DataFrame to convert from wide to long format\n",
    "long_df = pd.melt(wide_df, id_vars=['StationCod', 'LATITUDE', 'LONGITUDE'], var_name='Year', value_name='CSCI')\n",
    "\n",
    "long_df = long_df[(long_df['CSCI'] != 0) & (~long_df['CSCI'].isnull())]\n",
    "\n",
    "# Extract the year from the 'Year' column\n",
    "long_df['Year'] = long_df['Year'].str.extract(r'(\\d{4})')\n",
    "\n",
    "#print(\"Wide DataFrame:\")\n",
    "#print(wide_df)\n",
    "\n",
    "#print(\"\\nLong DataFrame:\")\n",
    "#print(long_df)\n",
    "\n",
    "# Use a lookup dictionary to filter out any data that is already present in SDE.Streams\n",
    "# Create a dictionary using Rest Service data\n",
    "service_url = 'https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/8'\n",
    "feature_layer = FeatureLayer(service_url)\n",
    "query_result = feature_layer.query()\n",
    "\n",
    "# Convert the query result to a Spatially Enabled DataFrame\n",
    "sdfStreamHab = query_result.sdf\n",
    "\n",
    "# Select specific columns for lookup\n",
    "columns_to_keep = ['SITE_NAME', 'STATION_TYPE', 'LATITUDE', 'LONGITUDE', 'LTINFO']\n",
    "sdfStreamHab = sdfStreamHab.loc[:, columns_to_keep]\n",
    "\n",
    "# Remove duplicates\n",
    "unique_values = sdfStreamHab.drop_duplicates()\n",
    "\n",
    "# Convert selected columns to a dictionary\n",
    "lookup_dict = unique_values.set_index('SITE_NAME').to_dict(orient='index')\n",
    "\n",
    "# Filter out rows where 'StationCod' is in the lookup dictionary\n",
    "filtered_df = long_df[~long_df['StationCod'].isin(lookup_dict)]\n",
    "\n",
    "\n",
    "\n",
    "# Add new fields and fill them with values\n",
    "filtered_df = filtered_df.copy()\n",
    "filtered_df.loc[:, 'Duration'] = 'One-time'\n",
    "filtered_df.loc[:, 'STATION_TYPE'] = 'Status'\n",
    "filtered_df.loc[:, 'LTINFO'] = 'Null'\n",
    "filtered_df['STATION_CODE'] = filtered_df['StationCod']\n",
    "#filtered_df.loc[:, 'STATION_CODE'] = sdfStreamHab['SITE_NAME']\n",
    "# Drop rows with null values in the COUNT_VALUE column\n",
    "filtered_df.dropna(subset=['CSCI'], inplace=True)\n",
    "\n",
    "print(\"Filtered DataFrame:\")\n",
    "print(filtered_df)\n",
    "\n",
    "# Field Mapping\n",
    "field_mapping = {\n",
    "    'StationCod': 'SITE_NAME',\n",
    "    'STATION_CODE': 'STATION_CODE',\n",
    "    'Year': 'YEAR_OF_COUNT',\n",
    "    'LATITUDE': 'LATITUDE',\n",
    "    'LONGITUDE': 'LONGITUDE',\n",
    "    'CSCI': 'COUNT_VALUE',\n",
    "    'STATION_TYPE': 'STATION_TYPE',\n",
    "    'LTINFO': 'LTINFO'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "ready_df = filtered_df.rename(columns=field_mapping).drop(columns=[col for col in filtered_df.columns if col not in field_mapping])\n",
    "\n",
    "# Drop rows with null values in the COUNT_VALUE column\n",
    "#ready_df.dropna(subset=['COUNT_VALUE'], inplace=True)\n",
    "\n",
    "# Convert DataFrame to Spatially Enabled DataFrame\n",
    "3#sedf = GeoAccessor.from_xy(ready_df, x_column='LONGITUDE', y_column='LATITUDE')\n",
    "\n",
    "# Convert the SEDF to a feature class without sanitizing columns\n",
    "#sedf.spatial.to_featureclass(location=os.path.join(workspace, 'CSCIfromIndex_Stagingg'), sanitize_columns=False)\n",
    "\n",
    "# Specify the input feature class (CSCIfromIndex_Staging) and the output projected feature class name\n",
    "#input_feature_class = os.path.join(workspace, 'CSCIfromIndex_Stagingg')\n",
    "#output_projected_feature_class = os.path.join(workspace, 'CSCIfromIndex_Projected_ready')\n",
    "\n",
    "# Specify the output spatial reference system (SR) using either a well-known ID (WKID) or a path to a .prj file\n",
    "#output_spatial_reference = arcpy.SpatialReference(26910)  \n",
    "\n",
    "# Project the feature class\n",
    "#arcpy.Project_management(input_feature_class, output_projected_feature_class, output_spatial_reference)\n",
    "\n",
    "#print(\"Projection completed.\")\n",
    "\n",
    "\n",
    "# Export to CSV\n",
    "ready_df.to_csv(os.path.join(working_folder, \"OldCSCIdata.csv\"), index=False)\n",
    "\n",
    "csv_path = r\"F:\\Research and Analysis\\Fisheries\\Streams\\Bioassessment\\California Stream Condition Index\\California Stream Condition Index\\OldCSCIdata.csv\" \n",
    "\n",
    "# Convert CSV to point feature class\n",
    "arcpy.management.XYTableToPoint(csv_path, \"oldStream_CSCI_Points\", \"LONGITUDE\", \"LATITUDE\")\n",
    "\n",
    "# Project to UTM Zone 10N\n",
    "arcpy.Project_management(\"oldStream_CSCI_Points\", \"old_CSCI_Projected\", arcpy.SpatialReference(26910))\n",
    "\n",
    "#print(\"Conversion and projection completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_df['StationCod'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert CSV to feature class\n",
    "# Convert DataFrame to CSV\n",
    "#csv_path = os.path.join(working_folder, \"OldCSCIdata.csv\")\n",
    "#ready_df.to_csv(csv_path, index=False) \n",
    "\n",
    "# Convert CSV to point feature class\n",
    "#arcpy.management.XYTableToPoint(csv_path, \"NewStream_CSCI_Points\", \"LONGITUDE\", \"LATITUDE\")\n",
    "\n",
    "# Project to UTM Zone 10N\n",
    "#arcpy.Project_management(\"NewStream_CSCI_Points\", \"NewStream_CSCI_Projected\", arcpy.SpatialReference(26910))\n",
    "\n",
    "#print(\"Conversion and projection completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis import GIS\n",
    "\n",
    "# Set up the GIS connection\n",
    "gis = GIS()\n",
    "\n",
    "# File paths and workspace\n",
    "workspace = r\"C:\\GIS\\Scratch.gdb\"\n",
    "arcpy.env.workspace = workspace\n",
    "\n",
    "# CSV file path\n",
    "csv_path = r\"F:\\Research and Analysis\\Fisheries\\Streams\\Bioassessment\\California Stream Condition Index\\California Stream Condition Index\\OldCSCIdata.csv\"\n",
    "\n",
    "# Read CSV into a Pandas DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert the Pandas DataFrame to a Spatially Enabled DataFrame\n",
    "sdf = pd.DataFrame.spatial.from_df(df)\n",
    "\n",
    "# SDEindexdata feature class name (change it as needed)\n",
    "SDEindexdata_fc = \"OldCSCIdata\"\n",
    "\n",
    "# Save the Spatially Enabled DataFrame to a feature class in the file geodatabase\n",
    "sdf.spatial.to_featureclass(location=os.path.join(workspace, SDEindexdata_fc))\n",
    "\n",
    "print(f\"CSV data imported to feature class '{SDEindexcscidata_fc}' in '{workspace}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
