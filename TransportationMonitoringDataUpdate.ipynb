{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring Data Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congestion Data Update\n",
    "* Data is from RITIS (ritis.org), which in turn uses INRIX data. Anyone at TRPA can get an account, but you need to sign a data form to get full access to all of the data. We're downloading the performance summaries for the segments/dates in question and pulling out the bidirectional travel times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages and setup notebook\n",
    "import arcpy\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "from arcgis.features import FeatureSet, GeoAccessor, GeoSeriesAccessor\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# # Set Pandas display options to show all rows and columns\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# in memory output file path\n",
    "wk_memory = \"memory\" + \"\\\\\"\n",
    "\n",
    "# set workspace and sde connections \n",
    "working_folder = \"C:\\GIS\"\n",
    "workspace      = \"C:\\GIS\\Scratch.gdb\"\n",
    "arcpy.env.workspace = \"C:\\GIS\\Scratch.gdb\"\n",
    "\n",
    "# network path to connection files\n",
    "filePath = \"C:\\\\GIS\\\\DB_CONNECT\"\n",
    "\n",
    "# database file path \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "\n",
    "travelTimeTable = os.path.join(sdeBase, \"sde.SDE.Travel_Times\")\n",
    "\n",
    "# setup connection string\n",
    "connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=sql12;DATABASE=sde;UID=sde;PWD=staff\"\n",
    "connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "engine = create_engine(connection_url)\n",
    "\n",
    "with engine.begin() as sdeConnect:\n",
    "    dfTTsde      = pd.read_sql(\"SELECT * FROM sde.SDE.Travel_Times\", sdeConnect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 455 entries, 0 to 454\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   OBJECTID          455 non-null    int64  \n",
      " 1   Name              455 non-null    object \n",
      " 2   Season            455 non-null    object \n",
      " 3   Year              442 non-null    float64\n",
      " 4   Value             433 non-null    float64\n",
      " 5   GlobalID          455 non-null    object \n",
      " 6   created_user      0 non-null      object \n",
      " 7   created_date      0 non-null      object \n",
      " 8   last_edited_user  0 non-null      object \n",
      " 9   last_edited_date  0 non-null      object \n",
      "dtypes: float64(2), int64(1), object(7)\n",
      "memory usage: 35.7+ KB\n"
     ]
    }
   ],
   "source": [
    "dfTTsde.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTTsde.to_csv(os.path.join(workspace,\"TravelTime_Staged.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OBJECTID',\n",
       " 'NAME',\n",
       " 'Shape_Leng',\n",
       " 'GlobalID',\n",
       " 'created_user',\n",
       " 'created_date',\n",
       " 'last_edited_user',\n",
       " 'last_edited_date',\n",
       " 'TT_Spring_22',\n",
       " 'TT_Summer_22',\n",
       " 'TT_Fall_22',\n",
       " 'TT_Winter_2122',\n",
       " 'TT_21',\n",
       " 'TT_Spring_21',\n",
       " 'TT_Summer_21',\n",
       " 'TT_Fall_21',\n",
       " 'TT_Winter_20to21',\n",
       " 'TT_20',\n",
       " 'TT_Spring_20',\n",
       " 'TT_Summer_20',\n",
       " 'TT_Fall_20',\n",
       " 'TT_Winter_1920',\n",
       " 'TT_19',\n",
       " 'TT_Spring_19',\n",
       " 'TT_Summer_19',\n",
       " 'TT_Fall_19',\n",
       " 'TT_Winter_1819',\n",
       " 'TT_18',\n",
       " 'TT_Spring_18',\n",
       " 'TT_Summer_18',\n",
       " 'TT_Fall_18',\n",
       " 'TT_Winter_1718',\n",
       " 'TT_17',\n",
       " 'TT_Spring_17',\n",
       " 'TT_Summer_17',\n",
       " 'TT_Fall_17',\n",
       " 'TT_Winter_1617',\n",
       " 'TT_16',\n",
       " 'TT_Spring_16',\n",
       " 'TT_Summer_16',\n",
       " 'TT_Fall_16',\n",
       " 'TT_Winter_1516',\n",
       " 'TT_15',\n",
       " 'TT_22',\n",
       " 'SpeedLimitTT',\n",
       " 'SHAPE']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# staging data\n",
    "stagingFC     = \"F:\\GIS\\PROJECTS\\Transportation\\Modeling and Monitoring\\Data\\TravelTimes.gdb\\Roads_JSCopy\"\n",
    "# Convert the feature set to a Pandas DataFrame\n",
    "df = pd.DataFrame.spatial.from_featureclass(stagingFC)\n",
    "df.columns.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Season</th>\n",
       "      <th>Year</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nevada SR 431</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2022</td>\n",
       "      <td>15.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nevada SR 28 (West of Incline)</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2022</td>\n",
       "      <td>8.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>California SR 267</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2022</td>\n",
       "      <td>4.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SR 89 (North of Tahoe City)</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2022</td>\n",
       "      <td>16.709999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California State Route 28</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2022</td>\n",
       "      <td>18.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>SR 207 Kingsbury Grade</td>\n",
       "      <td>All</td>\n",
       "      <td>2022</td>\n",
       "      <td>5.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>Pioneer Trail</td>\n",
       "      <td>All</td>\n",
       "      <td>2022</td>\n",
       "      <td>12.515000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>US 50 (Y to State Line)</td>\n",
       "      <td>All</td>\n",
       "      <td>2022</td>\n",
       "      <td>9.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>US 50 (Echo to Y)</td>\n",
       "      <td>All</td>\n",
       "      <td>2022</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>SR 89 (Luther Pass)</td>\n",
       "      <td>All</td>\n",
       "      <td>2022</td>\n",
       "      <td>13.215000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>468 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Name  Season  Year      Value\n",
       "0                     Nevada SR 431  Spring  2022  15.115000\n",
       "1    Nevada SR 28 (West of Incline)  Spring  2022   8.445000\n",
       "2                 California SR 267  Spring  2022   4.095000\n",
       "3       SR 89 (North of Tahoe City)  Spring  2022  16.709999\n",
       "4         California State Route 28  Spring  2022  18.305000\n",
       "..                              ...     ...   ...        ...\n",
       "463          SR 207 Kingsbury Grade     All  2022   5.300000\n",
       "464                   Pioneer Trail     All  2022  12.515000\n",
       "465         US 50 (Y to State Line)     All  2022   9.955000\n",
       "466               US 50 (Echo to Y)     All  2022  13.000000\n",
       "467             SR 89 (Luther Pass)     All  2022  13.215000\n",
       "\n",
       "[468 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to extract the second item or handle the case with only one \"_\"\n",
    "def extract_middle_item(value):\n",
    "    parts = value.split('_')\n",
    "    \n",
    "    if len(parts) == 2:\n",
    "        return \"All\"\n",
    "    elif len(parts) > 2:\n",
    "        return parts[1]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Specify columns to drop by name\n",
    "columns_to_drop = ['OBJECTID', \n",
    "                    'Shape_Leng',\n",
    "                    'GlobalID',\n",
    "                    'created_user',\n",
    "                    'created_date',\n",
    "                    'last_edited_user',\n",
    "                    'last_edited_date',\n",
    "                    'SpeedLimitTT',\n",
    "                    'SHAPE']\n",
    "\n",
    "# Drop the specified columns\n",
    "dfNew = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Specify the columns to keep \n",
    "columns_to_keep = ['NAME']\n",
    "\n",
    "# Melt the DataFrame by specifying the 'id_vars' parameter\n",
    "melted_df = pd.melt(dfNew, id_vars=columns_to_keep, var_name='TravelSeason', value_name='Value')\n",
    "\n",
    "dfTT = melted_df\n",
    "\n",
    "dfTT['Year']   = (\"20\"+dfTT['TravelSeason'].str[-2:]).astype(int)\n",
    "\n",
    "# Apply the function to create a new field 'NewField'\n",
    "dfTT['Season'] = dfTT['TravelSeason'].apply(extract_middle_item)\n",
    "# Rename the column\n",
    "dfTT = dfTT.rename(columns={'NAME': 'Name'})\n",
    "# Define the new column order\n",
    "new_column_order = ['Name', 'Season', 'Year', 'Value']\n",
    "\n",
    "# Set the new order and index for the columns\n",
    "dfFinal = dfTT[new_column_order]\n",
    "dfFinal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Field object at 0x1e762a47760[0x1e76420e030]>,\n",
       " <Field object at 0x1e762a47970[0x1e76420ed10]>,\n",
       " <Field object at 0x1e762a47730[0x1e76420eed0]>,\n",
       " <Field object at 0x1e762a47d30[0x1e76420ee70]>,\n",
       " <Field object at 0x1e762a47ee0[0x1e76420edf0]>,\n",
       " <Field object at 0x1e762a47cd0[0x1e76420eb50]>,\n",
       " <Field object at 0x1e762a47610[0x1e76420ef70]>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arcpy.ListFields(\"Congestion_Staging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field names have been altered successfully.\n"
     ]
    }
   ],
   "source": [
    "# List of field name changes (old_field_name: new_field_name)\n",
    "field_name_changes = {\n",
    "    'name'  : 'Name',\n",
    "    'season': 'Season',\n",
    "    'year'  : 'Year',\n",
    "    'value' : 'Value'\n",
    "}\n",
    "\n",
    "fc = os.path.join(workspace,\"Congestion_Staging\")\n",
    "\n",
    "# Loop through the field name changes and rename the fields\n",
    "for old_field_name, new_field_name in field_name_changes.items():\n",
    "    arcpy.management.AlterField(fc, old_field_name, new_field_name, new_field_name)\n",
    "\n",
    "print(\"Field names have been altered successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Disconnecting all users...\n",
      "Unregistering feature dataset as versioned...\n",
      "Finished unregistering feature dataset as versioned.\n",
      "\n",
      "Deleted all records in: C:\\GIS\\DB_CONNECT\\Vector.sde\\sde.SDE.Travel_Times\n",
      "\n",
      "\n",
      "Disconnecting all users...\n",
      "\n",
      "Registering feature dataset as versioned...\n",
      "\n",
      "Finished registering feature dataset as versioned.\n",
      "\n",
      "Updated C:\\GIS\\DB_CONNECT\\Vector.sde\\sde.SDE.Travel_Times\n"
     ]
    }
   ],
   "source": [
    "# replaces features in outfc with exact same schema\n",
    "def updateSDE(inputfc,outfc, fieldnames):\n",
    "        # disconnect all users\n",
    "    print(\"\\nDisconnecting all users...\")\n",
    "    arcpy.DisconnectUser(sdeBase, \"ALL\")\n",
    "\n",
    "    print (\"Unregistering feature dataset as versioned...\")\n",
    "    # unregister the sde feature class as versioned\n",
    "    arcpy.UnregisterAsVersioned_management(outfc,\"NO_KEEP_EDIT\",\"COMPRESS_DEFAULT\")\n",
    "    print (\"Finished unregistering feature dataset as versioned.\")\n",
    "    # set overwrite files envrionment setting to True\n",
    "    arcpy.env.overwriteOutput = True\n",
    "    # deletes all rows from the SDE feature class\n",
    "    arcpy.TruncateTable_management(outfc)\n",
    "    print (\"\\nDeleted all records in: {}\\n\".format(outfc))\n",
    "\n",
    "    # insert rows from Temporary feature class to SDE feature class\n",
    "    with arcpy.da.InsertCursor(outfc, fieldnames) as oCursor:\n",
    "        count = 0\n",
    "        with arcpy.da.SearchCursor(inputfc, fieldnames) as iCursor:\n",
    "            for row in iCursor:\n",
    "                oCursor.insertRow(row)\n",
    "                count += 1\n",
    "                if count % 100 == 0:\n",
    "                    print(\"Inserting record {0} into SDE table\".format(count))\n",
    "\n",
    "    # disconnect all users\n",
    "    print(\"\\nDisconnecting all users...\")\n",
    "    arcpy.DisconnectUser(sdeBase, \"ALL\")\n",
    "    print(\"\\nRegistering feature dataset as versioned...\")\n",
    "    # register SDE feature class as versioned\n",
    "    arcpy.RegisterAsVersioned_management(outfc, \"NO_EDITS_TO_BASE\")\n",
    "    print(\"\\nFinished registering feature dataset as versioned.\")\n",
    "    # confirm feature class was created\n",
    "    print(\"\\nUpdated \" + outfc)\n",
    "\n",
    "# # Convert the Pandas DataFrame to a NumPy structured array\n",
    "# dfFinal.spatial.to_table(location= os.path.join(workspace, \"Congestion_Staging\"))\n",
    "inputTable = \"Congestion_Staging\"\n",
    "field_names = ['Name', 'Season', 'Year', 'Value']\n",
    "\n",
    "updateSDE(inputTable, travelTimeTable, field_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.management.Append(\n",
    "    inputs=r\"C:\\GIS\\Scratch.gdb\\Congestion_Staging\",\n",
    "    target=r\"C:\\Users\\mbindl\\Documents\\ArcGIS\\Projects\\Vector.sde\\sde.SDE.Travel_Times\",\n",
    "    schema_type=\"NO_TEST\",\n",
    "    field_mapping=r'Name \"Name\" true true false 255 Text 0 0,First,#,C:\\GIS\\Scratch.gdb\\Congestion_Staging,name,0,60;Season \"Season\" true true false 255 Text 0 0,First,#,C:\\GIS\\Scratch.gdb\\Congestion_Staging,season,0,12;Year \"Year\" true true false 2 Short 0 5,First,#,C:\\GIS\\Scratch.gdb\\Congestion_Staging,year,-1,-1;Value \"Value\" true true false 8 Double 8 38,First,#,C:\\GIS\\Scratch.gdb\\Congestion_Staging,value,-1,-1;GlobalID \"GlobalID\" false false true 38 GlobalID 0 0,First,#',\n",
    "    subtype=\"\",\n",
    "    expression=\"\",\n",
    "    match_fields=None,\n",
    "    update_geometry=\"NOT_UPDATE_GEOMETRY\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T16:22:34.343189Z",
     "start_time": "2023-03-09T16:22:34.336183Z"
    }
   },
   "source": [
    "## Crash Data Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T23:49:44.069043Z",
     "start_time": "2023-03-09T23:49:43.829048Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jschmid\\AppData\\Local\\Temp\\ipykernel_26132\\1404647701.py:48: DtypeWarning: Columns (6,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfCACrash = pd.read_csv(caCrashes)\n"
     ]
    }
   ],
   "source": [
    "## Setup \n",
    "# import modules\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyodbc\n",
    "from datetime import datetime\n",
    "import arcpy\n",
    "import numpy as np\n",
    "import urllib\n",
    "#import geojson\n",
    "#import json\n",
    "#from urllib.request import urlopen\n",
    "from arcgis.features import FeatureSet, GeoAccessor, GeoSeriesAccessor\n",
    "#Test 2\n",
    "# setup workspace folder\n",
    "#workspace = \"//Trpa-fs01/gis/PROJECTS/ResearchAnalysis/Monitoring/Data/Crash/\"\n",
    "workspace = \"F:/gis/PROJECTS/ResearchAnalysis/Monitoring/Data/Crash/\"\n",
    "\n",
    "# setup environment variables\n",
    "arcpy.env.overwriteOutput = True\n",
    "#arcpy.env.workspace = \"//Trpa-fs01/GIS/PROJECTS/ResearchAnalysis/Monitoring/Data/Crash/CrashData/CrashData.gdb\"\n",
    "arcpy.env.workspace = \"F:/GIS/PROJECTS/ResearchAnalysis/Monitoring/Data/Crash/CrashData/CrashData.gdb\"\n",
    "\n",
    "# create a spatial reference object for the output coordinate system \n",
    "# output projection for data going into SDE should be UTM Zone 10N (EPSG: 26910)\n",
    "out_coordinate_system = arcpy.SpatialReference(26910)\n",
    "\n",
    "# network path to connection files\n",
    "#filePath = \"//Trpa-fs01/GIS/DB_CONNECT\"\n",
    "filePath = \"F:/GIS/DB_CONNECT\"\n",
    "\n",
    "# database file path \n",
    "sdeBase  = os.path.join(filePath, \"Vector.sde\")\n",
    "\n",
    "# SDE feature classes needed for spatial joins\n",
    "corridor = os.path.join(sdeBase, 'sde.SDE.Transportation\\sde.SDE.Corridor')\n",
    "trpa     = os.path.join(sdeBase, 'sde.SDE.Jurisdictions\\sde.SDE.TRPA_bdy')\n",
    "\n",
    "# define csv lat/long field names for xy table to point\n",
    "x_coords = 'POINT_X'\n",
    "y_coords = 'POINT_Y'\n",
    "\n",
    "# SDE feature class to update\n",
    "crashSDE  = os.path.join(sdeBase, 'sde.SDE.Transportation\\sde.SDE.Highway_Collisions')\n",
    "\n",
    "# Get Crash Data\n",
    "caCrashes = os.path.join(workspace, \"BothCACounties_Crashes_1321_Unclean.csv\")\n",
    "dfCACrash = pd.read_csv(caCrashes)\n",
    "\n",
    "nvCrashes = os.path.join(workspace, \"NV_LAKE TAHOE BASIN - ALL ROADS 2013-2020.csv\")\n",
    "dfNVCrash = pd.read_csv(nvCrashes)\n",
    "\n",
    "nvCrashes21 = os.path.join(workspace, \"[INSERT 2021 CRASH FILE]\")\n",
    "dfNVCrash21 = pd.read_csv(nvCrashes21)\n",
    "    \n",
    "# # in memory files\n",
    "memory = \"memory\" + \"\\\\\"\n",
    "\n",
    "# replaces features in outfc with exact same schema\n",
    "def updateSDE(inputfc,outfc, fieldnames):\n",
    "        # disconnect all users\n",
    "    print(\"\\nDisconnecting all users...\")\n",
    "    arcpy.DisconnectUser(sde, \"ALL\")\n",
    "\n",
    "    # deletes all rows from the SDE feature class\n",
    "    arcpy.TruncateTable_management(outfc)\n",
    "    print (\"\\nDeleted all records in: {}\\n\".format(outfc))\n",
    "\n",
    "    # insert rows from Temporary feature class to SDE feature class\n",
    "    with arcpy.da.InsertCursor(outfc, fieldnames) as oCursor:\n",
    "        count = 0\n",
    "        with arcpy.da.SearchCursor(inputfc, fieldnames) as iCursor:\n",
    "            for row in iCursor:\n",
    "                oCursor.insertRow(row)\n",
    "                count += 1\n",
    "                if count % 1000 == 0:\n",
    "                    print(\"Inserting record {0} into SDE table\".format(count))\n",
    "\n",
    "    # disconnect all users\n",
    "    print(\"\\nDisconnecting all users...\")\n",
    "    arcpy.DisconnectUser(sde, \"ALL\")\n",
    "    # confirm feature class was created\n",
    "    print(\"\\nUpdated \" + outfc)\n",
    "    \n",
    "# function to move spatial join data to parcel master staging\n",
    "def fieldJoinCalc(updateFC, updateFieldsList, sourceFC, sourceFieldsList):\n",
    "    from time import strftime  \n",
    "    print (\"Started data transfer: \" + strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "#     log.info(\"Started data transfer: \" + strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    # Use list comprehension to build a dictionary from arcpy SearchCursor  \n",
    "    valueDict = {r[0]:(r[1:]) for r in arcpy.da.SearchCursor(sourceFC, sourceFieldsList)}  \n",
    "   \n",
    "    with arcpy.da.UpdateCursor(updateFC, updateFieldsList) as updateRows:  \n",
    "        for updateRow in updateRows:  \n",
    "            # store the Join value of the row being updated in a keyValue variable  \n",
    "            keyValue = updateRow[0]  \n",
    "            # verify that the keyValue is in the Dictionary  \n",
    "            if keyValue in valueDict:  \n",
    "                # transfer the value stored under the keyValue from the dictionary to the updated field.  \n",
    "                updateRow[1] = valueDict[keyValue][0]  \n",
    "                updateRows.updateRow(updateRow)    \n",
    "    del valueDict  \n",
    "    print (\"Finished data transfer: \" + strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "#     log.info(\"Finished data transfer: \" + strftime(\"%Y-%m-%d %H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "10902    0\n",
      "10903    0\n",
      "10904    0\n",
      "10905    0\n",
      "10906    0\n",
      "Name: Num_Motorcyclist_Killed, Length: 10907, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## CA data translation\n",
    "# convert date/time and case info\n",
    "#convert state/county/city, year/date/time\n",
    "dfCACrash['State']       = \"CA\"\n",
    "dfCACrash['City']        = dfCACrash['CITY']\n",
    "dfCACrash['County']      = dfCACrash['COUNTY']\n",
    "dfCACrash['Year']        = dfCACrash['ACCIDENT_YEAR']\n",
    "dfCACrash['Date']        = dfCACrash['COLLISION_DATE']\n",
    "dfCACrash['4DigTime']    = dfCACrash['COLLISION_TIME'].astype(str).str.zfill(4)\n",
    "dfCACrash['Hour']        = dfCACrash['4DigTime'].str[:2]\n",
    "dfCACrash['Min']         = dfCACrash['4DigTime'].str[2:]\n",
    "dfCACrash['Time']        = dfCACrash['Hour']+\":\"+dfCACrash['Min']\n",
    "dfCACrash['Data_Source'] = \"CHP/SWITRS\"\n",
    "\n",
    "#Convert severity\n",
    "dfCACrash.loc[dfCACrash['COLLISION_SEVERITY'].isin([1]),  'Crash_Severity'] = 'Fatal'\n",
    "dfCACrash.loc[dfCACrash['COLLISION_SEVERITY'].isin([2]),  'Crash_Severity'] = 'Severe injury'\n",
    "dfCACrash.loc[dfCACrash['COLLISION_SEVERITY'].isin([3]),  'Crash_Severity'] = 'Other visible injury'\n",
    "dfCACrash.loc[dfCACrash['COLLISION_SEVERITY'].isin([4]),  'Crash_Severity'] = 'Complaint of pain'\n",
    "dfCACrash.loc[dfCACrash['COLLISION_SEVERITY'].isin([0]),  'Crash_Severity'] = 'Property damage only'\n",
    "#Convert crash type\n",
    "dfCACrash.loc[dfCACrash['TYPE_OF_COLLISION'].isin([\"A\"]),  'Crash_Type'] = 'Head-on'\n",
    "dfCACrash.loc[dfCACrash['TYPE_OF_COLLISION'].isin([\"B\"]),  'Crash_Type'] = 'Sideswipe'\n",
    "dfCACrash.loc[dfCACrash['TYPE_OF_COLLISION'].isin([\"C\"]),  'Crash_Type'] = 'Rear end'\n",
    "dfCACrash.loc[dfCACrash['TYPE_OF_COLLISION'].isin([\"D\"]),  'Crash_Type'] = 'Angle-broadside'\n",
    "dfCACrash.loc[dfCACrash['TYPE_OF_COLLISION'].isin([\"E\"]),  'Crash_Type'] = 'Hit object'\n",
    "dfCACrash.loc[dfCACrash['TYPE_OF_COLLISION'].isin([\"F\"]),  'Crash_Type'] = 'Overturned'\n",
    "dfCACrash.loc[dfCACrash['TYPE_OF_COLLISION'].isin([\"G\"]),  'Crash_Type'] = 'Vehicle/pedestrian'\n",
    "dfCACrash.loc[dfCACrash['TYPE_OF_COLLISION'].isin([\"H\"]),  'Crash_Type'] = 'Other'\n",
    "#convert lighting\n",
    "dfCACrash.loc[dfCACrash['LIGHTING'].isin([\"A\"]),  'Lighting'] = \"Daylight\"\n",
    "dfCACrash.loc[dfCACrash['LIGHTING'].isin([\"B\"]),  'Lighting'] = \"Dusk - Dawn\"\n",
    "dfCACrash.loc[dfCACrash['LIGHTING'].isin([\"C\"]),  'Lighting'] = \"Dark - Street Lights\"\n",
    "dfCACrash.loc[dfCACrash['LIGHTING'].isin([\"D\"]),  'Lighting'] = \"Dark - No Street Lights\"\n",
    "dfCACrash.loc[dfCACrash['LIGHTING'].isin([\"E\"]),  'Lighting'] = \"Dark - Street Lights Not Functioning\"\n",
    "dfCACrash.loc[dfCACrash['LIGHTING'].isin([\"-\"]),  'Lighting'] = \"Not Stated\"\n",
    "#Convert weather 1\n",
    "dfCACrash.loc[dfCACrash['WEATHER_1'].isin([\"A\"]),  'Weather_1'] = \"Clear\"\n",
    "dfCACrash.loc[dfCACrash['WEATHER_1'].isin([\"B\"]),  'Weather_1'] = \"Cloudy\"\n",
    "dfCACrash.loc[dfCACrash['WEATHER_1'].isin([\"C\"]),  'Weather_1'] = \"Raining\"\n",
    "dfCACrash.loc[dfCACrash['WEATHER_1'].isin([\"D\"]),  'Weather_1'] = \"Snowing\"\n",
    "dfCACrash.loc[dfCACrash['WEATHER_1'].isin([\"E\"]),  'Weather_1'] = \"Fog\"\n",
    "dfCACrash.loc[dfCACrash['WEATHER_1'].isin([\"F\"]),  'Weather_1'] = \"Other\"\n",
    "dfCACrash.loc[dfCACrash['WEATHER_1'].isin([\"G\"]),  'Weather_1'] = \"Windy\"\n",
    "dfCACrash.loc[dfCACrash['WEATHER_1'].isin([\"-\"]),  'Weather_1'] = \"Not Stated\"\n",
    "# convert weather 2\n",
    "dfCACrash.loc[dfCACrash['WEATHER_2'].isin([\"A\"]),  'Weather_2'] = \"Clear\"\n",
    "dfCACrash.loc[dfCACrash['WEATHER_2'].isin([\"B\"]),  'Weather_2'] = \"Cloudy\"\n",
    "dfCACrash.loc[dfCACrash['WEATHER_2'].isin([\"C\"]),  'Weather_2'] = \"Raining\"\n",
    "dfCACrash.loc[dfCACrash['WEATHER_2'].isin([\"D\"]),  'Weather_2'] = \"Snowing\"\n",
    "dfCACrash.loc[dfCACrash['WEATHER_2'].isin([\"E\"]),  'Weather_2'] = \"Fog\"\n",
    "dfCACrash.loc[dfCACrash['WEATHER_2'].isin([\"F\"]),  'Weather_2'] = \"Other\"\n",
    "dfCACrash.loc[dfCACrash['WEATHER_2'].isin([\"G\"]),  'Weather_2'] = \"Windy\"\n",
    "dfCACrash.loc[dfCACrash['WEATHER_2'].isin([\"-\"]),  'Weather_2'] = \"Not Stated\"\n",
    "#Convert violation\n",
    "dfCACrash.loc[dfCACrash['PCF_VIOL_CATEGORY'].isin([\"0\"]),  'Violation'] = \"Unknown\"\n",
    "dfCACrash.loc[dfCACrash['PCF_VIOL_CATEGORY'].isin([\"1\"]),  'Violation'] = \"Driving/Biking Under the Influence\"\n",
    "dfCACrash.loc[dfCACrash['PCF_VIOL_CATEGORY'].isin([\"2\"]),  'Violation'] = \"Impeding Traffic\"\n",
    "dfCACrash.loc[dfCACrash['PCF_VIOL_CATEGORY'].isin([\"3\"]),  'Violation'] = \"Unsafe Speed\"\n",
    "dfCACrash.loc[dfCACrash['PCF_VIOL_CATEGORY'].isin([\"4\"]),  'Violation'] = \"Following Too Closely\"\n",
    "dfCACrash.loc[dfCACrash['PCF_VIOL_CATEGORY'].isin([\"5\"]),  'Violation'] = \"Wrong Side of Road\"\n",
    "dfCACrash.loc[dfCACrash['PCF_VIOL_CATEGORY'].isin([\"6\"]),  'Violation'] = \"Improper Passing\"\n",
    "dfCACrash.loc[dfCACrash['PCF_VIOL_CATEGORY'].isin([\"7\"]),  'Violation'] = \"Unsafe Lane Change\"\n",
    "dfCACrash.loc[dfCACrash['PCF_VIOL_CATEGORY'].isin([\"8\"]),  'Violation'] = \"Improper Turning\"\n",
    "dfCACrash.loc[dfCACrash['PCF_VIOL_CATEGORY'].isin([\"9\"]),  'Violation'] = \"Auotomobile Right of Way\"\n",
    "dfCACrash.loc[dfCACrash['PCF_VIOL_CATEGORY'].isin([\"10\"]),  'Violation'] = \"Pedestrian Right of Way\"\n",
    "dfCACrash.loc[dfCACrash['PCF_VIOL_CATEGORY'].isin([\"11\"]),  'Violation'] = \"Pedestrian Violation\"\n",
    "dfCACrash.loc[dfCACrash['PCF_VIOL_CATEGORY'].isin([\"12\"]),  'Violation'] = \"Traffic Signals and Signs\"\n",
    "dfCACrash.loc[dfCACrash['PCF_VIOL_CATEGORY'].isin([\"13\"]),  'Violation'] = \"Hazardous Parking\"\n",
    "dfCACrash.loc[dfCACrash['PCF_VIOL_CATEGORY'].isin([\"17\"]),  'Violation'] = \"Other Hazardous Violation\"\n",
    "dfCACrash.loc[dfCACrash['PCF_VIOL_CATEGORY'].isin([\"18\"]),  'Violation'] = \"Other Than Driver or Pedestrian\"\n",
    "dfCACrash.loc[dfCACrash['PCF_VIOL_CATEGORY'].isin([\"21\"]),  'Violation'] = \"Unsafe Backing or Starting\"\n",
    "dfCACrash.loc[dfCACrash['PCF_VIOL_CATEGORY'].isin([\"22\"]),  'Violation'] = \"Other Improper Driving\"\n",
    "dfCACrash.loc[dfCACrash['PCF_VIOL_CATEGORY'].isin([\"-\"]),  'Violation'] = \"Not Stated\"\n",
    "#Convert road surface\n",
    "dfCACrash.loc[dfCACrash['ROAD_SURFACE'].isin([\"A\"]),  'Road_Surface'] = \"Dry\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_SURFACE'].isin([\"B\"]),  'Road_Surface'] = \"Wet\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_SURFACE'].isin([\"C\"]),  'Road_Surface'] = \"Snowy or Icy\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_SURFACE'].isin([\"D\"]),  'Road_Surface'] = \"Slippery (Muddy, Oily, etc.)\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_SURFACE'].isin([\"-\"]),  'Road_Surface'] = \"Not Stated\"\n",
    "#Convert road condition 1\n",
    "dfCACrash.loc[dfCACrash['ROAD_COND_1'].isin([\"A\"]),  'Road_Condition_1'] = \"Holes, Deep Ruts\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_COND_1'].isin([\"B\"]),  'Road_Condition_1'] = \"Loose Material on Roadway\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_COND_1'].isin([\"C\"]),  'Road_Condition_1'] = \"Obstruction on Roadway\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_COND_1'].isin([\"D\"]),  'Road_Condition_1'] = \"Construction or Repair Zone\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_COND_1'].isin([\"E\"]),  'Road_Condition_1'] = \"Reduced Roadway Width\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_COND_1'].isin([\"F\"]),  'Road_Condition_1'] = \"Flooded\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_COND_1'].isin([\"G\"]),  'Road_Condition_1'] = \"Other\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_COND_1'].isin([\"H\"]),  'Road_Condition_1'] = \"No Unusual Condition\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_COND_1'].isin([\"-\"]),  'Road_Condition_1'] = \"Not Stated\"\n",
    "# conver road condition 2\n",
    "dfCACrash.loc[dfCACrash['ROAD_COND_2'].isin([\"A\"]),  'Road_Condition_2'] = \"Holes, Deep Ruts\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_COND_2'].isin([\"B\"]),  'Road_Condition_2'] = \"Loose Material on Roadway\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_COND_2'].isin([\"C\"]),  'Road_Condition_2'] = \"Obstruction on Roadway\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_COND_2'].isin([\"D\"]),  'Road_Condition_2'] = \"Construction or Repair Zone\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_COND_2'].isin([\"E\"]),  'Road_Condition_2'] = \"Reduced Roadway Width\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_COND_2'].isin([\"F\"]),  'Road_Condition_2'] = \"Flooded\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_COND_2'].isin([\"G\"]),  'Road_Condition_2'] = \"Other\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_COND_2'].isin([\"H\"]),  'Road_Condition_2'] = \"No Unusual Condition\"\n",
    "dfCACrash.loc[dfCACrash['ROAD_COND_2'].isin([\"-\"]),  'Road_Condition_2'] = \"Not Stated\"\n",
    "#Convert pedestrian action\n",
    "dfCACrash.loc[dfCACrash['PED_ACTION'].isin([\"A\"]),  'Pedestrian_Action'] = \"No Pedestrian Involved\"\n",
    "dfCACrash.loc[dfCACrash['PED_ACTION'].isin([\"B\"]),  'Pedestrian_Action'] = \"Crossing in Crosswalk at Intersection\"\n",
    "dfCACrash.loc[dfCACrash['PED_ACTION'].isin([\"C\"]),  'Pedestrian_Action'] = \"Crossing in Crosswalk Not at Intersection\"\n",
    "dfCACrash.loc[dfCACrash['PED_ACTION'].isin([\"D\"]),  'Pedestrian_Action'] = \"Crossing Not in Crosswalk\"\n",
    "dfCACrash.loc[dfCACrash['PED_ACTION'].isin([\"E\"]),  'Pedestrian_Action'] = \"In Road, Including Shoulder\"\n",
    "dfCACrash.loc[dfCACrash['PED_ACTION'].isin([\"F\"]),  'Pedestrian_Action'] = \"Not in Road\"\n",
    "dfCACrash.loc[dfCACrash['PED_ACTION'].isin([\"G\"]),  'Pedestrian_Action'] = \"Approaching/Leaving School Bus\"\n",
    "dfCACrash.loc[dfCACrash['PED_ACTION'].isin([\"-\"]),  'Pedestrian_Action'] = \"Not Stated\"\n",
    "#Convert hit and run\n",
    "dfCACrash.loc[dfCACrash['HIT_AND_RUN'].isin([\"F\"]), 'Hit_and_Run'] = \"Felony\"\n",
    "dfCACrash.loc[dfCACrash['HIT_AND_RUN'].isin([\"M\"]), 'Hit_and_Run'] = \"Misdemeanor\"\n",
    "dfCACrash.loc[dfCACrash['HIT_AND_RUN'].isin([\"N\"]), 'Hit_and_Run'] = \"Not Hit and Run\"\n",
    "#Convert MVIW\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"A\"]), 'Motor_Vehicle_Interacted_With'] = \"Non-Collision\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"B\"]), 'Motor_Vehicle_Interacted_With'] = \"Pedestrian\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"C\"]), 'Motor_Vehicle_Interacted_With'] = \"Other Motor Vehicle\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"D\"]), 'Motor_Vehicle_Interacted_With'] = \"Motor Vehicle on Other Roadway\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"E\"]), 'Motor_Vehicle_Interacted_With'] = \"Parked Motor Vehicle\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"F\"]), 'Motor_Vehicle_Interacted_With'] = \"Train\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"G\"]), 'Motor_Vehicle_Interacted_With'] = \"Bicycle\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"H\"]), 'Motor_Vehicle_Interacted_With'] = \"Animal\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"I\"]), 'Motor_Vehicle_Interacted_With'] = \"Fixed Object\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"J\"]), 'Motor_Vehicle_Interacted_With'] = \"Other Object\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"0\"]), 'Motor_Vehicle_Interacted_With'] = \"Non-Collision and Additional Object\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"1\"]), 'Motor_Vehicle_Interacted_With'] = \"Pedestrian and Additional Object\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"2\"]), 'Motor_Vehicle_Interacted_With'] = \"Other Motor Vehicle and Additional Object\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"3\"]), 'Motor_Vehicle_Interacted_With'] = \"Motor Vehicle on Other Roadway and Additional Object\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"4\"]), 'Motor_Vehicle_Interacted_With'] = \"Parked Motor Vehicle and Additional Object\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"5\"]), 'Motor_Vehicle_Interacted_With'] = \"Train and Additional Object\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"6\"]), 'Motor_Vehicle_Interacted_With'] = \"Bicycle and Additional Object\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"7\"]), 'Motor_Vehicle_Interacted_With'] = \"Animal and Additional Object\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"8\"]), 'Motor_Vehicle_Interacted_With'] = \"Fixed Object and Additional Object\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"9\"]), 'Motor_Vehicle_Interacted_With'] = \"Other Object and Additional Object\"\n",
    "dfCACrash.loc[dfCACrash['MVIW'].isin([\"-\"]), 'Motor_Vehicle_Interacted_With'] = \"Not Stated\"\n",
    "#convert case ID\n",
    "dfCACrash['CA_Case_ID']            = dfCACrash['CASE_ID']\n",
    "dfCACrash['NV_Accident_Num']       = np.nan\n",
    "dfCACrash['NV_Accident_Rec_Num']   = np.nan\n",
    "#Convert number injured/killed\n",
    "dfCACrash['Num_Killed']            = dfCACrash['NUMBER_KILLED']\n",
    "dfCACrash['Num_Injured']           = dfCACrash['NUMBER_INJURED']\n",
    "dfCACrash['Num_Ped_Killed']        = dfCACrash['COUNT_PED_KILLED']\n",
    "dfCACrash['Num_Ped_Injured']       = dfCACrash['COUNT_PED_INJURED']\n",
    "dfCACrash['Num_Bicyclist_Killed']  = dfCACrash['COUNT_BICYCLIST_KILLED']\n",
    "dfCACrash['Num_Bicyclist_Injured'] = dfCACrash['COUNT_BICYCLIST_INJURED']\n",
    "dfCACrash['Num_Motorcyclist_Killed']  = dfCACrash['COUNT_MC_KILLED']\n",
    "dfCACrash['Num_Motorcyclist_Injured'] = dfCACrash['COUNT_MC_INJURED']\n",
    "print(dfCACrash['Num_Motorcyclist_Killed'])\n",
    "#Convert number vehicles/parties\n",
    "dfCACrash['Num_Vehicles']          = np.nan\n",
    "dfCACrash['Num_Parties']           = dfCACrash['PARTY_COUNT']\n",
    "#Convert alcohol/bike/ped involvement\n",
    "dfCACrash['Alcohol_Involved']      = dfCACrash['ALCOHOL_INVOLVED']\n",
    "dfCACrash['Pedestrian_Involved']   = dfCACrash['PEDESTRIAN_ACCIDENT']\n",
    "dfCACrash['Bicycle_Involved']      = dfCACrash['BICYCLE_ACCIDENT']\n",
    "dfCACrash['Motorcycle_Involved']   = dfCACrash['MOTORCYCLE_ACCIDENT']\n",
    "dfCACrash['Corridor_ID']           = np.nan\n",
    "\n",
    "# final list of fields\n",
    "dfCACrash = dfCACrash[['State',\n",
    "           'CA_Case_ID',\n",
    "           'NV_Accident_Num',\n",
    "           'NV_Accident_Rec_Num',\n",
    "           'Corridor_ID',\n",
    "           'County',\n",
    "           'City',\n",
    "           'Year',\n",
    "           'Date',\n",
    "           'Time',\n",
    "           'Weather_1',\n",
    "           'Weather_2',\n",
    "           'Crash_Severity',\n",
    "           'Num_Killed',\n",
    "           'Num_Injured',\n",
    "           'Num_Ped_Killed',\n",
    "           'Num_Ped_Injured',\n",
    "           'Num_Bicyclist_Killed',\n",
    "           'Num_Bicyclist_Injured',\n",
    "           'Num_Motorcyclist_Killed',\n",
    "           'Num_Motorcyclist_Injured',\n",
    "           'Crash_Type',\n",
    "           'Num_Vehicles',\n",
    "           'Num_Parties',\n",
    "           'Violation',\n",
    "           'Hit_and_Run',\n",
    "           'Motor_Vehicle_Interacted_With',\n",
    "           'Pedestrian_Action', \n",
    "           'Road_Condition_1',\n",
    "           'Road_Condition_2',\n",
    "           'Road_Surface',\n",
    "           'Lighting',\n",
    "           'Pedestrian_Involved',\n",
    "           'Bicycle_Involved',\n",
    "           'Motorcycle_Involved',\n",
    "           'Alcohol_Involved',\n",
    "           'Data_Source',\n",
    "           'POINT_X',\n",
    "           'POINT_Y']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crash Feature Classes Merged\n",
      "Started data transfer: 2023-12-08 11:49:51\n",
      "Finished data transfer: 2023-12-08 11:49:56\n",
      "Finished updating staging layer\n",
      "features deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## NV Data Transformation\n",
    "# set fields for time and case info\n",
    "dfNVCrash['CA_Case_ID']           = np.nan\n",
    "dfNVCrash['NV_Accident_Num']      = dfNVCrash['NV Accident Num']\n",
    "dfNVCrash['NV_Accident_Rec_Num']  = dfNVCrash['NV Accident Rec Num']\n",
    "dfNVCrash['City']                 = np.nan\n",
    "dfNVCrash['Year']                 = dfNVCrash['Collision_Year']\n",
    "dfNVCrash['Date']                 = dfNVCrash['Collision_Date']\n",
    "dfNVCrash['Time']                 = dfNVCrash['Collision_Time']\n",
    "dfNVCrash['Num_Vehicles']         = dfNVCrash['Total Vehicles']\n",
    "dfNVCrash['Num_Parties']          = np.nan\n",
    "dfNVCrash['Data_Source']          = \"NDOT\"\n",
    "\n",
    "# Convert NV crash type and severity\n",
    "dfNVCrash['Crash_Severity']       = dfNVCrash['COLLISION_SEVERITY']\n",
    "dfNVCrash.loc[dfNVCrash['Crash Type'].isin([\"ANGLE\"]),  'Crash_Type']         = 'Angle-broadside'\n",
    "dfNVCrash.loc[dfNVCrash['Crash Type'].isin([\"BACKING\"]),  'Crash_Type']       = 'Backing'\n",
    "dfNVCrash.loc[dfNVCrash['Crash Type'].isin([\"HEAD-ON\"]),  'Crash_Type']       = 'Head-on'\n",
    "dfNVCrash.loc[dfNVCrash['Crash Type'].isin([\"NON-COLLISION\"]),  'Crash_Type'] = 'Non-collision'\n",
    "dfNVCrash.loc[dfNVCrash['Crash Type'].isin([\"REAR-END\"]),  'Crash_Type']      = 'Rear end'\n",
    "dfNVCrash.loc[dfNVCrash['Crash Type'].isin([\"REAR-TO-REAR\"]),  'Crash_Type']  = 'Other'\n",
    "dfNVCrash.loc[dfNVCrash['Crash Type'].isin([\"SIDESWIPE, MEETING\"]),  'Crash_Type'] = 'Sideswipe'\n",
    "dfNVCrash.loc[dfNVCrash['Crash Type'].isin([\"SIDESWIPE, OVERTAKING\"]),  'Crash_Type'] = 'Sideswipe'\n",
    "dfNVCrash.loc[dfNVCrash['Crash Type'].isin([\"UNKNOWN\"]),  'Crash_Type'] = 'Unknown'\n",
    "#Convert # injured/killed\n",
    "dfNVCrash['Num_Killed']   = dfNVCrash['Fatalities']\n",
    "dfNVCrash['Num_Killed'].fillna(0)\n",
    "dfNVCrash['Num_Injured'] = dfNVCrash['Injured']\n",
    "dfNVCrash['Num_Injured'].fillna(0)\n",
    "dfNVCrash['Num_Ped_Killed'] = np.nan\n",
    "dfNVCrash['Num_Ped_Injured'] = np.nan\n",
    "dfNVCrash['Num_Bicyclist_Killed'] = np.nan\n",
    "dfNVCrash['Num_Bicyclist_Injured'] = np.nan\n",
    "dfNVCrash['Num_Motorcyclist_Killed'] = np.nan\n",
    "dfNVCrash['Num_Motorcyclist_Injured'] = np.nan\n",
    "# convert crash info\n",
    "dfNVCrash['Violation']  = \"N/A\"\n",
    "dfNVCrash['Hit_and_Run'] = \"N/A\"\n",
    "dfNVCrash['Motor_Vehicle_Interacted_With'] = \"N/A\"\n",
    "dfNVCrash['Pedestrian_Action'] = \"N/A\"\n",
    "# Process lighting\n",
    "dfNVCrash['Lighting'] = dfNVCrash['LIGHTING']\n",
    "dfNVCrash.loc[dfNVCrash['Lighting'].isin([\"Dusk-Dawn\"]),  'Lighting'] = \"Dusk - Dawn\"\n",
    "dfNVCrash.loc[dfNVCrash['Lighting'].isin([\"Dusk-dawn\"]),  'Lighting'] = \"Dusk - Dawn\"\n",
    "dfNVCrash.loc[dfNVCrash['Lighting'].isin([\"Dark - street lights\"]),  'Lighting'] = \"Dark - Street Lights\"\n",
    "dfNVCrash.loc[dfNVCrash['Lighting'].isin([\"Dark - no street lights\"]),  'Lighting'] = \"Dark - No Street Lights\"\n",
    "dfNVCrash.loc[dfNVCrash['Lighting'].isin([\"Dark - Unknown Lighting\"]),  'Lighting'] = \"Dark - Unknown Lighting\"\n",
    "dfNVCrash.loc[dfNVCrash['Lighting'].isin([\"Not stated\"]),  'Lighting'] = \"Not Stated\"\n",
    "#Process alcohol involvement (check outputs)\n",
    "dfNVCrash['V1 Driver Factors'] = dfNVCrash['V1 Driver Factors'].fillna(\"Not stated\")\n",
    "dfNVCrash['V2 Driver Factors'] = dfNVCrash['V2 Driver Factors'].fillna(\"Not stated\")\n",
    "dfNVCrash.loc[dfNVCrash['V1 Driver Factors'].str.contains(\"DRINKING\"), 'Alcohol_Involved'] = \"Y\"\n",
    "dfNVCrash.loc[dfNVCrash['V2 Driver Factors'].str.contains(\"DRINKING\"), 'Alcohol_Involved'] = \"Y\"\n",
    "#Process bike/ped involvement (check outputs)\n",
    "dfNVCrash['V1 All Events'] = dfNVCrash['V1 All Events'].fillna(\"Not stated\")\n",
    "dfNVCrash['V2 All Events'] = dfNVCrash['V2 All Events'].fillna(\"Not stated\")\n",
    "dfNVCrash.loc[dfNVCrash['V1 All Events'].str.contains(\"PEDESTRIAN\"), 'Pedestrian_Involved'] = \"Y\"\n",
    "dfNVCrash.loc[dfNVCrash['V2 All Events'].str.contains(\"PEDESTRIAN\"), 'Pedestrian_Involved'] = \"Y\"\n",
    "dfNVCrash.loc[dfNVCrash['V1 All Events'].str.contains(\"PEDAL CYCLE\"), 'Bicycle_Involved'] = \"Y\"\n",
    "dfNVCrash.loc[dfNVCrash['V2 All Events'].str.contains(\"PEDAL CYCLE\"), 'Bicycle_Involved'] = \"Y\"\n",
    "#Process motorcycle involvement (check outputs)\n",
    "dfNVCrash['V1 Type'] = dfNVCrash['V1 Type'].fillna(\"Not stated\")\n",
    "dfNVCrash['V2 Type'] = dfNVCrash['V1 Type'].fillna(\"Not stated\")\n",
    "dfNVCrash.loc[dfNVCrash['V1 Type'].str.contains(\"MOTORCYCLE\"), 'Motorcycle_Involved'] = \"Y\"\n",
    "dfNVCrash.loc[dfNVCrash['V1 Type'].str.contains(\"MOTORBIKE\"), 'Motorcycle_Involved'] = \"Y\"\n",
    "dfNVCrash.loc[dfNVCrash['V1 Type'].str.contains(\"MOPED\"), 'Motorcycle_Involved'] = \"Y\"\n",
    "dfNVCrash.loc[dfNVCrash['V2 Type'].str.contains(\"MOTORCYCLE\"), 'Motorcycle_Involved'] = \"Y\"\n",
    "dfNVCrash.loc[dfNVCrash['V2 Type'].str.contains(\"MOTORBIKE\"), 'Motorcycle_Involved'] = \"Y\"\n",
    "dfNVCrash.loc[dfNVCrash['V2 Type'].str.contains(\"MOPED\"), 'Motorcycle_Involved'] = \"Y\"\n",
    "# #Convert road surface\n",
    "dfNVCrash['Factors Roadway'] = dfNVCrash['Factors Roadway'].fillna(\"Not stated\")\n",
    "dfNVCrash['HWY Factors'] = dfNVCrash['HWY Factors'].fillna(\"Not stated\")\n",
    "dfNVCrash.loc[dfNVCrash['Factors Roadway'].str.contains(\"DRY\"), 'Road_Surface'] = \"Dry\"\n",
    "dfNVCrash.loc[dfNVCrash['Factors Roadway'].str.contains(\"WET\"), 'Road_Surface'] = \"Wet\"\n",
    "dfNVCrash.loc[dfNVCrash['Factors Roadway'].str.contains(\"WATER\"), 'Road_Surface'] = \"Wet\"\n",
    "dfNVCrash.loc[dfNVCrash['Factors Roadway'].str.contains(\"ICE\"), 'Road_Surface'] = \"Snowy or Icy\"\n",
    "dfNVCrash.loc[dfNVCrash['Factors Roadway'].str.contains(\"SNOW\"), 'Road_Surface'] = \"Snowy or Icy\"\n",
    "dfNVCrash.loc[dfNVCrash['Factors Roadway'].str.contains(\"SLUSH\"), 'Road_Surface'] = \"Snowy or Icy\"\n",
    "dfNVCrash.loc[dfNVCrash['Factors Roadway'].str.contains(\"OTHER\"), 'Road_Surface'] = \"Other\"\n",
    "dfNVCrash.loc[dfNVCrash['Factors Roadway'].str.contains(\"NA\"), 'Road_Surface'] = \"Not stated\"\n",
    "dfNVCrash.loc[dfNVCrash['Factors Roadway'].str.contains(\"UNKNOWN\"), 'Road_Surface'] = \"Unknown\"\n",
    "dfNVCrash.loc[dfNVCrash['Factors Roadway'].str.contains(\"Not stated\"), 'Road_Surface'] = \"Not stated\"\n",
    "#Convert road condition\n",
    "dfNVCrash.loc[dfNVCrash['HWY Factors'].str.contains(\"NONE\"), 'Road_Condition_1'] = \"No Unusual Condition\"\n",
    "dfNVCrash.loc[dfNVCrash['HWY Factors'].str.contains(\"UNKNOWN\"), 'Road_Condition_1'] = \"Unknown\"\n",
    "dfNVCrash.loc[dfNVCrash['HWY Factors'].str.contains(\"WET, ICY, SNOW, SLUSH\"), 'Road_Condition_1'] = \"Weather\"\n",
    "dfNVCrash.loc[dfNVCrash['HWY Factors'].str.contains(\"WEATHER\"), 'Road_Condition_1'] = \"Weather\"\n",
    "dfNVCrash.loc[dfNVCrash['HWY Factors'].str.contains(\"BACKUP\"), 'Road_Condition_1'] = \"Other\"\n",
    "dfNVCrash.loc[dfNVCrash['HWY Factors'].str.contains(\"GLARE\"), 'Road_Condition_1'] = \"Other\"\n",
    "dfNVCrash.loc[dfNVCrash['HWY Factors'].str.contains(\"OTHER\"), 'Road_Condition_1'] = \"Other\"\n",
    "dfNVCrash.loc[dfNVCrash['HWY Factors'].str.contains(\"VISUAL OBSTRUCTION\"), 'Road_Condition_1'] = \"Other\"\n",
    "dfNVCrash.loc[dfNVCrash['HWY Factors'].str.contains(\"ROAD OBSTRUCTION\"), 'Road_Condition_1'] = \"Obstruction on Roadway\"\n",
    "dfNVCrash.loc[dfNVCrash['HWY Factors'].str.contains(\"DEBRIS\"), 'Road_Condition_1'] = \"Loose Material on Roadway\"\n",
    "dfNVCrash.loc[dfNVCrash['HWY Factors'].str.contains(\"WORK ZONE\"), 'Road_Condition_1'] = \"Construction or Repair Zone\"\n",
    "dfNVCrash.loc[dfNVCrash['HWY Factors'].str.contains(\"ANIMAL\"), 'Road_Condition_1'] = \"Wildlife\"\n",
    "dfNVCrash.loc[dfNVCrash['HWY Factors'].str.contains(\"RUTS\"), 'Road_Condition_1'] = \"Holes, Deep Ruts\"\n",
    "dfNVCrash.loc[dfNVCrash['HWY Factors'].str.contains(\"Not stated\"), 'Road_Condition_1'] = \"Not Stated\"\n",
    "dfNVCrash['Road_Condition_2'] = np.nan\n",
    "\n",
    "#Rename to match CA data\n",
    "dfNVCrash['Weather_1']   = dfNVCrash['Weather']\n",
    "dfNVCrash['Weather_2']   = np.nan\n",
    "dfNVCrash['POINT_X']     = dfNVCrash['X']\n",
    "dfNVCrash['POINT_Y']     = dfNVCrash['Y']\n",
    "dfNVCrash['Corridor_ID'] = np.nan\n",
    "\n",
    "# final list of fields\n",
    "dfNVCrash = dfNVCrash[['State',\n",
    "           'CA_Case_ID',\n",
    "           'NV_Accident_Num',\n",
    "           'NV_Accident_Rec_Num',\n",
    "           'Corridor_ID',\n",
    "           'County',\n",
    "           'City',\n",
    "           'Year',\n",
    "           'Date',\n",
    "           'Time',\n",
    "           'Weather_1',\n",
    "           'Weather_2',\n",
    "           'Crash_Severity',\n",
    "           'Num_Killed',\n",
    "           'Num_Injured',\n",
    "           'Num_Ped_Killed',\n",
    "           'Num_Ped_Injured',\n",
    "           'Num_Bicyclist_Killed',\n",
    "           'Num_Bicyclist_Injured',\n",
    "           'Crash_Type',\n",
    "           'Num_Vehicles',\n",
    "           'Num_Parties',\n",
    "           'Violation',\n",
    "           'Hit_and_Run',\n",
    "           'Motor_Vehicle_Interacted_With',\n",
    "           'Pedestrian_Action', \n",
    "           'Road_Condition_1',\n",
    "           'Road_Condition_2',\n",
    "           'Road_Surface',\n",
    "           'Lighting',\n",
    "           'Pedestrian_Involved',\n",
    "           'Bicycle_Involved',\n",
    "           'Motorcycle_Involved',\n",
    "           'Alcohol_Involved',\n",
    "           'Data_Source',\n",
    "           'POINT_X',\n",
    "           'POINT_Y']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NV 2021 data transformation\n",
    "\n",
    "# set fields for time and case info\n",
    "dfNVCrash21['CA_Case_ID']           = np.nan\n",
    "dfNVCrash21['NV_Accident_Num']      = dfNVCrash21['NV Accident Num']\n",
    "dfNVCrash21['NV_Accident_Rec_Num']  = dfNVCrash21['NV Accident Rec Num']\n",
    "dfNVCrash21['City']                 = np.nan\n",
    "dfNVCrash21['Year']                 = dfNVCrash21['Collision_Year']\n",
    "dfNVCrash21['Date']                 = dfNVCrash21['Collision_Date']\n",
    "dfNVCrash21['Time']                 = dfNVCrash21['Collision_Time']\n",
    "dfNVCrash21['Num_Vehicles']         = dfNVCrash21['Total Vehicles']\n",
    "dfNVCrash21['Num_Parties']          = np.nan\n",
    "dfNVCrash21['Data_Source']          = \"NDOT\"\n",
    "\n",
    "# Convert NV crash type and severity\n",
    "dfNVCrash21['Crash_Severity']       = dfNVCrash21['COLLISION_SEVERITY']\n",
    "dfNVCrash21.loc[dfNVCrash21['VehCrashType'].isin([\"ANGLE\"]),  'Crash_Type']         = 'Angle-broadside'\n",
    "dfNVCrash21.loc[dfNVCrash21['VehCrashType'].isin([\"BACKING\"]),  'Crash_Type']       = 'Backing'\n",
    "dfNVCrash21.loc[dfNVCrash21['VehCrashType'].isin([\"HEAD-ON\"]),  'Crash_Type']       = 'Head-on'\n",
    "dfNVCrash21.loc[dfNVCrash21['VehCrashType'].isin([\"HEAD ON\"]),  'Crash_Type']       = 'Head-on'\n",
    "dfNVCrash21.loc[dfNVCrash21['VehCrashType'].isin([\"NON-COLLISION\"]),  'Crash_Type'] = 'Non-collision'\n",
    "dfNVCrash21.loc[dfNVCrash21['VehCrashType'].isin([\"REAR-END\"]),  'Crash_Type']      = 'Rear end'\n",
    "dfNVCrash21.loc[dfNVCrash21['VehCrashType'].isin([\"REAR-TO-REAR\"]),  'Crash_Type']  = 'Other'\n",
    "dfNVCrash21.loc[dfNVCrash21['VehCrashType'].isin([\"SIDESWIPE, MEETING\"]),  'Crash_Type'] = 'Sideswipe'\n",
    "dfNVCrash21.loc[dfNVCrash21['VehCrashType'].isin([\"SIDESWIPE - MEETING\"]),  'Crash_Type'] = 'Sideswipe'\n",
    "dfNVCrash21.loc[dfNVCrash21['VehCrashType'].isin([\"SIDESWIPE, OVERTAKING\"]),  'Crash_Type'] = 'Sideswipe'\n",
    "dfNVCrash21.loc[dfNVCrash21['VehCrashType'].isin([\"SIDESWIPE - OVERTAKING\"]),  'Crash_Type'] = 'Sideswipe'\n",
    "dfNVCrash21.loc[dfNVCrash21['VehCrashType'].isin([\"UNKNOWN\"]),  'Crash_Type'] = 'Unknown'\n",
    "#Convert # injured/killed\n",
    "dfNVCrash21['Num_Killed']   = dfNVCrash21['NumFatalities']\n",
    "dfNVCrash21['Num_Killed'].fillna(0)\n",
    "dfNVCrash21['Num_Injured'] = dfNVCrash21['Injured']\n",
    "dfNVCrash21['Num_Injured'].fillna(0)\n",
    "dfNVCrash21['Num_Ped_Killed'] = np.nan\n",
    "dfNVCrash21['Num_Ped_Injured'] = np.nan\n",
    "dfNVCrash21['Num_Bicyclist_Killed'] = np.nan\n",
    "dfNVCrash21['Num_Bicyclist_Injured'] = np.nan\n",
    "dfNVCrash21['Num_Motorcyclist_Killed'] = np.nan\n",
    "dfNVCrash21['Num_Motorcyclist_Injured'] = np.nan\n",
    "# convert crash info\n",
    "dfNVCrash21['Violation']  = \"N/A\"\n",
    "dfNVCrash21['Hit_and_Run'] = \"N/A\"\n",
    "dfNVCrash21['Motor_Vehicle_Interacted_With'] = \"N/A\"\n",
    "dfNVCrash21['Pedestrian_Action'] = \"N/A\"\n",
    "# Process lighting\n",
    "dfNVCrash21['Lighting'] = dfNVCrash21['LightCondition']\n",
    "dfNVCrash21.loc[dfNVCrash21['Lighting'].isin([\"Dusk\"]),  'Lighting'] = \"Dusk - Dawn\"\n",
    "dfNVCrash21.loc[dfNVCrash21['Lighting'].isin([\"Dawn\"]),  'Lighting'] = \"Dusk - Dawn\"\n",
    "dfNVCrash21.loc[dfNVCrash21['Lighting'].isin([\"Dark - street lights\"]),  'Lighting'] = \"Dark - Street Lights\"\n",
    "dfNVCrash21.loc[dfNVCrash21['Lighting'].isin([\"DARK - NO ROADWAY LIGHTING\"]),  'Lighting'] = \"Dark - No Street Lights\"\n",
    "dfNVCrash21.loc[dfNVCrash21['Lighting'].isin([\"Dark - Unknown Lighting\"]),  'Lighting'] = \"Dark - Unknown Lighting\"\n",
    "dfNVCrash21.loc[dfNVCrash21['Lighting'].isin([\"DAYLIGHT\"]),  'Lighting'] = \"Daylight\"\n",
    "dfNVCrash21.loc[dfNVCrash21['Lighting'].isin([\"Unknown\"]),  'Lighting'] = \"Not Stated\"\n",
    "#Process alcohol involvement (check outputs)\n",
    "dfNVCrash21['V1 Driver Factors'] = dfNVCrash21['V1 Driver Factors'].fillna(\"Not stated\")\n",
    "dfNVCrash21['V2 Driver Factors'] = dfNVCrash21['V2 Driver Factors'].fillna(\"Not stated\")\n",
    "dfNVCrash21.loc[dfNVCrash21['V1 Driver Factors'].str.contains(\"DRINKING\"), 'Alcohol_Involved'] = \"Y\"\n",
    "dfNVCrash21.loc[dfNVCrash21['V2 Driver Factors'].str.contains(\"DRINKING\"), 'Alcohol_Involved'] = \"Y\"\n",
    "#Process bike/ped involvement (check outputs)\n",
    "dfNVCrash21['V1 All Events'] = dfNVCrash21['V1 All Events'].fillna(\"Not stated\")\n",
    "dfNVCrash21['V2 All Events'] = dfNVCrash21['V2 All Events'].fillna(\"Not stated\")\n",
    "dfNVCrash21.loc[dfNVCrash21['V1 All Events'].str.contains(\"PEDESTRIAN\"), 'Pedestrian_Involved'] = \"Y\"\n",
    "dfNVCrash21.loc[dfNVCrash21['V2 All Events'].str.contains(\"PEDESTRIAN\"), 'Pedestrian_Involved'] = \"Y\"\n",
    "dfNVCrash21.loc[dfNVCrash21['V1 All Events'].str.contains(\"PEDAL CYCLE\"), 'Bicycle_Involved'] = \"Y\"\n",
    "dfNVCrash21.loc[dfNVCrash21['V2 All Events'].str.contains(\"PEDAL CYCLE\"), 'Bicycle_Involved'] = \"Y\"\n",
    "#Process motorcycle involvement (check outputs)\n",
    "dfNVCrash21['V1 Type'] = dfNVCrash21['V1 Type'].fillna(\"Not stated\")\n",
    "dfNVCrash21['V2 Type'] = dfNVCrash21['V1 Type'].fillna(\"Not stated\")\n",
    "dfNVCrash21.loc[dfNVCrash21['V1 Type'].str.contains(\"MOTORCYCLE\"), 'Motorcycle_Involved'] = \"Y\"\n",
    "dfNVCrash21.loc[dfNVCrash21['V1 Type'].str.contains(\"MOTORBIKE\"), 'Motorcycle_Involved'] = \"Y\"\n",
    "dfNVCrash21.loc[dfNVCrash21['V1 Type'].str.contains(\"MOPED\"), 'Motorcycle_Involved'] = \"Y\"\n",
    "dfNVCrash21.loc[dfNVCrash21['V2 Type'].str.contains(\"MOTORCYCLE\"), 'Motorcycle_Involved'] = \"Y\"\n",
    "dfNVCrash21.loc[dfNVCrash21['V2 Type'].str.contains(\"MOTORBIKE\"), 'Motorcycle_Involved'] = \"Y\"\n",
    "dfNVCrash21.loc[dfNVCrash21['V2 Type'].str.contains(\"MOPED\"), 'Motorcycle_Involved'] = \"Y\"\n",
    "# #Convert road surface\n",
    "dfNVCrash21['Factors Roadway'] = dfNVCrash21['Factors Roadway'].fillna(\"Not stated\")\n",
    "dfNVCrash21['HWY Factors'] = dfNVCrash21['HWY Factors'].fillna(\"Not stated\")\n",
    "dfNVCrash21.loc[dfNVCrash21['Factors Roadway'].str.contains(\"DRY\"), 'Road_Surface'] = \"Dry\"\n",
    "dfNVCrash21.loc[dfNVCrash21['Factors Roadway'].str.contains(\"WET\"), 'Road_Surface'] = \"Wet\"\n",
    "dfNVCrash21.loc[dfNVCrash21['Factors Roadway'].str.contains(\"WATER\"), 'Road_Surface'] = \"Wet\"\n",
    "dfNVCrash21.loc[dfNVCrash21['Factors Roadway'].str.contains(\"ICE\"), 'Road_Surface'] = \"Snowy or Icy\"\n",
    "dfNVCrash21.loc[dfNVCrash21['Factors Roadway'].str.contains(\"SNOW\"), 'Road_Surface'] = \"Snowy or Icy\"\n",
    "dfNVCrash21.loc[dfNVCrash21['Factors Roadway'].str.contains(\"SLUSH\"), 'Road_Surface'] = \"Snowy or Icy\"\n",
    "dfNVCrash21.loc[dfNVCrash21['Factors Roadway'].str.contains(\"OTHER\"), 'Road_Surface'] = \"Other\"\n",
    "dfNVCrash21.loc[dfNVCrash21['Factors Roadway'].str.contains(\"NA\"), 'Road_Surface'] = \"Not stated\"\n",
    "dfNVCrash21.loc[dfNVCrash21['Factors Roadway'].str.contains(\"UNKNOWN\"), 'Road_Surface'] = \"Unknown\"\n",
    "dfNVCrash21.loc[dfNVCrash21['Factors Roadway'].str.contains(\"Not stated\"), 'Road_Surface'] = \"Not stated\"\n",
    "#Convert road condition\n",
    "dfNVCrash21.loc[dfNVCrash21['RoadEnvironmentalFactors'].str.contains(\"NONE\"), 'Road_Condition_1'] = \"No Unusual Condition\"\n",
    "dfNVCrash21.loc[dfNVCrash21['RoadEnvironmentalFactors'].str.contains(\"UNKNOWN\"), 'Road_Condition_1'] = \"Unknown\"\n",
    "dfNVCrash21.loc[dfNVCrash21['RoadEnvironmentalFactors'].str.contains(\"WET, ICY, SNOW, SLUSH\"), 'Road_Condition_1'] = \"Weather\"\n",
    "dfNVCrash21.loc[dfNVCrash21['RoadEnvironmentalFactors'].str.contains(\"WEATHER\"), 'Road_Condition_1'] = \"Weather\"\n",
    "dfNVCrash21.loc[dfNVCrash21['RoadEnvironmentalFactors'].str.contains(\"BACKUP\"), 'Road_Condition_1'] = \"Other\"\n",
    "dfNVCrash21.loc[dfNVCrash21['RoadEnvironmentalFactors'].str.contains(\"GLARE\"), 'Road_Condition_1'] = \"Other\"\n",
    "dfNVCrash21.loc[dfNVCrash21['RoadEnvironmentalFactors'].str.contains(\"OTHER\"), 'Road_Condition_1'] = \"Other\"\n",
    "dfNVCrash21.loc[dfNVCrash21['RoadEnvironmentalFactors'].str.contains(\"VISUAL OBSTRUCTION\"), 'Road_Condition_1'] = \"Other\"\n",
    "dfNVCrash21.loc[dfNVCrash21['RoadEnvironmentalFactors'].str.contains(\"ROAD OBSTRUCTION\"), 'Road_Condition_1'] = \"Obstruction on Roadway\"\n",
    "dfNVCrash21.loc[dfNVCrash21['RoadEnvironmentalFactors'].str.contains(\"DEBRIS\"), 'Road_Condition_1'] = \"Loose Material on Roadway\"\n",
    "dfNVCrash21.loc[dfNVCrash21['RoadEnvironmentalFactors'].str.contains(\"WORK ZONE\"), 'Road_Condition_1'] = \"Construction or Repair Zone\"\n",
    "dfNVCrash21.loc[dfNVCrash21['RoadEnvironmentalFactors'].str.contains(\"ANIMAL\"), 'Road_Condition_1'] = \"Wildlife\"\n",
    "dfNVCrash21.loc[dfNVCrash21['RoadEnvironmentalFactors'].str.contains(\"RUTS\"), 'Road_Condition_1'] = \"Holes, Deep Ruts\"\n",
    "dfNVCrash21.loc[dfNVCrash21['RoadEnvironmentalFactors'].str.contains(\"Not stated\"), 'Road_Condition_1'] = \"Not Stated\"\n",
    "dfNVCrash21['Road_Condition_2'] = np.nan\n",
    "\n",
    "#Rename to match CA data\n",
    "dfNVCrash21['Weather_1']   = dfNVCrash21['Weather']\n",
    "dfNVCrash21['Weather_2']   = np.nan\n",
    "dfNVCrash21['POINT_X']     = dfNVCrash21['X']\n",
    "dfNVCrash21['POINT_Y']     = dfNVCrash21['Y']\n",
    "dfNVCrash21['Corridor_ID'] = np.nan\n",
    "\n",
    "# final list of fields\n",
    "dfNVCrash21 = dfNVCrash21[['State',\n",
    "           'CA_Case_ID',\n",
    "           'NV_Accident_Num',\n",
    "           'NV_Accident_Rec_Num',\n",
    "           'Corridor_ID',\n",
    "           'County',\n",
    "           'City',\n",
    "           'Year',\n",
    "           'Date',\n",
    "           'Time',\n",
    "           'Weather_1',\n",
    "           'Weather_2',\n",
    "           'Crash_Severity',\n",
    "           'Num_Killed',\n",
    "           'Num_Injured',\n",
    "           'Num_Ped_Killed',\n",
    "           'Num_Ped_Injured',\n",
    "           'Num_Bicyclist_Killed',\n",
    "           'Num_Bicyclist_Injured',\n",
    "           'Crash_Type',\n",
    "           'Num_Vehicles',\n",
    "           'Num_Parties',\n",
    "           'Violation',\n",
    "           'Hit_and_Run',\n",
    "           'Motor_Vehicle_Interacted_With',\n",
    "           'Pedestrian_Action', \n",
    "           'Road_Condition_1',\n",
    "           'Road_Condition_2',\n",
    "           'Road_Surface',\n",
    "           'Lighting',\n",
    "           'Pedestrian_Involved',\n",
    "           'Bicycle_Involved',\n",
    "           'Motorcycle_Involved',\n",
    "           'Alcohol_Involved',\n",
    "           'Data_Source',\n",
    "           'POINT_X',\n",
    "           'POINT_Y']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# os.remove(os.path.join(workspace, \"NV_Crash_New.csv\" ))\n",
    "\n",
    "# export dataframe to csv \n",
    "dfNVCrash.to_csv(os.path.join(workspace, \"NV_Crash_New.csv\" ))\n",
    "# get NV CSV for XY Table TO Point\n",
    "nvCSV = os.path.join(workspace, \"NV_Crash_New.csv\" )\n",
    "\n",
    "# name the output feature class\n",
    "nvFC  = 'NV_Crash_New'\n",
    "\n",
    "\n",
    "\n",
    "# Nevada data frame to feature class \n",
    "# input data is in NAD 1983 UTM Zone 11N coordinate system\n",
    "arcpy.management.XYTableToPoint(nvCSV, nvFC, \n",
    "                                x_coords, y_coords, \"\",\n",
    "                                # set prjoection transform to from\n",
    "                                arcpy.SpatialReference(26911))\n",
    "\n",
    "# output data for project tool\n",
    "output_NV_Crash_Project = \"NV_Crash_Project\"\n",
    "\n",
    "# project from UTM to WGS\n",
    "arcpy.Project_management(nvFC, output_NV_Crash_Project, out_coordinate_system)\n",
    "\n",
    "# os.remove(os.path.join(workspace, \"CA_Crash_New.csv\"))\n",
    "## CA Export\n",
    "# export dataframe to csv \n",
    "dfCACrash.to_csv(os.path.join(workspace, \"CA_Crash_New.csv\" ))\n",
    "\n",
    "# get NV CSV for XY Table TO Point\n",
    "caCSV = os.path.join(workspace, \"CA_Crash_New.csv\" )\n",
    "\n",
    "# name the output feature class\n",
    "caFC     = 'CA_Crash_New'\n",
    "\n",
    "# CA data frame to feature class\n",
    "arcpy.management.XYTableToPoint(caCSV, caFC, \n",
    "                                x_coords, y_coords, \"\",\n",
    "                                # set prjoection transform to from\n",
    "                                arcpy.SpatialReference(4326))\n",
    "\n",
    "# output data for project tool\n",
    "output_CA_Crash_Project = \"CA_Crash_Project\" \n",
    "\n",
    "# project from UTM to WGS\n",
    "arcpy.Project_management(caFC, output_CA_Crash_Project, out_coordinate_system)\n",
    "\n",
    "## Merge CA and NV\n",
    "# out merge fc\n",
    "tahoeCrash = \"Tahoe_Crash\"\n",
    "\n",
    "# input feature classes\n",
    "caCrash = \"CA_Crash_Project\"\n",
    "nvCrash = \"NV_Crash_Project\"\n",
    "\n",
    "# Create FieldMappings object to manage merge output fields\n",
    "fieldMappings = arcpy.FieldMappings()\n",
    "# Add all fields from all parcel staging layers\n",
    "fieldMappings.addTable(caCrash)\n",
    "fieldMappings.addTable(nvCrash)\n",
    "\n",
    "# Remove all output fields from the field mappings, except fields in field_master list\n",
    "for field in fieldMappings.fields:\n",
    "    if field.name not in [  'OBJECTID',\n",
    "                            'State',\n",
    "                            'CA_Case_ID',\n",
    "                            'NV_Accident_Num',\n",
    "                            'NV_Accident_Rec_Num',\n",
    "                            'Corridor_ID',\n",
    "                            'County',\n",
    "                            'City',\n",
    "                            'Year',\n",
    "                            'Date',\n",
    "                            'Time',\n",
    "                            'Weather_1',\n",
    "                            'Weather_2',\n",
    "                            'Crash_Severity',\n",
    "                            'Num_Killed',\n",
    "                            'Num_Injured',\n",
    "                            'Num_Ped_Killed',\n",
    "                            'Num_Ped_Injured',\n",
    "                            'Num_Bicyclist_Killed',\n",
    "                            'Num_Bicyclist_Injured',\n",
    "                            'Num_Motorcyclist_Killed',\n",
    "                            'Num_Motorcyclist_Injured',\n",
    "                            'Crash_Type',\n",
    "                            'Num_Vehicles',\n",
    "                            'Num_Parties',\n",
    "                            'Violation',\n",
    "                            'Hit_and_Run',\n",
    "                            'Motor_Vehicle_Interacted_With',\n",
    "                            'Pedestrian_Action', \n",
    "                            'Road_Condition_1',\n",
    "                            'Road_Condition_2',\n",
    "                            'Road_Surface',\n",
    "                            'Lighting',\n",
    "                            'Pedestrian_Involved',\n",
    "                            'Bicycle_Involved',\n",
    "                            'Alcohol_Involved',\n",
    "                            'Motorcycle_Involved',\n",
    "                            'Data_Source',\n",
    "                            'POINT_X',\n",
    "                            'POINT_Y',\n",
    "                            'SHAPE@']:\n",
    "        # remove everything else\n",
    "        fieldMappings.removeFieldMap(fieldMappings.findFieldMapIndex(field.name)) \n",
    "    \n",
    "# Use Merge tool to move features into single dataset\n",
    "arcpy.management.Merge([caCrash, nvCrash], tahoeCrash, fieldMappings)\n",
    "print(\"Crash Feature Classes Merged\")\n",
    "\n",
    "# ## Spatial Join of Corridor IDs\n",
    "# in memory points to be used for spatial join results\n",
    "corridorPoints = memory + 'CrashPoint_Corridor'\n",
    "# Spatial Join\n",
    "arcpy.SpatialJoin_analysis(tahoeCrash, corridor, corridorPoints, \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\", \"\", \"\")\n",
    "\n",
    "# use function to transfer spatial join results to crash stagin layer\n",
    "fieldJoinCalc(tahoeCrash, ['OBJECTID', 'Corridor_ID'], corridorPoints, ['OBJECTID','CORRIDOR_NAME'])\n",
    "print(\"Finished updating staging layer\")\n",
    "\n",
    "\n",
    "tempLayer = \"deleteLayers\"\n",
    "\n",
    "# Run MakeFeatureLayer\n",
    "arcpy.management.MakeFeatureLayer(tahoeCrash, tempLayer)\n",
    " \n",
    "arcpy.management.SelectLayerByLocation(tempLayer, \"have_their_center_in\", \n",
    "                                       trpa,\n",
    "                                       search_distance=\"\", \n",
    "                                       selection_type=\"NEW_SELECTION\", \n",
    "                                       invert_spatial_relationship=\"INVERT\")\n",
    " \n",
    "# Run GetCount and if some features have been selected, then \n",
    "#  run DeleteFeatures to remove the selected features.\n",
    "if int(arcpy.management.GetCount(tempLayer)[0]) > 0:\n",
    "    arcpy.management.DeleteFeatures(tempLayer)\n",
    "print(\"features deleted\")\n",
    "\n",
    "# outfc = \n",
    "# Update SDE - Truncate Append\n",
    "# updateSDE(tahoeCrash, outfc, fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Data Export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Photo Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from arcpy import da\n",
    "import os\n",
    "# setup workspace folder\n",
    "workspace = \"//Trpa-fs01/gis/GIS_DATA/Monitoring/StreamBioassessment/StreamBioassessment_Survey/Survey_Photos/2022\"\n",
    "\n",
    "\n",
    "# arcpy.env.workspace = \"C:\\GIS\\Scratch.gdb\"\n",
    "\n",
    "# network path to connection files\n",
    "filePath = \"C:\\\\GIS\\\\DB_CONNECT\"\n",
    "\n",
    "# database file path \n",
    "sdeBase = os.path.join(filePath,\"Collection.sde\")\n",
    "\n",
    "photoTable = os.path.join(sdeBase, 'sde_collection.SDE.StreamBio_Photo_2022__ATTACH')\n",
    "\n",
    "with da.SearchCursor(photoTable, ['DATA', 'ATT_NAME', 'ATTACHMENTID']) as cursor:\n",
    "    for item in cursor:\n",
    "        attachment = item[0]\n",
    "        filenum = \"ATT\" + str(item[2]) + \"_\"\n",
    "        filename = filenum + str(item[1])\n",
    "        open(workspace + os.sep + filename, 'wb').write(attachment.tobytes())\n",
    "        print(filename)\n",
    "        del item\n",
    "        del filenum\n",
    "        del filename\n",
    "        del attachment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Bike Ped Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### How to download the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* __Website:__ [Trafx Data](https://www.trafx.net/datanet/login)\n",
    "__username__: stevlin@trpa.org\n",
    "__password__: _CampOrkila_\n",
    "\n",
    "__TRT Login__\n",
    "__username__: info@tahoerimtrail.org\n",
    "__password__: _Trta8782!_\n",
    "\n",
    "> To get trafx data, hit the main page, then:\n",
    " - go to the analyze tab \n",
    " - select all sites and all date ranges \n",
    " - select daily counts \n",
    " - download csv and enter csv name below\n",
    "\n",
    "* __Website:__ [Ecovisio](https://www.eco-visio.net/Ecovisio/)\n",
    "__username__: _trpc.admin_\n",
    "__password__: _bikecount_\n",
    "\n",
    "> To get EcoVisio data: go to this [page](https://www.eco-visio.net/v5/#analysis::module=6), then:\n",
    " - hit the 'select all' text above the teal 'new counters' bar \n",
    " - press analyze selection in top left\n",
    " - under the period bar, select entire period\n",
    " - hit the spreadsheet symbol then click the spreadsheet option\n",
    " - hit the export symbol and chose csv\n",
    " - chose the default export options\n",
    " \n",
    " \n",
    " * __Save__ the file to the [workspace](//Trpa-fs01/gis/PROJECTS/ResearchAnalysis/Monitoring/Data/BikePed/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T21:20:01.066005Z",
     "start_time": "2022-08-19T21:19:46.492714Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyodbc\n",
    "from datetime import datetime\n",
    "import sqlalchemy\n",
    "import pyodbc\n",
    "import arcpy\n",
    "from arcgis.features import FeatureSet, GeoAccessor, GeoSeriesAccessor\n",
    "\n",
    "# setup workspace folder\n",
    "workspace = \"//Trpa-fs01/gis/PROJECTS/ResearchAnalysis/Monitoring/Data/BikePed/\"\n",
    "\n",
    "\n",
    "arcpy.env.workspace = \"C:\\GIS\\Scratch.gdb\"\n",
    "\n",
    "# network path to connection files\n",
    "filePath = \"C:\\\\GIS\\\\DB_CONNECT\"\n",
    "\n",
    "# database file path \n",
    "sdeBase = os.path.join(filePath, \"Vector.sde\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T21:20:01.121013Z",
     "start_time": "2022-08-19T21:20:01.071006Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# make sql database connection to BMP with pyodbc\n",
    "sdeTabular = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=sql12;DATABASE=sde_tabular;UID=sde;PWD=staff')\n",
    "\n",
    "# Final table to update\n",
    "outTable = \"sde_tabular.SDE.bike_ped_counter_tabular\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Get Current Bike Ped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T21:20:04.940375Z",
     "start_time": "2022-08-19T21:20:02.293148Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Parcels - create a dataframe from SDE Parcel Master\n",
    "counters = sdeBase + \"\\\\sde.SDE.Transportation\\\\sde.SDE.bike_ped_counter_spatial\"\n",
    "sdfBike = pd.DataFrame.spatial.from_featureclass(counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T21:20:04.981374Z",
     "start_time": "2022-08-19T21:20:04.946385Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['counter_13', 'counter_16', 'counter_17', 'counter_18',\n",
       "       'counter_19', 'counter_20', 'counter_21', 'counter_25',\n",
       "       'counter_27', 'counter_28', 'counter_29', 'counter_32',\n",
       "       'counter_33', 'counter_37', 'counter_38', 'counter_39',\n",
       "       'counter_40', 'counter_43', 'counter_44', 'counter_30',\n",
       "       'counter_36', 'counter_41', 'counter_42', 'counter_1', 'counter_2',\n",
       "       'counter_3', 'counter_4', 'counter_5', 'counter_6', 'counter_7',\n",
       "       'counter_8', 'counter_9', 'counter_10', 'counter_11', 'counter_12',\n",
       "       'counter_14', 'counter_15', 'counter_22', 'counter_24',\n",
       "       'counter_26', 'counter_34', 'counter_35', 'counter_45',\n",
       "       'counter_46', 'counter_47'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdfBike.counter_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T21:20:10.892905Z",
     "start_time": "2022-08-19T21:20:10.155854Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30390 entries, 0 to 30389\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   OBJECTID           30390 non-null  int64 \n",
      " 1   month_day_year     30390 non-null  object\n",
      " 2   counter_name       30390 non-null  object\n",
      " 3   month_of_year      30390 non-null  int64 \n",
      " 4   season_of_year     30390 non-null  object\n",
      " 5   count_of_bike_ped  30390 non-null  int64 \n",
      " 6   counter_category   30390 non-null  object\n",
      " 7   counter_id         28005 non-null  object\n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Get Bike Ped SQL Table\n",
    "dfBike      = pd.read_sql(\"SELECT * FROM sde_tabular.SDE.bike_ped_counter_tabular\", sdeTabular)\n",
    "dfBike.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T21:20:11.913985Z",
     "start_time": "2022-08-19T21:20:11.244922Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export OG data to workspace\n",
    "dfBike.to_csv(os.path.join(workspace,\"BikePed_Original.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T21:20:14.690229Z",
     "start_time": "2022-08-19T21:20:14.486235Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfBike = pd.read_csv(os.path.join(workspace,\"BikePed_Original.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T21:20:15.802328Z",
     "start_time": "2022-08-19T21:20:15.789354Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1/1/2017'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfBike.month_day_year.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T21:20:17.239456Z",
     "start_time": "2022-08-19T21:20:17.218485Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of counters: 44\n"
     ]
    }
   ],
   "source": [
    "n = len(dfBike.counter_name.unique())\n",
    "\n",
    "print(\"number of counters:\",n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T21:20:41.085598Z",
     "start_time": "2022-08-19T21:20:41.072601Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create dictionary of counter_name:counter_id to be used to assign counter_id to the new data\n",
    "counterDict = sdfBike.set_index('counter_name').to_dict()['counter_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T21:20:42.166684Z",
     "start_time": "2022-08-19T21:20:42.149692Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared-use path - Baldwin Beach : counter_13\n",
      "Shared-use path - Lake Forest : counter_16\n",
      "Shared-use path - Lakeshore Blvd : counter_17\n",
      "Shared-use path - Lakeside Trail : counter_18\n",
      "Shared-use path - Lakeview Commons : counter_19\n",
      "Shared-use path - Linear Park : counter_20\n",
      "Shared-use path - Pinedrops : counter_21\n",
      "Shared-use path - Ski Run Blvd : counter_25\n",
      "Shared-use path - South Tahoe Bikeway : counter_27\n",
      "Shared-use path - Sugar Pine Point  : counter_28\n",
      "Shared-use path - Sunnyside : counter_29\n",
      "Shared-use path - US 50 at Pioneer (Stateline) : counter_32\n",
      "Shared-use path - Zephyr Cove : counter_33\n",
      "Sidewalk - Carnelian Woods Avenue : counter_37\n",
      "Sidewalk - Emerald Bay : counter_38\n",
      "Sidewalk - Kings Beach Recreation Area : counter_39\n",
      "Sidewalk - Kingsbury Grade : counter_40\n",
      "Sidewalk - US 50 at Lake Pkwy : counter_43\n",
      "Sidewalk - Village Blvd : counter_44\n",
      "Shared-use path - Trout Creek bridge : counter_30\n",
      "Sidewalk - Al Tahoe Blvd : counter_36\n",
      "Sidewalk - Pioneer Trail at Meyers : counter_41\n",
      "Sidewalk - Sierra Blvd : counter_42\n",
      "East Shore trail - Hidden Beach : counter_1\n",
      "East Shore trail - Incline Village : counter_2\n",
      "floating pneumatic tube - for editing : counter_3\n",
      "Pneumatic Tube - Carnelian Woods Ave. : counter_4\n",
      "Pneumatic Tube - Fallen Leaf Rd : counter_5\n",
      "Pneumatic tube - Pioneer Trail at Sierra House : counter_6\n",
      "Pneumatic tube - Sierra Blvd : counter_7\n",
      "Pneumatic Tube - US 50 at Al Tahoe Blvd : counter_8\n",
      "Pneumatic Tube - US 50 at Ski Run Blvd : counter_9\n",
      "Pneumatic Tube - US 50 at Tahoe Keys Blvd : counter_10\n",
      "Pnuematic Tube - Kings Beach Recreation Area : counter_11\n",
      "Pnuematic Tube - Village Blvd : counter_12\n",
      "Shared-use path - Camp Richardson : counter_14\n",
      "Shared-use path - Homewood : counter_15\n",
      "Shared-use path - Rabe Meadows : counter_22\n",
      "Shared-use path - Sawmill : counter_24\n",
      "Shared-use path - Ski Run to El Dorado Beach : counter_26\n",
      "Shared-use path: Dollar Creek trail : counter_34\n",
      "Shared-use path: US 50 at Santa Fe Rd : counter_35\n",
      "Shared-use path - Sierra Blvd : counter_45\n",
      "Shared-use path - Al Tahoe : counter_46\n",
      "Shared-use path - LTCC : counter_47\n"
     ]
    }
   ],
   "source": [
    "for key,value in counterDict.items():\n",
    "    print(\"{} : {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Transform Trafx Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T21:21:58.825742Z",
     "start_time": "2022-08-19T21:21:58.670522Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trafxTRTA2021 = pd.read_csv(os.path.join(workspace, \"trafx_daily_trta_2021.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 611 entries, 0 to 610\n",
      "Data columns (total 19 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Day                           611 non-null    object \n",
      " 1   2021 Barker North Bike        0 non-null      float64\n",
      " 2   2021 Barker North Bike.1      144 non-null    float64\n",
      " 3   2021 Before Picnic Rock       136 non-null    float64\n",
      " 4   2021 Brockway Summit East     116 non-null    float64\n",
      " 5   2021 Brockway Summit West     116 non-null    float64\n",
      " 6   2021 Bryan Meadow South Bike  121 non-null    float64\n",
      " 7   2021 Bryan North Bike         126 non-null    float64\n",
      " 8   2021 Echo Lake                129 non-null    float64\n",
      " 9   2021 Genoa Peak Rd            136 non-null    float64\n",
      " 10  2021 Meiss Bike               122 non-null    float64\n",
      " 11  2021 Mt. Rose Gal. Falls      117 non-null    float64\n",
      " 12  2021 Scotts Lake              114 non-null    float64\n",
      " 13  2021 Showers North            117 non-null    float64\n",
      " 14  2021 Showers North Bike       117 non-null    float64\n",
      " 15  2021 South of Tunnel Creek    103 non-null    float64\n",
      " 16  2021 Spooner North            153 non-null    float64\n",
      " 17  2021 Spooner South            253 non-null    float64\n",
      " 18  2021 Star Lake                97 non-null     float64\n",
      "dtypes: float64(18), object(1)\n",
      "memory usage: 90.8+ KB\n"
     ]
    }
   ],
   "source": [
    "trafxTRTA2021.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 25 is out of bounds for axis 0 with size 19",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-550110fb89f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrafxTRTA2021\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrafxTRTA2021\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m26\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m27\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4305\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4307\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4308\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4309\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 25 is out of bounds for axis 0 with size 19"
     ]
    }
   ],
   "source": [
    "trafxTRTA2021.drop(trafxTRTA2021.columns[[25,26,27,28]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get a list of the fields that will be values\n",
    "dfCol = trafxTRTA2021.iloc[:,2:18]\n",
    "trafxList = dfCol.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# transfrom data from wide to long format\n",
    "dfTrafx = pd.melt(trafxTRTA2021.reset_index(), id_vars = ['Day'], value_vars = trafxList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# change the format of field to Date\n",
    "dfTrafx['Day'] = pd.to_datetime(dfTrafx['Day'])\n",
    "# change the format to MM-DD-YYYY\n",
    "dfTrafx['Day'] = dfTrafx['Day'].dt.strftime('%m-%d-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfTrafx = dfTrafx.rename(columns={'Day':'month_day_year', \n",
    "                        'variable':'counter_name', \n",
    "                        'value':'count_of_bike_ped'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_day_year</th>\n",
       "      <th>counter_name</th>\n",
       "      <th>count_of_bike_ped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>06-14-2020</td>\n",
       "      <td>2021 Barker North Bike.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>06-15-2020</td>\n",
       "      <td>2021 Barker North Bike.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06-16-2020</td>\n",
       "      <td>2021 Barker North Bike.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06-17-2020</td>\n",
       "      <td>2021 Barker North Bike.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>06-18-2020</td>\n",
       "      <td>2021 Barker North Bike.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9771</th>\n",
       "      <td>02-10-2022</td>\n",
       "      <td>2021 Spooner South</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9772</th>\n",
       "      <td>02-11-2022</td>\n",
       "      <td>2021 Spooner South</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9773</th>\n",
       "      <td>02-12-2022</td>\n",
       "      <td>2021 Spooner South</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9774</th>\n",
       "      <td>02-13-2022</td>\n",
       "      <td>2021 Spooner South</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9775</th>\n",
       "      <td>02-14-2022</td>\n",
       "      <td>2021 Spooner South</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9776 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     month_day_year              counter_name  count_of_bike_ped\n",
       "0        06-14-2020  2021 Barker North Bike.1                NaN\n",
       "1        06-15-2020  2021 Barker North Bike.1                NaN\n",
       "2        06-16-2020  2021 Barker North Bike.1                NaN\n",
       "3        06-17-2020  2021 Barker North Bike.1                NaN\n",
       "4        06-18-2020  2021 Barker North Bike.1                NaN\n",
       "...             ...                       ...                ...\n",
       "9771     02-10-2022        2021 Spooner South                0.0\n",
       "9772     02-11-2022        2021 Spooner South                0.0\n",
       "9773     02-12-2022        2021 Spooner South                0.0\n",
       "9774     02-13-2022        2021 Spooner South              114.0\n",
       "9775     02-14-2022        2021 Spooner South                NaN\n",
       "\n",
       "[9776 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrafx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T21:22:25.059884Z",
     "start_time": "2022-08-19T21:22:24.278816Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfTrafx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-8b86c3db4c58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdfTrafx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcounter_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfTrafx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcounter_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2021 \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdfTrafx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdfTrafx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"counter_name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"Barker North Bike.1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'counter_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Barker North Bike\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dfTrafx' is not defined"
     ]
    }
   ],
   "source": [
    "dfTrafx.counter_name = dfTrafx.counter_name.str.lstrip(\"2021 \")\n",
    "dfTrafx.loc[dfTrafx[\"counter_name\"]==\"Barker North Bike.1\", 'counter_name']=\"Barker North Bike\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_day_year</th>\n",
       "      <th>counter_name</th>\n",
       "      <th>count_of_bike_ped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>06-14-2020</td>\n",
       "      <td>Barker North Bike</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>06-15-2020</td>\n",
       "      <td>Barker North Bike</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06-16-2020</td>\n",
       "      <td>Barker North Bike</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06-17-2020</td>\n",
       "      <td>Barker North Bike</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>06-18-2020</td>\n",
       "      <td>Barker North Bike</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9771</th>\n",
       "      <td>02-10-2022</td>\n",
       "      <td>Spooner South</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9772</th>\n",
       "      <td>02-11-2022</td>\n",
       "      <td>Spooner South</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9773</th>\n",
       "      <td>02-12-2022</td>\n",
       "      <td>Spooner South</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9774</th>\n",
       "      <td>02-13-2022</td>\n",
       "      <td>Spooner South</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9775</th>\n",
       "      <td>02-14-2022</td>\n",
       "      <td>Spooner South</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9776 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     month_day_year       counter_name  count_of_bike_ped\n",
       "0        06-14-2020  Barker North Bike                NaN\n",
       "1        06-15-2020  Barker North Bike                NaN\n",
       "2        06-16-2020  Barker North Bike                NaN\n",
       "3        06-17-2020  Barker North Bike                NaN\n",
       "4        06-18-2020  Barker North Bike                NaN\n",
       "...             ...                ...                ...\n",
       "9771     02-10-2022      Spooner South                0.0\n",
       "9772     02-11-2022      Spooner South                0.0\n",
       "9773     02-12-2022      Spooner South                0.0\n",
       "9774     02-13-2022      Spooner South              114.0\n",
       "9775     02-14-2022      Spooner South                NaN\n",
       "\n",
       "[9776 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrafx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:11:07.171865Z",
     "start_time": "2022-03-28T21:11:07.075826Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trafx = pd.read_csv(os.path.join(workspace, \"trafx_daily.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:11:09.223049Z",
     "start_time": "2022-03-28T21:11:09.165013Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1966 entries, 0 to 1965\n",
      "Data columns (total 29 columns):\n",
      " #   Column                                                Non-Null Count  Dtype  \n",
      "---  ------                                                --------------  -----  \n",
      " 0   Day                                                   1966 non-null   object \n",
      " 1   Shared-use path - Baldwin Beach                       781 non-null    float64\n",
      " 2   Shared-use path - Lake Forest                         1131 non-null   float64\n",
      " 3   Shared-use path - Lakeshore Blvd                      1030 non-null   float64\n",
      " 4   Shared-use path - Lakeside Trail                      427 non-null    float64\n",
      " 5   Shared-use path - Lakeview Commons                    1189 non-null   float64\n",
      " 6   Shared-use path - Linear Park                         982 non-null    float64\n",
      " 7   Shared-use path - Pinedrops                           740 non-null    float64\n",
      " 8   Shared-use path - RoundHill                           313 non-null    float64\n",
      " 9   Shared-use path - Ski Run Blvd                        816 non-null    float64\n",
      " 10  Shared-use path - South Tahoe Bikeway                 508 non-null    float64\n",
      " 11  Shared-use path - Sugar Pine Point                    494 non-null    float64\n",
      " 12  Shared-use path - Sunnyside                           427 non-null    float64\n",
      " 13  Shared-use path - Trout Creek bridge                  351 non-null    float64\n",
      " 14  Shared-use path - US 50 at Pioneer (Stateline)        797 non-null    float64\n",
      " 15  Shared-use path - Zephyr Cove                         563 non-null    float64\n",
      " 16  Sidewalk - Al Tahoe Blvd                              500 non-null    float64\n",
      " 17  Sidewalk - Carnelian Woods Avenue                     950 non-null    float64\n",
      " 18  Sidewalk - Emerald Bay                                561 non-null    float64\n",
      " 19  Sidewalk - Kings Beach Recreation Area                1058 non-null   float64\n",
      " 20  Sidewalk - Kingsbury Grade                            652 non-null    float64\n",
      " 21  Sidewalk - Pioneer Trail at Meyers                    273 non-null    float64\n",
      " 22  Sidewalk - Sierra Blvd                                383 non-null    float64\n",
      " 23  Sidewalk - US 50 at Lake Pkwy                         933 non-null    float64\n",
      " 24  Sidewalk - Village Blvd                               929 non-null    float64\n",
      " 25  SPOT COUNT - Kahle Drive at Hwy 50                    173 non-null    float64\n",
      " 26  SPOT COUNT - Shared-use path - National Ave.          132 non-null    float64\n",
      " 27  SPOT COUNT - SR28 / SR267 (NW intersection sidewalk)  21 non-null     float64\n",
      " 28  SPOT COUNT - SR28 / SR267 (SE intersection sidewalk)  48 non-null     float64\n",
      "dtypes: float64(28), object(1)\n",
      "memory usage: 445.5+ KB\n"
     ]
    }
   ],
   "source": [
    "trafx.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Transform Trafx Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T21:22:42.239416Z",
     "start_time": "2022-08-19T21:22:42.205433Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trafx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-4a16a37f96dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrafx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrafx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m26\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m27\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'trafx' is not defined"
     ]
    }
   ],
   "source": [
    "trafx.drop(trafx.columns[[25,26,27,28]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:11:19.895988Z",
     "start_time": "2022-03-28T21:11:19.884972Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get a list of the fields that will be values\n",
    "dfCol = trafx.iloc[:,1:24]\n",
    "trafxList = dfCol.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:11:23.904328Z",
     "start_time": "2022-03-28T21:11:23.854326Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# transfrom data from wide to long format\n",
    "dfTrafx = pd.melt(trafx.reset_index(), id_vars = ['Day'], value_vars = trafxList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:11:27.454663Z",
     "start_time": "2022-03-28T21:11:26.276535Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# change the format of field to Date\n",
    "dfTrafx['Day'] = pd.to_datetime(dfTrafx['Day'])\n",
    "# change the format to MM-DD-YYYY\n",
    "dfTrafx['Day'] = dfTrafx['Day'].dt.strftime('%m-%d-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:11:28.705758Z",
     "start_time": "2022-03-28T21:11:28.690758Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfTrafx = dfTrafx.rename(columns={'Day':'month_day_year', \n",
    "                        'variable':'counter_name', \n",
    "                        'value':'count_of_bike_ped'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Transform EcoVision Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:11:31.925041Z",
     "start_time": "2022-03-28T21:11:31.852034Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eco = pd.read_csv(os.path.join(workspace, \"ecovision_daily.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:11:32.960135Z",
     "start_time": "2022-03-28T21:11:32.918131Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2002 entries, 0 to 2001\n",
      "Data columns (total 26 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Time                                            2002 non-null   object \n",
      " 1   East Shore trail - Hidden Beach                 980 non-null    float64\n",
      " 2   East Shore trail - Incline Village              916 non-null    float64\n",
      " 3   floating pneumatic tube - for editing           2 non-null      float64\n",
      " 4   Pneumatic Tube - Carnelian Woods Ave.           14 non-null     float64\n",
      " 5   Pneumatic Tube - Fallen Leaf Rd                 8 non-null      float64\n",
      " 6   Pneumatic tube - Pioneer Trail at Sierra House  17 non-null     float64\n",
      " 7   Pneumatic tube - Sierra Blvd                    4 non-null      float64\n",
      " 8   Pneumatic Tube - US 50 at Al Tahoe Blvd         13 non-null     float64\n",
      " 9   Pneumatic Tube - US 50 at Ski Run Blvd          12 non-null     float64\n",
      " 10  Pneumatic Tube - US 50 at Tahoe Keys Blvd       22 non-null     float64\n",
      " 11  Pnuematic Tube - Kings Beach Recreation Area    16 non-null     float64\n",
      " 12  Pnuematic Tube - Village Blvd                   16 non-null     float64\n",
      " 13  Shared-use path - Al Tahoe                      136 non-null    float64\n",
      " 14  Shared-use path - Camp Richardson               1704 non-null   float64\n",
      " 15  Shared-use path - Homewood                      1468 non-null   float64\n",
      " 16  Shared-use path - LTCC                          136 non-null    float64\n",
      " 17  Shared-use path - Rabe Meadows                  1818 non-null   float64\n",
      " 18  Shared-use path - Sawmill                       1913 non-null   float64\n",
      " 19  Shared-use path - Sierra Blvd                   172 non-null    float64\n",
      " 20  Shared-use path - Ski Run to El Dorado Beach    1402 non-null   float64\n",
      " 21  Shared-use path - Trout Creek bridge            1074 non-null   float64\n",
      " 22  Shared-use path - Truckee River Trail           1108 non-null   float64\n",
      " 23  Shared-use path: Dollar Creek trail             1189 non-null   float64\n",
      " 24  Shared-use path: US 50 at Santa Fe Rd           1836 non-null   float64\n",
      " 25  Unnamed: 25                                     0 non-null      float64\n",
      "dtypes: float64(25), object(1)\n",
      "memory usage: 406.8+ KB\n"
     ]
    }
   ],
   "source": [
    "eco.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:11:50.020664Z",
     "start_time": "2022-03-28T21:11:50.006660Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# drop columns\n",
    "eco.drop(eco.columns[[25]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:11:52.987926Z",
     "start_time": "2022-03-28T21:11:52.977927Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get a list of the fields that will be values\n",
    "dfCol = eco.iloc[:,1:24]\n",
    "ecoList = dfCol.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:15:52.339311Z",
     "start_time": "2022-03-28T21:15:52.302303Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# transfrom data from wide to long format\n",
    "dfEco = pd.melt(eco.reset_index(), id_vars = ['Time'], value_vars = ecoList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:15:55.763625Z",
     "start_time": "2022-03-28T21:15:53.790440Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# change the format of field to Date\n",
    "dfEco['Time'] = pd.to_datetime(dfEco['Time'])\n",
    "# change the format to MM-DD-YYYY\n",
    "dfEco['Time'] = dfEco['Time'].dt.strftime('%m-%d-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:15:55.784619Z",
     "start_time": "2022-03-28T21:15:55.770615Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfEco = dfEco.rename(columns={'Time':'month_day_year', \n",
    "                        'variable':'counter_name', \n",
    "                        'value':'count_of_bike_ped'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Merge and Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:15:57.036740Z",
     "start_time": "2022-03-28T21:15:57.013724Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# combine data frames\n",
    "df = pd.concat([dfEco, dfTrafx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = dfTrafx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:15:57.944810Z",
     "start_time": "2022-03-28T21:15:57.916806Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# drop NaN values\n",
    "df.dropna(subset = ['count_of_bike_ped'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:16:00.028996Z",
     "start_time": "2022-03-28T21:15:59.689965Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create the month field and set it's type\n",
    "df.insert(loc=3,column='month_of_year', value = pd.to_datetime(df['month_day_year']).dt.month)\n",
    "# calculate the values\n",
    "df['month_of_year'] = pd.to_datetime(df['month_day_year']).dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:16:03.944337Z",
     "start_time": "2022-03-28T21:16:03.909340Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# set the season field values\n",
    "df.loc[df['month_of_year'].isin([12,1,2,3]),  'season_of_year'] = 'Winter' \n",
    "df.loc[df['month_of_year'].isin([6,7,8,9]),   'season_of_year'] = 'Summer' \n",
    "df.loc[df['month_of_year'].isin([4,5,10,11]), 'season_of_year'] = 'Off-Season' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['counter_category']= 'TRTA Trafx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:16:09.469841Z",
     "start_time": "2022-03-28T21:16:09.434835Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# set the counter category values by comparing the list of station names created earliar\n",
    "df.loc[df['counter_name'].isin(ecoList), 'counter_category']= 'ecovision'\n",
    "df.loc[df['counter_name'].isin(trafxList), 'counter_category']= 'trafx'\n",
    "df.loc[df['counter_name'].isin(ecoList) & df['counter_name'].isin(trafxList), 'counter_category']= 'ecovision & trafx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* https://stackoverflow.com/questions/51881503/assign-a-dictionary-value-to-a-dataframe-column-based-on-dictionary-key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:16:16.093458Z",
     "start_time": "2022-03-28T21:16:15.924412Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create dictionary of counter_name:counter_id to be used to assign counter_id to the new data\n",
    "counterDict = dfBike.set_index('counter_name').to_dict()['counter_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:16:17.300553Z",
     "start_time": "2022-03-28T21:16:17.260537Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# set counter id based on counter_id in DF Bike using dictionary\n",
    "df[\"counter_id\"] = df[\"counter_name\"].apply(lambda x: counterDict.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:16:18.788676Z",
     "start_time": "2022-03-28T21:16:18.770668Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.counter_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:15:28.117145Z",
     "start_time": "2022-03-28T21:15:28.059139Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# new\n",
    "df.loc[df[\"counter_name\"]==\"Shared-use path - Sierra Blvd\", 'counter_id']=\"counter_44\"\n",
    "# new\n",
    "df.loc[df[\"counter_name\"]=='Shared-use path - Al Tahoe',     'counter_id']=\"counter_45\"\n",
    "# new\n",
    "df.loc[df[\"counter_name\"]=='Shared-use path - LTCC',         'counter_id']=\"counter_46\"\n",
    "# existing\n",
    "df.loc[df[\"counter_name\"]=='Shared-use path - Sawmill',      'counter_id']=\"counter_24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_day_year</th>\n",
       "      <th>counter_name</th>\n",
       "      <th>count_of_bike_ped</th>\n",
       "      <th>month_of_year</th>\n",
       "      <th>season_of_year</th>\n",
       "      <th>counter_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>07-01-2021</td>\n",
       "      <td>Barker North Bike</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>Summer</td>\n",
       "      <td>TRTA Trafx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>07-02-2021</td>\n",
       "      <td>Barker North Bike</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "      <td>Summer</td>\n",
       "      <td>TRTA Trafx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>07-03-2021</td>\n",
       "      <td>Barker North Bike</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7</td>\n",
       "      <td>Summer</td>\n",
       "      <td>TRTA Trafx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>07-04-2021</td>\n",
       "      <td>Barker North Bike</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7</td>\n",
       "      <td>Summer</td>\n",
       "      <td>TRTA Trafx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>07-05-2021</td>\n",
       "      <td>Barker North Bike</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>Summer</td>\n",
       "      <td>TRTA Trafx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9770</th>\n",
       "      <td>02-09-2022</td>\n",
       "      <td>Spooner South</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Winter</td>\n",
       "      <td>TRTA Trafx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9771</th>\n",
       "      <td>02-10-2022</td>\n",
       "      <td>Spooner South</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Winter</td>\n",
       "      <td>TRTA Trafx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9772</th>\n",
       "      <td>02-11-2022</td>\n",
       "      <td>Spooner South</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Winter</td>\n",
       "      <td>TRTA Trafx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9773</th>\n",
       "      <td>02-12-2022</td>\n",
       "      <td>Spooner South</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Winter</td>\n",
       "      <td>TRTA Trafx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9774</th>\n",
       "      <td>02-13-2022</td>\n",
       "      <td>Spooner South</td>\n",
       "      <td>114.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Winter</td>\n",
       "      <td>TRTA Trafx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2120 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     month_day_year       counter_name  count_of_bike_ped  month_of_year  \\\n",
       "382      07-01-2021  Barker North Bike                1.0              7   \n",
       "383      07-02-2021  Barker North Bike                5.0              7   \n",
       "384      07-03-2021  Barker North Bike                2.0              7   \n",
       "385      07-04-2021  Barker North Bike                4.0              7   \n",
       "386      07-05-2021  Barker North Bike                3.0              7   \n",
       "...             ...                ...                ...            ...   \n",
       "9770     02-09-2022      Spooner South                0.0              2   \n",
       "9771     02-10-2022      Spooner South                0.0              2   \n",
       "9772     02-11-2022      Spooner South                0.0              2   \n",
       "9773     02-12-2022      Spooner South                0.0              2   \n",
       "9774     02-13-2022      Spooner South              114.0              2   \n",
       "\n",
       "     season_of_year counter_category  \n",
       "382          Summer       TRTA Trafx  \n",
       "383          Summer       TRTA Trafx  \n",
       "384          Summer       TRTA Trafx  \n",
       "385          Summer       TRTA Trafx  \n",
       "386          Summer       TRTA Trafx  \n",
       "...             ...              ...  \n",
       "9770         Winter       TRTA Trafx  \n",
       "9771         Winter       TRTA Trafx  \n",
       "9772         Winter       TRTA Trafx  \n",
       "9773         Winter       TRTA Trafx  \n",
       "9774         Winter       TRTA Trafx  \n",
       "\n",
       "[2120 rows x 6 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save to CSV\n",
    "df.to_csv(os.path.join(workspace,\"BikePed_TRTA2021.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:16:41.663725Z",
     "start_time": "2022-03-28T21:16:41.639710Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['OBJECTID'] = df.reset_index().index\n",
    "df.set_index('OBJECTID',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:16:43.360865Z",
     "start_time": "2022-03-28T21:16:43.348865Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-54-2ab3c0d36b32>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-54-2ab3c0d36b32>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    reorder fields to match the sql table schema\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# get a list of the existing fields\n",
    "bikeList = dfBike.columns.tolist()\n",
    "del bikeList[0]\n",
    "#reorder fields to match the sql table schema\n",
    "dfNew = df[bikeList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T21:13:37.520265Z",
     "start_time": "2022-03-28T21:13:37.495269Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfNew' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-1d4c4a0e8e24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdfNew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dfNew' is not defined"
     ]
    }
   ],
   "source": [
    "dfNew.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save to CSV\n",
    "dfNew.to_csv(os.path.join(workspace,\"BikePed_New.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# update sql table\n",
    "dfNew.to_sql(outTable, engine, if_exists='replace', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>month_day_year</th>\n",
       "      <th>counter_name</th>\n",
       "      <th>month_of_year</th>\n",
       "      <th>season_of_year</th>\n",
       "      <th>count_of_bike_ped</th>\n",
       "      <th>counter_category</th>\n",
       "      <th>counter_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1003</td>\n",
       "      <td>0</td>\n",
       "      <td>07-01-2019</td>\n",
       "      <td>East Shore trail - Hidden Beach</td>\n",
       "      <td>7</td>\n",
       "      <td>Summer</td>\n",
       "      <td>1874.0</td>\n",
       "      <td>ecovision</td>\n",
       "      <td>counter_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1004</td>\n",
       "      <td>1</td>\n",
       "      <td>07-02-2019</td>\n",
       "      <td>East Shore trail - Hidden Beach</td>\n",
       "      <td>7</td>\n",
       "      <td>Summer</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>ecovision</td>\n",
       "      <td>counter_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>2</td>\n",
       "      <td>07-03-2019</td>\n",
       "      <td>East Shore trail - Hidden Beach</td>\n",
       "      <td>7</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2143.0</td>\n",
       "      <td>ecovision</td>\n",
       "      <td>counter_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1006</td>\n",
       "      <td>3</td>\n",
       "      <td>07-04-2019</td>\n",
       "      <td>East Shore trail - Hidden Beach</td>\n",
       "      <td>7</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2534.0</td>\n",
       "      <td>ecovision</td>\n",
       "      <td>counter_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1007</td>\n",
       "      <td>4</td>\n",
       "      <td>07-05-2019</td>\n",
       "      <td>East Shore trail - Hidden Beach</td>\n",
       "      <td>7</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2944.0</td>\n",
       "      <td>ecovision</td>\n",
       "      <td>counter_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29289</th>\n",
       "      <td>59413</td>\n",
       "      <td>29289</td>\n",
       "      <td>01-05-2022</td>\n",
       "      <td>Sidewalk - US 50 at Lake Pkwy</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>85.0</td>\n",
       "      <td>trafx</td>\n",
       "      <td>counter_43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29290</th>\n",
       "      <td>59414</td>\n",
       "      <td>29290</td>\n",
       "      <td>01-06-2022</td>\n",
       "      <td>Sidewalk - US 50 at Lake Pkwy</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>53.0</td>\n",
       "      <td>trafx</td>\n",
       "      <td>counter_43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29291</th>\n",
       "      <td>59415</td>\n",
       "      <td>29291</td>\n",
       "      <td>01-07-2022</td>\n",
       "      <td>Sidewalk - US 50 at Lake Pkwy</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>49.0</td>\n",
       "      <td>trafx</td>\n",
       "      <td>counter_43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29292</th>\n",
       "      <td>59416</td>\n",
       "      <td>29292</td>\n",
       "      <td>01-08-2022</td>\n",
       "      <td>Sidewalk - US 50 at Lake Pkwy</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>77.0</td>\n",
       "      <td>trafx</td>\n",
       "      <td>counter_43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29293</th>\n",
       "      <td>59417</td>\n",
       "      <td>29293</td>\n",
       "      <td>01-09-2022</td>\n",
       "      <td>Sidewalk - US 50 at Lake Pkwy</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>76.0</td>\n",
       "      <td>trafx</td>\n",
       "      <td>counter_43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29294 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  OBJECTID month_day_year                     counter_name  \\\n",
       "0       1003         0     07-01-2019  East Shore trail - Hidden Beach   \n",
       "1       1004         1     07-02-2019  East Shore trail - Hidden Beach   \n",
       "2       1005         2     07-03-2019  East Shore trail - Hidden Beach   \n",
       "3       1006         3     07-04-2019  East Shore trail - Hidden Beach   \n",
       "4       1007         4     07-05-2019  East Shore trail - Hidden Beach   \n",
       "...      ...       ...            ...                              ...   \n",
       "29289  59413     29289     01-05-2022    Sidewalk - US 50 at Lake Pkwy   \n",
       "29290  59414     29290     01-06-2022    Sidewalk - US 50 at Lake Pkwy   \n",
       "29291  59415     29291     01-07-2022    Sidewalk - US 50 at Lake Pkwy   \n",
       "29292  59416     29292     01-08-2022    Sidewalk - US 50 at Lake Pkwy   \n",
       "29293  59417     29293     01-09-2022    Sidewalk - US 50 at Lake Pkwy   \n",
       "\n",
       "       month_of_year season_of_year  count_of_bike_ped counter_category  \\\n",
       "0                  7         Summer             1874.0        ecovision   \n",
       "1                  7         Summer             1970.0        ecovision   \n",
       "2                  7         Summer             2143.0        ecovision   \n",
       "3                  7         Summer             2534.0        ecovision   \n",
       "4                  7         Summer             2944.0        ecovision   \n",
       "...              ...            ...                ...              ...   \n",
       "29289              1         Winter               85.0            trafx   \n",
       "29290              1         Winter               53.0            trafx   \n",
       "29291              1         Winter               49.0            trafx   \n",
       "29292              1         Winter               77.0            trafx   \n",
       "29293              1         Winter               76.0            trafx   \n",
       "\n",
       "       counter_id  \n",
       "0       counter_1  \n",
       "1       counter_1  \n",
       "2       counter_1  \n",
       "3       counter_1  \n",
       "4       counter_1  \n",
       "...           ...  \n",
       "29289  counter_43  \n",
       "29290  counter_43  \n",
       "29291  counter_43  \n",
       "29292  counter_43  \n",
       "29293  counter_43  \n",
       "\n",
       "[29294 rows x 9 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNew.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfNoID = df.loc[df['counter_id'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Shared- use path - Sierra Blvd', 'Shared-use path - Al Tahoe',\n",
       "       'Shared-use path - LTCC', 'Shared-use path - Sawmill',\n",
       "       'Shared-use path - Truckee River Trail'], dtype=object)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNoID.counter_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# spatial data frame of points\n",
    "sdfBike = os.path\n",
    "\n",
    "F:\\GIS\\DB_CONNECT\\Vector.sde\\sde.SDE.Transportation\\sde.SDE.bike_ped_counter_spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Production Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ParcelTables_to_ParcelFeatures.py\n",
    "Created: March 11th, 2022\n",
    "Last Updated: March 11th, 2022\n",
    "Mason Bindl, Tahoe Regional Planning Agency\n",
    "\n",
    "This python script was developed to transform data from Trafx and Ecovision\n",
    "\n",
    "This script uses Python 3.x and was designed to be used with \n",
    "the default ArcGIS Pro python enivorment \"arcgispro-py3-clone\", with\n",
    "no need for installing new libraries.\n",
    "\"\"\"\n",
    "#--------------------------------------------------------------------------------------------------------#\n",
    "# import packages and modules\n",
    "import arcpy\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "from arcgis.features import FeatureSet, GeoAccessor, GeoSeriesAccessor\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# set overwrite to true\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "# in memory output file path\n",
    "wk_memory = \"memory\" + \"\\\\\"\n",
    "\n",
    "# set workspace and sde connections \n",
    "working_folder = \"C:\\GIS\"\n",
    "workspace = \"//trpa-fs01/gis/PROJECTS/ResearchAnalysis/Monitoring/Data/BikePed/\"\n",
    "arcpy.env.workspace = \"F:/gis/PROJECTS/ResearchAnalysis/Monitoring/Data/BikePed/\"\n",
    "\n",
    "# make sql database connection to BMP with pyodbc\n",
    "sdeTabular = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=sql12;DATABASE=sde_tabular;UID=sde;PWD=staff')\n",
    "\n",
    "# Final table to update\n",
    "outTable = \"sde_tabular.SDE.bike_ped_counter_tabular\"\n",
    "\n",
    "# start a timer for the entire script run\n",
    "FIRSTstartTimer = datetime.now()\n",
    "\n",
    "# Create and open log file.\n",
    "complete_txt_path = os.path.join(working_folder, \"BikePed_ETL_Log.txt\")\n",
    "print (complete_txt_path)\n",
    "log = open(complete_txt_path, \"w\")\n",
    "\n",
    "# Write results to txt file\n",
    "log.write(\"Log: \" + str(FIRSTstartTimer) + \"\\n\")\n",
    "log.write(\"\\n\")\n",
    "log.write(\"Begin process:\\n\")\n",
    "log.write(\"Process started at: \" + str(FIRSTstartTimer) + \"\\n\")\n",
    "log.write(\"\\n\")\n",
    "\n",
    "#---------------------------------------------------------------------------------------#\n",
    "## GET DATA\n",
    "#---------------------------------------------------------------------------------------#\n",
    "# start timer for the get data requests\n",
    "startTimer = datetime.now()\n",
    "\n",
    "# Get Bike Ped SQL Table\n",
    "dfBikeOG = pd.read_sql(\"SELECT * FROM sde_tabular.SDE.bike_ped_counter_tabular\", sdeTabular)\n",
    "# export OG data to workspace\n",
    "dfBikeOG.to_csv(os.path.join(workspace,\"BikePed_Original.csv\"))\n",
    "\n",
    "# read in CSV of OG data\n",
    "dfBike = pd.read_csv(os.path.join(workspace,\"BikePed_Original.csv\"))\n",
    "\n",
    "#read in Trafx data download\n",
    "trafx = pd.read_csv(os.path.join(workspace, \"trafx_daily.csv\"))\n",
    "\n",
    "# read in Ecovision data download\n",
    "eco = pd.read_csv(os.path.join(workspace, \"ecovision_daily.csv\"))\n",
    "       \n",
    "# report how long it took to get the data\n",
    "endTimer = datetime.now() - startTimer\n",
    "print(\"\\nTime it took to get the data: {}\".format(endTimer))   \n",
    "log.write(\"\\nTime it took to get the data: {}\".format(endTimer)) \n",
    "#---------------------------------------------------------------------------------------#\n",
    "## Define Functions ##\n",
    "#---------------------------------------------------------------------------------------#\n",
    "##--------------------------------------------------------------------------------------------------------#\n",
    "## SEND EMAIL WITH LOG FILE ##\n",
    "##--------------------------------------------------------------------------------------------------------#\n",
    "# path to text file\n",
    "fileToSend = complete_txt_path\n",
    "\n",
    "# email parameters\n",
    "subject = \"Bike/Ped ETL Log File\"\n",
    "sender_email = \"infosys@trpa.org\"\n",
    "# password = ''\n",
    "receiver_email = \"gis@trpa.gov\"\n",
    "\n",
    "# send mail function\n",
    "def send_mail(body):\n",
    "    msg = MIMEMultipart()\n",
    "    msg['Subject'] = subject\n",
    "    msg['From'] = sender_email\n",
    "    msg['To'] = receiver_email\n",
    "\n",
    "    msgText = MIMEText('%s<br><br>Cheers,<br>GIS Team' % (body), 'html')\n",
    "    msg.attach(msgText)\n",
    "\n",
    "    attachment = MIMEText(open(fileToSend).read())\n",
    "    attachment.add_header(\"Content-Disposition\", \"attachment\", filename = os.path.basename(fileToSend))\n",
    "    msg.attach(attachment)\n",
    "\n",
    "    try:\n",
    "        with smtplib.SMTP(\"mail.smtp2go.com\", 25) as smtpObj:\n",
    "            smtpObj.ehlo()\n",
    "            smtpObj.starttls()\n",
    "#             smtpObj.login(sender_email, password)\n",
    "            smtpObj.sendmail(sender_email, receiver_email, msg.as_string())\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# replaces features in outfc with exact same schema\n",
    "def updateSDE(inputfc,outfc, fieldnames):\n",
    "        # disconnect all users\n",
    "    print(\"\\nDisconnecting all users...\")\n",
    "    arcpy.DisconnectUser(sde, \"ALL\")\n",
    "\n",
    "    # deletes all rows from the SDE feature class\n",
    "    arcpy.TruncateTable_management(outfc)\n",
    "    print (\"\\nDeleted all records in: {}\\n\".format(outfc))\n",
    "\n",
    "    # insert rows from Temporary feature class to SDE feature class\n",
    "    with arcpy.da.InsertCursor(outfc, fieldnames) as oCursor:\n",
    "        count = 0\n",
    "        with arcpy.da.SearchCursor(inputfc, fieldnames) as iCursor:\n",
    "            for row in iCursor:\n",
    "                oCursor.insertRow(row)\n",
    "                count += 1\n",
    "                if count % 1000 == 0:\n",
    "                    print(\"Inserting record {0} into SDE table\".format(count))\n",
    "\n",
    "    # disconnect all users\n",
    "    print(\"\\nDisconnecting all users...\")\n",
    "    arcpy.DisconnectUser(sde, \"ALL\")\n",
    "    # confirm feature class was created\n",
    "    print(\"\\nUpdated \" + outfc)\n",
    "\n",
    "try:\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    ## CREATE STAGING TABLE ##\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    # start timer for the get data requests\n",
    "    startTimer = datetime.now()\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    \n",
    "    # drop fields\n",
    "    trafx.drop(trafx.columns[[25,26,27,28]], axis=1, inplace=True)\n",
    "    \n",
    "    # get a list of the fields that will be values\n",
    "    dfCol = trafx.iloc[:,1:24]\n",
    "    trafxList = dfCol.columns.to_list()\n",
    "    \n",
    "    # transfrom data from wide to long format\n",
    "    dfTrafx = pd.melt(trafx.reset_index(), id_vars = ['Day'], value_vars = trafxList)\n",
    "\n",
    "    # change the format of field to Date\n",
    "    dfTrafx['Day'] = pd.to_datetime(dfTrafx['Day'])\n",
    "    # change the format to MM-DD-YYYY\n",
    "    dfTrafx['Day'] = dfTrafx['Day'].dt.strftime('%m-%d-%Y')\n",
    "    # rename fields\n",
    "    dfTrafx = dfTrafx.rename(columns={'Day':'month_day_year', \n",
    "                        'variable':'counter_name', \n",
    "                        'value':'count_of_bike_ped'})\n",
    "\n",
    "    # drop columns\n",
    "    eco.drop(eco.columns[[25, 26]], axis=1, inplace=True)\n",
    "    # get a list of the fields that will be values\n",
    "    dfCol = eco.iloc[:,1:24]\n",
    "    ecoList = dfCol.columns.to_list()\n",
    "    # transfrom data from wide to long format\n",
    "    dfEco = pd.melt(eco.reset_index(), id_vars = ['Time'], value_vars = ecoList)\n",
    "    # change the format of field to Date\n",
    "    dfEco['Time'] = pd.to_datetime(dfEco['Time'])\n",
    "    # change the format to MM-DD-YYYY\n",
    "    dfEco['Time'] = dfEco['Time'].dt.strftime('%m-%d-%Y')\n",
    "    # rename fields\n",
    "    dfEco = dfEco.rename(columns={'Time':'month_day_year', \n",
    "                        'variable':'counter_name', \n",
    "                        'value':'count_of_bike_ped'})\n",
    "\n",
    "    # combine data frames\n",
    "    df = pd.concat([dfEco, dfTrafx])\n",
    "    # drop NaN values\n",
    "    df.dropna(subset = ['count_of_bike_ped'], inplace=True)\n",
    "    # create the month field and set it's type\n",
    "    df.insert(loc=3,column='month_of_year', value = pd.to_datetime(df['month_day_year']).dt.month)\n",
    "    # calculate the values\n",
    "    df['month_of_year'] = pd.to_datetime(df['month_day_year']).dt.month\n",
    "\n",
    "    # set the season field values\n",
    "    df.loc[df['month_of_year'].isin([12,1,2,3]),  'season_of_year'] = 'Winter' \n",
    "    df.loc[df['month_of_year'].isin([6,7,8,9]),   'season_of_year'] = 'Summer' \n",
    "    df.loc[df['month_of_year'].isin([4,5,10,11]), 'season_of_year'] = 'Off-Season' \n",
    "\n",
    "    # set the counter category values by comparing the list of station names created earliar\n",
    "    df.loc[df['counter_name'].isin(ecoList), 'counter_category']= 'ecovision'\n",
    "    df.loc[df['counter_name'].isin(trafxList), 'counter_category']= 'trafx'\n",
    "    df.loc[df['counter_name'].isin(ecoList) & df['counter_name'].isin(trafxList), 'counter_category']= 'ecovision & trafx'\n",
    "    \n",
    "    # create dictionary of counter_name:counter_id to be used to assign counter_id to the new data\n",
    "    counterDict = dfBike.set_index('counter_name').to_dict()['counter_id']\n",
    "    # set counter id based on counter_id in DF Bike using dictionary\n",
    "    df[\"counter_id\"] = df[\"counter_name\"].apply(lambda x: counterDict.get(x))\n",
    "    \n",
    "    # set OBJECTID field to be the index\n",
    "    df['OBJECTID'] = df.reset_index().index\n",
    "    df.set_index('OBJECTID',inplace=True)\n",
    "\n",
    "    # save to CSV\n",
    "    df.to_csv(os.path.join(workspace,\"BikePed_New.csv\"))\n",
    "\n",
    "    #sde connection to disconnect users\n",
    "    sde = \"C:\\\\GIS\\\\DB_CONNECT\\\\Tabular.sde\"\n",
    "\n",
    "    # Change this to the path of your input feature class\n",
    "    inputfc = os.path.join(workspace,\"BikePed_New.csv\")\n",
    "\n",
    "    # Change this to the path of your output FC\n",
    "    outfc = os.path.join(sde,\"sde_tabular.SDE.bike_ped_counter_tabular\")\n",
    "\n",
    "    # Get field objects from source FC\n",
    "    dsc = arcpy.Describe(inputfc)\n",
    "    fields = dsc.fields\n",
    "\n",
    "    # # List all field names except the OID field\n",
    "    fieldnames = [field.name for field in fields if field.name != \"Field1\"]\n",
    "\n",
    "    updateSDE(inputfc, outfc, fieldnames)\n",
    "\n",
    "    # report how long it took to run the script\n",
    "    endTimer = datetime.now() - startTimer\n",
    "    print (\"\\nTime it took to run this script: {}\".format(endTimer))\n",
    " \n",
    "    # report how long it took to run the script\n",
    "    FINALendTimer = datetime.now() - FIRSTstartTimer\n",
    "    print (\"\\nTime it took to run this script: {}\".format(FINALendTimer))\n",
    "\n",
    "    log.write(\"\\nTime it took to run this script: {}\".format(FINALendTimer))\n",
    "    log.close()\n",
    "    \n",
    "    header = \"SUCCESS - The Bike/Ped data was updated.\"\n",
    "    # send email with header based on try/except result\n",
    "    send_mail(header)\n",
    "    print('Sending email...')\n",
    "\n",
    "# catch any arcpy errors\n",
    "except arcpy.ExecuteError:\n",
    "    print(arcpy.GetMessages())\n",
    "    log.write(arcpy.GetMessages())\n",
    "    log.close()\n",
    "    \n",
    "    header = \"ERROR - Arcpy Exception - Check Log\"\n",
    "    # send email with header based on try/except result\n",
    "    send_mail(header)\n",
    "    print('Sending email...')\n",
    "\n",
    "# catch system errors\n",
    "except Exception:\n",
    "    e = sys.exc_info()[1]\n",
    "    print(e.args[0])\n",
    "    log.write(str(e))\n",
    "    log.close()\n",
    "    \n",
    "    header = \"ERROR - System Error - Check Log\"\n",
    "    # send email with header based on try/except result\n",
    "    send_mail(header)\n",
    "    print('Sending email...')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
