{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install arcgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureSet, GeoAccessor, GeoSeriesAccessor, FeatureLayer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.geometry import SpatialReference\n",
    "#import geopandas as gpd to use spatial.reference stuff\n",
    "gis = GIS()\n",
    "# # Set Pandas display options to show all rows and columns\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# in memory output file path\n",
    "wk_memory = \"memory\" + \"\\\\\"\n",
    "\n",
    "\n",
    "# set workspace and sde connections \n",
    "#working_folder = \"C:\\GIS\"\n",
    "\n",
    "#workspace      = \"C:\\GIS\\Scratch.gdb\"\n",
    "#arcpy.env.workspace = \"C:\\GIS\\Scratch.gdb\"\n",
    "#workspace ='F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "#arcpy.env.workspace = 'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "\n",
    "# network path to connection files\n",
    "filePath = \"F:\\\\GIS\\\\DB_CONNECT\"\n",
    "\n",
    "#path to GDB's to update and master data\n",
    "master_path = \"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "SEZ_Master = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "#set workspace for connection to GDB\n",
    "#workspace=master_path\n",
    "# database file paths \n",
    "### SDE Collection New data collected is put into SDE.Survey under the indicator name\n",
    "### SDE Vector is where the data will go \n",
    "#sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "sdeBase    = os.path.join(filePath, \"SarahVector.sde\")\n",
    "#sdeCollect = os.path.join(filepath, \"SarahCollect.sde\")\n",
    "\n",
    "# setup connection string???\n",
    "connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=sql12;DATABASE=sde;UID=sde;PWD=staff\"\n",
    "connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "engine = create_engine(connection_url)\n",
    "\n",
    "#with engine.begin() as sdeConnect:\n",
    " #   erosiondf      = pd.read_sql(\"SELECT * FROM sde.SDE.Stream_Erosion\", sdeConnect)\n",
    "\n",
    "# local variables sdata is starting data and f data is finishing datatables\n",
    "ffdata = os.path.join(sdeBase, \"sde.SDE.Monitoring\")\n",
    "sdata = os.path.join(sdeCollect, \"sde.SDE.Survey\")\n",
    "fdata = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "\n",
    "##Tables we get the data from in Collect\n",
    "sezsurveytable = os.path.join(sdata, \"sde.SDE.sez_survey\")\n",
    "erosiondata = os.path.join(sdata, \"sde.SDE.Stream_Erosion\")\n",
    "incisiondata = os.path.join(sdata, \"sde.SDE.sez_channel_incision\")\n",
    "invasivedata = os.path.join(sdata, \"sde.SDE.sez_invasive_plant\")\n",
    "headcutdata = os.path.join(sdata, \"sde.SDE.sez_stream_headcut\")\n",
    "\n",
    "#make this a spatial df\n",
    "streamdata = os.path.join(ffdata, \"sde.SDE.Stream\")\n",
    "\n",
    "\n",
    "#Staging Tables currently living in SEZ_Data.GDB\n",
    "stage_bank_stability = os.path.join(master_path, \"bank_stability\") \n",
    "stage_All_SEZ_Scores = os.path.join(master_path, \"All_SEZ_Scores\")\n",
    "stage_biotic_integrity = os.path.join(master_path, \"biotic_integrity\")\n",
    "stage_headcuts = os.path.join(master_path, \"headcuts_table\")\n",
    "stage_incision = os.path.join(master_path, \"incision\")\n",
    "stage_invasives = os.path.join(master_path, \"invasives\")\n",
    "\n",
    "\n",
    "\n",
    "#Final table to append to\n",
    "#finalSEZtable = os.path.join(fdata, \"sde.SDE.SEZ_Assessment_Unit\")\n",
    "#finalSEZtable = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "\n",
    "\n",
    "# network path to connection files??????\n",
    "#filePath = \"C:\\\\GIS\\\\DB_CONNECT\"\n",
    "# network path to connection files\n",
    "#filePath = r'F:\\Research and Analysis\\Workspace\\Sarah'\n",
    "\n",
    "#--------------------------------------------#\n",
    "#Notes to self\n",
    "#--------------------------------------------#\n",
    "#F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb. \n",
    "###'Assessment_Unit_Master' has all data for SEZ\n",
    "###“Bioassessment Sample Locations” has stream sites in SEZs\n",
    "\n",
    "gdbworking_folder = \"F:\\GIS\\GIS_DATA\\Monitoring\"\n",
    "headcutgdbfolder = os.path.join(gdbworking_folder, \"Stream_Headcut\", \"StreamHeadcut_Survey\")\n",
    "invasivegdbfolder = os.path.join(gdbworking_folder, \"Invasive_Species\", \"Invasive_Species_Survey\")\n",
    "## GDB with Raw Data straight from S123 not in the original folder (that one is not edited)\n",
    "headcut23gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2023.gdb\")\n",
    "headcut22gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2022.gdb\")\n",
    "headcut20gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2020.gdb\")\n",
    "headcut19gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2019.gdb\")\n",
    "#erosion23gdb = os.path.join(gdbworking_folder, \"Stream_Erosion\",\"Stream_Erosion_Survey\", \"Stream_Erosion_Survey_2023.gdb\")\n",
    "#erosion22gdb = os.path.join(working_folder, \"Stream_Erosion\",\"Stream_Erosion_Survey\", \"Stream_Erosion_Survey_2022.gdb\")\n",
    "#erosion20gdb = os.path.join(working_folder, \"Stream_Erosion\",\"Stream_Erosion_Survey\", \"Stream_Erosion_Survey_2020.gdb\")\n",
    "#channelincision23gdb = os.path.join(working_folder,\"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2023.gdb\")\n",
    "#channelincision22gdb = os.path.join(working_folder, \"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2022.gdb\")\n",
    "#channelincision20gdb = os.path.join(working_folder, \"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2020.gdb\")\n",
    "invasiveplant23gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2023.gdb\")\n",
    "invasiveplant22gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2022.gdb\")\n",
    "invasiveplant20gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2020.gdb\")\n",
    "invasiveplant19gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2019.gdb\")\n",
    "\n",
    "#This is thelocatoin for the final SEZ GDB to be updated in the gdb on f drive in the AssessmentUnits Master (polygon) i believe\n",
    "#FinalGDBtoupdate:F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data \\SEZ_Data.gdb\n",
    "\n",
    "#Location of USFS Invasive Species Data\n",
    "#https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer\n",
    "\n",
    "#Monitoring Dashboard location\n",
    "#Finalsdelocation:f'Vector.SDE' Sde.Monitoring Sde. SEZ_Assessment_Unit\n",
    "\n",
    "#Threshold Location? sde.tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup Dictionary for SEZ ID's -Old code uses AssessmentUnit_Master feature class in SEZ_Data.gdb- don't use for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------#\n",
    "#Create look up dictionary for SEZ_ID fill in--currently works for stream erosion code--\n",
    "#-----------------------------------------------------------------------------------#\n",
    "#Add in SEZ_Survey table to match up parentglobalid to assessment unit name so we can match an sez_id\n",
    "#----------------------------------------------------------------##Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "# Set workspace environment\n",
    "arcpy.env.workspace = master_path\n",
    "\n",
    "# Specify the feature class name SEZ_Master\n",
    "#feature_class = \"AssessmentUnits_Master\"\n",
    "# Create a cursor to iterate over the rows in the feature class\n",
    "fields = ['Assessment_Unit_Name', 'SEZ_ID']\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "# Iterate over the rows in the second feature class and append to data list\n",
    "with arcpy.da.SearchCursor(SEZ_Master, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "# Convert the data to a Pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Define function to update SEZ ID lookup dictionary\n",
    "def update_SEZID_lookup_dict(df, lookup_dict):\n",
    "    for index, row in df.iterrows():\n",
    "        # Update SEZ_ID column in DataFrame with data from the lookup dictionary\n",
    "        df.at[index, 'SEZ_ID'] = lookup_dict.get(row['Assessment_Unit_Name'], None)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Assuming 'Assessment_Unit_Name' is the common identifier between DataFrame and lookup dictionary\n",
    "selected_columns = ['Assessment_Unit_Name', 'SEZ_ID']\n",
    "\n",
    "# Drop duplicates based on 'Assessment_Unit_Name' and keep the first occurrence\n",
    "unique_values = df.drop_duplicates(subset='Assessment_Unit_Name', keep='first')\n",
    "\n",
    "# Convert selected columns to dictionary\n",
    "lookup_dict = unique_values[selected_columns].set_index('Assessment_Unit_Name').to_dict(orient='index')\n",
    "\n",
    "# Display the dictionary\n",
    "print(lookup_dict)\n",
    "\n",
    "# Update 'SEZ_ID' in other DataFrame with data from the lookup dictionary do this for each indicator...\n",
    "#Other_df = update_SEZID_lookup_dict(Other_df, lookup_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------#\n",
    "# DONT USE YETT Create look up dictionary for SEZ_ID fill in with Global ID,SEZ ID, and Assessment Unit Name\n",
    "\n",
    "# Set workspace environment\n",
    "arcpy.env.workspace = master_path\n",
    "\n",
    "# Define the fields for SEZ_Master and the additional feature class\n",
    "Masterfields = ['Assessment_Unit_Name', 'SEZ_ID']\n",
    "surveyfields = ['assessment_unit_name', 'GlobalID']  # Add any additional fields you need\n",
    "\n",
    "# Create empty lists to store data\n",
    "Masterdata = []\n",
    "surveydata = []\n",
    "\n",
    "# Iterate over the rows in the SEZ_Master feature class and append to data list\n",
    "with arcpy.da.SearchCursor(SEZ_Master, Masterfields) as cursor:\n",
    "    for row in cursor:\n",
    "        Masterdata.append(row)\n",
    "\n",
    "# Iterate over the rows in the additional feature class and append to data list\n",
    "with arcpy.da.SearchCursor(sezsurveytable, surveyfields) as cursor:\n",
    "    for row in cursor:\n",
    "        surveydata.append(row)\n",
    "\n",
    "# Convert the data to Pandas DataFrames\n",
    "Masterdf = pd.DataFrame(Masterdata, columns=Masterfields)\n",
    "surveydf = pd.DataFrame(surveydata, columns=surveyfields)\n",
    "# Convert the column name in surveydf to match Masterdf\n",
    "surveydf = surveydf.rename(columns={'assessment_unit_name': 'Assessment_Unit_Name'})\n",
    "# Merge the DataFrames on 'Assessment_Unit_Name'\n",
    "SEZIDdf = pd.merge(Masterdf, surveydf, on='Assessment_Unit_Name', how='inner')\n",
    "\n",
    "# Check for missing values in 'GlobalID' or 'SEZ_ID'\n",
    "missing_values = SEZIDdf[SEZIDdf['GlobalID'].isna() | SEZIDdf['SEZ_ID'].isna()]\n",
    "\n",
    "# Replace missing values in 'SEZ_ID' with NaN\n",
    "SEZIDdf.loc[missing_values.index, 'SEZ_ID'] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(SEZIDdf)\n",
    "# Assuming 'Assessment_Unit_Name' is the common identifier between DataFrame and lookup dictionary\n",
    "selected_columns = ['Assessment_Unit_Name', 'SEZ_ID', 'GlobalID']\n",
    "\n",
    "# Drop duplicates based on 'Assessment_Unit_Name' and keep the first occurrence\n",
    "unique_values = SEZIDdf.drop_duplicates(subset='Assessment_Unit_Name', keep='first')\n",
    "\n",
    "# Convert selected columns to dictionary\n",
    "lookup_dict = unique_values[selected_columns].set_index('Assessment_Unit_Name').to_dict(orient='index')\n",
    "\n",
    "# Display the dictionary\n",
    "print(lookup_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEZ ID based off of excel lookup dictionary larger polygons only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Angora Creek - tributary': 519, 'Angora Creek - upper': 520, 'Angora meadows - 1': 87, 'Angora meadows - 2': 90, 'Angora meadows - 3': 91, 'Angora meadows - 4': 143, 'Angora meadows - 5': 144, 'Angora meadows - 6': 142, 'Angora meadows - 7': 89, 'Angora meadows - 8': 88, 'Angora meadows - 9': 92, 'Angora meadows tributary - 1': 217, 'Angora meadows tributary - 2': 99, 'Angora meadows tributary - 3': 97, 'Angora meadows tributary - 4': 94, 'Angora meadows tributary - 5': 214, 'Angora meadows tributary - 6': 146, 'Angora meadows tributary - 7': 93, 'Angora meadows tributary - 8': 95, 'Angora meadows tributary - 9': 96, 'Angora tributary': 446, 'Antone meadows': 187, 'Baldwin marsh - 1': 160, 'Benwood meadows - 1': 129, 'Benwood meadows - 2': 131, 'Big Meadow - 1': 47, 'Big Meadow - 2': 48, 'Big Meadow - 3': 38, 'Big Meadow - 4': 39, 'Big Meadow - 5': 37, 'Big Meadow - 6': 40, 'Big meadow - 7': 126, 'Big Meadow Creek - lower': 521, 'Big Meadow Creek - upper': 491, 'Big Meadow Creek - upper 2': 522, 'Bijou meadows - 1': 115, 'Bijou meadows - 2': 116, 'Bijou meadows - 3': 164, 'Bijou meadows - private': 114, 'Bijou meadows - tributary 1': 226, 'Bijou Park Creek meadows - 1': 219, 'Bijou Park Creek meadows - 2': 166, 'Bijou Park Creek meadows - 3': 167, 'Bijou Park Creek meadows - 4': 163, 'Bijou Park Creek meadows - 5': 489, 'Bijou Park Creek meadows - 6': 488, 'Blackwood Creek - lower 1': 523, 'Blackwood Creek - lower 2': 596, 'Blackwood Creek - middle 1': 597, 'Blackwood Creek - middle 2': 524, 'Blackwood Creek - middle 3': 598, 'Blackwood Creek - middle 4': 525, 'Blackwood Creek - Upper 1': 599, 'Blackwood Creek - Upper 2': 526, 'Blackwood Creek - upper 3': 527, 'Blackwood meadows - 1': 66, 'Blackwood meadows - 3': 65, 'Buck Lake meadows': 176, 'Buddhas meadow': 168, 'Burke Creek - middle': 528, 'Burke Creek - upper': 503, 'Burke Creek meadows - 1': 171, 'Burke Creek meadows - 2': 3, 'Burke Creek tributary': 461, 'Burton Creek - lower': 633, 'Burton Creek - upper': 529, 'Carnelian Canyon Creek - lower': 620, 'Carnelian Canyon Creek - upper': 621, 'Cascade Creek - lower': 501, 'Cascade Creek - upper': 498, 'Casino meadows': 170, 'Christmas Valley meadows - 1': 136, 'Christmas Valley meadows - 2': 134, 'Christmas Valley meadows - 3': 130, 'Christmas Valley meadows - 4': 225, 'Cold Creek - Highland Woods': 110, 'Cold Creek - middle': 530, 'Cold Creek - tributary 1': 532, 'Cold Creek - tributary 2': 533, 'Cold Creek - tributary 3': 531, 'Cold Creek - upper': 534, 'Cold Creek tributary - 4': 465, 'Cold Creek tributary - 5': 466, 'Colony Inn meadows - lower': 169, 'Colony Inn meadows - upper': 485, 'Cookhouse meadow': 128, 'Deer Creek - headwaters': 535, 'Deer Creek - lower': 537, 'Deer Creek - middle': 538, 'Deer Creek - middle 2': 539, 'Deer Creek - upper': 536, 'Deer Creek meadows': 30, 'Dollar Creek - lower': 540, 'Dollar Creek - upper': 512, 'Eagle Creek': 497, 'Echo Creek - below lake': 541, 'Echo Creek - upper': 494, 'Edgewood Creek - middle': 542, 'Edgewood Creek tributary - 2 - headwaters': 462, 'Edgewood Creek tributary - 2 - upper': 635, 'Edgewood Creek tributary - 2 - lower': 476, 'Edgewood Creek tributary - 3 - lower': 628, 'Edgewood Creek tributary - 3 - upper': 629, 'Edgewood meadows': 221, 'Elks Club meadows - 1': 141, 'Elks Club meadows - 2': 85, 'Fallen Leaf meadows - 1': 154, 'Fallen Leaf meadows - 2': 155, 'Fallen Leaf meadows - 3': 147, 'Fallen Leaf meadows - 4': 76, 'First Creek - lower': 543, 'First Creek - upper': 516, 'Freel Meadows - 1': 25, 'Freel Meadows - 2': 24, 'Gardner meadow': 150, 'General Creek - lower': 544, 'General Creek - middle': 506, 'General Creek - upper': 545, 'General Creek meadows': 61, 'Ginny Lake Meadows': 193, 'Glen Alpine Creek - lower': 546, 'Glen Alpine Creek - upper': 606, 'Glenbrook Creek - middle': 510, 'Glenbrook Creek - upper': 547, 'Glenbrook meadows - 1': 177, 'Glenbrook meadows - 2': 178, 'Golden Bear meadows - 1': 212, 'Golden Bear meadows - 2': 196, 'Grass Lake Creek': 609, 'Grass Lake meadow': 127, 'Griff Creek - lower': 514, 'Griff Creek - tributary': 548, 'Griff Creek - upper': 549, 'Griff Creek meadows': 35, 'Haypress Meadows': 50, 'Heavenly Valley Creek - middle': 550, 'Heavenly Valley Creek - upper': 499, 'Heavenly Valley Creek meadows - 1': 111, 'Heavenly Valley Creek meadows - 2': 112, 'Heavenly Valley Creek meadows - 3': 152, 'Heavenly Valley Creek meadows - 4': 113, 'Hell Hole meadows - 1': 230, 'Hell Hole Meadows - 2': 8, 'Hidden Valley Creek - lower': 551, 'Hidden Valley Creek - upper': 552, 'High meadows - 1': 203, 'High meadows - 2': 198, 'High Meadows - 3': 200, 'High meadows - 4': 202, 'High meadows - 6': 437, 'Homewood Canyon Creek - lower': 600, 'Homewood Canyon Creek - upper': 508, 'Incline Creek - lower': 553, 'Incline Creek - middle 1': 554, 'Incline Creek - middle 2': 555, 'Incline Creek - middle 3': 636, 'Incline Creek - ski run': 556, 'Incline Creek - upper': 557, 'Incline Lake meadows - 1': 192, 'Incline Lake meadows - 2': 190, 'Incline Village tributary - 1': 458, 'Incline Village tributary - 2': 459, 'Incline Village tributary - 3': 460, 'Incline Village tributary - 4': 457, 'Kahle meadows - 2': 204, 'Kahle meadows - 3': 172, 'Kahle meadows - 4': 468, 'Kahle meadows - 5': 118, 'Kahle meadows - 6': 469, 'Kahle meadows - 7': 119, 'Kahle meadows - 8': 486, 'Kingsbury meadows': 222, 'Lake Forest meadows - 1': 185, 'Lake Forest meadows - 2': 186, 'Lake Forest meadows - 3': 184, 'Lake Forest meadows - 4': 487, 'Lake Forest meadows - 5': 223, 'Lake Forest meadows - 6': 183, 'Lake Forest tributary': 624, 'Lakeshore meadows': 72, 'Logan House Creek - lower': 558, 'Logan House Creek - upper': 507, 'Logan House meadow': 13, 'Lonely Gulch Creek - lower': 559, 'Lonely Gulch Creek - middle': 560, 'Lonely Gulch Creek - upper': 504, 'Madden Creek': 561, 'Marlette Creek - lower': 562, 'Marlette Creek - old dam site': 564, 'Marlette Creek - south fork (lower)': 642, 'Marlette Creek - south fork (upper)': 563, 'Marlette Creek - upper': 631, 'Marlette Lake meadows': 32, 'McFaul Creek - lower': 565, 'McFual meadow': 71, 'McKinney Creek - lower': 566, 'McKinney Creek - middle': 567, 'McKinney Creek - upper': 505, 'McKinney tributary - 1': 626, 'McKinney tributary - 2': 453, 'Meeks Bay meadows - 1': 205, 'Meeks Bay meadows - 2': 207, 'Meeks Bay meadows - 3': 36, 'Meeks Bay meadows - 4': 206, 'Meeks Creek - upper': 502, 'Meeks Bay Lagoon': 235, 'Meiss meadows - 1': 123, 'Meiss meadows - 2': 122, 'Meiss meadows - 3': 124, 'Meiss meadows - 4': 210, 'Meiss meadows - 5': 209, 'Meyers meadow': 218, 'Meyers tributary - 1': 447, 'Meyers tributary - 2': 467, 'Mill Creek - lower': 568, 'Mill Creek - upper': 515, 'Mill Creek meadows': 234, 'Mount Rainier Drive meadows - 1': 215, 'Mount Rainier Drive meadows - 2': 216, 'Muskawi Drive meadows': 83, 'North Logan House Creek': 509, 'North Logan House meadows': 12, 'North Zephyr Creek - lower': 570, 'North Zephyr Creek - middle': 615, 'North Zephyr Creek - tributary': 569, 'North Zephyr Creek - upper': 571, 'Nottaway Drive meadows': 213, 'Osgood Creek - above road': 572, 'Osgood Creek - below road': 573, 'Osgood Swamp': 482, 'Paige meadows': 182, 'Pope marsh meadow': 483, 'Quail Creek - lower': 574, 'Quail Creek - upper': 575, 'Quail Creek meadow': 26, 'Rosewood Creek - lower': 576, 'Rosewood Creek - middle 1': 586, 'Rosewood Creek - middle 2': 587, 'Rosewood Creek - middle 3': 588, 'Rubicon Creek': 601, 'Rubicon Creek - tributary': 602, 'Rubicon Meadows': 60, 'Saxon Creek - headwaters': 495, 'Saxon Creek - upper': 641, 'Saxon Creek meadows - above Fountain Place 1': 2, 'Saxon Creek meadows - above Fountain Place 2': 102, 'Saxon Creek meadows - below Fountain Place': 1, 'Saxon Creek tributary meadows - 1': 101, 'Saxon Creek tributary meadows - 3': 100, 'Saxon Creek tributary meadows - 4': 140, 'Saxon Creek tributary meadows - 5': 197, 'Saxon Creek tributary meadows - 6': 139, 'Saxon Creek tributary meadows - 7': 231, 'Second Creek - lower': 591, 'Second Creek - lower 2': 592, 'Second Creek - middle': 593, 'Second Creek - upper': 517, 'Secret Harbor Creek - lower': 618, 'Secret Harbor Creek - upper': 619, 'Sierra Tract wetlands': 211, 'Ski Run meadows': 220, 'Sky meadows': 79, 'Skylandia SEZ': 622, 'Slaughterhouse Creek - lower': 614, 'Slaughterhouse Creek - middle': 616, 'Slaughterhouse Creek - upper': 617, 'Slaughterhouse Meadows - 1': 6, 'Slaughterhouse meadows - 2': 180, 'small meadow 1': 11, 'small meadow 10': 64, 'small meadow 100': 77, 'small meadow 105': 125, 'small meadow 111': 132, 'small meadow 112': 133, 'small meadow 113': 135, 'small meadow 116': 181, 'small meadow 13': 20, 'small meadow 14': 19, 'small meadow 15': 18, 'small meadow 16': 21, 'small meadow 17': 28, 'small meadow 19': 63, 'small meadow 2': 10, 'small meadow 20': 623, 'small meadow 21': 67, 'small meadow 22': 53, 'small meadow 23': 54, 'small meadow 24': 56, 'small meadow 25': 55, 'small meadow 26': 57, 'small meadow 27': 58, 'small meadow 28': 59, 'small meadow 29': 174, 'small meadow 3': 70, 'small meadow 30': 175, 'small meadow 32': 51, 'small meadow 35': 49, 'small meadow 36': 52, 'small meadow 40': 74, 'small meadow 5': 29, 'small meadow 50': 23, 'small meadow 51': 42, 'small meadow 52': 41, 'small meadow 54': 22, 'small meadow 56': 46, 'small meadow 57': 73, 'small meadow 58': 173, 'small meadow 59': 14, 'small meadow 6': 17, 'small meadow 7': 27, 'small meadow 8': 69, 'small meadow 82': 232, 'small meadow 9': 68, 'small meadow 92': 31, 'small meadow 93': 33, 'small meadow 95': 43, 'small meadow 96': 44, 'small meadow 98': 62, 'small meadow 99': 75, 'Snow Creek tributary - 1': 451, 'Snow Creek tributary - 2': 452, 'Snow Creek wetlands - 1': 188, 'Snow Creek wetlands - 2': 189, 'South Lake Tahoe - wetland 1': 229, 'South Lake Tahoe airport': 484, 'South Lake Tahoe tributary - 1': 463, 'South Lake Tahoe tributary - 2': 464, 'South Lake Tahoe tributary - 3': 627, 'Spooner meadows - 1': 445, 'Spooner meadows - 2': 431, 'Spooner meadows - 3': 120, 'Spooner Meadows - 4': 5, 'Spooner meadows - 5': 121, 'Star Lake meadows': 45, 'Susquehana meadows - 1': 108, 'Susquehana meadows - 2': 107, 'Tahoe City meadow': 224, 'Tahoe City tributary - 1': 449, 'Tahoe City tributary - 2': 450, 'Tahoe Island meadows - 1': 158, 'Tahoe Island meadows - 2': 156, 'Tahoe Keys': 162, 'Tahoe Paradise golf course': 632, 'Tahoe Valley meadows - 1': 153, 'Tahoe Valley meadows - 2': 228, 'Tahoe Vista meadows': 227, 'Tallac Creek - abv highway - 1': 334, 'Tallac Creek - abv highway - 2': 604, 'Tallac Creek - tributary': 500, 'Tallac marsh': 159, 'Tallac meadows': 157, 'Taylor Creek': 605, 'Taylor Creek marsh': 208, 'Third Creek - headwaters': 585, 'Third Creek - lower': 577, 'Third Creek - lower 2': 578, 'Third Creek - middle': 581, 'Third Creek - middle 1': 579, 'Third Creek - middle 2': 580, 'Third Creek - upper 1': 582, 'Third Creek - upper 2': 584, 'Third Creek - upper 3': 583, 'Third Creek meadows - 1': 191, 'Third Creek meadows - 3': 34, 'Third Creek meadows - 4': 195, 'Third Creek meadows - 6': 194, 'Third Creek meadows - 7': 16, 'Third Creek meadows - 8': 15, 'Trout Creek - Highland Woods': 109, 'Trout Creek - tributary 2': 613, 'Trout Creek - tributary 3': 612, 'Trout Creek - upper': 496, 'Trout Creek above Black Bart': 149, 'Trout Creek below Black Bart': 161, 'Trout Creek headwaters meadows - 1': 137, 'Trout Creek headwaters meadows - 2': 9, 'Trout Creek meadows - above Fountain Place': 103, 'Trout Creek meadows - above Pioneer 1': 148, 'Trout Creek meadows - above Pioneer 2': 106, 'Trout Creek meadows - above Pioneer 3': 105, 'Trout Creek meadows - above Pioneer 4': 104, 'Upper Truckee River - Meyers': 138, 'Upper Truckee River - Tahoe Paradise': 7, 'UTR - Airport reach': 82, 'UTR - Christmas Valley 1': 608, 'UTR - Christmas Valley 3': 493, 'UTR - golf course meadows': 86, 'UTR - Johnson meadows - 2': 151, 'UTR - Johnson meadows - 3': 81, 'UTR - middle': 492, 'UTR - Reach 5': 80, 'UTR - Reach 6': 84, 'UTR - tributary 1': 610, 'UTR - tributary 3': 611, 'UTR - upper': 490, 'UTR - Washoe Meadows': 607, 'UTR marsh - Trout Creek side': 165, 'UTR Marsh - UTR side': 78, 'Van Sickle meadows': 117, 'Ward Creek - lower': 594, 'Ward Creek - middle': 595, 'Ward Creek - upper': 511, 'Ward Creek meadow': 625, 'Washoan Blvd meadows': 145, 'Washoe State Parks meadow - 1': 4, 'Washoe State Parks meadow - 2': 98, 'Watson Creek': 513, 'West Shore tributary - 1': 455, 'West Shore tributary - 2 - lower': 639, 'West Shore tributary - 2 - upper': 448, 'West Shore tributary - 3': 456, 'West Shore tributary - 4': 454, 'Woods Creek - lower': 589, 'Woods Creek - middle': 590, 'Woods Creek - upper': 518, 'High meadows - 5': 201}\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------#\n",
    "#Create look up dictionary for SEZ_ID fill in--currently works for stream erosion code--- change to use excel so its not messy\n",
    "#-----------------------------------------------------------------------------------#\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "excel_data = pd.read_excel(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\SEZID lookup.xlsx\")  # Replace 'your_excel_file.xlsx' with the path to your Excel file\n",
    "\n",
    "#Define Empty look up dataframe\n",
    "lookup_dict = {}\n",
    "\n",
    "for index, row in excel_data.iterrows():\n",
    "    lookup_dict[row['Assessment_Unit_Name']] = row['SEZ_ID']\n",
    "\n",
    "# See dictionary where keys are Assessment Unit Names and values are SEZ IDs\n",
    "print(lookup_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# Display the dictionary using pprint\n",
    "pprint.pprint(lookup_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Grading each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grading for each parameter \n",
    "#Defining Grade for Bank Stability based on Erosiondf[percent_unstable]\n",
    "def categorize_erosion(Percent_Unstable):\n",
    "    if 0 <= Percent_Unstable < 5:\n",
    "        return 'A'\n",
    "    elif 5 <= Percent_Unstable < 20:\n",
    "        return 'B'\n",
    "    elif 20 <= Percent_Unstable < 50:\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'D'\n",
    "    \n",
    "#Scoring based off of grading - check this\n",
    "def score_indicator(Rating):\n",
    "    if  Rating == 'A':\n",
    "        return '12'\n",
    "    elif Rating == 'B':\n",
    "        return '9'\n",
    "    elif Rating == 'C':\n",
    "        return '6'\n",
    "    else:\n",
    "        return '3'\n",
    "\n",
    "#Define Grade for Incision based off of incisino ratio\n",
    "\n",
    "def categorize_incision(bankfull_ratio):\n",
    "    if 0 <= bankfull_ratio < 1.2:\n",
    "        return 'A'\n",
    "    elif 1.2 <= bankfull_ratio < 1.6:\n",
    "        return 'B'\n",
    "    elif 1.6 <= bankfull_ratio < 2.1:\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'D'\n",
    "\n",
    "#Define Grade for Bioassessment Score\n",
    "def categorize_csci(biotic_integrity):\n",
    "     if   biotic_integrity > 0.92:\n",
    "        return 'A'\n",
    "     elif 0.79 < biotic_integrity <= 0.92:\n",
    "        return 'B'\n",
    "     elif 0.62 < biotic_integrity <= 0.79:\n",
    "        return 'C'\n",
    "     else:\n",
    "        return 'D'\n",
    "\n",
    "#Define Priority List Level of Invasive Plant Species\n",
    "    \n",
    "\n",
    " #Define Grade for Invasive Species based off of Priority List\n",
    "#A=0, B=1 level 2, C 2 level 2 or 1 level 1, D=3+ level 2 or 2+ level 1\n",
    "def rate_invasive(priority):\n",
    "    if priority['Group 3'] == 1:\n",
    "        return 'B'  \n",
    "    elif priority['Group 3'] == 2 or priority['Group 1'] == 1 or priority['Group 2'] == 1:\n",
    "        return 'C'  \n",
    "    elif priority['Group 3'] >= 3 or priority['Group 1'] >= 2 or priority['Group 2'] >= 2 or (priority['Group 1'] + priority['Group 2'] >= 2):   \n",
    "       return 'D' \n",
    "    else:\n",
    "        return 'A'  # Assign score A (no invasives)     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Define Size for Headcut based off of headcut size\n",
    "##A = 0 headcut, B 1+small headcut\n",
    "def categorize_headcut(headcutdepth):\n",
    "    if pd.isnull(headcutdepth) or headcutdepth == 0:\n",
    "        return np.nan\n",
    "    elif 0.1 <= headcutdepth < 0.5:\n",
    "        return 'small'\n",
    "    elif 0.5 <= headcutdepth < 1:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'large'\n",
    "\n",
    "#define rating for headcut health per sez\n",
    "def rate_headcut(row):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if row['large'] >= 1:\n",
    "        return 'D'  # Assign score D\n",
    "    elif row['medium'] >= 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif row['small'] >= 1:\n",
    "       return 'B'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no headcuts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Erosion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Get Data\n",
    "#----------------------------------------------------------------##Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "erosionfields = ['Assessment_Unit_Name', 'Shape.STLength()', 'Bank_Type', 'Survey_Date']\n",
    "#erosiondf = pd.DataFrame.from_records(data=arcpy.da.SearchCursor(erosiondata, erosionfields), columns=erosionfields)\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    " # Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(erosiondata, erosionfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "erosiondf = pd.DataFrame(data, columns=erosionfields)\n",
    "\n",
    "# Replace NaN values in 'Assessment_Unit_Name' column with 'Skylandia SEZ'\n",
    "#erosiondf['Assessment_Unit_Name'] = erosiondf['Assessment_Unit_Name'].fillna('Skylandia SEZ')\n",
    "# Replace specific values in 'Assessment_Unit_Name' column\n",
    "erosiondf['Assessment_Unit_Name'] = erosiondf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh'})\n",
    "\n",
    "\n",
    "#Add SEZ_ID column using lookup dictionary\n",
    "#erosiondf['SEZ_ID'] = erosiondf['Assessment_Unit_Name'].map(lambda x: lookup_dict.get(x, {}).get('SEZ_ID', None))\n",
    "\n",
    "#This code is for the excel look up dictionary\n",
    "erosiondf['SEZ_ID'] = erosiondf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "erosiondf['SEZ_ID'] = erosiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "erosiondf['SEZ_ID'] = erosiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#erosiondf = erosiondf.merge(df, on='Assessment_Unit_Name', how='left')\n",
    "\n",
    "#calculate year column \n",
    "erosiondf['Year'] = erosiondf['Survey_Date'].dt.year\n",
    "\n",
    "# Replace 'both_banks' with 'Both Banks' in Bank_Type column\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['both_banks', 'Both banks'], 'Both Banks' )\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['one_bank', 'One bank'], 'One Bank')\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['no_bank', 'No bank'], 'No Bank')\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Process Data\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "# Initialize variables\n",
    "erosiondf['bank_multiplier'] = erosiondf['Bank_Type'].apply(lambda x: 2 if x == 'Both Banks' else (1 if x == 'One Bank' else 0))\n",
    "\n",
    "\n",
    "# Calculate the product of 'Shape.STLength()' and 'bank_multiplier' to get the eroded banks per row\n",
    "erosiondf['eroded_banks_per_row'] = erosiondf['Shape.STLength()'] * erosiondf['bank_multiplier']\n",
    "\n",
    "# Group by Assessment_Unit_Name and year and sum the lengths of banks for each unit to get total banks assessed\n",
    "erosiondf['banks_assessed_per_unit'] = erosiondf.groupby(['Assessment_Unit_Name', 'Year'])['Shape.STLength()'].transform('sum') * 2\n",
    "\n",
    "# Group by Assessment_Unit_Name and sum the eroded banks per row for each unit\n",
    "erosiondf['SEZ_total_eroded'] = erosiondf.groupby(['Assessment_Unit_Name', 'Year'])['eroded_banks_per_row'].transform('sum')\n",
    "\n",
    "# Calculate percent unstable Bank Stability per Assessment Unit\n",
    "erosiondf['Bank_Stability_Percent_Unstable'] = (erosiondf['SEZ_total_eroded'] / erosiondf['banks_assessed_per_unit']) * 100\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score\n",
    "#----------------------------------------------------------------#\n",
    "erosiondf['Bank_Stability_Rating']=erosiondf['Bank_Stability_Percent_Unstable'].apply(categorize_erosion)\n",
    "erosiondf['Bank_Stability_Score']= erosiondf['Bank_Stability_Rating'].apply(score_indicator)\n",
    "\n",
    "erosiondf['Bank_Stability_Data_Source'] = 'TRPA'\n",
    "\n",
    "erosiondf.head()\n",
    "\n",
    "print(erosiondf)\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to bank_stability called stage_bank_stability GDB location\n",
    "#----------------------------------------------------------------#\n",
    "# Define the name of the feature class\n",
    "feature_class_name = 'bank_stability'\n",
    "\n",
    "# Define the full path to the feature class\n",
    "feature_class_path = stage_bank_stability \n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Bank_Stability_Data_Source': 'Bank_Stability_Data_Source',\n",
    "    'Bank_Stability_Percent_Unstable': 'Bank_Stability_Percent_Unstable',\n",
    "    'Bank_Stability_Rating': 'Bank_Stability_Rating',\n",
    "    'Bank_Stability_Score': 'Bank_Stability_Score',\n",
    "    'SEZ_ID': 'SEZ_ID'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "bank_stabilitydf = erosiondf.rename(columns=field_mapping).drop(columns=[col for col in erosiondf.columns if col not in field_mapping])\n",
    "\n",
    "readydf = bank_stabilitydf.groupby(['SEZ_ID', 'Year']).first().reset_index()\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "# Fix data type of Year so it writes to table\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y', errors='coerce')\n",
    "\n",
    "# Setting the frequency to 'Y' for year\n",
    "#readydf['Year'] = readydf['Year'].dt.to_period('Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "#Write dataframe to fc in SEZ_Data.GDB\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "with arcpy.da.InsertCursor(stage_bank_stability, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")\n",
    "\n",
    "\n",
    "#Write dataframe to sde.collect.bank.stability eventually, current code write it to GDB in SEZ_Data.GDB\n",
    "# Set environment workspace to your SDE connection file\n",
    "#arcpy.env.workspace = master_path\n",
    "\n",
    "# Convert DataFrame to Feature Class\n",
    "#output_feature_class = \"ErosionUpdate\"  # Name for the output feature class\n",
    "#output_fc_path = os.path.join(arcpy.env.workspace, output_feature_class)\n",
    "\n",
    "# Assuming your DataFrame is already converted to a feature class\n",
    "# Replace \"path_to_your_feature_class\" with the actual path to your feature class\n",
    "#arcpy.conversion.TableToTable(\"path_to_your_feature_class\", arcpy.env.workspace, output_feature_class)\n",
    "\n",
    "# Overwrite Feature Class in SDE\n",
    "# Replace \"path_to_your_dataframe\" with the actual path to your DataFrame\n",
    "#arcpy.management.CopyFeatures(\"path_to_your_dataframe\", output_fc_path)\n",
    "\n",
    "\n",
    "#print(ready_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(readydf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to CSV\n",
    "#csv_path = os.path.join(working_folder, \"Erosiondatamaster.csv\")\n",
    "#ready_df.to_csv(csv_path, index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Incision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Get Data-- all the data we need is in sde.collect.sde.survey.sde.sez_survey\n",
    "#----------------------------------------------------------------##Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "incisionfields = ['Assessment_Unit_Name', 'incision_ratio', 'survey_date']\n",
    "#incisiondf = pd.DataFrame.from_records(data=arcpy.da.SearchCursor(erosiondata, erosionfields), columns=erosionfields)\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    " # Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(sezsurveytable, incisionfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "incisiondf = pd.DataFrame(data, columns=incisionfields)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Add correct info to dataframe\n",
    "#----------------------------------------------------------------#\n",
    "#use this until we fix the domain\n",
    "incisiondf['Assessment_Unit_Name'] = incisiondf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh'})\n",
    "# Create a new column 'SEZ ID'\n",
    "#This code is for the excel look up dictionary\n",
    "incisiondf['SEZ_ID'] = incisiondf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#calculate year column \n",
    "incisiondf['Year'] = incisiondf['survey_date'].dt.year\n",
    "\n",
    "\n",
    "#incisiondf['SEZ ID'] = np.nan\n",
    "\n",
    "# Iterate through the rows in incisiondf\n",
    "#for index, row in incisiondf.iterrows():\n",
    " #   parentglobalid = row['parentglobalid']\n",
    "    \n",
    "      # Check if the parent_global_id exists in the lookup dictionary\n",
    "  #  if parentglobalid in lookup_dict:\n",
    "        # If it exists, retrieve the corresponding SEZ ID and fill it into SEZ ID column\n",
    "   #     corresponding_entry = lookup_dict[parentglobalid]\n",
    "    #    assert row['GlobalID'] == parentglobalid, \"ParentGlobalID does not match GlobalID in the lookup dictionary\"\n",
    "     #   incisiondf.at[index, 'SEZ_ID'] = corresponding_entry['SEZ_ID']\n",
    "\n",
    "# Display the updated incisiondf\n",
    "print(incisiondf)\n",
    "\n",
    "#Add SEZ_ID column using lookup dictionary\n",
    "#incisiondf['SEZ_ID'] = SEZIDdf['GlobalID'].map(lambda x: lookup_dict.get(x, {}).get('SEZ_ID', None))\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "#incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "#incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#incisiondf = incisiondf.merge(df, on='Assessment_Unit_Name', how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score\n",
    "#----------------------------------------------------------------#\n",
    "incisiondf['Incision_Rating']=incisiondf['incision_ratio'].apply(categorize_incision)\n",
    "incisiondf['Incision_Score']= incisiondf['Incision_Rating'].apply(score_indicator)\n",
    "\n",
    "incisiondf['Incision_Data_Source'] = 'TRPA'\n",
    "\n",
    "incisiondf.head()\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to incision called stage_incision GDB location\n",
    "#----------------------------------------------------------------#\n",
    "# Define the name of the feature class\n",
    "feature_class_name = 'incision'\n",
    "\n",
    "# Define the full path to the feature class\n",
    "feature_class_path = stage_incision \n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Incision_Data_Source': 'Incision_Data_Source',\n",
    "    'incision_ratio': 'Incision_Ratio',\n",
    "    'Incision_Rating': 'Incision_Rating',\n",
    "    'Incision_Score': 'Incision_Score',\n",
    "    'SEZ_ID': 'SEZ_ID'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "incisionfinaldf = incisiondf.rename(columns=field_mapping).drop(columns=[col for col in incisiondf.columns if col not in field_mapping])\n",
    "\n",
    "readydf = incisionfinaldf.groupby(['SEZ_ID', 'Year']).first().reset_index()\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "#Write dataframe to fc in SEZ_Data.GDB\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "with arcpy.da.InsertCursor(stage_incision, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invasive Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looped with date and geometry query and loop Haven't tried yet\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the USFS REST endpoint\n",
    "usfsrest = \"https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer/0\"\n",
    "usfsfeature_layer = FeatureLayer(usfsrest)\n",
    "\n",
    "# Define the spatial filter condition to retrieve data within the Tahoe area polygon\n",
    "tahoe_geometry = \"POLYGON ((-120.0 39.0, -120.0 38.8, -119.7 38.8, -119.7 39.0, -120.0 39.0))\"\n",
    "\n",
    "# Define the date after which you want the data\n",
    "start_date = datetime(2020, 1, 1)\n",
    "\n",
    "# Convert start date to Unix timestamp (milliseconds since epoch)\n",
    "start_timestamp = int(start_date.timestamp() * 1000)\n",
    "\n",
    "# Define query parameters\n",
    "query_params = {\n",
    "    'geometryType': 'esriGeometryPolygon',\n",
    "    'spatialRel': 'esriSpatialRelWithin',  \n",
    "    'geometry': tahoe_geometry,\n",
    "    'out_fields': '*',  \n",
    "    'returnGeometry': True,  \n",
    "    'returnTrueCurves': False,  \n",
    "    'returnIdsOnly': False,  \n",
    "    'returnCountOnly': False,  \n",
    "    'returnExtentOnly': False,  \n",
    "    'returnDistinctValues': False,  \n",
    "    'orderByFields': '',  \n",
    "    'groupByFieldsForStatistics': '',  \n",
    "    'outStatistics': '',  \n",
    "    'resultOffset': 0,  \n",
    "    'resultRecordCount': 2000,  # Adjust this based on the maximum records per query\n",
    "    'f': 'pjson'  ,\n",
    "    'where': f\"DATE_COLLECTED >= {start_timestamp}\"  # Filter for dates after 2020\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop until all records are retrieved\n",
    "while True:\n",
    "    # Perform query to retrieve data\n",
    "    result = usfsfeature_layer.query(\n",
    "        geometryFilter=query_params['geometry'], \n",
    "        where=query_params['where'],  # Include the where clause\n",
    "        out_fields=query_params['out_fields'], \n",
    "        returnGeometry=query_params['returnGeometry'], \n",
    "        returnTrueCurves=query_params['returnTrueCurves'], \n",
    "        returnIdsOnly=query_params['returnIdsOnly'], \n",
    "        returnCountOnly=query_params['returnCountOnly'], \n",
    "        returnExtentOnly=query_params['returnExtentOnly'], \n",
    "        returnDistinctValues=query_params['returnDistinctValues'], \n",
    "        orderByFields=query_params['orderByFields'], \n",
    "        groupByFieldsForStatistics=query_params['groupByFieldsForStatistics'], \n",
    "        outStatistics=query_params['outStatistics'], \n",
    "        resultOffset=query_params['resultOffset'], \n",
    "        resultRecordCount=query_params['resultRecordCount'], \n",
    "        f=query_params['f']\n",
    "    ).sdf\n",
    "    \n",
    "    # Check if there are records returned\n",
    "    if len(result) == 0:\n",
    "        break  # Exit loop if no more records\n",
    "    \n",
    "    # Append the dataframe to the list\n",
    "    dfs.append(result)\n",
    "    \n",
    "    # Update offset for the next query\n",
    "    query_params['resultOffset'] += query_params['resultRecordCount']\n",
    "\n",
    "# Concatenate all dataframes\n",
    "usfssdf = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Convert the 'DATE_COLLECTED' column to datetime format\n",
    "usfssdf['DATE_COLLECTED'] = pd.to_datetime(usfssdf['DATE_COLLECTED'])\n",
    "\n",
    "# Extract date portion\n",
    "usfssdf['DATE_COLLECTED'] = usfssdf['DATE_COLLECTED'].dt.date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try with looping and geometry straight from the gpt, works but takes 95 minutes!\n",
    "import pandas as pd\n",
    "from arcgis.features import FeatureLayer\n",
    "\n",
    "# Define the USFS REST endpoint\n",
    "usfsrest = \"https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer/0\"\n",
    "usfsfeature_layer = FeatureLayer(usfsrest)\n",
    "\n",
    "# Define the spatial filter condition to retrieve data within the Tahoe area polygon\n",
    "tahoe_geometry = \"POLYGON ((-120.0 39.0, -120.0 38.8, -119.7 38.8, -119.7 39.0, -120.0 39.0))\"\n",
    "\n",
    "# Define query parameters\n",
    "query_params = {\n",
    "    'geometryType': 'esriGeometryPolygon',\n",
    "    'spatialRel': 'esriSpatialRelWithin',  \n",
    "    'geometry': tahoe_geometry,\n",
    "    'out_fields': '*',  \n",
    "    'returnGeometry': True,  \n",
    "    'returnTrueCurves': False,  \n",
    "    'returnIdsOnly': False,  \n",
    "    'returnCountOnly': False,  \n",
    "    'returnExtentOnly': False,  \n",
    "    'returnDistinctValues': False,  \n",
    "    'orderByFields': '',  \n",
    "    'groupByFieldsForStatistics': '',  \n",
    "    'outStatistics': '',  \n",
    "    'resultOffset': 0,  \n",
    "    'resultRecordCount': 2000,  # Adjust this based on the maximum records per query\n",
    "    'f': 'pjson'  \n",
    "}\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop until all records are retrieved\n",
    "while True:\n",
    "    # Perform query to retrieve data\n",
    "    result = usfsfeature_layer.query(\n",
    "        geometryFilter=query_params['geometry'], \n",
    "        out_fields=query_params['out_fields'], \n",
    "        returnGeometry=query_params['returnGeometry'], \n",
    "        returnTrueCurves=query_params['returnTrueCurves'], \n",
    "        returnIdsOnly=query_params['returnIdsOnly'], \n",
    "        returnCountOnly=query_params['returnCountOnly'], \n",
    "        returnExtentOnly=query_params['returnExtentOnly'], \n",
    "        returnDistinctValues=query_params['returnDistinctValues'], \n",
    "        orderByFields=query_params['orderByFields'], \n",
    "        groupByFieldsForStatistics=query_params['groupByFieldsForStatistics'], \n",
    "        outStatistics=query_params['outStatistics'], \n",
    "        resultOffset=query_params['resultOffset'], \n",
    "        resultRecordCount=query_params['resultRecordCount'], \n",
    "        f=query_params['f']\n",
    "    ).sdf\n",
    "    \n",
    "    # Check if there are records returned\n",
    "    if len(result) == 0:\n",
    "        break  # Exit loop if no more records\n",
    "    \n",
    "    # Append the dataframe to the list\n",
    "    dfs.append(result)\n",
    "    \n",
    "    # Update offset for the next query\n",
    "    query_params['resultOffset'] += query_params['resultRecordCount']\n",
    "\n",
    "# Concatenate all dataframes\n",
    "usfssdf = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Convert the 'date_field' column to datetime format\n",
    "usfssdf['DATE_COLLECTED'] = pd.to_datetime(usfssdf['DATE_COLLECTED'])\n",
    "\n",
    "# Extract date portion\n",
    "usfssdf['DATE_COLLECTED'] = usfssdf['DATE_COLLECTED'].dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter usfs data down to correct timeframe ... make the data smalller from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2024, 12, 31)\n",
    "\n",
    "# Convert start and end dates to Pandas Timestamp objects\n",
    "start_timestamp = pd.Timestamp(start_date)\n",
    "end_timestamp = pd.Timestamp(end_date)\n",
    "\n",
    "# Convert the 'DATE_COLLECTED' column to datetime format\n",
    "usfssdf['DATE_COLLECTED'] = pd.to_datetime(usfssdf['DATE_COLLECTED'])\n",
    "\n",
    "# Perform the comparison after converting Timestamps to dates\n",
    "query = (usfssdf['DATE_COLLECTED'].dt.date >= start_date.date()) & (usfssdf['DATE_COLLECTED'].dt.date <= end_date.date())\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = usfssdf[query].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEZsdf = pd.DataFrame.spatial.from_featureclass(SEZ_Master)\n",
    "#SEZsdf = Geoaccessor.from_features(SEZ_Master)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Target feature Assessment Unit Polygons\n",
    "#SEZ_Master = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb/AssessmentUnits_Master\"\n",
    "\n",
    "# Create the spatially enabled DataFrame (sdf) for target feature SEZ assessment units\n",
    "#SEZsdf = pd.DataFrame.spatial.from_featureclass(SEZ_Master)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Now you have a Spatially Enabled DataFrame (SEZsdf) from the feature layer in the geodatabase\n",
    "\n",
    "#spatial reference stuff\n",
    "usfssdf.spatial.sr = SEZsdf.spatial.sr\n",
    "\n",
    "#CAsdf.spatial.set_spatial_reference(SEZsdf.spatial.sr)\n",
    "#perform spatial join\n",
    "#joinedsdf = SEZsdf.spatial.join(usfssdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calfora Data points--dates only go to 2012?\n",
    "# dates only go to 2012 hereCArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines07/MapServer/1\"\n",
    "#CArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines27/MapServer/0\"\n",
    "# Create a feature layer object\n",
    "#cafeature_layer = FeatureLayer(CArest)\n",
    "#Query caflora layer to that it only shows 2020-2023 or only county?\n",
    "#start_year = 'January 1, 2020'\n",
    "#end_year= 'December 31, 2023'\n",
    "# Convert to datetime objects\n",
    "#start_date = pd.to_datetime(start_year)\n",
    "#end_date = pd.to_datetime(end_year)\n",
    "\n",
    "#Query before or after??\n",
    "#query = (CAsdf['Date_']>= start_date) & (CAsdf['Date_']<= end_date)\n",
    "\n",
    "#CA_filtereddf = CAsdf.join[query].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis.features import GeoAccessor\n",
    "#Target feature Assessment Unit Polygons\n",
    "SEZ_Master = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb/AssessmentUnits_Master\"\n",
    "\n",
    "# Create the spatially enabled DataFrame (sdf) for target feature SEZ assessment units\n",
    "SEZsdf = pd.DataFrame.spatial.from_featureclass(SEZ_Master)\n",
    "#Calfora Data points--dates only go to 2012?\n",
    "# dates only go to 2012 hereCArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines07/MapServer/1\"\n",
    "#CArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines27/MapServer/0\"\n",
    "# Create a feature layer object\n",
    "#cafeature_layer = FeatureLayer(CArest)\n",
    "\n",
    "# Query the features and convert them into a spatial DataFrame\n",
    "#CAsdf = cafeature_layer.query().sdf\n",
    "\n",
    "#USFS Data polygons#\n",
    "#usfsrest= \"https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer/0\"\n",
    "#usfsfeature_layer = FeatureLayer(usfsrest)\n",
    "#usfssdf = usfsfeature_layer.query().sdf\n",
    "\n",
    "#Query caflora layer to that it only shows 2020-2023 or only county?\n",
    "#start_year = 'January 1, 2020'\n",
    "#end_year= 'December 31, 2023'\n",
    "# Convert to datetime objects\n",
    "#start_date = pd.to_datetime(start_year)\n",
    "#end_date = pd.to_datetime(end_year)\n",
    "\n",
    "#Query before or after??\n",
    "#query = (CAsdf['Date_']>= start_date) & (CAsdf['Date_']<= end_date)\n",
    "\n",
    "#CA_filtereddf = CAsdf.join[query].copy()\n",
    "#spatial reference stuff\n",
    "usfssdf.spatial.sr = SEZsdf.spatial.sr\n",
    "\n",
    "#CAsdf.spatial.set_spatial_reference(SEZsdf.spatial.sr)\n",
    "#perform spatial join\n",
    "joinedsdf = SEZsdf.spatial.join(usfssdf)\n",
    "\n",
    "#arcpy.env.workspace = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Data Management 2023.gdb\"\n",
    "\n",
    "# Define the target and join layers\n",
    "#target_feature = \"AssessmentUnits_Master\"  # Path to the target feature class or layer\n",
    "#join_feature = \"externalinvasivesez\"  # Path to the join feature class or layer\n",
    "\n",
    "# Define the output feature class\n",
    "#out_feature_class = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Data Management 2023.gdb\\spatial_join_output\"\n",
    "\n",
    "# Perform spatial join\n",
    "#arcpy.analysis.SpatialJoin(\n",
    " #   target_features=target_feature,\n",
    "  #  join_features=join_feature,\n",
    "   # out_feature_class=out_feature_class,\n",
    "    #join_operation=\"JOIN_ONE_TO_ONE\",  # or \"JOIN_ONE_TO_MANY\" based on your requirements\n",
    "    #join_type=\"KEEP_ALL\",  # Keep all target features, even if they don't intersect with any join features\n",
    "    #match_option=\"INTERSECT\",  # Perform the spatial join based on intersection\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Assessment_Unit_Name        plant_type  \\\n",
      "0       Saxon Creek meadows - below Fountain Place              None   \n",
      "1     Saxon Creek meadows - above Fountain Place 1      bull thistle   \n",
      "2                          Burke Creek meadows - 2              None   \n",
      "3                    Washoe State Parks meadow - 1              None   \n",
      "4                              Spooner Meadows - 4              None   \n",
      "...                                            ...               ...   \n",
      "1235                    Blackwood Creek - middle 4     Wooly_mullein   \n",
      "1236                           Saxon Creek - upper      bull_thistle   \n",
      "1237                           Saxon Creek - upper  reed_canarygrass   \n",
      "1238                      UTR - Christmas Valley 3     Wooly_mullein   \n",
      "1239                      UTR - Christmas Valley 3       oxeye_daisy   \n",
      "\n",
      "           SCIENTIFIC               created_date Eradicated Source  total  \\\n",
      "0                None                        NaT       None   USFS    NaN   \n",
      "1     Cirsium vulgare 2023-07-31 00:00:00.000000         No   USFS    NaN   \n",
      "2                None                        NaT       None   USFS    NaN   \n",
      "3                None                        NaT       None   USFS    NaN   \n",
      "4                None                        NaT       None   USFS    NaN   \n",
      "...               ...                        ...        ...    ...    ...   \n",
      "1235              NaN 2023-09-27 23:27:42.000000        NaN   TRPA    NaN   \n",
      "1236              NaN 2023-09-27 23:27:53.000001        NaN   TRPA    NaN   \n",
      "1237              NaN 2023-09-27 23:27:53.000001        NaN   TRPA    NaN   \n",
      "1238              NaN 2023-09-27 23:27:59.000000        NaN   TRPA    NaN   \n",
      "1239              NaN 2023-09-27 23:27:59.000000        NaN   TRPA    NaN   \n",
      "\n",
      "      percent_cover other  Species_Level  \n",
      "0               NaN   NaN            NaN  \n",
      "1               NaN   NaN            NaN  \n",
      "2               NaN   NaN            NaN  \n",
      "3               NaN   NaN            NaN  \n",
      "4               NaN   NaN            NaN  \n",
      "...             ...   ...            ...  \n",
      "1235            NaN  None            4.0  \n",
      "1236            NaN  None            3.0  \n",
      "1237            NaN  None            2.0  \n",
      "1238            NaN  None            4.0  \n",
      "1239            NaN  None            4.0  \n",
      "\n",
      "[1240 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#Invasive Species\n",
    "\n",
    "\n",
    "#Path to external data usfs \n",
    "usfsdata = os.path.join(master_path, \"usfsinvasivesez24\")\n",
    "\n",
    "\n",
    "# Paths to the feature classes\n",
    "invasive19fc = os.path.join(invasiveplant19gdb, \"Invasive_Species_2019\")\n",
    "invasive20fc = os.path.join(invasiveplant20gdb, \"Invasive_Species_2020\")\n",
    "invasive22fc = os.path.join(invasiveplant22gdb, \"Invasive_Species_2022\")\n",
    "invasive23fc = os.path.join(invasiveplant23gdb, \"sez_invasive_plant\")\n",
    "\n",
    "\n",
    "#Use gdb because they have assessment unit name in them... 2022 i had to manually add in PRO\n",
    "invasive23fields = ['Assessment_Unit', 'invasives_percent_cover', 'invasives_number_of_species','invasives_plant_type', 'Species_Level', 'invasive_type_other', 'created_date']\n",
    "invasive22fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover', 'Invasives_Number_of_Species', 'InvasiveType_Other', 'Survey_Date']\n",
    "invasive20fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover', 'Invasives_Number_of_Species', 'Other', 'Survey_Date']\n",
    "invasive19fields = ['SITE_ID', 'TOTAL_SPECIES', 'SURVEY_DATE', 'INVASIVE_PLANT', 'PERCENT_COVER' ]\n",
    "usfsfields = ['Assessment_Unit_Name', 'PLANT_COMM', 'SCIENTIFIC', 'DATE_COLLE', 'Eradicated']\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "\n",
    "# Function to read feature class into DataFrame\n",
    "def feature_class_to_dataframe(feature_class, fields):\n",
    "    data = [row for row in arcpy.da.SearchCursor(feature_class, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Read feature classes into DataFrames\n",
    "# Read feature classes into DataFrames\n",
    "invasive19df = feature_class_to_dataframe(invasive19fc, invasive19fields)\n",
    "invasive20df = feature_class_to_dataframe(invasive20fc, invasive20fields)\n",
    "invasive22df = feature_class_to_dataframe(invasive22fc, invasive22fields)\n",
    "invasive23df = feature_class_to_dataframe(invasive23fc, invasive23fields)\n",
    "usfs23df = feature_class_to_dataframe(usfsdata, usfsfields)\n",
    "\n",
    "\n",
    "invasive19df['Source'] = 'TRPA'\n",
    "invasive20df['Source'] = 'TRPA'\n",
    "invasive22df['Source'] = 'TRPA'\n",
    "invasive23df['Source'] = 'TRPA'\n",
    "usfs23df['Source'] = 'USFS'\n",
    "\n",
    "# Rename fields\n",
    "invasive19df.rename(columns={'SITE_ID': 'Assessment_Unit_Name', 'INVASIVE_PLANT': 'plant_type', 'TOTAL_SPECIES': 'total',  'PERCENT_COVER': 'percent_cover', 'SURVEY_DATE': 'created_date','Source':'Source'}, inplace=True)\n",
    "invasive20df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'Other': 'other', 'Invasives_Percent_Cover': 'percent_cover','Invasives_Number_of_Species': 'total', 'Survey_Date': 'created_date', 'Source':'Source'}, inplace=True)\n",
    "invasive22df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'InvasiveType_Other': 'other', 'Invasives_Percent_Cover': 'percent_cover','Invasives_Number_of_Species': 'total', 'Survey_Date': 'created_date','Source':'Source'}, inplace=True)\n",
    "invasive23df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'invasives_percent_cover': 'percent_cover', 'invasives_number_of_species': 'total', 'invasives_plant_type': 'plant_type', 'invasive_type_other': 'other', 'created_date': 'created_date','Source':'Source'}, inplace=True)\n",
    "usfs23df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'PLANT_COMM':'plant_type', 'DATE_COLLE':'created_date', 'Source':'Source'}, inplace=True)\n",
    "\n",
    "\n",
    "# Concatenate DataFrames\n",
    "invasivedf = pd.concat([usfs23df, invasive19df, invasive20df, invasive22df, invasive23df], ignore_index=True)\n",
    "\n",
    "# Display merged DataFrame\n",
    "print(invasivedf)\n",
    "\n",
    "#Define SEZ ID based on Assessment_Unit_Name\n",
    "invasivedf['SEZ_ID'] = invasivedf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].astype(int)\n",
    "invasivedf['Assessment_Unit_Name'] = invasivedf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky Meadows meadow - 1': 'Sky meadows'})\n",
    "\n",
    "#calculate year column \n",
    "invasivedf['Year'] = invasivedf['created_date'].dt.year\n",
    "\n",
    "\n",
    "#---------------------------#\n",
    "    #Prep Data\n",
    "#---------------------------#\n",
    "# Replace 'other' or 'Other' in 'plant_type' column with values from 'other' column\n",
    "invasivedf['plant_type'] = invasivedf.apply(lambda row: row['other'] if pd.notna(row['plant_type']) and row['plant_type'].lower() in ['other', 'Other'] else row['plant_type'], axis=1)\n",
    "\n",
    "# Drop the 'other' column\n",
    "invasivedf.drop(columns=['other'], inplace=True)\n",
    "\n",
    "# Function to separate plant types and create new rows\n",
    "def separate_species(df):\n",
    "    # Split plant types by comma and create new rows\n",
    "    df['plant_type'] = df['plant_type'].str.split(',')\n",
    "    df = df.explode('plant_type')\n",
    "    return df\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "invasivedf = separate_species(invasivedf)\n",
    "\n",
    "# Remove rows where data source is USFS and plant type is NaN\n",
    "invasivedf = invasivedf[~((invasivedf['Source'] == 'USFS') & invasivedf['plant_type'].isna())]\n",
    "\n",
    "# Capitalize the first word and replace underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.split('_').str[0].str.capitalize() + ' ' + invasivedf['plant_type'].str.split('_').str[1:].str.join(' ')\n",
    "\n",
    "# Remove spaces after the words and capitalize the first word while replacing underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].astype(str)\n",
    "\n",
    "\n",
    "# Remove any spaces at the beginning or end of a word\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.strip()\n",
    "\n",
    "\n",
    "# Remove rows where data source is USFS and plant type is NaN\n",
    "invasivedf = invasivedf[~((invasivedf['Source'] == 'USFS') & invasivedf['plant_type'].isna())]\n",
    "#Remove Eradicated \n",
    "\n",
    "# Filter out rows where 'eradicated' column is 'Yes'\n",
    "invasivedf = invasivedf[invasivedf['Eradicated'] != 'Yes']\n",
    "\n",
    "# Remove duplicates based on SEZ, Year, and plant_type\n",
    "invasivedf = invasivedf.drop_duplicates(subset=['SEZ_ID', 'Year', 'plant_type'])\n",
    "#----------------------------------------------------#\n",
    "#Create Look up dictionary to assign priority group for each plant and then use it\n",
    "#----------------------------------------------------#\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "csv_data = pd.read_csv(r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Invasives Priority lookup.csv\") \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "Invasives_lookup = {}\n",
    "\n",
    "key = 'Common'\n",
    "values = ['Scientific', 'Priority'] \n",
    "\n",
    "Invasives_lookup= csv_data.set_index(key)[values].to_dict(orient='index')\n",
    "\n",
    "# Define a custom function to map plant types to priorities\n",
    "def map_priority(plant_type):\n",
    "    if pd.isnull(plant_type):\n",
    "        return np.nan  # Return NaN for NaN values\n",
    "    else:\n",
    "        # Extract the priority from the dictionary, or return 'Unknown' if not found\n",
    "        plant_info = Invasives_lookup.get(plant_type)\n",
    "        if plant_info:\n",
    "            return plant_info['Priority']\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "\n",
    "#Create a new column [Scientific based on look up dictionary\n",
    "#invasivedf['Scientific']=invasivedf['plant_type'].map(map_priority)\n",
    "\n",
    "# Create a new column 'Priority' based on the mapping from the dictionary\n",
    "invasivedf['Priority'] = invasivedf['plant_type'].map(map_priority)\n",
    "\n",
    "#invasivedf['Priority'] = pd.to_numeric(invasivedf['Priority'], errors='coerce')\n",
    "\n",
    "# Create a new column 'Number_of_species' based on the presence of plant types\n",
    "invasivedf['Number_of_species'] = np.where(invasivedf['plant_type'].notnull(), 1, 0)\n",
    "\n",
    "# Initialize a new DataFrame to store the summary\n",
    "#summary_per_unit_year = pd.DataFrame(columns=['Assessment_Unit_Name', 'Year', 'Total_Species', 'Priority_1', 'Priority_2', 'Priority_3', 'Unknown'])\n",
    "#invasive_summary = invasivedf.groupby(['Assessment_Unit_Name', 'Year', 'Priority']).size().reset_index(name='Count')\n",
    "#invasive_summary_groups = invasive_summary.pivot_table(index=['Assessment_Unit_Name', 'Year'], columns='Priority', values='Count', fill_value=0)\n",
    "# Iterate over each row in the DataFrame\n",
    "#for index, row in invasivedf.iterrows():\n",
    "    # Get the Assessment Unit Name, Year, Priority, and Total Species for the current row\n",
    " #   unit_name = row['Assessment_Unit_Name']\n",
    "  #  year = row['Year']\n",
    "   # priority = row['Priority']\n",
    "    #total_species = row['Number_of_species']\n",
    "    \n",
    "    # Check if the row's priority is NaN (Unknown)\n",
    "    #if pd.isna(priority):\n",
    "     #   priority = 'Unknown'\n",
    "    \n",
    "    # Check if the combination of Assessment Unit and Year already exists in the summary DataFrame\n",
    "    #if ((summary_per_unit_year['Assessment_Unit_Name'] == unit_name) & (summary_per_unit_year['Year'] == year)).any():\n",
    "        # If it exists, increment the count for the corresponding priority level and update the total species count\n",
    "     #   summary_per_unit_year.loc[(summary_per_unit_year['Assessment_Unit_Name'] == unit_name) & (summary_per_unit_year['Year'] == year), priority] += 1\n",
    "      #  summary_per_unit_year.loc[(summary_per_unit_year['Assessment_Unit_Name'] == unit_name) & (summary_per_unit_year['Year'] == year), 'Total_Species'] += total_species\n",
    "    #else:\n",
    "        # If it doesn't exist, create a new row with counts initialized to 1 for the corresponding priority level\n",
    "     #   new_row = {'Assessment_Unit_Name': unit_name, 'Year': year, 'Total_Species': total_species, 'Priority_1': 0, 'Priority_2': 0, 'Priority_3': 0, 'Unknown': 0}\n",
    "      #  new_row[priority] += 1\n",
    "       # summary_per_unit_year = summary_per_unit_year.append(new_row, ignore_index=True)\n",
    "\n",
    "# Display the summary DataFrame\n",
    "#print(summary_per_unit_year)\n",
    "\n",
    "\n",
    "#Summarize the SEZ Unit for invasives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "plant_type= 'Cheatgrass'\n",
    "plant_info = Invasives_lookup.get(plant_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Barb goatgrass': {'Scientific': 'Aegilops triuncialis', 'Priority': 1}, 'Carolina horsenettle ': {'Scientific': 'Solanum carolinense', 'Priority': 1}, 'French broom ': {'Scientific': 'Genista monspessulana', 'Priority': 1}, 'Garlic mustard ': {'Scientific': 'Alliaria petiolata', 'Priority': 1}, 'Jointed goatgrass ': {'Scientific': 'Aegilops cylindrical', 'Priority': 1}, 'Leafy spurge ': {'Scientific': 'Euphorbia virgatae', 'Priority': 1}, 'Mediterranean sage ': {'Scientific': 'Salvia aethiopis', 'Priority': 1}, 'Medusahead ': {'Scientific': 'Taeniatherum caput-medusae', 'Priority': 1}, 'Myrtle spurge ': {'Scientific': 'Euphorbia myrsinites', 'Priority': 1}, 'Oblong spurge ': {'Scientific': 'Euphorbia oblongata', 'Priority': 1}, 'Purple starthistle': {'Scientific': 'Centaurea calcitrap', 'Priority': 1}, 'Spanish broom': {'Scientific': 'Spartium junceum', 'Priority': 1}, 'Squarrose knapweed ': {'Scientific': 'Centaurea virgata  ', 'Priority': 1}, 'Stinkwort ': {'Scientific': 'Dittrichia graveolens', 'Priority': 1}, 'Tamarisk/saltcedar ': {'Scientific': 'Tamarix spp.', 'Priority': 1}, 'Black locust ': {'Scientific': 'Robinia pseudoacacia', 'Priority': 2}, 'Bouncing Bet ': {'Scientific': 'Saponaria officinalis', 'Priority': 2}, 'Canada thistle': {'Scientific': 'Cirsium arvense', 'Priority': 2}, 'Diffuse knapweed ': {'Scientific': 'Centaurea diffusa', 'Priority': 2}, 'Dyer’s woad ': {'Scientific': 'Isatis tinctoria', 'Priority': 2}, 'Hairy whitetop ': {'Scientific': 'Lepidium appelianum', 'Priority': 2}, 'Whitetop': {'Scientific': 'Lepidium draba', 'Priority': 2}, 'Himalayan blackberry ': {'Scientific': 'Rubus armeniacus', 'Priority': 2}, 'Hoary alyssum ': {'Scientific': 'Berteroa incana', 'Priority': 2}, 'Hoary cress ': {'Scientific': 'Cardaria species', 'Priority': 2}, 'Musk thistle  ': {'Scientific': 'Carduus nutans', 'Priority': 2}, 'Poison hemlock': {'Scientific': 'Conium maculatum', 'Priority': 2}, 'Purple loosestrife ': {'Scientific': 'Lythrum salicaria', 'Priority': 2}, 'Reed canarygrass ': {'Scientific': 'Phalaris arundinacea', 'Priority': 2}, 'Rush skeletonweed ': {'Scientific': 'Chondrilla juncea', 'Priority': 2}, 'Russian knapweed ': {'Scientific': 'Acroptilon repens', 'Priority': 2}, 'Russian thistle ': {'Scientific': 'Salsola tragus  ', 'Priority': 2}, 'Spotted knapweed ': {'Scientific': 'Centaurea stoebe ssp. Micranthos', 'Priority': 2}, 'Sulfur cinquefoil': {'Scientific': 'Potentilla recta', 'Priority': 2}, 'Teasel ': {'Scientific': 'Dipsacus fullonum', 'Priority': 2}, 'Tree of Heaven ': {'Scientific': 'Ailanthus altissima  ', 'Priority': 2}, 'Yellow starthistle ': {'Scientific': 'Centaurea solstitialis', 'Priority': 2}, 'Puncturevine ': {'Scientific': 'Tribulus terrestris', 'Priority': 2}, 'Scotch thistle ': {'Scientific': 'Onopordum acanthium', 'Priority': 2}, 'Bull thistle': {'Scientific': 'Cirsium vulgare', 'Priority': 3}, 'Scotch broom ': {'Scientific': 'Cytisus scoparius', 'Priority': 3}, 'Common bindweed ': {'Scientific': 'Convolvulus arvensis', 'Priority': 3}, 'Dalmatian toadflax ': {'Scientific': 'Linaria dalmatica', 'Priority': 3}, 'Everlasting peavine ': {'Scientific': 'Lathyrus latifolius', 'Priority': 3}, 'Klamathweed ': {'Scientific': 'Hypericum perforatum', 'Priority': 3}, 'Perennial pepperweed ': {'Scientific': 'Lepidium latifolium', 'Priority': 3}, 'Yellow toadflax ': {'Scientific': 'Linaria vulgaris', 'Priority': 3}, 'Cheatgrass ': {'Scientific': 'Bromus tectorum', 'Priority': 4}, 'False salsify': {'Scientific': 'Tragopogon dubius', 'Priority': 4}, 'Oxeye daisy ': {'Scientific': 'Leucanthemum vulgare', 'Priority': 4}, 'Quackgrass ': {'Scientific': 'Elymus repens', 'Priority': 4}, 'White sweetclover ': {'Scientific': 'Melilotus alba', 'Priority': 4}, 'Wooly mullein ': {'Scientific': 'Verbascum thapsus', 'Priority': 4}, 'Yellow sweetclover ': {'Scientific': 'Melilotus officinalis', 'Priority': 4}, 'Common mullein': {'Scientific': 'Verbascum thapsus', 'Priority': 4}, 'Common st. johnswort': {'Scientific': 'Hypericum perforatum', 'Priority': 3}, 'Nodding plumeless thistle': {'Scientific': 'Carduus nutans', 'Priority': 2}, 'Sweetclover ': {'Scientific': 'Melilotus', 'Priority': 4}, 'Broadleaf Pepperweed': {'Scientific': 'Lepidium latifolium', 'Priority': 3}, 'Field Bindweed': {'Scientific': 'Convolvulus arvensis', 'Priority': 3}, 'Sulphur cinquefoil': {'Scientific': 'Potentilla recta', 'Priority': 2}, 'Broadleaved pepperweed': {'Scientific': 'Lepidium latifolium', 'Priority': 3}, 'Reed canary grass ': {'Scientific': 'Phalaris arundinacea', 'Priority': 2}}\n"
     ]
    }
   ],
   "source": [
    "print(Invasives_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the Excel file into a DataFrame\n",
    "csv_data = pd.read_csv(r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Invasives Priority lookup.csv\") \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "Invasives_lookup = {}\n",
    "\n",
    "key = 'Common'\n",
    "values = ['Scientific', 'Priority'] \n",
    "\n",
    "Invasives_lookup= csv_data.set_index(key)[values].to_dict(orient='index')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of 'plant_type' column in DataFrame: object\n",
      "Data type of value for key Barb goatgrass in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Carolina horsenettle  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key French broom  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Garlic mustard  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Jointed goatgrass  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Leafy spurge  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Mediterranean sage  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Medusahead  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Myrtle spurge  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Oblong spurge  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Purple starthistle in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Spanish broom in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Squarrose knapweed  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Stinkwort  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Tamarisk/saltcedar  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Black locust  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Bouncing Bet  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Canada thistle  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Diffuse knapweed  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Dyer’s woad  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Hairy whitetop  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Whitetop in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Himalayan blackberry  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Hoary alyssum  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Hoary cress  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Musk thistle   in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Poison hemlock in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Purple loosestrife  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Reed canarygrass  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Rush skeletonweed  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Russian knapweed  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Russian thistle  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Spotted knapweed  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Sulfur cinquefoil in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Teasel  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Tree of Heaven  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Yellow starthistle  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Puncturevine  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Scotch thistle  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Bull thistle in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Scotch broom  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Common bindweed  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Dalmatian toadflax  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Everlasting peavine  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Klamathweed  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Perennial pepperweed  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Yellow toadflax  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Cheatgrass  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key False salsify in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Oxeye daisy  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Quackgrass  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key White sweetclover  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Wooly mullein  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Yellow sweetclover  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Common mullein in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Common st. johnswort in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Nodding plumeless thistle in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Sweetclover  in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Broadleaf Pepperweed in lookup dictionary: <class 'dict'>\n",
      "Data type of value for key Field Bindweed in lookup dictionary: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Check the data type of 'plant_type' column in the DataFrame\n",
    "print(\"Data type of 'plant_type' column in DataFrame:\", invasivedf['plant_type'].dtype)\n",
    "\n",
    "# Check the data type of values in the lookup dictionary\n",
    "for key, value in Invasives_lookup.items():\n",
    "    print(\"Data type of value for key\", key, \"in lookup dictionary:\", type(value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before assignment: {}\n",
      "After assignment: {'Barb goatgrass': {'Scientific': 'Aegilops triuncialis', 'Priority': 1}, 'Carolina horsenettle ': {'Scientific': 'Solanum carolinense', 'Priority': 1}, 'French broom ': {'Scientific': 'Genista monspessulana', 'Priority': 1}, 'Garlic mustard ': {'Scientific': 'Alliaria petiolata', 'Priority': 1}, 'Jointed goatgrass ': {'Scientific': 'Aegilops cylindrical', 'Priority': 1}, 'Leafy spurge ': {'Scientific': 'Euphorbia virgatae', 'Priority': 1}, 'Mediterranean sage ': {'Scientific': 'Salvia aethiopis', 'Priority': 1}, 'Medusahead ': {'Scientific': 'Taeniatherum caput-medusae', 'Priority': 1}, 'Myrtle spurge ': {'Scientific': 'Euphorbia myrsinites', 'Priority': 1}, 'Oblong spurge ': {'Scientific': 'Euphorbia oblongata', 'Priority': 1}, 'Purple starthistle': {'Scientific': 'Centaurea calcitrap', 'Priority': 1}, 'Spanish broom': {'Scientific': 'Spartium junceum', 'Priority': 1}, 'Squarrose knapweed ': {'Scientific': 'Centaurea virgata  ', 'Priority': 1}, 'Stinkwort ': {'Scientific': 'Dittrichia graveolens', 'Priority': 1}, 'Tamarisk/saltcedar ': {'Scientific': 'Tamarix spp.', 'Priority': 1}, 'Black locust ': {'Scientific': 'Robinia pseudoacacia', 'Priority': 2}, 'Bouncing Bet ': {'Scientific': 'Saponaria officinalis', 'Priority': 2}, 'Canada thistle ': {'Scientific': 'Cirsium arvense', 'Priority': 2}, 'Diffuse knapweed ': {'Scientific': 'Centaurea diffusa', 'Priority': 2}, 'Dyer’s woad ': {'Scientific': 'Isatis tinctoria', 'Priority': 2}, 'Hairy whitetop ': {'Scientific': 'Lepidium appelianum', 'Priority': 2}, 'Whitetop': {'Scientific': 'Lepidium draba', 'Priority': 2}, 'Himalayan blackberry ': {'Scientific': 'Rubus armeniacus', 'Priority': 2}, 'Hoary alyssum ': {'Scientific': 'Berteroa incana', 'Priority': 2}, 'Hoary cress ': {'Scientific': 'Cardaria species', 'Priority': 2}, 'Musk thistle  ': {'Scientific': 'Carduus nutans', 'Priority': 2}, 'Poison hemlock': {'Scientific': 'Conium maculatum', 'Priority': 2}, 'Purple loosestrife ': {'Scientific': 'Lythrum salicaria', 'Priority': 2}, 'Reed canarygrass ': {'Scientific': 'Phalaris arundinacea', 'Priority': 2}, 'Rush skeletonweed ': {'Scientific': 'Chondrilla juncea', 'Priority': 2}, 'Russian knapweed ': {'Scientific': 'Acroptilon repens', 'Priority': 2}, 'Russian thistle ': {'Scientific': 'Salsola tragus  ', 'Priority': 2}, 'Spotted knapweed ': {'Scientific': 'Centaurea stoebe ssp. Micranthos', 'Priority': 2}, 'Sulfur cinquefoil': {'Scientific': 'Potentilla recta', 'Priority': 2}, 'Teasel ': {'Scientific': 'Dipsacus fullonum', 'Priority': 2}, 'Tree of Heaven ': {'Scientific': 'Ailanthus altissima  ', 'Priority': 2}, 'Yellow starthistle ': {'Scientific': 'Centaurea solstitialis', 'Priority': 2}, 'Puncturevine ': {'Scientific': 'Tribulus terrestris', 'Priority': 2}, 'Scotch thistle ': {'Scientific': 'Onopordum acanthium', 'Priority': 2}, 'Bull thistle': {'Scientific': 'Cirsium vulgare', 'Priority': 3}, 'Scotch broom ': {'Scientific': 'Cytisus scoparius', 'Priority': 3}, 'Common bindweed ': {'Scientific': 'Convolvulus arvensis', 'Priority': 3}, 'Dalmatian toadflax ': {'Scientific': 'Linaria dalmatica', 'Priority': 3}, 'Everlasting peavine ': {'Scientific': 'Lathyrus latifolius', 'Priority': 3}, 'Klamathweed ': {'Scientific': 'Hypericum perforatum', 'Priority': 3}, 'Perennial pepperweed ': {'Scientific': 'Lepidium latifolium', 'Priority': 3}, 'Yellow toadflax ': {'Scientific': 'Linaria vulgaris', 'Priority': 3}, 'Cheatgrass ': {'Scientific': 'Bromus tectorum', 'Priority': 4}, 'False salsify': {'Scientific': 'Tragopogon dubius', 'Priority': 4}, 'Oxeye daisy ': {'Scientific': 'Chrysanthemum leucanthemum', 'Priority': 4}, 'Quackgrass ': {'Scientific': 'Elymus repens', 'Priority': 4}, 'White sweetclover ': {'Scientific': 'Melilotus alba', 'Priority': 4}, 'Wooly mullein ': {'Scientific': 'Verbascum thapsus', 'Priority': 4}, 'Yellow sweetclover ': {'Scientific': 'Melilotus officinalis', 'Priority': 4}, 'Common mullein': {'Scientific': 'Verbascum thapsus', 'Priority': 4}, 'Common st. johnswort': {'Scientific': 'Hypericum perforatum', 'Priority': 3}, 'Nodding plumeless thistle': {'Scientific': 'Carduus nutans', 'Priority': 2}, 'Sweetclover ': {'Scientific': nan, 'Priority': 4}, 'Broadleaf Pepperweed': {'Scientific': 'Lepidium latifolium', 'Priority': 3}, 'Field Bindweed': {'Scientific': 'Convolvulus arvensis', 'Priority': 3}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "csv_data = pd.read_csv(r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Invasives Priority lookup.csv\")\n",
    "\n",
    "# Define an empty dictionary for the lookup\n",
    "invasives_lookup = {}\n",
    "\n",
    "# Print a message before the assignment\n",
    "print(\"Before assignment:\", invasives_lookup)\n",
    "\n",
    "# Iterate over rows in the CSV data\n",
    "for index, row in csv_data.iterrows():\n",
    "    common_name = row['Common']\n",
    "    scientific_name = row['Scientific']\n",
    "    priority = int(row['Priority'])\n",
    "    \n",
    "    # Assign values to the dictionary\n",
    "    invasives_lookup[common_name] = {'Scientific': scientific_name, 'Priority': priority}\n",
    "\n",
    "# Print a message after the assignment\n",
    "print(\"After assignment:\", invasives_lookup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HEadcuts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Headcuts\n",
    "#--------------------------------#\n",
    "#Get Data from GDB's with assessment unit name \n",
    "#--------------------------------#\n",
    "\n",
    "# Paths to the feature classes\n",
    "headcut19fc = os.path.join(headcut19gdb, \"Stream_Headcut_2019\")\n",
    "headcut20fc = os.path.join(headcut20gdb, \"Stream_Headcut_2020\")\n",
    "headcut22fc = os.path.join(headcut22gdb, \"Stream_Headcut_2022\")\n",
    "headcut23fc = os.path.join(headcut23gdb, \"sez_stream_headcut\")\n",
    "\n",
    "\n",
    "#Use gdb because they have assessment unit name in them... 2022 i had to manually add in PRO\n",
    "headcut23fields = ['Assessment_Unit', 'headcut_depth', 'created_date']\n",
    "headcut22fields = ['Assessment_Unit', 'Headcut_Depth', 'synced_date']\n",
    "headcut20fields = ['Assessment_Unit_Name', 'Headcut_Depth','Survey_Date']\n",
    "headcut19fields = ['SITE_NAME', 'HEADCUT_DEPTH', 'SURVEY_DATE' ]\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "\n",
    "# Function to read feature class into DataFrame\n",
    "def feature_class_to_dataframe(feature_class, fields):\n",
    "    data = [row for row in arcpy.da.SearchCursor(feature_class, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Read feature classes into DataFrames\n",
    "# Read feature classes into DataFrames\n",
    "headcut19df = feature_class_to_dataframe(headcut19fc, headcut19fields)\n",
    "headcut20df = feature_class_to_dataframe(headcut20fc, headcut20fields)\n",
    "headcut22df = feature_class_to_dataframe(headcut22fc, headcut22fields)\n",
    "headcut23df = feature_class_to_dataframe(headcut23fc, headcut23fields)\n",
    "\n",
    "# Rename fields\n",
    "headcut19df.rename(columns={'SITE_NAME': 'Assessment_Unit_Name', 'HEADCUT_DEPTH': 'headcut_depth', 'SURVEY_DATE': 'created_date'}, inplace=True)\n",
    "headcut20df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'Headcut_Depth': 'headcut_depth', 'Survey_Date': 'created_date'}, inplace=True)\n",
    "headcut22df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'Headcut_Depth': 'headcut_depth', 'synced_date': 'created_date'}, inplace=True)\n",
    "headcut23df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'headcut_depth': 'headcut_depth', 'created_date': 'created_date'}, inplace=True)\n",
    "\n",
    "# Concatenate DataFrames\n",
    "headcutdf = pd.concat([headcut19df, headcut20df, headcut22df, headcut23df], ignore_index=True)\n",
    "\n",
    "# Display merged DataFrame\n",
    "print(headcutdf)\n",
    "\n",
    "#hopefully this is just one time\n",
    "\n",
    "\n",
    "#--------------------------------#\n",
    "#Get Data from exported table where Assessment Unit is joined---parentglobalids don't match up for years before 2023. need to redo sde.collect 2019-2022 data append.... \n",
    "#--------------------------------#\n",
    "\n",
    "#--------------------------------#\n",
    "#Get Data from SDE.Collect...instead of GDB\n",
    "#--------------------------------#\n",
    "#----------------------------------------------------------------#\n",
    "#Add correct info to dataframe\n",
    "#----------------------------------------------------------------#\n",
    "# Create a new column 'SEZ ID'\n",
    "#This code is for the excel look up dictionary---ADD last? this is goo dfor QA on assessment unit names because 0's tell you the name is wrong\n",
    "headcutdf['SEZ_ID'] = headcutdf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "headcutdf['SEZ_ID'] = headcutdf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "headcutdf['SEZ_ID'] = headcutdf['SEZ_ID'].astype(int)\n",
    "headcutdf['Assessment_Unit_Name'] = headcutdf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky meadows - 1': 'Sky Meadows'})\n",
    "\n",
    "#calculate year column \n",
    "headcutdf['Year'] = headcutdf['created_date'].dt.year\n",
    "\n",
    "#----------------------------------------------#\n",
    "# Process Data\n",
    "#----------------------------------------------#\n",
    "# assign small, medium, large to headcut\n",
    "headcutdf['Headcut_Size']=headcutdf['headcut_depth'].apply(categorize_headcut)\n",
    "\n",
    "\n",
    "# Group by 'SEZ_ID', 'Year', and 'Headcut_Size', and count the number of occurrences for each group\n",
    "headcut_summary = headcutdf.groupby(['Assessment_Unit_Name', 'Year', 'Headcut_Size']).size().reset_index(name='Count')\n",
    "\n",
    "print(type(headcut_summary))\n",
    "# Pivot the table to have 'Headcut_Size' categories as columns\n",
    "headcut_summary_sml = headcut_summary.pivot_table(index=['Assessment_Unit_Name', 'Year'], columns='Headcut_Size', values='Count', fill_value=0)\n",
    "\n",
    "# Reset the index to flatten the DataFrame\n",
    "headcut_summary_sml.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the summarized DataFrame\n",
    "print(headcut_summary_sml)\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score, # of Headcuts\n",
    "#----------------------------------------------------------------#\n",
    "    \n",
    "    # Process Data\n",
    "def rate_headcut(row):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if row['large'] >= 1:\n",
    "        return 'D'  # Assign score D\n",
    "    elif row['medium'] >= 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif row['small'] >= 1:\n",
    "       return 'B'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no headcuts)\n",
    "\n",
    "# Apply the rating function to the summary DataFrame\n",
    "headcut_summary_sml['Headcuts_Rating'] = headcut_summary_sml.apply(rate_headcut, axis=1)\n",
    "\n",
    "#Calculate total number of headcuts per sez per year\n",
    "headcut_summary_sml['Number_of_Headcuts']= headcut_summary_sml[['large', 'medium', 'small']].sum(axis=1)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "headcut_summary_sml['Headcuts_Score']= headcut_summary_sml['Headcuts_Rating'].apply(score_indicator)\n",
    "\n",
    "#Add Datasource\n",
    "headcut_summary_sml['Headcuts_Data_Source'] = 'TRPA'\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to incision called stage_incision GDB location\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Headcuts_Data_Source': 'Headcuts_Data_Source',\n",
    "    'Number_of_Headcuts': 'Number_of_Headcuts',\n",
    "    'Headcuts_Rating': 'Headcuts_Rating',\n",
    "    'Headcuts_Score': 'Headcuts_Score',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'small': 'small',\n",
    "    'medium': 'medium',\n",
    "    'large': 'large'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = headcut_summary_sml.rename(columns=field_mapping).drop(columns=[col for col in headcut_summary_sml.columns if col not in field_mapping])\n",
    "\n",
    "readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "with arcpy.da.InsertCursor(stage_headcuts, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in External Data from USFS and Calflora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------#\n",
    "#Get Data from external sources and spatially join to our data\n",
    "#--------------------------------#\n",
    "\n",
    "\n",
    "#Calfora Data\n",
    "CArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines07/MapServer/1\"\n",
    "#CAfc = os.path.join(CArest, \"Non-Native and Invasive Plants - Calflora\")\n",
    "CAsdf = pd.DataFrame.spatial.from_layer(CArest)\n",
    "# Delete the existing feature layer if it exists\n",
    "\n",
    "CA_fc = arcpy.MakeFeatureLayer_management(CArest, \"Calfora_invasives\")\n",
    "\n",
    "\n",
    "usfsrest= \"https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer/0\"\n",
    "#usfsfc =os.path.join(usfsrest, \"Current Invasive Plant Locations\") \n",
    "#usfssdf = pd.DataFrame.spatial.from_layer(usfsrest)\n",
    "if arcpy.Exists(\"usfs_layer\"):\n",
    "    arcpy.Delete_management(\"usfs_layer\")\n",
    "usfs_fc = arcpy.MakeFeatureLayer_management(usfsrest, \"usfs_layer\")\n",
    "#Spatially join usfs layers to see if any of the points or polygons intersect with our sez's123\n",
    "\n",
    "# Set the workspace environment\n",
    "arcpy.env.workspace = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Data Management 2023.gdb\"\n",
    "\n",
    "# Define the target and join layers\n",
    "target_feature = SEZ_Master #\"AssessmentUnit_Master\"\n",
    "CAjoin_feature = CA_fc\n",
    "USFSjoin_feature = usfs_fc\n",
    "\n",
    "# Define the output feature class\n",
    "out_feature_class = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Data Management 2023.gdb\\externalinvasivesez\"\n",
    "\n",
    "# Define the fields you want to keep from both layers\n",
    "#field_mappings = arcpy.FieldMappings()\n",
    "#field_mappings.addTable(target_features)\n",
    "#field_mappings.addTable(join_features)\n",
    "\n",
    "# Keep only specific fields from the target layer\n",
    "#keep_fields_target = [\"Assessment_Unit_Name\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_target:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "#Keep only specific fields from the calfora join layer\n",
    "#keep_fields_CA = [\"CName\", \"SName\", \"Source\", \"Obs_Date\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_CA:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "# Keep only specific fields from the usfs join_features\n",
    "#keep_fields_USFS = [\"SCIENTIFIC_NAME\", \"COMMON_NAME\", \"DATE_COLLECTED\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_USFS:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "\n",
    "# Perform spatial join with specified field mappings\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=SEZ_Master,\n",
    "    join_features=[CAjoin_feature],\n",
    "    out_feature_class=out_feature_class,\n",
    "    join_operation=\"JOIN_ONE_TO_MANY\",\n",
    "    join_type=\"KEEP_ALL\"\n",
    "    #field_mapping=field_mappings\n",
    ")\n",
    "\n",
    "#Create Dataframe for spatially joined data\n",
    "# Define the fields you want to retrieve from the output feature class\n",
    "fields = [\"Assessment_Unit_Name\", \"CName\", \"SName\", \"Source\", \"Obs_Date\", \"SCIENTIFIC_NAME\", \"COMMON_NAME\", \"DATE_COLLECTED\"]\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "data = []\n",
    "\n",
    "# Use SearchCursor to read data from the output feature class\n",
    "with arcpy.da.SearchCursor(out_feature_class, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the list of tuples into a pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bioassessment/ Biotic Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Could not load the dataset: 'str' object has no attribute 'filter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\arcgis\\features\\geo\\_accessor.py\u001b[0m in \u001b[0;36mfrom_layer\u001b[1;34m(layer)\u001b[0m\n\u001b[0;32m   3102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3103\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfrom_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3104\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\arcgis\\features\\geo\\_io\\serviceops.py\u001b[0m in \u001b[0;36mfrom_layer\u001b[1;34m(layer, query)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \"\"\"\n\u001b[1;32m--> 181\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'filter'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22940\\898155792.py\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Convert the data to a Pandas DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mstreamsdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGeoAccessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstreamdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#streamsedf = GeoAccessor.from_xy(streamdata, x_column='LONGITUDE', y_column='LATITUDE')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\arcgis\\features\\geo\\_accessor.py\u001b[0m in \u001b[0;36mfrom_layer\u001b[1;34m(layer)\u001b[0m\n\u001b[0;32m   3110\u001b[0m             )\n\u001b[0;32m   3111\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3112\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Could not load the dataset: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3114\u001b[0m     \u001b[1;31m# ----------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Could not load the dataset: 'str' object has no attribute 'filter'"
     ]
    }
   ],
   "source": [
    "#Bioassessment scores- get all stream data into sde.Stream first... then look at Biotic Integrit Data Source to find which stream was used to evaluate each meadow.. this will help with percent of stream miles I believe\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Get Data-- all the data we need is in sde.collect.sde.survey.sde.sez_survey\n",
    "#----------------------------------------------------------------##Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "\n",
    "#Create SEDF setup\n",
    "#streamdata is the path to the feature class in sde\n",
    "#stream_fl = FeatureLayer(streamdata)\n",
    "#Get CSCI scores for streams\n",
    "#streamfields = ['SITE_NAME', 'COUNT_VALUE', 'YEAR_OF_COUNT', 'LATITUDE', 'LONGITUDE']\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "streamsdf = GeoAccessor.from_layer(streamdata)\n",
    "\n",
    "#streamsedf = GeoAccessor.from_xy(streamdata, x_column='LONGITUDE', y_column='LATITUDE')\n",
    "\n",
    "\n",
    "#spatial join this layer to asessment unit master layer\n",
    "#ASsessment unit master layer is called SEZ_Master\n",
    "\n",
    "#spatial join of sez to stream data\n",
    "#bioticsdf = SEZsdf.spatial.join(streamsdf, how='inner')\n",
    "#----------------------------------------------------------------#\n",
    "#Add correct info to dataframe\n",
    "#----------------------------------------------------------------#\n",
    "#use this until we fix the domain\n",
    "#bioticsdf['Assessment_Unit_Name'] = bioticsdf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh'})\n",
    "# Create a new column 'SEZ ID'\n",
    "#This code is for the csv look up dictionary\n",
    "#bioticsdf['SEZ_ID'] = bioticsdf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "#bioticsdf['SEZ_ID'] = bioticsdf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "#bioticsdf['SEZ_ID'] = bioticsdf['SEZ_ID'].astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conifer Encroachment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aquatic Organism /Fish Passage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code to make it a spatially enabled dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to conver to spatially enable geodataframe\n",
    "# Create a new DataFrame Final_df with columns based on the field mapping\n",
    "#df = df.rename(columns=field_mapping)\n",
    "\n",
    "# Convert DataFrame to Spatially Enabled DataFrame\n",
    "#sedf = GeoAccessor.from_xy(df, x_column='LONGITUDE', y_column='LATITUDE')\n",
    "\n",
    "# Convert the SEDF to a feature class without sanitizing columns\n",
    "#sedf.spatial.to_featureclass(location=os.path.join(workspace, 'PlanAreaNoise_23_Staging'), sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
