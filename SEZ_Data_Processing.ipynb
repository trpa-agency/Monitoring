{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install arcgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureSet, GeoAccessor, GeoSeriesAccessor, FeatureLayer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.geometry import SpatialReference\n",
    "#import geopandas as gpd to use spatial.reference stuff\n",
    "gis = GIS()\n",
    "# # Set Pandas display options to show all rows and columns\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# in memory output file path\n",
    "wk_memory = \"memory\" + \"\\\\\"\n",
    "\n",
    "\n",
    "# set workspace and sde connections \n",
    "#working_folder = \"C:\\GIS\"\n",
    "\n",
    "#workspace      = \"C:\\GIS\\Scratch.gdb\"\n",
    "#arcpy.env.workspace = \"C:\\GIS\\Scratch.gdb\"\n",
    "#workspace ='F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "#arcpy.env.workspace = 'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "\n",
    "# network path to connection files\n",
    "filePath = \"F:\\\\GIS\\\\DB_CONNECT\"\n",
    "\n",
    "#path to GDB's to update and master data\n",
    "master_path = \"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "SEZ_Master = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "#set workspace for connection to GDB\n",
    "#workspace=master_path\n",
    "# database file paths \n",
    "### SDE Collection New data collected is put into SDE.Survey under the indicator name\n",
    "### SDE Vector is where the data will go \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "#sdeBase    = os.path.join(filePath, \"SarahVector.sde\")\n",
    "#sdeCollect = os.path.join(filepath, \"SarahCollect.sde\")\n",
    "\n",
    "# setup connection string???\n",
    "#connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=sql12;DATABASE=sde;UID=sde;PWD=staff\"\n",
    "#connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "#engine = create_engine(connection_url)\n",
    "\n",
    "#with engine.begin() as sdeConnect:\n",
    " #   erosiondf      = pd.read_sql(\"SELECT * FROM sde.SDE.Stream_Erosion\", sdeConnect)\n",
    "\n",
    "# local variables sdata is starting data and f data is finishing datatables\n",
    "ffdata = os.path.join(sdeBase, \"sde.SDE.Monitoring\")\n",
    "sdata = os.path.join(sdeCollect, \"sde.SDE.Survey\")\n",
    "fdata = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "\n",
    "##Tables we get the data from in Collect 2010-2022 globalids don'tmatch\n",
    "sezsurveytable = os.path.join(sdata, \"sde.SDE.sez_survey\")\n",
    "erosiondata = os.path.join(sdata, \"sde.SDE.Stream_Erosion\")\n",
    "incisiondata = os.path.join(sdata, \"sde.SDE.sez_channel_incision\")\n",
    "invasivedata = os.path.join(sdata, \"sde.SDE.sez_invasive_plant\")\n",
    "headcutdata = os.path.join(sdata, \"sde.SDE.sez_stream_headcut\")\n",
    "\n",
    "#make this a spatial df\n",
    "streamdata = os.path.join(ffdata, \"sde.SDE.Stream\")\n",
    "\n",
    "\n",
    "#Staging Tables currently living in SEZ_Data.GDB\n",
    "stage_bank_stability = os.path.join(master_path, \"bank_stability\") \n",
    "stage_All_SEZ_Scores = os.path.join(master_path, \"All_SEZ_Scores\")\n",
    "stage_biotic_integrity = os.path.join(master_path, \"biotic_integrity\")\n",
    "stage_headcuts = os.path.join(master_path, \"headcuts_table\")\n",
    "stage_incision = os.path.join(master_path, \"incision\")\n",
    "stage_invasives = os.path.join(master_path, \"invasives\")\n",
    "stage_vegetation = os.path.join(master_path, \"vegetation_vigor\")\n",
    "stage_conifer = os.path.join(master_path, \"conifer_encroachment\")\n",
    "stage_aquatic = os.path.join(master_path, \"aquatic_organism_passage_table\")\n",
    "stage_ditches = os.path.join(master_path, \"ditches\")\n",
    "stage_habitat = os.path.join(master_path, \"habitat_fragmentation\")\n",
    "#Final table to append to\n",
    "#finalSEZtable = os.path.join(ffdata, \"sde.SDE.SEZ_Assessment_Unit\")\n",
    "#finalSEZtable = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "\n",
    "\n",
    "# network path to connection files??????\n",
    "#filePath = \"C:\\\\GIS\\\\DB_CONNECT\"\n",
    "# network path to connection files\n",
    "#filePath = r'F:\\Research and Analysis\\Workspace\\Sarah'\n",
    "\n",
    "#--------------------------------------------#\n",
    "#Notes to self\n",
    "#--------------------------------------------#\n",
    "#F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb. \n",
    "###'Assessment_Unit_Master' has all data for SEZ\n",
    "###“Bioassessment Sample Locations” has stream sites in SEZs\n",
    "\n",
    "gdbworking_folder = \"F:\\GIS\\GIS_DATA\\Monitoring\"\n",
    "headcutgdbfolder = os.path.join(gdbworking_folder, \"Stream_Headcut\", \"StreamHeadcut_Survey\")\n",
    "invasivegdbfolder = os.path.join(gdbworking_folder, \"Invasive_Species\", \"Invasive_Species_Survey\")\n",
    "## GDB with Raw Data straight from S123 not in the original folder (that one is not edited)\n",
    "headcut23gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2023.gdb\")\n",
    "headcut22gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2022.gdb\")\n",
    "headcut20gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2020.gdb\")\n",
    "headcut19gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2019.gdb\")\n",
    "#erosion23gdb = os.path.join(gdbworking_folder, \"Stream_Erosion\",\"Stream_Erosion_Survey\", \"Stream_Erosion_Survey_2023.gdb\")\n",
    "#erosion22gdb = os.path.join(working_folder, \"Stream_Erosion\",\"Stream_Erosion_Survey\", \"Stream_Erosion_Survey_2022.gdb\")\n",
    "#erosion20gdb = os.path.join(working_folder, \"Stream_Erosion\",\"Stream_Erosion_Survey\", \"Stream_Erosion_Survey_2020.gdb\")\n",
    "#channelincision23gdb = os.path.join(working_folder,\"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2023.gdb\")\n",
    "#channelincision22gdb = os.path.join(working_folder, \"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2022.gdb\")\n",
    "#channelincision20gdb = os.path.join(working_folder, \"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2020.gdb\")\n",
    "invasiveplant23gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2023.gdb\")\n",
    "invasiveplant22gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2022.gdb\")\n",
    "invasiveplant20gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2020.gdb\")\n",
    "invasiveplant19gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2019.gdb\")\n",
    "\n",
    "#This is thelocatoin for the final SEZ GDB to be updated in the gdb on f drive in the AssessmentUnits Master (polygon) i believe\n",
    "#FinalGDBtoupdate:F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data \\SEZ_Data.gdb\n",
    "\n",
    "#Location of USFS Invasive Species Data\n",
    "#https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer\n",
    "\n",
    "#Monitoring Dashboard location\n",
    "#Finalsdelocation:f'Vector.SDE' Sde.Monitoring Sde. SEZ_Assessment_Unit\n",
    "\n",
    "#Threshold Location? sde.tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't Use-Lookup Dictionary for SEZ ID's -Old code uses AssessmentUnit_Master feature class in SEZ_Data.gdb- don't use for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------#\n",
    "#Create look up dictionary for SEZ_ID fill in--currently works for stream erosion code--\n",
    "#-----------------------------------------------------------------------------------#\n",
    "#Add in SEZ_Survey table to match up parentglobalid to assessment unit name so we can match an sez_id\n",
    "#----------------------------------------------------------------##Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "# Set workspace environment\n",
    "arcpy.env.workspace = master_path\n",
    "\n",
    "# Specify the feature class name SEZ_Master\n",
    "#feature_class = \"AssessmentUnits_Master\"\n",
    "# Create a cursor to iterate over the rows in the feature class\n",
    "fields = ['Assessment_Unit_Name', 'SEZ_ID']\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "# Iterate over the rows in the second feature class and append to data list\n",
    "with arcpy.da.SearchCursor(SEZ_Master, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "# Convert the data to a Pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Define function to update SEZ ID lookup dictionary\n",
    "def update_SEZID_lookup_dict(df, lookup_dict):\n",
    "    for index, row in df.iterrows():\n",
    "        # Update SEZ_ID column in DataFrame with data from the lookup dictionary\n",
    "        df.at[index, 'SEZ_ID'] = lookup_dict.get(row['Assessment_Unit_Name'], None)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Assuming 'Assessment_Unit_Name' is the common identifier between DataFrame and lookup dictionary\n",
    "selected_columns = ['Assessment_Unit_Name', 'SEZ_ID']\n",
    "\n",
    "# Drop duplicates based on 'Assessment_Unit_Name' and keep the first occurrence\n",
    "unique_values = df.drop_duplicates(subset='Assessment_Unit_Name', keep='first')\n",
    "\n",
    "# Convert selected columns to dictionary\n",
    "lookup_dict = unique_values[selected_columns].set_index('Assessment_Unit_Name').to_dict(orient='index')\n",
    "\n",
    "# Display the dictionary\n",
    "print(lookup_dict)\n",
    "\n",
    "# Update 'SEZ_ID' in other DataFrame with data from the lookup dictionary do this for each indicator...\n",
    "#Other_df = update_SEZID_lookup_dict(Other_df, lookup_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------#\n",
    "# DONT USE YETT Create look up dictionary for SEZ_ID fill in with Global ID,SEZ ID, and Assessment Unit Name\n",
    "\n",
    "# Set workspace environment\n",
    "arcpy.env.workspace = master_path\n",
    "\n",
    "# Define the fields for SEZ_Master and the additional feature class\n",
    "Masterfields = ['Assessment_Unit_Name', 'SEZ_ID']\n",
    "surveyfields = ['assessment_unit_name', 'GlobalID']  # Add any additional fields you need\n",
    "\n",
    "# Create empty lists to store data\n",
    "Masterdata = []\n",
    "surveydata = []\n",
    "\n",
    "# Iterate over the rows in the SEZ_Master feature class and append to data list\n",
    "with arcpy.da.SearchCursor(SEZ_Master, Masterfields) as cursor:\n",
    "    for row in cursor:\n",
    "        Masterdata.append(row)\n",
    "\n",
    "# Iterate over the rows in the additional feature class and append to data list\n",
    "with arcpy.da.SearchCursor(sezsurveytable, surveyfields) as cursor:\n",
    "    for row in cursor:\n",
    "        surveydata.append(row)\n",
    "\n",
    "# Convert the data to Pandas DataFrames\n",
    "Masterdf = pd.DataFrame(Masterdata, columns=Masterfields)\n",
    "surveydf = pd.DataFrame(surveydata, columns=surveyfields)\n",
    "# Convert the column name in surveydf to match Masterdf\n",
    "surveydf = surveydf.rename(columns={'assessment_unit_name': 'Assessment_Unit_Name'})\n",
    "# Merge the DataFrames on 'Assessment_Unit_Name'\n",
    "SEZIDdf = pd.merge(Masterdf, surveydf, on='Assessment_Unit_Name', how='inner')\n",
    "\n",
    "# Check for missing values in 'GlobalID' or 'SEZ_ID'\n",
    "missing_values = SEZIDdf[SEZIDdf['GlobalID'].isna() | SEZIDdf['SEZ_ID'].isna()]\n",
    "\n",
    "# Replace missing values in 'SEZ_ID' with NaN\n",
    "SEZIDdf.loc[missing_values.index, 'SEZ_ID'] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(SEZIDdf)\n",
    "# Assuming 'Assessment_Unit_Name' is the common identifier between DataFrame and lookup dictionary\n",
    "selected_columns = ['Assessment_Unit_Name', 'SEZ_ID', 'GlobalID']\n",
    "\n",
    "# Drop duplicates based on 'Assessment_Unit_Name' and keep the first occurrence\n",
    "unique_values = SEZIDdf.drop_duplicates(subset='Assessment_Unit_Name', keep='first')\n",
    "\n",
    "# Convert selected columns to dictionary\n",
    "lookup_dict = unique_values[selected_columns].set_index('Assessment_Unit_Name').to_dict(orient='index')\n",
    "\n",
    "# Display the dictionary\n",
    "print(lookup_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEZ ID based off of excel lookup dictionary larger polygons only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Angora Creek - tributary': 519, 'Angora Creek - upper': 520, 'Angora meadows - 1': 87, 'Angora meadows - 2': 90, 'Angora meadows - 3': 91, 'Angora meadows - 4': 143, 'Angora meadows - 5': 144, 'Angora meadows - 6': 142, 'Angora meadows - 7': 89, 'Angora meadows - 8': 88, 'Angora meadows - 9': 92, 'Angora meadows tributary - 1': 217, 'Angora meadows tributary - 2': 99, 'Angora meadows tributary - 3': 97, 'Angora meadows tributary - 4': 94, 'Angora meadows tributary - 5': 214, 'Angora meadows tributary - 6': 146, 'Angora meadows tributary - 7': 93, 'Angora meadows tributary - 8': 95, 'Angora meadows tributary - 9': 96, 'Angora tributary': 446, 'Antone meadows': 187, 'Baldwin marsh - 1': 160, 'Benwood meadows - 1': 129, 'Benwood meadows - 2': 131, 'Big Meadow - 1': 47, 'Big Meadow - 2': 48, 'Big Meadow - 3': 38, 'Big Meadow - 4': 39, 'Big Meadow - 5': 37, 'Big Meadow - 6': 40, 'Big meadow - 7': 126, 'Big Meadow Creek - lower': 521, 'Big Meadow Creek - upper': 491, 'Big Meadow Creek - upper 2': 522, 'Bijou meadows - 1': 115, 'Bijou meadows - 2': 116, 'Bijou meadows - 3': 164, 'Bijou meadows - private': 114, 'Bijou meadows - tributary 1': 226, 'Bijou Park Creek meadows - 1': 219, 'Bijou Park Creek meadows - 2': 166, 'Bijou Park Creek meadows - 3': 167, 'Bijou Park Creek meadows - 4': 163, 'Bijou Park Creek meadows - 5': 489, 'Bijou Park Creek meadows - 6': 488, 'Blackwood Creek - lower 1': 523, 'Blackwood Creek - lower 2': 596, 'Blackwood Creek - middle 1': 597, 'Blackwood Creek - middle 2': 524, 'Blackwood Creek - middle 3': 598, 'Blackwood Creek - middle 4': 525, 'Blackwood Creek - Upper 1': 599, 'Blackwood Creek - Upper 2': 526, 'Blackwood Creek - upper 3': 527, 'Blackwood meadows - 1': 66, 'Blackwood meadows - 3': 65, 'Buck Lake meadows': 176, 'Buddhas meadow': 168, 'Burke Creek - middle': 528, 'Burke Creek - upper': 503, 'Burke Creek meadows - 1': 171, 'Burke Creek meadows - 2': 3, 'Burke Creek tributary': 461, 'Burton Creek - lower': 633, 'Burton Creek - upper': 529, 'Carnelian Canyon Creek - lower': 620, 'Carnelian Canyon Creek - upper': 621, 'Cascade Creek - lower': 501, 'Cascade Creek - upper': 498, 'Casino meadows': 170, 'Christmas Valley meadows - 1': 136, 'Christmas Valley meadows - 2': 134, 'Christmas Valley meadows - 3': 130, 'Christmas Valley meadows - 4': 225, 'Cold Creek - Highland Woods': 110, 'Cold Creek - middle': 530, 'Cold Creek - tributary 1': 532, 'Cold Creek - tributary 2': 533, 'Cold Creek - tributary 3': 531, 'Cold Creek - upper': 534, 'Cold Creek tributary - 4': 465, 'Cold Creek tributary - 5': 466, 'Colony Inn meadows - lower': 169, 'Colony Inn meadows - upper': 485, 'Cookhouse meadow': 128, 'Deer Creek - headwaters': 535, 'Deer Creek - lower': 537, 'Deer Creek - middle': 538, 'Deer Creek - middle 2': 539, 'Deer Creek - upper': 536, 'Deer Creek meadows': 30, 'Dollar Creek - lower': 540, 'Dollar Creek - upper': 512, 'Eagle Creek': 497, 'Echo Creek - below lake': 541, 'Echo Creek - upper': 494, 'Edgewood Creek - middle': 542, 'Edgewood Creek tributary - 2 - headwaters': 462, 'Edgewood Creek tributary - 2 - upper': 635, 'Edgewood Creek tributary - 2 - lower': 476, 'Edgewood Creek tributary - 3 - lower': 628, 'Edgewood Creek tributary - 3 - upper': 629, 'Edgewood meadows': 221, 'Elks Club meadows - 1': 141, 'Elks Club meadows - 2': 85, 'Fallen Leaf meadows - 1': 154, 'Fallen Leaf meadows - 2': 155, 'Fallen Leaf meadows - 3': 147, 'Fallen Leaf meadows - 4': 76, 'First Creek - lower': 543, 'First Creek - upper': 516, 'Freel Meadows - 1': 25, 'Freel Meadows - 2': 24, 'Gardner meadow': 150, 'General Creek - lower': 544, 'General Creek - middle': 506, 'General Creek - upper': 545, 'General Creek meadows': 61, 'Ginny Lake Meadows': 193, 'Glen Alpine Creek - lower': 546, 'Glen Alpine Creek - upper': 606, 'Glenbrook Creek - middle': 510, 'Glenbrook Creek - upper': 547, 'Glenbrook meadows - 1': 177, 'Glenbrook meadows - 2': 178, 'Golden Bear meadows - 1': 212, 'Golden Bear meadows - 2': 196, 'Grass Lake Creek': 609, 'Grass Lake meadow': 127, 'Griff Creek - lower': 514, 'Griff Creek - tributary': 548, 'Griff Creek - upper': 549, 'Griff Creek meadows': 35, 'Haypress Meadows': 50, 'Heavenly Valley Creek - middle': 550, 'Heavenly Valley Creek - upper': 499, 'Heavenly Valley Creek meadows - 1': 111, 'Heavenly Valley Creek meadows - 2': 112, 'Heavenly Valley Creek meadows - 3': 152, 'Heavenly Valley Creek meadows - 4': 113, 'Hell Hole meadows - 1': 230, 'Hell Hole Meadows - 2': 8, 'Hidden Valley Creek - lower': 551, 'Hidden Valley Creek - upper': 552, 'High meadows - 1': 203, 'High meadows - 2': 198, 'High Meadows - 3': 200, 'High meadows - 4': 202, 'High meadows - 6': 437, 'Homewood Canyon Creek - lower': 600, 'Homewood Canyon Creek - upper': 508, 'Incline Creek - lower': 553, 'Incline Creek - middle 1': 554, 'Incline Creek - middle 2': 555, 'Incline Creek - middle 3': 636, 'Incline Creek - ski run': 556, 'Incline Creek - upper': 557, 'Incline Lake meadows - 1': 192, 'Incline Lake meadows - 2': 190, 'Incline Village tributary - 1': 458, 'Incline Village tributary - 2': 459, 'Incline Village tributary - 3': 460, 'Incline Village tributary - 4': 457, 'Kahle meadows - 2': 204, 'Kahle meadows - 3': 172, 'Kahle meadows - 4': 468, 'Kahle meadows - 5': 118, 'Kahle meadows - 6': 469, 'Kahle meadows - 7': 119, 'Kahle meadows - 8': 486, 'Kingsbury meadows': 222, 'Lake Forest meadows - 1': 185, 'Lake Forest meadows - 2': 186, 'Lake Forest meadows - 3': 184, 'Lake Forest meadows - 4': 487, 'Lake Forest meadows - 5': 223, 'Lake Forest meadows - 6': 183, 'Lake Forest tributary': 624, 'Lakeshore meadows': 72, 'Logan House Creek - lower': 558, 'Logan House Creek - upper': 507, 'Logan House meadow': 13, 'Lonely Gulch Creek - lower': 559, 'Lonely Gulch Creek - middle': 560, 'Lonely Gulch Creek - upper': 504, 'Madden Creek': 561, 'Marlette Creek - lower': 562, 'Marlette Creek - old dam site': 564, 'Marlette Creek - south fork (lower)': 642, 'Marlette Creek - south fork (upper)': 563, 'Marlette Creek - upper': 631, 'Marlette Lake meadows': 32, 'McFaul Creek - lower': 565, 'McFual meadow': 71, 'McKinney Creek - lower': 566, 'McKinney Creek - middle': 567, 'McKinney Creek - upper': 505, 'McKinney tributary - 1': 626, 'McKinney tributary - 2': 453, 'Meeks Bay meadows - 1': 205, 'Meeks Bay meadows - 2': 207, 'Meeks Bay meadows - 3': 36, 'Meeks Bay meadows - 4': 206, 'Meeks Creek - upper': 502, 'Meeks Bay Lagoon': 235, 'Meiss meadows - 1': 123, 'Meiss meadows - 2': 122, 'Meiss meadows - 3': 124, 'Meiss meadows - 4': 210, 'Meiss meadows - 5': 209, 'Meyers meadow': 218, 'Meyers tributary - 1': 447, 'Meyers tributary - 2': 467, 'Mill Creek - lower': 568, 'Mill Creek - upper': 515, 'Mill Creek meadows': 234, 'Mount Rainier Drive meadows - 1': 215, 'Mount Rainier Drive meadows - 2': 216, 'Muskawi Drive meadows': 83, 'North Logan House Creek': 509, 'North Logan House meadows': 12, 'North Zephyr Creek - lower': 570, 'North Zephyr Creek - middle': 615, 'North Zephyr Creek - tributary': 569, 'North Zephyr Creek - upper': 571, 'Nottaway Drive meadows': 213, 'Osgood Creek - above road': 572, 'Osgood Creek - below road': 573, 'Osgood Swamp': 482, 'Paige meadows': 182, 'Pope marsh meadow': 483, 'Quail Creek - lower': 574, 'Quail Creek - upper': 575, 'Quail Creek meadow': 26, 'Rosewood Creek - lower': 576, 'Rosewood Creek - middle 1': 586, 'Rosewood Creek - middle 2': 587, 'Rosewood Creek - middle 3': 588, 'Rubicon Creek': 601, 'Rubicon Creek - tributary': 602, 'Rubicon Meadows': 60, 'Saxon Creek - headwaters': 495, 'Saxon Creek - upper': 641, 'Saxon Creek meadows - above Fountain Place 1': 2, 'Saxon Creek meadows - above Fountain Place 2': 102, 'Saxon Creek meadows - below Fountain Place': 1, 'Saxon Creek tributary meadows - 1': 101, 'Saxon Creek tributary meadows - 3': 100, 'Saxon Creek tributary meadows - 4': 140, 'Saxon Creek tributary meadows - 5': 197, 'Saxon Creek tributary meadows - 6': 139, 'Saxon Creek tributary meadows - 7': 231, 'Second Creek - lower': 591, 'Second Creek - lower 2': 592, 'Second Creek - middle': 593, 'Second Creek - upper': 517, 'Secret Harbor Creek - lower': 618, 'Secret Harbor Creek - upper': 619, 'Sierra Tract wetlands': 211, 'Ski Run meadows': 220, 'Sky meadows': 79, 'Skylandia SEZ': 622, 'Slaughterhouse Creek - lower': 614, 'Slaughterhouse Creek - middle': 616, 'Slaughterhouse Creek - upper': 617, 'Slaughterhouse Meadows - 1': 6, 'Slaughterhouse meadows - 2': 180, 'small meadow 1': 11, 'small meadow 10': 64, 'small meadow 100': 77, 'small meadow 105': 125, 'small meadow 111': 132, 'small meadow 112': 133, 'small meadow 113': 135, 'small meadow 116': 181, 'small meadow 13': 20, 'small meadow 14': 19, 'small meadow 15': 18, 'small meadow 16': 21, 'small meadow 17': 28, 'small meadow 19': 63, 'small meadow 2': 10, 'small meadow 20': 623, 'small meadow 21': 67, 'small meadow 22': 53, 'small meadow 23': 54, 'small meadow 24': 56, 'small meadow 25': 55, 'small meadow 26': 57, 'small meadow 27': 58, 'small meadow 28': 59, 'small meadow 29': 174, 'small meadow 3': 70, 'small meadow 30': 175, 'small meadow 32': 51, 'small meadow 35': 49, 'small meadow 36': 52, 'small meadow 40': 74, 'small meadow 5': 29, 'small meadow 50': 23, 'small meadow 51': 42, 'small meadow 52': 41, 'small meadow 54': 22, 'small meadow 56': 46, 'small meadow 57': 73, 'small meadow 58': 173, 'small meadow 59': 14, 'small meadow 6': 17, 'small meadow 7': 27, 'small meadow 8': 69, 'small meadow 82': 232, 'small meadow 9': 68, 'small meadow 92': 31, 'small meadow 93': 33, 'small meadow 95': 43, 'small meadow 96': 44, 'small meadow 98': 62, 'small meadow 99': 75, 'Snow Creek tributary - 1': 451, 'Snow Creek tributary - 2': 452, 'Snow Creek wetlands - 1': 188, 'Snow Creek wetlands - 2': 189, 'South Lake Tahoe - wetland 1': 229, 'South Lake Tahoe airport': 484, 'South Lake Tahoe tributary - 1': 463, 'South Lake Tahoe tributary - 2': 464, 'South Lake Tahoe tributary - 3': 627, 'Spooner meadows - 1': 445, 'Spooner meadows - 2': 431, 'Spooner meadows - 3': 120, 'Spooner Meadows - 4': 5, 'Spooner meadows - 5': 121, 'Star Lake meadows': 45, 'Susquehana meadows - 1': 108, 'Susquehana meadows - 2': 107, 'Tahoe City meadow': 224, 'Tahoe City tributary - 1': 449, 'Tahoe City tributary - 2': 450, 'Tahoe Island meadows - 1': 158, 'Tahoe Island meadows - 2': 156, 'Tahoe Keys': 162, 'Tahoe Paradise golf course': 632, 'Tahoe Valley meadows - 1': 153, 'Tahoe Valley meadows - 2': 228, 'Tahoe Vista meadows': 227, 'Tallac Creek - abv highway - 1': 334, 'Tallac Creek - abv highway - 2': 604, 'Tallac Creek - tributary': 500, 'Tallac marsh': 159, 'Tallac meadows': 157, 'Taylor Creek': 605, 'Taylor Creek marsh': 208, 'Third Creek - headwaters': 585, 'Third Creek - lower': 577, 'Third Creek - lower 2': 578, 'Third Creek - middle': 581, 'Third Creek - middle 1': 579, 'Third Creek - middle 2': 580, 'Third Creek - upper 1': 582, 'Third Creek - upper 2': 584, 'Third Creek - upper 3': 583, 'Third Creek meadows - 1': 191, 'Third Creek meadows - 3': 34, 'Third Creek meadows - 4': 195, 'Third Creek meadows - 6': 194, 'Third Creek meadows - 7': 16, 'Third Creek meadows - 8': 15, 'Trout Creek - Highland Woods': 109, 'Trout Creek - tributary 2': 613, 'Trout Creek - tributary 3': 612, 'Trout Creek - upper': 496, 'Trout Creek above Black Bart': 149, 'Trout Creek below Black Bart': 161, 'Trout Creek headwaters meadows - 1': 137, 'Trout Creek headwaters meadows - 2': 9, 'Trout Creek meadows - above Fountain Place': 103, 'Trout Creek meadows - above Pioneer 1': 148, 'Trout Creek meadows - above Pioneer 2': 106, 'Trout Creek meadows - above Pioneer 3': 105, 'Trout Creek meadows - above Pioneer 4': 104, 'Upper Truckee River - Meyers': 138, 'Upper Truckee River - Tahoe Paradise': 7, 'UTR - Airport reach': 82, 'UTR - Christmas Valley 1': 608, 'UTR - Christmas Valley 3': 493, 'UTR - golf course meadows': 86, 'UTR - Johnson meadows - 2': 151, 'UTR - Johnson meadows - 3': 81, 'UTR - middle': 492, 'UTR - Reach 5': 80, 'UTR - Reach 6': 84, 'UTR - tributary 1': 610, 'UTR - tributary 3': 611, 'UTR - upper': 490, 'UTR - Washoe Meadows': 607, 'UTR marsh - Trout Creek side': 165, 'UTR Marsh - UTR side': 78, 'Van Sickle meadows': 117, 'Ward Creek - lower': 594, 'Ward Creek - middle': 595, 'Ward Creek - upper': 511, 'Ward Creek meadow': 625, 'Washoan Blvd meadows': 145, 'Washoe State Parks meadow - 1': 4, 'Washoe State Parks meadow - 2': 98, 'Watson Creek': 513, 'West Shore tributary - 1': 455, 'West Shore tributary - 2 - lower': 639, 'West Shore tributary - 2 - upper': 448, 'West Shore tributary - 3': 456, 'West Shore tributary - 4': 454, 'Woods Creek - lower': 589, 'Woods Creek - middle': 590, 'Woods Creek - upper': 518, 'High meadows - 5': 201}\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------#\n",
    "#Create look up dictionary for SEZ_ID fill in--currently works for stream erosion code--- change to use excel so its not messy\n",
    "#-----------------------------------------------------------------------------------#\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "excel_data = pd.read_excel(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\SEZID lookup.xlsx\")  # Replace 'your_excel_file.xlsx' with the path to your Excel file\n",
    "\n",
    "#Define Empty look up dataframe\n",
    "lookup_dict = {}\n",
    "\n",
    "for index, row in excel_data.iterrows():\n",
    "    lookup_dict[row['Assessment_Unit_Name']] = row['SEZ_ID']\n",
    "\n",
    "# See dictionary where keys are Assessment Unit Names and values are SEZ IDs\n",
    "print(lookup_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# Display the dictionary using pprint\n",
    "pprint.pprint(lookup_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Grading each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grading for each parameter \n",
    "#Defining Grade for Bank Stability based on Erosiondf[percent_unstable]\n",
    "def categorize_erosion(Percent_Unstable):\n",
    "    if 0 <= Percent_Unstable < 5:\n",
    "        return 'A'\n",
    "    elif 5 <= Percent_Unstable < 20:\n",
    "        return 'B'\n",
    "    elif 20 <= Percent_Unstable < 50:\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'D'\n",
    "    \n",
    "#Scoring based off of grading - check this\n",
    "def score_indicator(Rating):\n",
    "    if  Rating == 'A':\n",
    "        return '12'\n",
    "    elif Rating == 'B':\n",
    "        return '9'\n",
    "    elif Rating == 'C':\n",
    "        return '6'\n",
    "    else:\n",
    "        return '3'\n",
    "\n",
    "#Define Grade for Incision based off of incisino ratio\n",
    "\n",
    "def categorize_incision(bankfull_ratio):\n",
    "    if 0 <= bankfull_ratio < 1.2:\n",
    "        return 'A'\n",
    "    elif 1.2 <= bankfull_ratio < 1.6:\n",
    "        return 'B'\n",
    "    elif 1.6 <= bankfull_ratio < 2.1:\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'D'\n",
    "\n",
    "#Define Grade for Bioassessment Score\n",
    "def categorize_csci(biotic_integrity):\n",
    "     if   biotic_integrity > 0.92:\n",
    "        return 'A'\n",
    "     elif 0.79 < biotic_integrity <= 0.92:\n",
    "        return 'B'\n",
    "     elif 0.62 < biotic_integrity <= 0.79:\n",
    "        return 'C'\n",
    "     else:\n",
    "        return 'D'\n",
    "\n",
    "#Define Priority List Level of Invasive Plant Species\n",
    "    \n",
    "\n",
    "# Process Data\n",
    "def rate_invasive(priority):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if (priority[3] + priority[4] == 1):\n",
    "        return 'B'  # Assign score D\n",
    "    elif (priority[3] + priority[4] == 2) or priority[1] == 1 or priority[2] == 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif (priority[3] + priority[4] >= 3)or priority[1] >= 2 or priority[2] >= 2 or (priority[1] + priority[2] >= 2):  \n",
    "       return 'D'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no invasives of any priority are present)\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Define Size for Headcut based off of headcut size\n",
    "##A = 0 headcut, B 1+small headcut\n",
    "def categorize_headcut(headcutdepth):\n",
    "    if pd.isnull(headcutdepth) or headcutdepth == 0:\n",
    "        return np.nan\n",
    "    elif 0.1 <= headcutdepth < 0.5:\n",
    "        return 'small'\n",
    "    elif 0.5 <= headcutdepth < 1:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'large'\n",
    "\n",
    "#define rating for headcut health per sez\n",
    "def rate_headcut(row):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if row['large'] >= 1:\n",
    "        return 'D'  # Assign score D\n",
    "    elif row['medium'] >= 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif row['small'] >= 1:\n",
    "       return 'B'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no headcuts)\n",
    "\n",
    "\n",
    "#define rating SEZ Rating\n",
    "def rate_SEZ(percent):\n",
    "    if 0 <= percent < .69:\n",
    "        return 'D'\n",
    "    elif .7 <= percent < 79:\n",
    "        return 'C'\n",
    "    elif 80 <= percent <= 89:\n",
    "        return 'B'\n",
    "    else:\n",
    "        return 'A'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Erosion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Get Data\n",
    "#----------------------------------------------------------------#\n",
    "#Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "erosionfields = ['Assessment_Unit_Name', 'Shape.STLength()', 'Bank_Type', 'Survey_Date']\n",
    "#erosiondf = pd.DataFrame.from_records(data=arcpy.da.SearchCursor(erosiondata, erosionfields), columns=erosionfields)\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    " # Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(erosiondata, erosionfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "erosiondf = pd.DataFrame(data, columns=erosionfields)\n",
    "\n",
    "# Replace NaN values in 'Assessment_Unit_Name' column with 'Skylandia SEZ'\n",
    "#erosiondf['Assessment_Unit_Name'] = erosiondf['Assessment_Unit_Name'].fillna('Skylandia SEZ')\n",
    "# Replace specific values in 'Assessment_Unit_Name' column\n",
    "erosiondf['Assessment_Unit_Name'] = erosiondf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh'})\n",
    "\n",
    "\n",
    "#Add SEZ_ID column using lookup dictionary\n",
    "#erosiondf['SEZ_ID'] = erosiondf['Assessment_Unit_Name'].map(lambda x: lookup_dict.get(x, {}).get('SEZ_ID', None))\n",
    "\n",
    "#This code is for the excel look up dictionary\n",
    "erosiondf['SEZ_ID'] = erosiondf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "erosiondf['SEZ_ID'] = erosiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "erosiondf['SEZ_ID'] = erosiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#erosiondf = erosiondf.merge(df, on='Assessment_Unit_Name', how='left')\n",
    "\n",
    "#calculate year column \n",
    "erosiondf['Year'] = erosiondf['Survey_Date'].dt.year\n",
    "\n",
    "# Replace 'both_banks' with 'Both Banks' in Bank_Type column\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['both_banks', 'Both banks'], 'Both Banks' )\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['one_bank', 'One bank'], 'One Bank')\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['no_bank', 'No bank'], 'No Bank')\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Process Data\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "# Initialize variables\n",
    "erosiondf['bank_multiplier'] = erosiondf['Bank_Type'].apply(lambda x: 2 if x == 'Both Banks' else (1 if x == 'One Bank' else 0))\n",
    "\n",
    "\n",
    "# Calculate the product of 'Shape.STLength()' and 'bank_multiplier' to get the eroded banks per row\n",
    "erosiondf['eroded_banks_per_row'] = erosiondf['Shape.STLength()'] * erosiondf['bank_multiplier']\n",
    "\n",
    "# Group by Assessment_Unit_Name and year and sum the lengths of banks for each unit to get total banks assessed\n",
    "erosiondf['banks_assessed_per_unit'] = erosiondf.groupby(['Assessment_Unit_Name', 'Year'])['Shape.STLength()'].transform('sum') * 2\n",
    "\n",
    "# Group by Assessment_Unit_Name and sum the eroded banks per row for each unit\n",
    "erosiondf['SEZ_total_eroded'] = erosiondf.groupby(['Assessment_Unit_Name', 'Year'])['eroded_banks_per_row'].transform('sum')\n",
    "\n",
    "# Calculate percent unstable Bank Stability per Assessment Unit\n",
    "erosiondf['Bank_Stability_Percent_Unstable'] = (erosiondf['SEZ_total_eroded'] / erosiondf['banks_assessed_per_unit']) * 100\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score\n",
    "#----------------------------------------------------------------#\n",
    "erosiondf['Bank_Stability_Rating']=erosiondf['Bank_Stability_Percent_Unstable'].apply(categorize_erosion)\n",
    "erosiondf['Bank_Stability_Score']= erosiondf['Bank_Stability_Rating'].apply(score_indicator)\n",
    "\n",
    "erosiondf['Bank_Stability_Data_Source'] = 'TRPA'\n",
    "\n",
    "erosiondf.head()\n",
    "\n",
    "print(erosiondf)\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to bank_stability called stage_bank_stability GDB location\n",
    "#----------------------------------------------------------------#\n",
    "# Define the name of the feature class\n",
    "feature_class_name = 'bank_stability'\n",
    "\n",
    "# Define the full path to the feature class\n",
    "feature_class_path = stage_bank_stability \n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Bank_Stability_Data_Source': 'Bank_Stability_Data_Source',\n",
    "    'Bank_Stability_Percent_Unstable': 'Bank_Stability_Percent_Unstable',\n",
    "    'Bank_Stability_Rating': 'Bank_Stability_Rating',\n",
    "    'Bank_Stability_Score': 'Bank_Stability_Score',\n",
    "    'SEZ_ID': 'SEZ_ID'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "bank_stabilitydf = erosiondf.rename(columns=field_mapping).drop(columns=[col for col in erosiondf.columns if col not in field_mapping])\n",
    "\n",
    "readydf = bank_stabilitydf.groupby(['SEZ_ID', 'Year']).first().reset_index()\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "# Fix data type of Year so it writes to table\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y', errors='coerce')\n",
    "\n",
    "# Setting the frequency to 'Y' for year\n",
    "#readydf['Year'] = readydf['Year'].dt.to_period('Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "#Write dataframe to fc in SEZ_Data.GDB\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(stage_bank_stability, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")\n",
    "\n",
    "\n",
    "#Write dataframe to sde.collect.bank.stability eventually, current code write it to GDB in SEZ_Data.GDB\n",
    "# Set environment workspace to your SDE connection file\n",
    "#arcpy.env.workspace = master_path\n",
    "\n",
    "# Convert DataFrame to Feature Class\n",
    "#output_feature_class = \"ErosionUpdate\"  # Name for the output feature class\n",
    "#output_fc_path = os.path.join(arcpy.env.workspace, output_feature_class)\n",
    "\n",
    "# Assuming your DataFrame is already converted to a feature class\n",
    "# Replace \"path_to_your_feature_class\" with the actual path to your feature class\n",
    "#arcpy.conversion.TableToTable(\"path_to_your_feature_class\", arcpy.env.workspace, output_feature_class)\n",
    "\n",
    "# Overwrite Feature Class in SDE\n",
    "# Replace \"path_to_your_dataframe\" with the actual path to your DataFrame\n",
    "#arcpy.management.CopyFeatures(\"path_to_your_dataframe\", output_fc_path)\n",
    "\n",
    "\n",
    "#print(ready_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(readydf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to CSV\n",
    "#csv_path = os.path.join(working_folder, \"Erosiondatamaster.csv\")\n",
    "#ready_df.to_csv(csv_path, index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Incision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Get Data-- all the data we need is in sde.collect.sde.survey.sde.sez_survey\n",
    "#----------------------------------------------------------------##Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "incisionfields = ['Assessment_Unit_Name', 'incision_ratio', 'survey_date']\n",
    "#incisiondf = pd.DataFrame.from_records(data=arcpy.da.SearchCursor(erosiondata, erosionfields), columns=erosionfields)\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    " # Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(sezsurveytable, incisionfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "incisiondf = pd.DataFrame(data, columns=incisionfields)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Add correct info to dataframe\n",
    "#----------------------------------------------------------------#\n",
    "#use this until we fix the domain\n",
    "incisiondf['Assessment_Unit_Name'] = incisiondf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh'})\n",
    "# Create a new column 'SEZ ID'\n",
    "#This code is for the excel look up dictionary\n",
    "incisiondf['SEZ_ID'] = incisiondf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#calculate year column \n",
    "incisiondf['Year'] = incisiondf['survey_date'].dt.year\n",
    "\n",
    "\n",
    "#incisiondf['SEZ ID'] = np.nan\n",
    "\n",
    "# Iterate through the rows in incisiondf\n",
    "#for index, row in incisiondf.iterrows():\n",
    " #   parentglobalid = row['parentglobalid']\n",
    "    \n",
    "      # Check if the parent_global_id exists in the lookup dictionary\n",
    "  #  if parentglobalid in lookup_dict:\n",
    "        # If it exists, retrieve the corresponding SEZ ID and fill it into SEZ ID column\n",
    "   #     corresponding_entry = lookup_dict[parentglobalid]\n",
    "    #    assert row['GlobalID'] == parentglobalid, \"ParentGlobalID does not match GlobalID in the lookup dictionary\"\n",
    "     #   incisiondf.at[index, 'SEZ_ID'] = corresponding_entry['SEZ_ID']\n",
    "\n",
    "# Display the updated incisiondf\n",
    "print(incisiondf)\n",
    "\n",
    "#Add SEZ_ID column using lookup dictionary\n",
    "#incisiondf['SEZ_ID'] = SEZIDdf['GlobalID'].map(lambda x: lookup_dict.get(x, {}).get('SEZ_ID', None))\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "#incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "#incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#incisiondf = incisiondf.merge(df, on='Assessment_Unit_Name', how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score\n",
    "#----------------------------------------------------------------#\n",
    "incisiondf['Incision_Rating']=incisiondf['incision_ratio'].apply(categorize_incision)\n",
    "incisiondf['Incision_Score']= incisiondf['Incision_Rating'].apply(score_indicator)\n",
    "\n",
    "incisiondf['Incision_Data_Source'] = 'TRPA'\n",
    "\n",
    "incisiondf.head()\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to incision called stage_incision GDB location\n",
    "#----------------------------------------------------------------#\n",
    "# Define the name of the feature class\n",
    "feature_class_name = 'incision'\n",
    "\n",
    "# Define the full path to the feature class\n",
    "feature_class_path = stage_incision \n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Incision_Data_Source': 'Incision_Data_Source',\n",
    "    'incision_ratio': 'Incision_Ratio',\n",
    "    'Incision_Rating': 'Incision_Rating',\n",
    "    'Incision_Score': 'Incision_Score',\n",
    "    'SEZ_ID': 'SEZ_ID'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "incisionfinaldf = incisiondf.rename(columns=field_mapping).drop(columns=[col for col in incisiondf.columns if col not in field_mapping])\n",
    "\n",
    "readydf = incisionfinaldf.groupby(['SEZ_ID', 'Year']).first().reset_index()\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "#Write dataframe to fc in SEZ_Data.GDB\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(stage_incision, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invasive Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calfora Data points--dates only go to 2012?\n",
    "# dates only go to 2012 hereCArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines07/MapServer/1\"\n",
    "#CArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines27/MapServer/0\"\n",
    "# Create a feature layer object\n",
    "#cafeature_layer = FeatureLayer(CArest)\n",
    "#Query caflora layer to that it only shows 2020-2023 or only county?\n",
    "#start_year = 'January 1, 2020'\n",
    "#end_year= 'December 31, 2023'\n",
    "# Convert to datetime objects\n",
    "#start_date = pd.to_datetime(start_year)\n",
    "#end_date = pd.to_datetime(end_year)\n",
    "\n",
    "#Query before or after??\n",
    "#query = (CAsdf['Date_']>= start_date) & (CAsdf['Date_']<= end_date)\n",
    "\n",
    "#CA_filtereddf = CAsdf.join[query].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code using REST Service- most likely will reuse this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from arcgis.features import FeatureLayer\n",
    "from utils import get_fs_data_spatial_query\n",
    "\n",
    "# Define the USFS REST endpoint\n",
    "usfsrest = \"https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer/0\"\n",
    "where    = \"FS_UNIT_ID = '0519'\"\n",
    "\n",
    "# Query the feature layer\n",
    "sdfUSFS = get_fs_data_spatial_query(usfsrest, where)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spatial join of sdf and sez master\n",
    "\n",
    "\n",
    "# Create the spatially enabled DataFrame (sdf) for target feature SEZ assessment units\n",
    "SEZsdf = pd.DataFrame.spatial.from_featureclass(SEZ_Master)\n",
    "#spatial reference stuff\n",
    "sdfUSFS.spatial.sr = SEZsdf.spatial.sr\n",
    "\n",
    "#CAsdf.spatial.set_spatial_reference(SEZsdf.spatial.sr)\n",
    "#perform spatial join\n",
    "usfsdata = SEZsdf.spatial.join(sdfUSFS, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(invasivedf['plant_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "#SET UP DATA WRANGLE\n",
    "#-----------------------------------------\n",
    "\n",
    "#Path to external data usfs with rest service--This assumes rest service is up to date to 2023\n",
    "\n",
    "# Paths to the feature classes\n",
    "invasive19fc = os.path.join(invasiveplant19gdb, \"Invasive_Species_2019\")\n",
    "invasive20fc = os.path.join(invasiveplant20gdb, \"Invasive_Species_2020\")\n",
    "invasive22fc = os.path.join(invasiveplant22gdb, \"Invasive_Species_2022\")\n",
    "invasive23fc = os.path.join(invasiveplant23gdb, \"sez_invasive_plant\")\n",
    "\n",
    "\n",
    "#Use gdb because they have assessment unit name in them... 2022 i had to manually add in PRO\n",
    "invasive23fields = ['Assessment_Unit', 'invasives_percent_cover','invasives_plant_type', 'Species_Level', 'invasive_type_other', 'created_date']\n",
    "invasive22fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover', 'InvasiveType_Other', 'Survey_Date']\n",
    "invasive20fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover',  'Other', 'Survey_Date']\n",
    "invasive19fields = ['SITE_ID', 'SURVEY_DATE', 'INVASIVE_PLANT', 'PERCENT_COVER' ]\n",
    "usfsfields = ['Assessment_Unit_Name', 'COMMON_NAME', 'SCIENTIFIC_NAME']\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "\n",
    "# Function to read feature class into DataFrame\n",
    "def feature_class_to_dataframe(feature_class, fields):\n",
    "    data = [row for row in arcpy.da.SearchCursor(feature_class, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Read feature classes into DataFrames\n",
    "# Read feature classes into DataFrames\n",
    "invasive19df = feature_class_to_dataframe(invasive19fc, invasive19fields)\n",
    "invasive20df = feature_class_to_dataframe(invasive20fc, invasive20fields)\n",
    "invasive22df = feature_class_to_dataframe(invasive22fc, invasive22fields)\n",
    "invasive23df = feature_class_to_dataframe(invasive23fc, invasive23fields)\n",
    "# Create a new DataFrame with only the specified columns\n",
    "# Convert SpatialDataFrame to DataFrame\n",
    "\n",
    "usfsdf = usfsdata[usfsfields]\n",
    "#usfsdf = usfsdata.drop(columns='SHAPE')\n",
    "\n",
    "print(usfsdf)\n",
    "#usfs23df = feature_class_to_dataframe(usfsdata, usfsfields)\n",
    "\n",
    "\n",
    "invasive19df['Source'] = 'TRPA'\n",
    "invasive20df['Source'] = 'TRPA'\n",
    "invasive22df['Source'] = 'TRPA'\n",
    "invasive23df['Source'] = 'TRPA'\n",
    "usfsdf['Source'] = 'USFS'\n",
    "\n",
    "#usfsdf['Year'] = '2023'\n",
    "\n",
    "\n",
    "# Rename fields\n",
    "invasive19df.rename(columns={'SITE_ID': 'Assessment_Unit_Name', 'INVASIVE_PLANT': 'plant_type', 'PERCENT_COVER': 'percent_cover', 'SURVEY_DATE': 'created_date','Source':'Source'}, inplace=True)\n",
    "invasive20df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date', 'Source':'Source'}, inplace=True)\n",
    "invasive22df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'InvasiveType_Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date','Source':'Source'}, inplace=True)\n",
    "invasive23df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'invasives_percent_cover': 'percent_cover', 'invasives_plant_type': 'plant_type', 'invasive_type_other': 'other', 'created_date': 'created_date','Source':'Source'}, inplace=True)\n",
    "usfsdf.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'COMMON_NAME': 'plant_type', 'Source':'Source'}, inplace=True)\n",
    "#Remove null plant types for usfs data\n",
    "usfsdf = usfsdf[~usfsdf['plant_type'].isna()]\n",
    "\n",
    "\n",
    "# Concatenate DataFrames\n",
    "invasivedf = pd.concat([usfsdf, invasive19df, invasive20df, invasive22df, invasive23df], ignore_index=True)\n",
    "# Display merged DataFrame\n",
    "print(invasivedf)\n",
    "\n",
    "\n",
    "#Define SEZ ID based on Assessment_Unit_Name for QA on SEZ Name \n",
    "invasivedf['SEZ_ID'] = invasivedf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "#invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].astype(int)\n",
    "invasivedf['Assessment_Unit_Name'] = invasivedf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky Meadows meadow - 1': 'Sky meadows'})\n",
    "# Remove rows where data source is USFS and plant type is NaN\n",
    "#invasivedf = invasivedf[~((invasivedf['Source'] == 'USFS') & invasivedf['plant_type'].isna())]\n",
    "\n",
    "# Set 'Year' column based on data source\n",
    "\n",
    "invasivedf.loc[invasivedf['Source'] == 'USFS', 'Year'] = '2023'\n",
    "invasivedf.loc[invasivedf['Source'] == 'TRPA', 'Year'] = invasivedf['created_date'].dt.year\n",
    "\n",
    "#invasivedf.reset_index(drop=True, inplace=True)\n",
    "invasivedf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#invasivedf = invasivedf.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip this\n",
    "invasivedf_values_unique = invasivedf.values.flatten()\n",
    "is_unique = len(invasivedf_values_unique) == len(set(invasivedf_values_unique))\n",
    "print(is_unique)\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = invasivedf[invasivedf.duplicated(keep=False)]\n",
    "\n",
    "# Display duplicate rows\n",
    "print(duplicate_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------#\n",
    "    #Prep Plant_type Data\n",
    "#---------------------------#\n",
    "#Make a dataframe to capture 'other' plants in trpa data and then add it to invasive df\n",
    "other_plants_df = invasivedf[['Source', 'Year', 'SEZ_ID', 'Assessment_Unit_Name', 'Year', 'other']].copy()\n",
    "\n",
    "#Get rid of Null values\n",
    "other_plants_df = other_plants_df[~other_plants_df['other'].isna()]\n",
    "\n",
    "#invasivedf.reset_index(drop=True, inplace=True)\n",
    "other_plants_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "#Rename 'other to plant_type\n",
    "other_plants_df.rename(columns={'other': 'plant_type'}, inplace=True)\n",
    "\n",
    "#invasivedf = invasivedf.drop_duplicates()\n",
    "\n",
    "\n",
    "# Concatenate other_plants_df with invasivedf JUST DO THIS MANUALLY \n",
    "#invasivedf = pd.concat([invasivedf, other_plants_df], ignore_index=True)\n",
    "\n",
    "#Append\n",
    "#invasivesdf=invasivedf.append(other_plants_df)\n",
    "#Concatenate the new DataFrame with the existing invasivedf\n",
    "#invasivedf = pd.concat([invasivedf, other_plants_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Replace various representations of null values with 'none'\n",
    "null_representations = ['<null>', '<Null>', '', 'NA', 'N/A', 'nan', 'NaN', 'None', 'NULL', None]\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace(null_representations, 'none')\n",
    "\n",
    "# Split plant types by comma and create new rows\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.split(pat=',')\n",
    "invasivedf = invasivedf.explode('plant_type')\n",
    "\n",
    "# Capitalize the first word and replace underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.split('_').str[0].str.capitalize() + ' ' + invasivedf['plant_type'].str.split('_').str[1:].str.join(' ')\n",
    "\n",
    "# Remove spaces after the words and capitalize the first word while replacing underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].astype(str)\n",
    "\n",
    "\n",
    "# Remove any spaces at the beginning or end of a word\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.strip()\n",
    "\n",
    "\n",
    "#---------------------#\n",
    "#These replacements are so the same weed with different common name doesn't get counted twice toward score MAY NEED TO ADD MORE DESCREPENCIES in coming years if more pop up or find more elopquent solution\n",
    "#---------------------#\n",
    "# Replace 'Common mullein' with 'Wooly mullein' in the 'plant_type' column \n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common mullein', 'Wooly mullein')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Nodding plumeless thistle', 'Musk thistle')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Field bindweed', 'Common bindweed')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common st. johnswort', 'Klamathweed')\n",
    "# Replace 'Broadleaf and leaved' with 'Perennial' in the 'plant_type' column\n",
    "# Define the replacements dictionary\n",
    "replacements = {'Broadleaf Pepperweed': 'Perennial pepperweed', 'Broadleaved pepperweed': 'Perennial pepperweed'}\n",
    "\n",
    "# Replace values in the 'plant_type' column using the dictionary\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace(replacements)\n",
    "\n",
    "#Replace Sulphur Cinquefoil with sulfur\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sulphur cinquefoil', 'Sulfur cinquefoil')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sweetclover', 'White sweetclover')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Reed canary grass', 'Reed canarygrass')\n",
    "#Replace\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Salt cedar', 'Tamarisk')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Butter and eggs', 'Yellow toadflax')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Canada cottonthistle', 'Canada thistle')\n",
    "# Replace empty strings or other placeholders with NaN\n",
    "#invasivedf['plant_type'] = invasivedf['plant_type'].replace('', np.nan)\n",
    "\n",
    "# Drop duplicates based on 'Assessment_Unit_Name' and 'Year' in the remaining DataFrame\n",
    "invasivedf = invasivedf.drop_duplicates(subset=['Assessment_Unit_Name', 'Year', 'plant_type'], keep='first')\n",
    "\n",
    "\n",
    "grouped_df = invasivedf.groupby(['Assessment_Unit_Name', 'Year'])['plant_type']\n",
    "\n",
    "# Aggregate the plant types into one column separated by commas\n",
    "combined_plant_types = grouped_df.apply(lambda x: ', '.join(x)).reset_index(name='all_plant_types')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group, data in invasivedf.groupby(['Assessment_Unit_Name', 'Year']):\n",
    "    print(group)\n",
    "    print(data)\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------#\n",
    "#Create Look up dictionary to assign priority group for each plant and then use it\n",
    "#----------------------------------------------------#\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "csv_data = pd.read_csv(r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Invasives Priority lookup.csv\") \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "Invasives_lookup = {}\n",
    "\n",
    "key = 'Common'\n",
    "values = ['Scientific', 'Priority'] \n",
    "\n",
    "Invasives_lookup= csv_data.set_index(key)[values].to_dict(orient='index')\n",
    "\n",
    "# Define a custom function to map plant types to priorities\n",
    "def map_priority(plant_type):\n",
    "    if pd.isnull(plant_type):\n",
    "        return 'None' # Return NaN for NaN values\n",
    "    else:\n",
    "        # Extract the priority from the dictionary, or return 'Unknown' if not found\n",
    "        plant_info = Invasives_lookup.get(plant_type)\n",
    "        if plant_info:\n",
    "            return plant_info['Priority']\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "\n",
    "\n",
    "# Create a new column 'Priority' based on the mapping from the dictionary\n",
    "invasivedf['Priority'] = invasivedf['plant_type'].map(map_priority)\n",
    "\n",
    "print(invasivedf.columns)\n",
    "#invasivedf['Priority'] = pd.to_numeric(invasivedf['Priority'], errors='coerce')\n",
    "\n",
    "# group by assessment unit and year and summarize the priority level of plants in each unit\n",
    "invasive_summary = invasivedf.groupby(['Assessment_Unit_Name', 'Year', 'Priority','Source'], dropna=False).size().reset_index(name='Count')\n",
    "\n",
    "invasive_summary_priority = invasive_summary.pivot_table(index=['Assessment_Unit_Name', 'Year','Source'], columns='Priority', values='Count', fill_value=0)\n",
    "\n",
    "\n",
    "# Reset the index to flatten the DataFrame\n",
    "invasive_summary_priority.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Grade, Score, # of Invasives\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "# Process Data\n",
    "def rate_invasive(priority):\n",
    "    if (priority[3] + priority[4] == 1):\n",
    "        return 'B'  # Assign score D\n",
    "    elif (priority[3] + priority[4] == 2) or priority[1] == 1 or priority[2] == 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif (priority[3] + priority[4] >= 3)or priority[1] >= 2 or priority[2] >= 2 or (priority[1] + priority[2] >= 2):  \n",
    "       return 'D'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no invasives of any priority are present)\n",
    "\n",
    "# Apply the rating function to the summary DataFrame\n",
    "invasive_summary_priority['Invasives_Rating'] = invasive_summary_priority[[1, 2, 3, 4]].apply(rate_invasive, axis=1)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "invasive_summary_priority['Invasives_Score']= invasive_summary_priority['Invasives_Rating'].apply(score_indicator) \n",
    "\n",
    "#Calculate total number of invasives per sez per year\n",
    "invasive_summary_priority['Number_of_Invasives']= invasive_summary_priority[[1, 2, 3, 4]].sum(axis=1)\n",
    "\n",
    "#Define SEZ ID based on Assessment_Unit_Name for QA on SEZ Name \n",
    "invasive_summary_priority['SEZ_ID'] = invasive_summary_priority['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "invasive_summary_priority['all_plants']= combined_plant_types['all_plant_types']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Prep and post ending dataframe to invasives table in SEZ_Data.GDB\n",
    "#----------------------------------------------------------------#\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Source': 'Invasives_Data_Source',\n",
    "    'Number_of_Invasives': 'Invasives_Number_of_Invasives',\n",
    "    'Invasives_Rating': 'Invasives_Rating',\n",
    "    'Invasives_Score': 'Invasives_Scores',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'percent_cover': 'Invasives_Percent_Cover',\n",
    "    'all_plants': 'Invasives_Plant_Types',\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = invasive_summary_priority.rename(columns=field_mapping).drop(columns=[col for col in invasive_summary_priority.columns if col not in field_mapping])\n",
    "\n",
    "readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "#with arcpy.da.InsertCursor(stage_invasives, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invasive with gdb and usfs pre joined layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invasive Species\n",
    "\n",
    "\n",
    "#Path to external data usfs \n",
    "usfsdata = os.path.join(master_path, \"usfsinvasivesez24\")\n",
    "#Path to external data Calflora aka State Park Data\n",
    "#calfloradata = os.path.join(master_path, \"\")\n",
    "\n",
    "# Paths to the feature classes\n",
    "invasive19fc = os.path.join(invasiveplant19gdb, \"Invasive_Species_2019\")\n",
    "invasive20fc = os.path.join(invasiveplant20gdb, \"Invasive_Species_2020\")\n",
    "invasive22fc = os.path.join(invasiveplant22gdb, \"Invasive_Species_2022\")\n",
    "invasive23fc = os.path.join(invasiveplant23gdb, \"sez_invasive_plant\")\n",
    "\n",
    "\n",
    "#Use gdb because they have assessment unit name in them... 2022 i had to manually add in PRO\n",
    "invasive23fields = ['Assessment_Unit', 'invasives_percent_cover','invasives_plant_type', 'Species_Level', 'invasive_type_other', 'created_date']\n",
    "invasive22fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover', 'InvasiveType_Other', 'Survey_Date']\n",
    "invasive20fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover',  'Other', 'Survey_Date']\n",
    "invasive19fields = ['SITE_ID', 'SURVEY_DATE', 'INVASIVE_PLANT', 'PERCENT_COVER' ]\n",
    "usfsfields = ['Assessment_Unit_Name', 'PLANT_COMM', 'SCIENTIFIC', 'DATE_COLLE', 'Eradicated']\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "\n",
    "# Function to read feature class into DataFrame\n",
    "def feature_class_to_dataframe(feature_class, fields):\n",
    "    data = [row for row in arcpy.da.SearchCursor(feature_class, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Read feature classes into DataFrames\n",
    "# Read feature classes into DataFrames\n",
    "invasive19df = feature_class_to_dataframe(invasive19fc, invasive19fields)\n",
    "invasive20df = feature_class_to_dataframe(invasive20fc, invasive20fields)\n",
    "invasive22df = feature_class_to_dataframe(invasive22fc, invasive22fields)\n",
    "invasive23df = feature_class_to_dataframe(invasive23fc, invasive23fields)\n",
    "usfs23df = feature_class_to_dataframe(usfsdata, usfsfields)\n",
    "\n",
    "\n",
    "invasive19df['Source'] = 'TRPA'\n",
    "invasive20df['Source'] = 'TRPA'\n",
    "invasive22df['Source'] = 'TRPA'\n",
    "invasive23df['Source'] = 'TRPA'\n",
    "usfs23df['Source'] = 'USFS'\n",
    "\n",
    "# Rename fields\n",
    "invasive19df.rename(columns={'SITE_ID': 'Assessment_Unit_Name', 'INVASIVE_PLANT': 'plant_type', 'PERCENT_COVER': 'percent_cover', 'SURVEY_DATE': 'created_date','Source':'Source'}, inplace=True)\n",
    "invasive20df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date', 'Source':'Source1'}, inplace=True)\n",
    "invasive22df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'InvasiveType_Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date','Source':'Source1'}, inplace=True)\n",
    "invasive23df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'invasives_percent_cover': 'percent_cover', 'invasives_plant_type': 'plant_type', 'invasive_type_other': 'other', 'created_date': 'created_date','Source':'Source1'}, inplace=True)\n",
    "usfs23df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'PLANT_COMM':'plant_type', 'DATE_COLLE':'created_date', 'Source':'Source2'}, inplace=True)\n",
    "\n",
    "\n",
    "# Concatenate DataFrames\n",
    "invasivedf = pd.concat([usfs23df, invasive19df, invasive20df, invasive22df, invasive23df], ignore_index=True)\n",
    "\n",
    "# Display merged DataFrame\n",
    "print(invasivedf)\n",
    "\n",
    "#Define SEZ ID based on Assessment_Unit_Name\n",
    "invasivedf['SEZ_ID'] = invasivedf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].astype(int)\n",
    "invasivedf['Assessment_Unit_Name'] = invasivedf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky Meadows meadow - 1': 'Sky meadows'})\n",
    "\n",
    "#calculate year column \n",
    "invasivedf['Year'] = invasivedf['created_date'].dt.year\n",
    "\n",
    "\n",
    "#---------------------------#\n",
    "    #Prep Data\n",
    "#---------------------------#\n",
    "# Replace 'other' or 'Other' in 'plant_type' column with values from 'other' column\n",
    "invasivedf['plant_type'] = invasivedf.apply(lambda row: row['other'] if pd.notna(row['plant_type']) and row['plant_type'].lower() in ['other', 'Other'] else row['plant_type'], axis=1)\n",
    "\n",
    "# Drop the 'other' column\n",
    "invasivedf.drop(columns=['other'], inplace=True)\n",
    "\n",
    "# Function to separate plant types and create new rows\n",
    "def separate_species(df):\n",
    "    # Split plant types by comma and create new rows\n",
    "    df['plant_type'] = df['plant_type'].str.split(',')\n",
    "    df = df.explode('plant_type')\n",
    "    return df\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "invasivedf = separate_species(invasivedf)\n",
    "\n",
    "# Remove rows where data source is USFS and plant type is NaN\n",
    "invasivedf = invasivedf[~((invasivedf['Source'] == 'USFS') & invasivedf['plant_type'].isna())]\n",
    "\n",
    "# Capitalize the first word and replace underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.split('_').str[0].str.capitalize() + ' ' + invasivedf['plant_type'].str.split('_').str[1:].str.join(' ')\n",
    "\n",
    "# Remove spaces after the words and capitalize the first word while replacing underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].astype(str)\n",
    "\n",
    "\n",
    "# Remove any spaces at the beginning or end of a word\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.strip()\n",
    "\n",
    "\n",
    "# Remove rows where data source is USFS and plant type is NaN\n",
    "invasivedf = invasivedf[~((invasivedf['Source'] == 'USFS') & invasivedf['plant_type'].isna())]\n",
    "#Remove Eradicated \n",
    "\n",
    "# Filter out rows where 'eradicated' column is 'Yes'\n",
    "invasivedf = invasivedf[invasivedf['Eradicated'] != 'Yes']\n",
    "\n",
    "#These replacements are so the same weed with different common name doesn't get counted twice toward score MAY NEED TO ADD MORE DESCREPENCIES in coming years if more pop up or find more elopquent solution\n",
    "# Replace 'Common mullein' with 'Wooly mullein' in the 'plant_type' column \n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common mullein', 'Wooly mullein')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Nodding plumeless thistle', 'Musk thistle')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Field bindweed', 'Common bindweed')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common st. johnswort', 'Klamathweed')\n",
    "# Replace 'Broadleaf and leaved' with 'Perennial' in the 'plant_type' column\n",
    "# Define the replacements dictionary\n",
    "replacements = {'Broadleaf Pepperweed': 'Perennial pepperweed', 'Broadleaved pepperweed': 'Perennial pepperweed'}\n",
    "\n",
    "# Replace values in the 'plant_type' column using the dictionary\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace(replacements)\n",
    "\n",
    "#Replace Sulphur Cinquefoil with sulfur\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sulphur cinquefoil', 'Sulfur cinquefoil')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sweetclover', 'White sweetclover')\n",
    "\n",
    "#Replace\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Salt cedar', 'Tamarisk')\n",
    "\n",
    "# Now, drop duplicates based on the specified subset of columns\n",
    "invasivedf = invasivedf.drop_duplicates(subset=['SEZ_ID', 'Year', 'plant_type'])\n",
    "\n",
    "# Reset index if needed\n",
    "invasivedf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Remove duplicates based on SEZ, Year, and plant_type\n",
    "invasivedf = invasivedf.drop_duplicates(subset=['SEZ_ID', 'Year', 'plant_type'])\n",
    "#----------------------------------------------------#\n",
    "#Create Look up dictionary to assign priority group for each plant and then use it\n",
    "#----------------------------------------------------#\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "csv_data = pd.read_csv(r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Invasives Priority lookup.csv\") \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "Invasives_lookup = {}\n",
    "\n",
    "key = 'Common'\n",
    "values = ['Scientific', 'Priority'] \n",
    "\n",
    "Invasives_lookup= csv_data.set_index(key)[values].to_dict(orient='index')\n",
    "\n",
    "# Define a custom function to map plant types to priorities\n",
    "def map_priority(plant_type):\n",
    "    if pd.isnull(plant_type):\n",
    "        return np.nan  # Return NaN for NaN values\n",
    "    else:\n",
    "        # Extract the priority from the dictionary, or return 'Unknown' if not found\n",
    "        plant_info = Invasives_lookup.get(plant_type)\n",
    "        if plant_info:\n",
    "            return plant_info['Priority']\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "\n",
    "#Create a new column [Scientific based on look up dictionary\n",
    "#invasivedf['Scientific']=invasivedf['plant_type'].map(map_priority)\n",
    "\n",
    "# Create a new column 'Priority' based on the mapping from the dictionary\n",
    "invasivedf['Priority'] = invasivedf['plant_type'].map(map_priority)\n",
    "\n",
    "#invasivedf['Priority'] = pd.to_numeric(invasivedf['Priority'], errors='coerce')\n",
    "# group by assessment unit and year and summarize the priority level of plants in each unit\n",
    "invasive_summary = invasivedf.groupby(['Assessment_Unit_Name', 'Year', 'Priority']).size().reset_index(name='Count')\n",
    "\n",
    "invasive_summary_priority2 = invasive_summary.pivot_table(index=['Assessment_Unit_Name', 'Year'], columns='Priority', values='Count', fill_value=0)\n",
    "\n",
    "# Reset the index to flatten the DataFrame\n",
    "invasive_summary_priority2.reset_index(inplace=True)\n",
    "\n",
    "#invasive_summary_priority['Source'] = invasivedf['Source']\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score, # of Headcuts\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "# Process Data\n",
    "def rate_invasive(priority):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if (priority[3] + priority[4] == 1):\n",
    "        return 'B'  # Assign score D\n",
    "    elif (priority[3] + priority[4] == 2) or priority[1] == 1 or priority[2] == 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif (priority[3] + priority[4] >= 3)or priority[1] >= 2 or priority[2] >= 2 or (priority[1] + priority[2] >= 2):  \n",
    "       return 'B'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no invasives of any priority are present)\n",
    "\n",
    "# Apply the rating function to the summary DataFrame\n",
    "invasive_summary_priority2['Invasives_Rating'] = invasive_summary_priority2.apply(rate_invasive, axis=1)\n",
    "\n",
    "#Calculate total number of invasives per sez per year\n",
    "invasive_summary_priority2['Number_of_Invasives']= invasive_summary_priority2[[1, 2, 3, 4,'Unknown']].sum(axis=1)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "invasive_summary_priority2['Invasives_Score']= invasive_summary_priority2['Invasives_Rating'].apply(score_indicator)    \n",
    "\n",
    "# make a columns in invasive summary that totals up percent cover per sez/year\n",
    "invasive_summary_priority2['Invasives_Percent_Cover'] = invasivedf.groupby(['SEZ_ID', 'Year'])['percent_cover'].sum().reset_index(drop=True)\n",
    "\n",
    "# combine the source column so that it shows all data sources that contributed to the data\n",
    "# Group invasivedf by 'SEZ_ID' and 'Year' and concatenate 'Data Source 1' values\n",
    "#data_source_1_combined = invasivedf.groupby(['SEZ_ID', 'Year'])['Source 1'].apply(lambda x: ', '.join(x.dropna())).reset_index()\n",
    "\n",
    "# Group invasivedf by 'SEZ_ID' and 'Year' and concatenate 'Data Source 2' values\n",
    "#data_source_2_combined = invasivedf.groupby(['SEZ_ID', 'Year'])['Source 2'].apply(lambda x: ', '.join(x.dropna())).reset_index()\n",
    "\n",
    "# Merge data_source_1_combined and data_source_2_combined on 'SEZ_ID' and 'Year'\n",
    "#merged_data_sources = pd.merge(data_source_1_combined, data_source_2_combined, on=['SEZ_ID', 'Year'], how='outer')\n",
    "\n",
    "# Combine 'Data Source 1' and 'Data Source 2' values with a comma separator\n",
    "#merged_data_sources['Data_Sources'] = merged_data_sources.apply(lambda row: ', '.join(filter(None, [row['Source 1'], row['Source 2']])), axis=1)\n",
    "\n",
    "# Drop the individual 'Data Source 1' and 'Data Source 2' columns\n",
    "#merged_data_sources.drop(columns=['Source 1', 'Source 2'], inplace=True)\n",
    "\n",
    "# Merge merged_data_sources with invasive_summary_priority on 'SEZ_ID' and 'Year'\n",
    "#invasive_summary_priority = pd.merge(invasive_summary_priority, merged_data_sources, on=['SEZ_ID', 'Year'], how='left')\n",
    "\n",
    "#invasive_summary_priority['Data_Sources']= invasivedfinvasivedf.groupby(['SED_ID', 'Year'])[Data Source 1 ] merge with DataSource 1 separate with comma if there are both \n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to incision called stage_incision GDB location\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Data_Sources': 'Invasives_Data_Source',\n",
    "    'Number_of_Invasives': 'Invasives_Number_of_Invasives',\n",
    "    'Invasives_Rating': 'Invasives_Rating',\n",
    "    'Invasives_Score': 'Invasives_Scores',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'percent_cover': 'Invasives_Percent_Cover',\n",
    "    'plant_type': 'Invasives_Plant_Type',\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "#readydf = invasive_summary_priority.rename(columns=field_mapping).drop(columns=[col for col in headcut_summary_sml.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "#print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "#data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "#field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(stage_headcuts, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data type of 'plant_type' column in the DataFrame\n",
    "print(\"Data type of 'plant_type' column in DataFrame:\", invasivedf['plant_type'].dtype)\n",
    "\n",
    "# Check the data type of values in the lookup dictionary\n",
    "for key, value in Invasives_lookup.items():\n",
    "    print(\"Data type of value for key\", key, \"in lookup dictionary:\", type(value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HEadcuts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Headcuts\n",
    "#--------------------------------#\n",
    "#Get Data from GDB's with assessment unit name \n",
    "#--------------------------------#\n",
    "\n",
    "# Paths to the feature classes\n",
    "headcut19fc = os.path.join(headcut19gdb, \"Stream_Headcut_2019\")\n",
    "headcut20fc = os.path.join(headcut20gdb, \"Stream_Headcut_2020\")\n",
    "headcut22fc = os.path.join(headcut22gdb, \"Stream_Headcut_2022\")\n",
    "headcut23fc = os.path.join(headcut23gdb, \"sez_stream_headcut\")\n",
    "\n",
    "\n",
    "#Use gdb because they have assessment unit name in them... 2022 i had to manually add in PRO\n",
    "headcut23fields = ['Assessment_Unit', 'headcut_depth', 'created_date']\n",
    "headcut22fields = ['Assessment_Unit', 'Headcut_Depth', 'synced_date']\n",
    "headcut20fields = ['Assessment_Unit_Name', 'Headcut_Depth','Survey_Date']\n",
    "headcut19fields = ['SITE_NAME', 'HEADCUT_DEPTH', 'SURVEY_DATE' ]\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "\n",
    "# Function to read feature class into DataFrame\n",
    "def feature_class_to_dataframe(feature_class, fields):\n",
    "    data = [row for row in arcpy.da.SearchCursor(feature_class, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Read feature classes into DataFrames\n",
    "# Read feature classes into DataFrames\n",
    "headcut19df = feature_class_to_dataframe(headcut19fc, headcut19fields)\n",
    "headcut20df = feature_class_to_dataframe(headcut20fc, headcut20fields)\n",
    "headcut22df = feature_class_to_dataframe(headcut22fc, headcut22fields)\n",
    "headcut23df = feature_class_to_dataframe(headcut23fc, headcut23fields)\n",
    "\n",
    "# Rename fields\n",
    "headcut19df.rename(columns={'SITE_NAME': 'Assessment_Unit_Name', 'HEADCUT_DEPTH': 'headcut_depth', 'SURVEY_DATE': 'created_date'}, inplace=True)\n",
    "headcut20df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'Headcut_Depth': 'headcut_depth', 'Survey_Date': 'created_date'}, inplace=True)\n",
    "headcut22df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'Headcut_Depth': 'headcut_depth', 'synced_date': 'created_date'}, inplace=True)\n",
    "headcut23df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'headcut_depth': 'headcut_depth', 'created_date': 'created_date'}, inplace=True)\n",
    "\n",
    "# Concatenate DataFrames\n",
    "headcutdf = pd.concat([headcut19df, headcut20df, headcut22df, headcut23df], ignore_index=True)\n",
    "\n",
    "# Display merged DataFrame\n",
    "print(headcutdf)\n",
    "\n",
    "#hopefully this is just one time\n",
    "\n",
    "\n",
    "#--------------------------------#\n",
    "#Get Data from exported table where Assessment Unit is joined---parentglobalids don't match up for years before 2023. need to redo sde.collect 2019-2022 data append.... \n",
    "#--------------------------------#\n",
    "\n",
    "#--------------------------------#\n",
    "#Get Data from SDE.Collect...instead of GDB\n",
    "#--------------------------------#\n",
    "#----------------------------------------------------------------#\n",
    "#Add correct info to dataframe\n",
    "#----------------------------------------------------------------#\n",
    "# Create a new column 'SEZ ID'\n",
    "#This code is for the excel look up dictionary---ADD last? this is goo dfor QA on assessment unit names because 0's tell you the name is wrong\n",
    "headcutdf['SEZ_ID'] = headcutdf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "headcutdf['SEZ_ID'] = headcutdf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "headcutdf['SEZ_ID'] = headcutdf['SEZ_ID'].astype(int)\n",
    "headcutdf['Assessment_Unit_Name'] = headcutdf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky meadows - 1': 'Sky Meadows'})\n",
    "\n",
    "#calculate year column \n",
    "headcutdf['Year'] = headcutdf['created_date'].dt.year\n",
    "\n",
    "#----------------------------------------------#\n",
    "# Process Data\n",
    "#----------------------------------------------#\n",
    "# assign small, medium, large to headcut\n",
    "headcutdf['Headcut_Size']=headcutdf['headcut_depth'].apply(categorize_headcut)\n",
    "\n",
    "\n",
    "# Group by 'SEZ_ID', 'Year', and 'Headcut_Size', and count the number of occurrences for each group\n",
    "headcut_summary = headcutdf.groupby(['Assessment_Unit_Name', 'Year', 'Headcut_Size']).size().reset_index(name='Count')\n",
    "\n",
    "print(type(headcut_summary))\n",
    "# Pivot the table to have 'Headcut_Size' categories as columns\n",
    "headcut_summary_sml = headcut_summary.pivot_table(index=['Assessment_Unit_Name', 'Year'], columns='Headcut_Size', values='Count', fill_value=0)\n",
    "\n",
    "# Reset the index to flatten the DataFrame\n",
    "headcut_summary_sml.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the summarized DataFrame\n",
    "print(headcut_summary_sml)\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score, # of Headcuts\n",
    "#----------------------------------------------------------------#\n",
    "    \n",
    "    # Process Data\n",
    "def rate_headcut(row):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if row['large'] >= 1:\n",
    "        return 'D'  # Assign score D\n",
    "    elif row['medium'] >= 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif row['small'] >= 1:\n",
    "       return 'B'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no headcuts)\n",
    "\n",
    "# Apply the rating function to the summary DataFrame\n",
    "headcut_summary_sml['Headcuts_Rating'] = headcut_summary_sml.apply(rate_headcut, axis=1)\n",
    "\n",
    "#Calculate total number of headcuts per sez per year\n",
    "headcut_summary_sml['Number_of_Headcuts']= headcut_summary_sml[['large', 'medium', 'small']].sum(axis=1)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "headcut_summary_sml['Headcuts_Score']= headcut_summary_sml['Headcuts_Rating'].apply(score_indicator)\n",
    "\n",
    "#Add Datasource\n",
    "headcut_summary_sml['Headcuts_Data_Source'] = 'TRPA'\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to incision called stage_incision GDB location\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Headcuts_Data_Source': 'Headcuts_Data_Source',\n",
    "    'Number_of_Headcuts': 'Number_of_Headcuts',\n",
    "    'Headcuts_Rating': 'Headcuts_Rating',\n",
    "    'Headcuts_Score': 'Headcuts_Score',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'small': 'small',\n",
    "    'medium': 'medium',\n",
    "    'large': 'large'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = headcut_summary_sml.rename(columns=field_mapping).drop(columns=[col for col in headcut_summary_sml.columns if col not in field_mapping])\n",
    "\n",
    "readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "with arcpy.da.InsertCursor(stage_headcuts, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in External Data from USFS and Calflora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------#\n",
    "#Get Data from external sources and spatially join to our data\n",
    "#--------------------------------#\n",
    "\n",
    "\n",
    "#Calfora Data\n",
    "CArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines07/MapServer/1\"\n",
    "#CAfc = os.path.join(CArest, \"Non-Native and Invasive Plants - Calflora\")\n",
    "CAsdf = pd.DataFrame.spatial.from_layer(CArest)\n",
    "# Delete the existing feature layer if it exists\n",
    "\n",
    "CA_fc = arcpy.MakeFeatureLayer_management(CArest, \"Calfora_invasives\")\n",
    "\n",
    "\n",
    "usfsrest= \"https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer/0\"\n",
    "#usfsfc =os.path.join(usfsrest, \"Current Invasive Plant Locations\") \n",
    "#usfssdf = pd.DataFrame.spatial.from_layer(usfsrest)\n",
    "if arcpy.Exists(\"usfs_layer\"):\n",
    "    arcpy.Delete_management(\"usfs_layer\")\n",
    "usfs_fc = arcpy.MakeFeatureLayer_management(usfsrest, \"usfs_layer\")\n",
    "#Spatially join usfs layers to see if any of the points or polygons intersect with our sez's123\n",
    "\n",
    "# Set the workspace environment\n",
    "arcpy.env.workspace = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Data Management 2023.gdb\"\n",
    "\n",
    "# Define the target and join layers\n",
    "target_feature = SEZ_Master #\"AssessmentUnit_Master\"\n",
    "CAjoin_feature = CA_fc\n",
    "USFSjoin_feature = usfs_fc\n",
    "\n",
    "# Define the output feature class\n",
    "out_feature_class = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Data Management 2023.gdb\\externalinvasivesez\"\n",
    "\n",
    "# Define the fields you want to keep from both layers\n",
    "#field_mappings = arcpy.FieldMappings()\n",
    "#field_mappings.addTable(target_features)\n",
    "#field_mappings.addTable(join_features)\n",
    "\n",
    "# Keep only specific fields from the target layer\n",
    "#keep_fields_target = [\"Assessment_Unit_Name\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_target:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "#Keep only specific fields from the calfora join layer\n",
    "#keep_fields_CA = [\"CName\", \"SName\", \"Source\", \"Obs_Date\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_CA:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "# Keep only specific fields from the usfs join_features\n",
    "#keep_fields_USFS = [\"SCIENTIFIC_NAME\", \"COMMON_NAME\", \"DATE_COLLECTED\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_USFS:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "\n",
    "# Perform spatial join with specified field mappings\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=SEZ_Master,\n",
    "    join_features=[CAjoin_feature],\n",
    "    out_feature_class=out_feature_class,\n",
    "    join_operation=\"JOIN_ONE_TO_MANY\",\n",
    "    join_type=\"KEEP_ALL\"\n",
    "    #field_mapping=field_mappings\n",
    ")\n",
    "\n",
    "#Create Dataframe for spatially joined data\n",
    "# Define the fields you want to retrieve from the output feature class\n",
    "fields = [\"Assessment_Unit_Name\", \"CName\", \"SName\", \"Source\", \"Obs_Date\", \"SCIENTIFIC_NAME\", \"COMMON_NAME\", \"DATE_COLLECTED\"]\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "data = []\n",
    "\n",
    "# Use SearchCursor to read data from the output feature class\n",
    "with arcpy.da.SearchCursor(out_feature_class, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the list of tuples into a pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bioassessment/ Biotic Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bioassessment scores- get all stream data into sde.Stream first... then look at Biotic Integrit Data Source to find which stream was used to evaluate each meadow.. this will help with percent of stream miles I believe\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Get Data-- all the data we need is in sde.collect.sde.survey.sde.sez_survey\n",
    "#----------------------------------------------------------------##Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "\n",
    "#Create SEDF setup\n",
    "#streamdata is the path to the feature class in sde\n",
    "# Set the workspace to your SDE connection file\n",
    "arcpy.env.workspace = streamdata\n",
    "feature_class= \"Stream\"\n",
    "\n",
    "# Convert feature class to a pandas DataFrame\n",
    "fields = [field.name for field in arcpy.ListFields(feature_class)]\n",
    "\n",
    "# Create DataFrame\n",
    "streamsdf = pd.DataFrame.spatial.from_featureclass(feature_class, columns=fields)\n",
    "\n",
    "# Create the spatially enabled DataFrame (sdf) for target feature SEZ assessment units\n",
    "SEZsdf = pd.DataFrame.spatial.from_featureclass(SEZ_Master)\n",
    "#spatial reference stuff\n",
    "streamsdf.spatial.sr = SEZsdf.spatial.sr\n",
    "\n",
    "#perform spatial join of sde.stream and sez units\n",
    "thesdf = SEZsdf.spatial.join(streamsdf, how='inner')\n",
    "\n",
    "#Notes to self, Stream Miles?\n",
    "#Keep only Riverine?, this may be what the smaller polygons are for \n",
    "# Filter for SEZ type 'Riverine'\n",
    "#riverine_df = bioticsdf[bioticsdf['Feature_Type'] == 'Riverine']\n",
    "\n",
    "#if the layer contains Riverine? or just for any spatial join.. see what it does\n",
    "#spatial join this layer to asessment unit master layer\n",
    "#ASsessment unit master layer is called SEZ_Master\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------#\n",
    "#DATA PREP\n",
    "#----------------------#\n",
    "# List of columns to keep\n",
    "columns_to_keep = ['Assessment_Unit_Name', 'SEZ_Type', 'Feature_Type', 'SEZ_ID','SITE_NAME', 'COUNT_VALUE', 'YEAR_OF_COUNT', 'STATION_TYPE', 'LONGITUDE', 'LATITUDE', ]\n",
    "##Try this instead\n",
    "# Select only the desired columns\n",
    "bioticdf = thesdf.loc[:, columns_to_keep].copy()  \n",
    "\n",
    "#DATA PREP\n",
    "# Filter for years 2020 to 2023\n",
    "filtered_df = bioticdf.loc[(bioticdf['YEAR_OF_COUNT'] >= 2020) & (bioticdf['YEAR_OF_COUNT'] <= 2023)].copy()\n",
    "\n",
    "# Define SEZ ID based on Assessment_Unit_Name for QA on SEZ Name THIS METHOD USES LARGER POLYGONES \n",
    "filtered_df.loc[:, 'SEZ_ID'] = filtered_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "filtered_df.loc[:, 'SEZ_ID'] = filtered_df['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "filtered_df.loc[:, 'SEZ_ID'] = filtered_df['SEZ_ID'].astype(int)\n",
    "\n",
    "# Replace values in the 'Assessment_Unit_Name' column\n",
    "filtered_df.loc[:, 'Assessment_Unit_Name'] = filtered_df['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky Meadows meadow - 1': 'Sky meadows'})\n",
    "\n",
    "# Add data source information\n",
    "filtered_df['Source'] = 'TRPA, ' + filtered_df['SITE_NAME'].astype(str) + ', ' + filtered_df['YEAR_OF_COUNT'].astype(str)\n",
    "\n",
    "#Group by Year and Assessment Unit and Site NAME and remove duplicates\n",
    "\n",
    "filtered_df['SITE_NAME'] = filtered_df['SITE_NAME'].str.strip()\n",
    "filtered_df['YEAR_OF_COUNT'] = filtered_df['YEAR_OF_COUNT'].astype(str).str.strip().astype(int)\n",
    "filtered_df['YEAR_OF_COUNT'] = pd.to_numeric(filtered_df['YEAR_OF_COUNT'], errors='coerce')\n",
    "\n",
    "\n",
    "# Group by Assessment_Unit_Name, SITE_NAME, and YEAR_OF_COUNT and drop duplicates\n",
    "BIdf = filtered_df.groupby(['SEZ_ID', 'SITE_NAME', 'YEAR_OF_COUNT', 'COUNT_VALUE']).apply(lambda x: x.drop_duplicates()).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_df['YEAR_OF_COUNT'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------#\n",
    "#Grade and Score biotic integrity\n",
    "#----------------------#\n",
    "\n",
    "#Rate the score\n",
    "#ef categorize_csci(biotic_integrity):\n",
    "# Apply the rating function to the summary DataFrame\n",
    "BIdf['Biotic_Rating'] = BIdf['COUNT_VALUE'].apply(categorize_csci)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "BIdf['Biotic_Score']= BIdf['Biotic_Rating'].apply(score_indicator) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Prep and post ending dataframe to invasives table in SEZ_Data.GDB\n",
    "#----------------------------------------------------------------#\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'YEAR_OF_COUNT': 'Year',\n",
    "    'Source': 'Biotic_Integrity_Data_Source',\n",
    "    'COUNT_VALUE': 'Biotic_Integrity_CSCI',\n",
    "    'Biotic_Rating': 'Biotic_Integrity_Rating',\n",
    "    'Biotic_Score': 'Biotic_Integrity_Score',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = BIdf.rename(columns=field_mapping).drop(columns=[col for col in BIdf.columns if col not in field_mapping])\n",
    "\n",
    "readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_biotic_integrity, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")\n",
    "\n",
    "#Delete duplicates yourself.. not that much data to go through, can't figure out why it won't remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conifer Encroachment STaging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import arcpy\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Conifer_Encroachment_Data_Sourc',\n",
    "                        'Conifer_Encroachment_Rating',                    \n",
    "                        'Conifer_Encroachment_Percent_En',\n",
    "                        'Conifer_Encroachment_Score',\n",
    "                        'SEZ_ID',\n",
    "                        'Assessment_Unit_Name',\n",
    "                        'ConiferEncroachment_Comments']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    conifer_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Conifer_Encroachment_Data_Source Conifer_Encroachment_Rating  \\\n",
      "0                   TRPA LIDAR, 2009                           D   \n",
      "1                   TRPA LIDAR, 2009                           D   \n",
      "2                   TRPA LIDAR, 2009                           A   \n",
      "3                   TRPA LIDAR, 2009                           C   \n",
      "4                   TRPA LIDAR, 2009                           A   \n",
      "..                               ...                         ...   \n",
      "636                             None                        None   \n",
      "637                             None                        None   \n",
      "638                             None                        None   \n",
      "639                             None                        None   \n",
      "640                             None                        None   \n",
      "\n",
      "     Conifer_Percent_Encroached  Conifer_Encroachment_Score  SEZ_ID  \\\n",
      "0                          96.0                         3.0       1   \n",
      "1                          99.0                         3.0       2   \n",
      "2                           5.0                        12.0       3   \n",
      "3                          50.0                         6.0       4   \n",
      "4                           4.0                        12.0       5   \n",
      "..                          ...                         ...     ...   \n",
      "636                         NaN                         NaN     639   \n",
      "637                         NaN                         NaN     640   \n",
      "638                         NaN                         NaN     641   \n",
      "639                         NaN                         NaN     642   \n",
      "640                         NaN                         NaN     643   \n",
      "\n",
      "                             Assessment_Unit_Name  \\\n",
      "0      Saxon Creek meadows - below Fountain Place   \n",
      "1    Saxon Creek meadows - above Fountain Place 1   \n",
      "2                         Burke Creek meadows - 2   \n",
      "3                   Washoe State Parks meadow - 1   \n",
      "4                             Spooner Meadows - 4   \n",
      "..                                            ...   \n",
      "636              West Shore tributary - 2 - lower   \n",
      "637                           Saxon Creek - upper   \n",
      "638                           Saxon Creek - upper   \n",
      "639           Marlette Creek - south fork (lower)   \n",
      "640           Marlette Creek - south fork (lower)   \n",
      "\n",
      "                          ConiferEncroachment_Comments  Year  \n",
      "0          very heavy conifer encroachment since 1940.  2020  \n",
      "1          very heavy conifer encroachment since 1940.  2020  \n",
      "2                              no conifer encroachment  2020  \n",
      "3    not much change in conifer levels since 1940 b...  2020  \n",
      "4                              no conifer encroachment  2020  \n",
      "..                                                 ...   ...  \n",
      "636                                               None  2020  \n",
      "637                                               None  2020  \n",
      "638                                               None  2020  \n",
      "639                                               None  2020  \n",
      "640                                               None  2020  \n",
      "\n",
      "[641 rows x 8 columns]\n",
      "Data appended to table successfully.\n"
     ]
    }
   ],
   "source": [
    "#Prep Data\n",
    "conifer_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Conifer_Encroachment_Data_Sourc': 'Conifer_Encroachment_Data_Source',\n",
    "                'Conifer_Encroachment_Rating': 'Conifer_Encroachment_Rating',                    \n",
    "                'Conifer_Encroachment_Percent_En': 'Conifer_Percent_Encroached',\n",
    "                'Conifer_Encroachment_Score': 'Conifer_Encroachment_Score',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'ConiferEncroachment_Comments': 'ConiferEncroachment_Comments'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = conifer_df.rename(columns=field_mapping).drop(columns=[col for col in conifer_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_conifer, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aquatic Organism Passage STagin table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['OBJECTID', 'Shape', 'Acres', 'Assessment_Unit_Name',\n",
      "       'AquaticOrganismPassage_Barriers', 'AquaticOrganismPassage_DataSour',\n",
      "       'AquaticOrganismPassage_NumberOf', 'AquaticOrganismPassage_Rating',\n",
      "       'AquaticOrganismPassage_Score', 'AquaticOrganismPassage_StreamMi',\n",
      "       'Bank_Stability_Data_Source', 'Bank_Stability_Percent_Unstable',\n",
      "       'Bank_Stability_Rating', 'Bank_Stability_Score',\n",
      "       'Biotic_Integrity_CSCI', 'Biotic_Integrity_Data_Source',\n",
      "       'Biotic_Integrity_Rating', 'Biotic_Integrity_Score', 'Comments',\n",
      "       'Conifer_Encroachment_Data_Sourc', 'Conifer_Encroachment_Percent_En',\n",
      "       'Conifer_Encroachment_Rating', 'Conifer_Encroachment_Score',\n",
      "       'Ditches_Data_Source', 'Ditches_Length', 'Ditches_Meadow_Length',\n",
      "       'Ditches_Percent', 'Ditches_Rating', 'Ditches_Score',\n",
      "       'Habitat_Fragmentation_Data_Sour', 'Habitat_Fragmentation_Imperviou',\n",
      "       'Habitat_Fragmentation_Percent_I', 'Habitat_Fragmentation_Rating',\n",
      "       'Habitat_Fragmentation_Score', 'Headcuts_Data_Source',\n",
      "       'Headcuts_Number_of_Headcuts', 'Headcuts_Rating', 'Headcuts_Score',\n",
      "       'Incision_Data_Source', 'Incision_Ratio', 'Incision_Rating',\n",
      "       'Incision_Score', 'Invasives_Data_Source',\n",
      "       'Invasives_Number_of_Invasives', 'Invasive_Percent_Cover',\n",
      "       'Invasives_Plant_Types', 'Invasive_Rating', 'Invasives_Scores',\n",
      "       'SEZ_Type', 'VegetationVigor_DataSource', 'VegetationVigor_Rating',\n",
      "       'VegetationVigor_Raw', 'VegetationVigor_Score', 'Ownership_Primary',\n",
      "       'Ownership_Secondary', 'Ownership_Secondary_2', 'Ownership_Secondary_3',\n",
      "       'Final_Percent', 'Final_Points_Possible', 'Final_Rating',\n",
      "       'Final_Total_Points', 'Feature_Type', 'ConiferEncroachment_Comments',\n",
      "       'NDVI_ID', 'GlobalID', 'created_user', 'created_date',\n",
      "       'last_edited_user', 'last_edited_date', 'Shape_Length', 'Shape_Area',\n",
      "       'SEZ_ID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(SEZ_Master.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aquatic Organism /Fish Passage- only old data for now\n",
    "import os\n",
    "import pandas as pd\n",
    "import arcpy\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['AquaticOrganismPassage_Barriers',\n",
    "                        'AquaticOrganismPassage_DataSour',                    \n",
    "                        'AquaticOrganismPassage_NumberOf',\n",
    "                        'AquaticOrganismPassage_Rating',\n",
    "                        'SEZ_ID',\n",
    "                        'Assessment_Unit_Name',\n",
    "                        'AquaticOrganismPassage_Score',\n",
    "                        'AquaticOrganismPassage_StreamMi']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    AOP_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     AOP_BarriersPerMile          AOP_DataSource  AOP_NumberofBarriers  \\\n",
      "0                    0.0  USFS / TRPA 2009, 2019                   0.0   \n",
      "1                    0.0  USFS / TRPA 2009, 2019                   0.0   \n",
      "2                    NaN  USFS / TRPA 2009, 2019                   NaN   \n",
      "3                    NaN  USFS / TRPA 2009, 2019                   NaN   \n",
      "4                    NaN  USFS / TRPA 2009, 2019                   NaN   \n",
      "..                   ...                     ...                   ...   \n",
      "636                  NaN  USFS / TRPA 2009, 2019                   NaN   \n",
      "637                  0.0  USFS / TRPA 2009, 2019                   0.0   \n",
      "638                  0.0  USFS / TRPA 2009, 2019                   0.0   \n",
      "639                 10.0  USFS / TRPA 2009, 2019                   2.0   \n",
      "640                 10.0  USFS / TRPA 2009, 2019                   2.0   \n",
      "\n",
      "    AOP_Rating  SEZ_ID                          Assessment_Unit_Name  \\\n",
      "0            A       1    Saxon Creek meadows - below Fountain Place   \n",
      "1            A       2  Saxon Creek meadows - above Fountain Place 1   \n",
      "2         None       3                       Burke Creek meadows - 2   \n",
      "3         None       4                 Washoe State Parks meadow - 1   \n",
      "4         None       5                           Spooner Meadows - 4   \n",
      "..         ...     ...                                           ...   \n",
      "636       None     639              West Shore tributary - 2 - lower   \n",
      "637          A     640                           Saxon Creek - upper   \n",
      "638          A     641                           Saxon Creek - upper   \n",
      "639          D     642           Marlette Creek - south fork (lower)   \n",
      "640          D     643           Marlette Creek - south fork (lower)   \n",
      "\n",
      "     AOP_Score  AOP_StreamMiles  Year  \n",
      "0         12.0              NaN  2020  \n",
      "1         12.0              NaN  2020  \n",
      "2          NaN              NaN  2020  \n",
      "3          NaN              NaN  2020  \n",
      "4          NaN              NaN  2020  \n",
      "..         ...              ...   ...  \n",
      "636        NaN              NaN  2020  \n",
      "637       12.0              NaN  2020  \n",
      "638       12.0              NaN  2020  \n",
      "639        3.0              0.2  2020  \n",
      "640        3.0              0.2  2020  \n",
      "\n",
      "[641 rows x 9 columns]\n",
      "Data appended to table successfully.\n"
     ]
    }
   ],
   "source": [
    "#Prep Data\n",
    "AOP_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'AquaticOrganismPassage_Barriers': 'AOP_BarriersPerMile',\n",
    "                'AquaticOrganismPassage_DataSour': 'AOP_DataSource',                    \n",
    "                'AquaticOrganismPassage_NumberOf': 'AOP_NumberofBarriers',\n",
    "                'AquaticOrganismPassage_Score': 'AOP_Score',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'AquaticOrganismPassage_StreamMi': 'AOP_StreamMiles',\n",
    "                'AquaticOrganismPassage_Rating': 'AOP_Rating'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = AOP_df.rename(columns=field_mapping).drop(columns=[col for col in AOP_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_aquatic, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HAbitat Fragmentation STaging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create STaging Tables for External Data Put in STagin Table\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Habitat_Fragmentation_Data_Sour',\n",
    "                        'Habitat_Fragmentation_Imperviou',\n",
    "                        'Habitat_Fragmentation_Percent_I',\n",
    "                        'Habitat_Fragmentation_Rating',\n",
    "                        'Habitat_Fragmentation_Score',\n",
    "                        'SEZ_ID', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    HabFrag_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Habitat_Frag_Data_Source  Habitat_Frag_Impervious_Acres  \\\n",
      "0           TRPA LIDAR, 2009                           0.00   \n",
      "1           TRPA LIDAR, 2009                           0.00   \n",
      "2           TRPA LIDAR, 2009                           0.00   \n",
      "3           TRPA LIDAR, 2009                           0.01   \n",
      "4           TRPA LIDAR, 2009                           0.00   \n",
      "..                       ...                            ...   \n",
      "636         TRPA, LIDAR 2009                           0.29   \n",
      "637          TRPA LIDAR 2009                           0.00   \n",
      "638          TRPA LIDAR 2009                           0.00   \n",
      "639          TRPA LIDAR 2009                           0.03   \n",
      "640          TRPA LIDAR 2009                           0.03   \n",
      "\n",
      "     HAbitat_Frag_Percent_Impervious Habitat_Frag_Rating  Habitat_Frag_Score  \\\n",
      "0                           0.000000                   A                12.0   \n",
      "1                           0.000000                   A                12.0   \n",
      "2                           0.000000                   A                12.0   \n",
      "3                           0.027997                   A                12.0   \n",
      "4                           0.000000                   A                12.0   \n",
      "..                               ...                 ...                 ...   \n",
      "636                        51.700000                   D                 3.0   \n",
      "637                         0.000000                   A                12.0   \n",
      "638                         0.000000                   A                12.0   \n",
      "639                         1.100000                   B                 9.0   \n",
      "640                         1.100000                   B                 9.0   \n",
      "\n",
      "     SEZ_ID                          Assessment_Unit_Name  Year  \n",
      "0         1    Saxon Creek meadows - below Fountain Place  2020  \n",
      "1         2  Saxon Creek meadows - above Fountain Place 1  2020  \n",
      "2         3                       Burke Creek meadows - 2  2020  \n",
      "3         4                 Washoe State Parks meadow - 1  2020  \n",
      "4         5                           Spooner Meadows - 4  2020  \n",
      "..      ...                                           ...   ...  \n",
      "636     639              West Shore tributary - 2 - lower  2020  \n",
      "637     640                           Saxon Creek - upper  2020  \n",
      "638     641                           Saxon Creek - upper  2020  \n",
      "639     642           Marlette Creek - south fork (lower)  2020  \n",
      "640     643           Marlette Creek - south fork (lower)  2020  \n",
      "\n",
      "[641 rows x 8 columns]\n",
      "Data appended to table successfully.\n"
     ]
    }
   ],
   "source": [
    "#Prep Data\n",
    "HabFrag_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Habitat_Fragmentation_Data_Sour': 'Habitat_Frag_Data_Source',\n",
    "                'Habitat_Fragmentation_Percent_I': 'HAbitat_Frag_Percent_Impervious',                    \n",
    "                'Habitat_Fragmentation_Score': 'Habitat_Frag_Score',\n",
    "                'Habitat_Fragmentation_Imperviou': 'Habitat_Frag_Impervious_Acres',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'Habitat_Fragmentation_Rating': 'Habitat_Frag_Rating'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = HabFrag_df.rename(columns=field_mapping).drop(columns=[col for col in HabFrag_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_habitat, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ditches STaging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create STaging Tables for External Data Put in STagin Table\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Ditches_Data_Source',\n",
    "                        'Ditches_Length',\n",
    "                        'Ditches_Meadow_Length',\n",
    "                        'Ditches_Percent',\n",
    "                        'Ditches_Rating',\n",
    "                        'Ditches_Score',\n",
    "                        'SEZ_ID', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    Ditch_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Ditches_Data_Source  Ditches_Length  Ditches_Meadow_Length  \\\n",
      "0            TRPA, 2019             0.0                    NaN   \n",
      "1            TRPA, 2019             0.0                    NaN   \n",
      "2            TRPA, 2019             0.0                    NaN   \n",
      "3            TRPA, 2019             0.0                    NaN   \n",
      "4            TRPA, 2019             0.0                    NaN   \n",
      "..                  ...             ...                    ...   \n",
      "636          TRPA, 2019          1312.0                 2295.0   \n",
      "637                None             NaN                    NaN   \n",
      "638                None             NaN                    NaN   \n",
      "639                None             NaN                    NaN   \n",
      "640                None             NaN                    NaN   \n",
      "\n",
      "     Ditches_Percent Ditches_Rating  Ditches_Score  SEZ_ID  \\\n",
      "0                0.0              A           12.0       1   \n",
      "1                0.0              A           12.0       2   \n",
      "2                0.0              A           12.0       3   \n",
      "3                0.0              A           12.0       4   \n",
      "4                0.0              A           12.0       5   \n",
      "..               ...            ...            ...     ...   \n",
      "636             57.1              D            3.0     639   \n",
      "637              NaN           None            NaN     640   \n",
      "638              NaN           None            NaN     641   \n",
      "639              NaN           None            NaN     642   \n",
      "640              NaN           None            NaN     643   \n",
      "\n",
      "                             Assessment_Unit_Name  Year  \n",
      "0      Saxon Creek meadows - below Fountain Place  2020  \n",
      "1    Saxon Creek meadows - above Fountain Place 1  2020  \n",
      "2                         Burke Creek meadows - 2  2020  \n",
      "3                   Washoe State Parks meadow - 1  2020  \n",
      "4                             Spooner Meadows - 4  2020  \n",
      "..                                            ...   ...  \n",
      "636              West Shore tributary - 2 - lower  2020  \n",
      "637                           Saxon Creek - upper  2020  \n",
      "638                           Saxon Creek - upper  2020  \n",
      "639           Marlette Creek - south fork (lower)  2020  \n",
      "640           Marlette Creek - south fork (lower)  2020  \n",
      "\n",
      "[641 rows x 9 columns]\n",
      "Data appended to table successfully.\n"
     ]
    }
   ],
   "source": [
    "#Prep Data\n",
    "Ditch_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Ditches_Data_Source': 'Ditches_Data_Source',\n",
    "                'Ditches_Length': 'Ditches_Length',                    \n",
    "                'Ditches_Meadow_Length': 'Ditches_Meadow_Length',\n",
    "                'Ditches_Percent': 'Ditches_Percent',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'Ditches_Rating': 'Ditches_Rating',\n",
    "                'Ditches_Score': 'Ditches_Score'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = Ditch_df.rename(columns=field_mapping).drop(columns=[col for col in Ditch_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_ditches, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vegetation Vigor- old data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create STaging Tables for External Data Put in STagin Table\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['VegetationVigor_DataSource',\n",
    "                        'NDVI_ID',\n",
    "                        'VegetationVigor_Raw',\n",
    "                        'VegetationVigor_Rating',\n",
    "                        'VegetationVigor_Score',\n",
    "                        'SEZ_ID', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    vegetation_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    VegetationVigor_DataSource   NDVI_ID  VegetationVigor_Raw  \\\n",
      "0                     DRI 2020       N/A                  NaN   \n",
      "1                     DRI 2020       N/A                  NaN   \n",
      "2                     DRI 2020  TRPA_190                  NaN   \n",
      "3                     DRI 2020       N/A                  NaN   \n",
      "4                     DRI 2020  TRPA_245                  NaN   \n",
      "..                         ...       ...                  ...   \n",
      "636                   DRI 2020      None                  NaN   \n",
      "637                   DRI 2020      None                  NaN   \n",
      "638                   DRI 2020      None                  NaN   \n",
      "639                   DRI 2020      None                  NaN   \n",
      "640                   DRI 2020      None                  NaN   \n",
      "\n",
      "    VegetationVigor_Rating  VegetationVigor_Score  SEZ_ID  \\\n",
      "0                       NA                    NaN       1   \n",
      "1                       NA                    NaN       2   \n",
      "2                        A                   12.0       3   \n",
      "3                       NA                    NaN       4   \n",
      "4                        C                    6.0       5   \n",
      "..                     ...                    ...     ...   \n",
      "636                   None                    NaN     639   \n",
      "637                   None                    NaN     640   \n",
      "638                   None                    NaN     641   \n",
      "639                   None                    NaN     642   \n",
      "640                   None                    NaN     643   \n",
      "\n",
      "                             Assessment_Unit_Name  Year  \n",
      "0      Saxon Creek meadows - below Fountain Place  2020  \n",
      "1    Saxon Creek meadows - above Fountain Place 1  2020  \n",
      "2                         Burke Creek meadows - 2  2020  \n",
      "3                   Washoe State Parks meadow - 1  2020  \n",
      "4                             Spooner Meadows - 4  2020  \n",
      "..                                            ...   ...  \n",
      "636              West Shore tributary - 2 - lower  2020  \n",
      "637                           Saxon Creek - upper  2020  \n",
      "638                           Saxon Creek - upper  2020  \n",
      "639           Marlette Creek - south fork (lower)  2020  \n",
      "640           Marlette Creek - south fork (lower)  2020  \n",
      "\n",
      "[641 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "#Prep Data\n",
    "vegetation_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'VegetationVigor_DataSource': 'VegetationVigor_DataSource',\n",
    "                'NDVI_ID': 'NDVI_ID',                    \n",
    "                'VegetationVigor_Raw': 'VegetationVigor_Raw',\n",
    "                'VegetationVigor_Rating': 'VegetationVigor_Rating',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'VegetationVigor_Score': 'VegetationVigor_Score'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = vegetation_df.rename(columns=field_mapping).drop(columns=[col for col in vegetation_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_vegetation, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEZ data from 2020 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All SEZ Scores from current Data\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Acres',\n",
    "                        'Final_Percent',\n",
    "                        'Final_Points_Possible',\n",
    "                        'Final_Rating',\n",
    "                        'Final_Total_Points',\n",
    "                        'SEZ_ID',\n",
    "                        'Comments',\n",
    "                        'SEZ_Type', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    SEZ20_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Acres  Final_Percent  Final_Points_Possible Final_Rating  \\\n",
      "0    16.661797      86.111111                  108.0            B   \n",
      "1    12.805647      80.555556                  108.0            B   \n",
      "2     4.919027      91.666667                   72.0            A   \n",
      "3    35.718239      75.000000                   60.0            C   \n",
      "4    18.761409      91.666667                   72.0            A   \n",
      "..         ...            ...                    ...          ...   \n",
      "636   0.566975      62.500000                   48.0            D   \n",
      "637   1.350239      83.333333                   72.0            B   \n",
      "638  20.885109      83.333333                   72.0            B   \n",
      "639   2.533502      56.600000                   60.0            D   \n",
      "640   0.268518      56.600000                   60.0            D   \n",
      "\n",
      "     Final_Total_Points  SEZ_ID  \\\n",
      "0                  93.0       1   \n",
      "1                  87.0       2   \n",
      "2                  66.0       3   \n",
      "3                  45.0       4   \n",
      "4                  66.0       5   \n",
      "..                  ...     ...   \n",
      "636                30.0     639   \n",
      "637                60.0     640   \n",
      "638                60.0     641   \n",
      "639                24.0     642   \n",
      "640                24.0     643   \n",
      "\n",
      "                                              Comments  \\\n",
      "0    Moderately incised but the channel is mostly s...   \n",
      "1    Very incised with some channel instability. Me...   \n",
      "2    Sagebrush encroachment is occuring into upper ...   \n",
      "3    Meadow was used as car racing track until 1980...   \n",
      "4    No indications of degradation. Lake is not nat...   \n",
      "..                                                 ...   \n",
      "636  Ditched through most of the SEZ and surrounded...   \n",
      "637  Parts of channel are very incised and not conn...   \n",
      "638  Parts of channel are very incised and not conn...   \n",
      "639  Heavy bank erosion, headcuts, and severely inc...   \n",
      "640  Heavy bank erosion, headcuts, and severely inc...   \n",
      "\n",
      "                            SEZ_Type  \\\n",
      "0                   Channeled Meadow   \n",
      "1                   Channeled Meadow   \n",
      "2               Non-Channeled Meadow   \n",
      "3               Non-Channeled Meadow   \n",
      "4               Non-Channeled Meadow   \n",
      "..                               ...   \n",
      "636                         Forested   \n",
      "637             Riverine (Perennial)   \n",
      "638  Riverine (Perennial) + Forested   \n",
      "639  Riverine (Perennial) + Forested   \n",
      "640             Riverine (Perennial)   \n",
      "\n",
      "                             Assessment_Unit_Name  Year  \n",
      "0      Saxon Creek meadows - below Fountain Place  2020  \n",
      "1    Saxon Creek meadows - above Fountain Place 1  2020  \n",
      "2                         Burke Creek meadows - 2  2020  \n",
      "3                   Washoe State Parks meadow - 1  2020  \n",
      "4                             Spooner Meadows - 4  2020  \n",
      "..                                            ...   ...  \n",
      "636              West Shore tributary - 2 - lower  2020  \n",
      "637                           Saxon Creek - upper  2020  \n",
      "638                           Saxon Creek - upper  2020  \n",
      "639           Marlette Creek - south fork (lower)  2020  \n",
      "640           Marlette Creek - south fork (lower)  2020  \n",
      "\n",
      "[641 rows x 10 columns]\n",
      "Data appended to table successfully.\n"
     ]
    }
   ],
   "source": [
    "#Prep Data\n",
    "SEZ20_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Acres': 'Acres',\n",
    "                'SEZ_Type': 'SEZ_Type',                    \n",
    "                'Final_Percent': 'Final_Percent',\n",
    "                'Final_Total_Points': 'Final_Total_Points',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'Final_Points_Possible': 'Final_Points_Possible',\n",
    "                'Final_Rating': 'Final_Rating',\n",
    "                'Comments': 'Comments'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = SEZ20_df.rename(columns=field_mapping).drop(columns=[col for col in SEZ20_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_All_SEZ_Scores, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All_SEZ_Scores Final Calculations for SEZ Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEZ_Data.GDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# path to table in SEZ_Data.GDB\n",
    "#stage_All_SEZ_Scores\n",
    "stage_bank_stability = os.path.join(master_path, \"bank_stability\") \n",
    "stage_All_SEZ_Scores = os.path.join(master_path, \"All_SEZ_Scores\")\n",
    "stage_biotic_integrity = os.path.join(master_path, \"biotic_integrity\")\n",
    "stage_headcuts = os.path.join(master_path, \"headcuts_table\")\n",
    "stage_incision = os.path.join(master_path, \"incision\")\n",
    "stage_invasives = os.path.join(master_path, \"invasives\")\n",
    "#fdata = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "#Create dataframes from all tables\n",
    "#Function to convert a table to a Pandas DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Load each table from the geodatabase into a DataFrame New data \n",
    "#dfbanks = table_to_dataframe(os.path.join(master_path, \"bank_stability\"))\n",
    "#dfbiotic = table_to_dataframe(os.path.join(master_path, \"biotic_integrity\"))\n",
    "#dfheadcuts = table_to_dataframe(os.path.join(master_path, \"headcuts_table\"))\n",
    "#dfincision = table_to_dataframe(os.path.join(master_path, \"incision\"))\n",
    "#dfinvasives = table_to_dataframe(os.path.join(master_path, \"invasives\"))\n",
    "dfvegetation = table_to_dataframe(os.path.join(master_path, \"vegetation_vigor\"))\n",
    "#dfconifer = table_to_dataframe(os.path.join(master_path, \"incision\"))\n",
    "#dfAOP = table_to_dataframe(os.path.join(master_path, \"invasives\"))\n",
    "#dfditch = table_to_dataframe(os.path.join(master_path, \"vegetation\"))\n",
    "#dfhabitat = table_to_dataframe(os.path.join(master_path, \"vegetation\"))\n",
    "\n",
    "# Need scores from indicators in this table and must calculate all SEZS.. \n",
    "#MAster_df = table_to_dataframe(os.path.join(fdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Data with REST SERVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Paths to Staging tables in SDE... via REST service\n",
    "# Use rest service to get data \n",
    "#Setup\n",
    "import pandas as pd\n",
    "import os\n",
    "from arcgis.features import FeatureLayer\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def get_fs_data(bank_stability_url):\n",
    "    feature_layer = FeatureLayer(bank_stability_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "def get_fs_data(biotic_integrity_url):\n",
    "    feature_layer = FeatureLayer(biotic_integrity_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "def get_fs_data(conifer_url):\n",
    "    feature_layer = FeatureLayer(conifer_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "def get_fs_data(ditches_url):\n",
    "    feature_layer = FeatureLayer(ditches_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "def get_fs_data(invasives_url):\n",
    "    feature_layer = FeatureLayer(invasives_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "def get_fs_data(Hab_Frag_url):\n",
    "    feature_layer = FeatureLayer(Hab_Frag_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "def get_fs_data(vegetation_url):\n",
    "    feature_layer = FeatureLayer(vegetation_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "def get_fs_data(incision_url):\n",
    "    feature_layer = FeatureLayer(incision_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "def get_fs_data(headcuts_url):\n",
    "    feature_layer = FeatureLayer(headcuts_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "def get_fs_data(AOP_url):\n",
    "    feature_layer = FeatureLayer(AOP_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "def get_fs_data(all_sez_scores_url):\n",
    "    feature_layer = FeatureLayer(all_sez_scores_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "def get_fs_data(SEZ_url):\n",
    "    feature_layer = FeatureLayer(SEZ_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "bank_stability_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/4\"\n",
    "biotic_integrity_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/5\"\n",
    "conifer_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/6\"\n",
    "ditches_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/7\"\n",
    "invasives_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/11\"\n",
    "Hab_Frag_url = 'https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/8'\n",
    "vegetation_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/12\"\n",
    "incision_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/10\"\n",
    "headcuts_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/9\"\n",
    "AOP_url= \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/3\"\n",
    "all_sez_scores_url = \"\"\n",
    "\n",
    "SEZ_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/0\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Where data will be appended\n",
    "#All_SEZ_Scores_df = table_to_dataframe(os.path.join(master_path, \"All_SEZ_Scores\"))\n",
    "\n",
    "#Where do the comments come from??? Maybe SDE Collect in SEZ Site information?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframes from Rest Services\n",
    "\n",
    "dfbanks = get_fs_data(bank_stability_url)\n",
    "dfbiotic = get_fs_data(biotic_integrity_url)\n",
    "dfconifer = get_fs_data(conifer_url)\n",
    "dfditch = get_fs_data(ditches_url)\n",
    "dfinvasive = get_fs_data(invasives_url)\n",
    "dfhabitat = get_fs_data(Hab_Frag_url)\n",
    "dfvegetation = get_fs_data(vegetation_url)\n",
    "dfincision = get_fs_data(incision_url)\n",
    "dfheadcuts = get_fs_data(headcuts_url)\n",
    "dfAOP = get_fs_data(AOP_url)\n",
    "dfSEZ = get_fs_data(SEZ_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Assessment_Unit_Name  Year  Biotic_Integrity_Score\n",
      "0                           Angora meadows - 6  2020                     9.0\n",
      "1                               Big Meadow - 1  2022                     6.0\n",
      "2                     Big Meadow Creek - lower  2020                    12.0\n",
      "3                   Big Meadow Creek - upper 2  2021                     6.0\n",
      "4                    Blackwood Creek - Upper 2  2020                    12.0\n",
      "5                   Blackwood Creek - middle 2  2020                    12.0\n",
      "6                          Burke Creek - upper  2020                    12.0\n",
      "7                        Cascade Creek - upper  2022                     6.0\n",
      "8                 Christmas Valley meadows - 1  2020                    12.0\n",
      "9                 Christmas Valley meadows - 2  2022                    12.0\n",
      "10                         Cold Creek - middle  2022                    12.0\n",
      "11                     Deer Creek - headwaters  2020                     9.0\n",
      "12                         Deer Creek - middle  2020                    12.0\n",
      "13                      General Creek - middle  2022                    12.0\n",
      "14                         Griff Creek - upper  2022                    12.0\n",
      "15              Heavenly Valley Creek - middle  2022                     6.0\n",
      "16                 Hidden Valley Creek - upper  2022                     3.0\n",
      "17                       Incline Creek - lower  2020                    12.0\n",
      "18                   Logan House Creek - lower  2020                    12.0\n",
      "19                      McKinney Creek - lower  2020                    12.0\n",
      "20                      McKinney Creek - lower  2022                    12.0\n",
      "21                         Meeks Creek - upper  2020                    12.0\n",
      "22                         Meeks Creek - upper  2021                    12.0\n",
      "23                         Meeks Creek - upper  2022                    12.0\n",
      "24                           Meiss meadows - 1  2020                     6.0\n",
      "25                  North Zephyr Creek - upper  2020                    12.0\n",
      "26                   Osgood Creek - below road  2020                    12.0\n",
      "27                         Quail Creek - upper  2022                     9.0\n",
      "28                   Rosewood Creek - middle 2  2020                    10.5\n",
      "29                    Saxon Creek - headwaters  2022                    12.0\n",
      "30                         Saxon Creek - upper  2022                     6.0\n",
      "31                       Third Creek - lower 2  2020                    12.0\n",
      "32                         Trout Creek - upper  2020                    12.0\n",
      "33                         Trout Creek - upper  2022                    10.2\n",
      "34                Trout Creek above Black Bart  2022                     3.0\n",
      "35                Trout Creek below Black Bart  2020                     6.0\n",
      "36  Trout Creek meadows - above Fountain Place  2022                    12.0\n",
      "37       Trout Creek meadows - above Pioneer 3  2022                     3.0\n",
      "38                         UTR - Airport reach  2022                     3.0\n",
      "39                    UTR - Christmas Valley 1  2020                    12.0\n",
      "40                    UTR - Christmas Valley 1  2022                     9.0\n",
      "41                    UTR - Christmas Valley 3  2022                    12.0\n",
      "42                   UTR - Johnson meadows - 2  2022                     6.0\n",
      "43                               UTR - Reach 5  2022                     3.0\n",
      "44                                UTR - middle  2022                     6.0\n",
      "45                           UTR - tributary 3  2020                    12.0\n",
      "46                                 UTR - upper  2022                     6.0\n",
      "47                        UTR Marsh - UTR side  2020                     3.0\n",
      "48                        UTR Marsh - UTR side  2022                     3.0\n",
      "49                         Ward Creek - middle  2020                    12.0\n",
      "50                          Ward Creek - upper  2020                     9.0\n",
      "51                                Watson Creek  2020                    12.0\n",
      "52                                Watson Creek  2022                     3.0\n",
      "53                             small meadow 57  2022                     3.0\n"
     ]
    }
   ],
   "source": [
    "#Prep data?\n",
    "\n",
    "# Assuming dfbiotic is already loaded and has columns 'Assessment_Unit_Name', 'Year', and 'biotic_integrity_score'\n",
    "\n",
    "# Function to average scores for each Year and Assessment_Unit_Name\n",
    "def average_biotic_scores(dfbiotic, unit_col='Assessment_Unit_Name', year_col='Year', score_col='Biotic_Integrity_Score'):\n",
    "    # Convert year to numeric, if not already\n",
    "    #df[year_col] = pd.to_numeric(df[year_col], errors='coerce')\n",
    "    # Group by Assessment Unit and Year and calculate the mean of the scores\n",
    "    averaged_df = dfbiotic.groupby([unit_col, year_col], as_index=False)[score_col].mean()\n",
    "    return averaged_df\n",
    "\n",
    "# Apply the function to dfbiotic\n",
    "averaged_biotic_df = average_biotic_scores(dfbiotic)\n",
    "\n",
    "# Output the resulting DataFrame\n",
    "print(averaged_biotic_df)\n",
    "\n",
    "\n",
    "#Keep only score column and SEZ ID and Year for each df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns after reduction for score column 'Bank_Stability_Score': ['SEZ_ID', 'Assessment_Unit_Name', 'Year', 'Bank_Stability_Score']\n",
      "   SEZ_ID                          Assessment_Unit_Name  Year  \\\n",
      "0       0                     Kings Beach tributary - 5  2019   \n",
      "1       1    Saxon Creek meadows - below Fountain Place  2019   \n",
      "2       1    Saxon Creek meadows - below Fountain Place  2022   \n",
      "3       2  Saxon Creek meadows - above Fountain Place 1  2019   \n",
      "4       2  Saxon Creek meadows - above Fountain Place 1  2022   \n",
      "\n",
      "   Bank_Stability_Score  \n",
      "0                  12.0  \n",
      "1                  12.0  \n",
      "2                   6.0  \n",
      "3                   9.0  \n",
      "4                   9.0  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['SEZ_ID'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3612\\382317093.py\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# Keep only required columns for each DataFrame and inspect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_col\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdfs_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mreduced_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeep_required_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mreduced_dfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduced_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3612\\382317093.py\u001b[0m in \u001b[0;36mkeep_required_columns\u001b[1;34m(df, score_col, sez_id_col, unit_col, year_col)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Define a function to keep only the required columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mkeep_required_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msez_id_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SEZ_ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Assessment_Unit_Name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myear_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Year'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mreduced_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msez_id_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myear_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_col\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Ensure a copy is made to avoid SettingWithCopyWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\nColumns after reduction for score column '{score_col}': {reduced_df.columns.tolist()}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduced_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Display the first few rows of the reduced DataFrame for inspection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3765\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3766\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3767\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"columns\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3769\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5874\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5876\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5877\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5878\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5937\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5938\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{not_found} not in index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5940\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['SEZ_ID'] not in index\""
     ]
    }
   ],
   "source": [
    "##THIS NEEDS SOME WORK it is not putting the columns together correectly. We are trying to add the most up to date data for each indicator to the same row and then calculate the final score\n",
    "\n",
    "# Define a function to keep only the required columns\n",
    "def keep_required_columns(df, score_col, sez_id_col='SEZ_ID', unit_col='Assessment_Unit_Name', year_col='Year'):\n",
    "    reduced_df = df[[sez_id_col, unit_col, year_col, score_col]].copy()  # Ensure a copy is made to avoid SettingWithCopyWarning\n",
    "    print(f\"\\nColumns after reduction for score column '{score_col}': {reduced_df.columns.tolist()}\")\n",
    "    print(reduced_df.head())  # Display the first few rows of the reduced DataFrame for inspection\n",
    "    return reduced_df\n",
    "\n",
    "# List of DataFrames along with their corresponding score columns\n",
    "dfs_info = [\n",
    "    (dfbanks, 'Bank_Stability_Score'),\n",
    "    (averaged_biotic_df, 'Biotic_Integrity_Score'),\n",
    "    (dfconifer, 'Conifer_Encroachment_Score'),\n",
    "    (dfditch, 'Ditches_Score'),\n",
    "    (dfinvasive, 'Invasives_Score'),\n",
    "    (dfhabitat, 'Habitat_Frag_Score'),\n",
    "    (dfvegetation, 'VegetationVigor_Score'),\n",
    "    (dfincision, 'Incision_Score'),\n",
    "    (dfheadcuts, 'Headcuts_Score'),\n",
    "    (dfAOP, 'AOP_Score')\n",
    "]\n",
    "\n",
    "# List to store reduced DataFrames\n",
    "reduced_dfs = []\n",
    "\n",
    "# Keep only required columns for each DataFrame and inspect\n",
    "for df, score_col in dfs_info:\n",
    "    reduced_df = keep_required_columns(df, score_col)\n",
    "    reduced_dfs.append(reduced_df)\n",
    "\n",
    "# Verify the content of reduced_dfs\n",
    "for i, df in enumerate(reduced_dfs):\n",
    "    print(f\"\\nReduced DataFrame {i} for score column '{dfs_info[i][1]}':\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Assessment_Unit_Name  Year  Bank_Stability_Score  \\\n",
      "0      Angora Creek - tributary  2023                  12.0   \n",
      "1            Angora meadows - 1  2019                  12.0   \n",
      "2            Angora meadows - 2  2019                  12.0   \n",
      "3            Angora meadows - 3  2022                   9.0   \n",
      "4            Angora meadows - 6  2019                  12.0   \n",
      "..                          ...   ...                   ...   \n",
      "876          Logan House meadow  2019                   NaN   \n",
      "877  Slaughterhouse Meadows - 1  2019                   NaN   \n",
      "878      Susquehana meadows - 1  2019                   NaN   \n",
      "879          Van Sickle meadows  2019                   NaN   \n",
      "880              small meadow 5  2019                   NaN   \n",
      "\n",
      "     Biotic_Integrity_Score  Conifer_Encroachment_Score  Ditches_Score  \\\n",
      "0                       NaN                         NaN            NaN   \n",
      "1                       NaN                         NaN            NaN   \n",
      "2                       NaN                         NaN            NaN   \n",
      "3                       NaN                         NaN            NaN   \n",
      "4                       NaN                         NaN            NaN   \n",
      "..                      ...                         ...            ...   \n",
      "876                     NaN                         NaN            NaN   \n",
      "877                     NaN                         NaN            NaN   \n",
      "878                     NaN                         NaN            NaN   \n",
      "879                     NaN                         NaN            NaN   \n",
      "880                     NaN                         NaN            NaN   \n",
      "\n",
      "     Invasives_Scores  Habitat_Frag_Score  VegetationVigor_Score  \\\n",
      "0                 3.0                 NaN                    NaN   \n",
      "1                 6.0                 NaN                    NaN   \n",
      "2                 NaN                 NaN                    NaN   \n",
      "3                 NaN                 NaN                    NaN   \n",
      "4                 NaN                 NaN                    NaN   \n",
      "..                ...                 ...                    ...   \n",
      "876               NaN                 NaN                    NaN   \n",
      "877               NaN                 NaN                    NaN   \n",
      "878               NaN                 NaN                    NaN   \n",
      "879               NaN                 NaN                    NaN   \n",
      "880               NaN                 NaN                    NaN   \n",
      "\n",
      "     Incision_Score  Headcuts_Score  AOP_Score  \n",
      "0               3.0             9.0        NaN  \n",
      "1              12.0             NaN        NaN  \n",
      "2               NaN             NaN        NaN  \n",
      "3               6.0             NaN        NaN  \n",
      "4               NaN             NaN        NaN  \n",
      "..              ...             ...        ...  \n",
      "876             NaN             9.0        NaN  \n",
      "877             NaN             3.0        NaN  \n",
      "878             NaN             6.0        NaN  \n",
      "879             NaN             6.0        NaN  \n",
      "880             NaN             3.0        NaN  \n",
      "\n",
      "[881 rows x 12 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snewsome\\AppData\\Local\\Temp\\ipykernel_3612\\2926912764.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[year_col] = pd.to_numeric(df[year_col], errors='coerce')\n",
      "C:\\Users\\snewsome\\AppData\\Local\\Temp\\ipykernel_3612\\2926912764.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[year_col] = pd.to_numeric(df[year_col], errors='coerce')\n",
      "C:\\Users\\snewsome\\AppData\\Local\\Temp\\ipykernel_3612\\2926912764.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[year_col] = pd.to_numeric(df[year_col], errors='coerce')\n",
      "C:\\Users\\snewsome\\AppData\\Local\\Temp\\ipykernel_3612\\2926912764.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[year_col] = pd.to_numeric(df[year_col], errors='coerce')\n",
      "C:\\Users\\snewsome\\AppData\\Local\\Temp\\ipykernel_3612\\2926912764.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[year_col] = pd.to_numeric(df[year_col], errors='coerce')\n",
      "C:\\Users\\snewsome\\AppData\\Local\\Temp\\ipykernel_3612\\2926912764.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[year_col] = pd.to_numeric(df[year_col], errors='coerce')\n",
      "C:\\Users\\snewsome\\AppData\\Local\\Temp\\ipykernel_3612\\2926912764.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[year_col] = pd.to_numeric(df[year_col], errors='coerce')\n",
      "C:\\Users\\snewsome\\AppData\\Local\\Temp\\ipykernel_3612\\2926912764.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[year_col] = pd.to_numeric(df[year_col], errors='coerce')\n",
      "C:\\Users\\snewsome\\AppData\\Local\\Temp\\ipykernel_3612\\2926912764.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[year_col] = pd.to_numeric(df[year_col], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def keep_required_columns(df, score_col, unit_col='Assessment_Unit_Name', year_col='Year'):\n",
    "    return df[[unit_col, year_col, score_col]]\n",
    "\n",
    "# Define a function to get the most recent year data for each SEZ Assessment Unit\n",
    "def get_most_recent_data(df, unit_col='Assessment_Unit_Name', year_col='Year'):\n",
    "    df[year_col] = pd.to_numeric(df[year_col], errors='coerce')\n",
    "    most_recent_df = df.sort_values(by=[unit_col, year_col]).groupby(unit_col).tail(1)\n",
    "    return most_recent_df\n",
    "\n",
    "# List of DataFrames along with their corresponding score columns\n",
    "dfs_info = [\n",
    "    (dfbanks, 'Bank_Stability_Score'),\n",
    "    (averaged_biotic_df, 'Biotic_Integrity_Score'),\n",
    "    (dfconifer, 'Conifer_Encroachment_Score'),\n",
    "    (dfditch, 'Ditches_Score'),\n",
    "    (dfinvasive, 'Invasives_Scores'),\n",
    "    (dfhabitat, 'Habitat_Frag_Score'),\n",
    "    (dfvegetation, 'VegetationVigor_Score'),\n",
    "    (dfincision, 'Incision_Score'),\n",
    "    (dfheadcuts, 'Headcuts_Score'),\n",
    "    (dfAOP, 'AOP_Score')\n",
    "]\n",
    "\n",
    "# List to store reduced DataFrames\n",
    "reduced_dfs = []\n",
    "\n",
    "# Keep only required columns for each DataFrame\n",
    "for df, score_col in dfs_info:\n",
    "    reduced_df = keep_required_columns(df, score_col)\n",
    "    most_recent_df = get_most_recent_data(reduced_df)\n",
    "    reduced_dfs.append(most_recent_df)\n",
    "\n",
    "# Merge DataFrames in the reduced_dfs list on 'Assessment_Unit_Name' and 'Year'\n",
    "merged_df = reduced_dfs[0]\n",
    "for df in reduced_dfs[1:]:\n",
    "    merged_df = pd.merge(merged_df, df, on=['Assessment_Unit_Name', 'Year'], how='outer')\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Scores for Each SEZ This will go into table All_SEZ_Scores\n",
    "\n",
    "# Calculate the final score by summing up the values in each score column\n",
    "merged_df['Final_Total_Points'] = merged_df.iloc[:, 2:].sum(axis=1, skipna=True)\n",
    "\n",
    "\n",
    "merged_df['Final_Points_Possible']= dfSEZ['Final_Points_Possible']\n",
    "\n",
    "merged_df['SEZ_Type'] = dfSEZ['Final_Total_Points']\n",
    "\n",
    "# Calculate the final rating\n",
    "#merged_df['Final_Percent'] = merged_df['Final_Total_Points'] / merged_df['Final_Points_Possible']\n",
    "\n",
    "#merged_df['Final_Rating']= merged_df['Final_Percent'].apply(rate_SEZ)\n",
    "\n",
    "#merged_df['Final_Score']= merged_df['Final_Rating'].apply(score_indicator)\n",
    "\n",
    "#\n",
    "\n",
    "#merged_df['Acres']=dfSEZ['Acres']\n",
    "\n",
    "\n",
    "#Use Score indicator definition to calculate the grade for the SEZ... or is this where acres comes in\n",
    "#Calculate the Final Rating for the SEZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#DONT USE FOR NOWKeep only score column and SEZ ID and Year for each df\n",
    "# Define a function to keep only the required columns\n",
    "def keep_required_columns(df, score_col, sez_id_col='Assessment_Unit_Name', year_col='Year'):\n",
    "    return df[[sez_id_col, year_col, score_col]]\n",
    "\n",
    "# List of DataFrames along with their corresponding score columns\n",
    "dfs_info = [\n",
    "    (dfbanks, 'Bank_Stability_Score'),\n",
    "    (averaged_biotic_df, 'Biotic_Integrity_Score'),\n",
    "    (dfconifer, 'Conifer_Encroachment_Score'),\n",
    "    (dfditch, 'Ditches_Score'),\n",
    "    (dfinvasive, 'Invasives_Scores'),\n",
    "    (dfhabitat, 'Habitat_Frag_Score'),\n",
    "    (dfvegetation, 'VegetationVigor_Score'),\n",
    "    (dfincision, 'Incision_Score'),\n",
    "    (dfheadcuts, 'Headcuts_Score'),\n",
    "    (dfAOP, 'AOP_Score')\n",
    "]\n",
    "\n",
    "# List to store reduced DataFrames\n",
    "reduced_dfs = []\n",
    "\n",
    "# Keep only required columns for each DataFrame\n",
    "for df, score_col in dfs_info:\n",
    "    reduced_df = keep_required_columns(df, score_col)\n",
    "    reduced_dfs.append(reduced_df)\n",
    "\n",
    "\n",
    "# Merge DataFrames in the reduced_dfs list on 'Assessment_Unit_Name' and 'Year'\n",
    "#merged_df = reduced_dfs[0]\n",
    "#for df in reduced_dfs[1:]:\n",
    " #   merged_df = pd.merge(merged_df, df, on=['Assessment_Unit_Name', 'Year'], how='outer')\n",
    "\n",
    "#Drop duplicate rows based on 'Assessment_Unit_Name' and 'Year'\n",
    "#unique_merged_df = merged_df.drop_duplicates(subset=['Assessment_Unit_Name', 'Year'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "Passing 'suffixes' which cause duplicate columns {'GlobalID_x', 'created_date_x', 'SEZ_ID_x', 'created_user_x', 'last_edited_user_x', 'OBJECTID_x', 'last_edited_date_x'} is not allowed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3612\\772720264.py\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mmerged_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmost_recent_dfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmost_recent_dfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mmerged_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Assessment_Unit_Name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'outer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     )\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    809\u001b[0m         \u001b[0mjoin_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_join_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         result = self._reindex_and_concat(\n\u001b[0m\u001b[0;32m    812\u001b[0m             \u001b[0mjoin_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m         )\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_reindex_and_concat\u001b[1;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[0;32m    761\u001b[0m         \u001b[0mright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m         llabels, rlabels = _items_overlap_with_suffix(\n\u001b[0m\u001b[0;32m    764\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m         )\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_items_overlap_with_suffix\u001b[1;34m(left, right, suffixes)\u001b[0m\n\u001b[0;32m   2624\u001b[0m         \u001b[0mdups\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2625\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2626\u001b[1;33m         raise MergeError(\n\u001b[0m\u001b[0;32m   2627\u001b[0m             \u001b[1;34mf\"Passing 'suffixes' which cause duplicate columns {set(dups)} is \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2628\u001b[0m             \u001b[1;34mf\"not allowed.\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMergeError\u001b[0m: Passing 'suffixes' which cause duplicate columns {'GlobalID_x', 'created_date_x', 'SEZ_ID_x', 'created_user_x', 'last_edited_user_x', 'OBJECTID_x', 'last_edited_date_x'} is not allowed."
     ]
    }
   ],
   "source": [
    "# List of DataFrames\n",
    "dfs = [dfbanks, averaged_biotic_df, dfconifer, dfditch, dfinvasive, dfhabitat, dfvegetation, dfincision, dfheadcuts, dfAOP]\n",
    "\n",
    "# Filter each DataFrame to keep only the most recent year for each SEZ Assessment Unit\n",
    "most_recent_dfs = [get_most_recent_data(df) for df in dfs]\n",
    "\n",
    "# Merge DataFrames on 'SEZ Assessment Unit' and 'Year'\n",
    "merged_df = most_recent_dfs[0]\n",
    "for df in most_recent_dfs[1:]:\n",
    "    merged_df = pd.merge(merged_df, df, on=['Assessment_Unit_Name', 'Year'], how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "Passing 'suffixes' which cause duplicate columns {'GlobalID_x', 'created_date_x', 'SEZ_ID_x', 'created_user_x', 'last_edited_user_x', 'OBJECTID_x', 'last_edited_date_x'} is not allowed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3612\\565579921.py\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mmerged_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmost_recent_dfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmost_recent_dfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mmerged_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Assessment_Unit_Name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'outer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Merge DataFrames in the reduced_dfs list on 'SEZ Assessment Unit' and 'Year'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     )\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    809\u001b[0m         \u001b[0mjoin_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_join_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         result = self._reindex_and_concat(\n\u001b[0m\u001b[0;32m    812\u001b[0m             \u001b[0mjoin_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m         )\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_reindex_and_concat\u001b[1;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[0;32m    761\u001b[0m         \u001b[0mright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m         llabels, rlabels = _items_overlap_with_suffix(\n\u001b[0m\u001b[0;32m    764\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m         )\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_items_overlap_with_suffix\u001b[1;34m(left, right, suffixes)\u001b[0m\n\u001b[0;32m   2624\u001b[0m         \u001b[0mdups\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2625\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2626\u001b[1;33m         raise MergeError(\n\u001b[0m\u001b[0;32m   2627\u001b[0m             \u001b[1;34mf\"Passing 'suffixes' which cause duplicate columns {set(dups)} is \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2628\u001b[0m             \u001b[1;34mf\"not allowed.\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMergeError\u001b[0m: Passing 'suffixes' which cause duplicate columns {'GlobalID_x', 'created_date_x', 'SEZ_ID_x', 'created_user_x', 'last_edited_user_x', 'OBJECTID_x', 'last_edited_date_x'} is not allowed."
     ]
    }
   ],
   "source": [
    "#combine all dataframes and process data to get final scores for SEZs\n",
    "\n",
    "# List of DataFrames\n",
    "dfs = [dfbanks, averaged_biotic_df, dfconifer, dfditch, dfinvasive, dfhabitat, dfvegetation, dfincision, dfheadcuts, dfAOP]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to get the most recent year data for each SEZ Assessment Unit\n",
    "def get_most_recent_data(df, unit_col='Assessment_Unit_Name', year_col='Year'):\n",
    "    df[year_col] = pd.to_numeric(df[year_col], errors='coerce')\n",
    "    most_recent_df = df.sort_values(by=[unit_col, year_col]).groupby(unit_col).tail(1)\n",
    "    return most_recent_df\n",
    "\n",
    "\n",
    "# Filter each DataFrame to keep only the most recent year for each SEZ Assessment Unit\n",
    "most_recent_dfs = [get_most_recent_data(df) for df in dfs]\n",
    "\n",
    "# Merge DataFrames on 'SEZ Assessment Unit'\n",
    "merged_df = most_recent_dfs[0]\n",
    "for df in most_recent_dfs[1:]:\n",
    "    merged_df = pd.merge(merged_df, df, on=['Assessment_Unit_Name', 'Year'], how='outer')\n",
    "\n",
    "# Merge DataFrames in the reduced_dfs list on 'SEZ Assessment Unit' and 'Year'\n",
    "#merged_df = reduced_dfs[0]\n",
    "#for df in reduced_dfs[1:]:\n",
    " #   merged_df = pd.merge(merged_df, df, on=['Assessment_Unit_Name', 'Year'], how='outer')\n",
    "\n",
    "\n",
    "# Calculate the final score based on the columns available (example: summing scores)\n",
    "# Adjust the calculation as needed based on your specific scoring criteria\n",
    "# For example, if each DataFrame has a column 'score' we can sum them:\n",
    "#score_columns = [col for col in merged_df.columns if 'score' in col]\n",
    "#merged_df['Final_Total_Points'] = merged_df[score_columns].sum(axis=1, skipna=True)\n",
    "\n",
    "# Display the final DataFrame with SEZ Assessment Unit and Final Score\n",
    "#final_score_df = merged_df[['Assessment_Unit_Name', 'Final_Total_Points']]\n",
    "\n",
    "# Output the final DataFrame\n",
    "#print(final_score_df)\n",
    "\n",
    "# Concatenate along rows (stacking DataFrames vertically)\n",
    "#concatenated_df = pd.concat([bank_stability_df, biotic_integrity_df, headcuts_df, incision_df, invasives_df], ignore_index=True)\n",
    "\n",
    "#Create a df with only the scores for each indicator and then get the sum across the row to get a final score? or just use the big table and pick out score columsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by assessment unit name, SEZ ID, and SEZ type, and sum the scores for each group\n",
    "final_df = combined_df.groupby(['Assessment_Unit_Name', 'SEZ_ID', 'SEZ_Type', 'Year']).agg({'Score': 'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames on common columns \"Assessment_Unit_Name\" and \"Year\"\n",
    "#merged_df = pd.merge(bank_stability_df, biotic_integrity_df, on=[\"Assessment_Unit_Name\", \"Year\", \"SEZ_ID\"], how=\"left\", suffixes=('_bank', '_biotic'))\n",
    "#merged_df = pd.merge(merged_df, headcuts_df, on=[\"Assessment_Unit_Name\", \"Year\", \"SEZ_ID\"], how=\"left\", suffixes=('_merged', '_headcuts'))\n",
    "#merged_df = pd.merge(merged_df, incision_df, on=[\"Assessment_Unit_Name\", \"Year\", \"SEZ_ID\"], how=\"left\", suffixes=('_merged', '_incision'))\n",
    "#merged_df = pd.merge(merged_df, invasives_df, on=[\"Assessment_Unit_Name\", \"Year\", \"SEZ_ID\"], how=\"left\", suffixes=('_merged', '_invasives'))\n",
    "#merged_df = pd.merge(merged_df, vegetation_df, on =[\"Assessment_Unit_Name\", \"Year\"], how=\"left\", suffixes=('_merged', '_veg')))\n",
    "#This isn't quite working.. some double records and some \n",
    "#Master df add year\n",
    "#MAster_df['Year']= 2019 \n",
    "# Assuming your DataFrame is named df, and \"OBJECTID\" is the column you want to drop\n",
    "#MAster_df.drop(columns=[\"OBJECTID\"], inplace=True)\n",
    "#Combine all data into one table???\n",
    "#Alldatadf= pd.merge(merged_df, MAster_df, on=[\"Assessment_Unit_Name\", \"Year\",\"SEZ_ID\"], how=\"left\", suffixes=('_merged', '_olddata'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# New approach for creating our final table.. \n",
    "\n",
    "#Get Baseline information about each SEZ and then fill this in as we go. \n",
    "keep_only = ['Assessment_Unit_Name', 'SEZ_ID', 'SEZ_Type', 'Points_Possible'] \n",
    "All_SEZdf= MAster_df[keep_only].copy()\n",
    "\n",
    "merged_df\n",
    "\n",
    "Input new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Prep and post ending dataframe to All Scores table in SEZ_Data.GDB\n",
    "#----------------------------------------------------------------#\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    '': 'Year',\n",
    "    '': 'Comments',\n",
    "    '': 'SEZ_Type',\n",
    "    '': 'Final_Rating',\n",
    "    '': 'Final_Percent',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    '': 'Final_Points_Possible',\n",
    "    '': 'Final_Total_Points'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = BIdf.rename(columns=field_mapping).drop(columns=[col for col in BIdf.columns if col not in field_mapping])\n",
    "\n",
    "readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code to make it a spatially enabled dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to conver to spatially enable geodataframe\n",
    "# Create a new DataFrame Final_df with columns based on the field mapping\n",
    "#df = df.rename(columns=field_mapping)\n",
    "\n",
    "# Convert DataFrame to Spatially Enabled DataFrame\n",
    "#sedf = GeoAccessor.from_xy(df, x_column='LONGITUDE', y_column='LATITUDE')\n",
    "\n",
    "# Convert the SEDF to a feature class without sanitizing columns\n",
    "#sedf.spatial.to_featureclass(location=os.path.join(workspace, 'PlanAreaNoise_23_Staging'), sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
