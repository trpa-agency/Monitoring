{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: arcgis in c:\\program files\\arcgis\\pro\\bin\\python\\envs\\arcgispro-py3\\lib\\site-packages (2.2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install arcgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureSet, GeoAccessor, GeoSeriesAccessor, FeatureLayer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.geometry import SpatialReference\n",
    "#import geopandas as gpd to use spatial.reference stuff\n",
    "gis = GIS()\n",
    "# # Set Pandas display options to show all rows and columns\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# in memory output file path\n",
    "wk_memory = \"memory\" + \"\\\\\"\n",
    "\n",
    "\n",
    "# set workspace and sde connections \n",
    "#working_folder = \"C:\\GIS\"\n",
    "\n",
    "#workspace      = \"C:\\GIS\\Scratch.gdb\"\n",
    "#arcpy.env.workspace = \"C:\\GIS\\Scratch.gdb\"\n",
    "#workspace ='F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "#arcpy.env.workspace = 'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "\n",
    "# network path to connection files\n",
    "filePath = \"F:\\\\GIS\\\\DB_CONNECT\"\n",
    "\n",
    "#path to GDB's to update and master data\n",
    "master_path = \"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "SEZ_Master = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "#set workspace for connection to GDB\n",
    "#workspace=master_path\n",
    "# database file paths \n",
    "### SDE Collection New data collected is put into SDE.Survey under the indicator name\n",
    "### SDE Vector is where the data will go \n",
    "#sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "sdeBase    = os.path.join(filePath, \"SarahVector.sde\")\n",
    "#sdeCollect = os.path.join(filepath, \"SarahCollect.sde\")\n",
    "\n",
    "# setup connection string???\n",
    "connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=sql12;DATABASE=sde;UID=sde;PWD=staff\"\n",
    "connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "engine = create_engine(connection_url)\n",
    "\n",
    "#with engine.begin() as sdeConnect:\n",
    " #   erosiondf      = pd.read_sql(\"SELECT * FROM sde.SDE.Stream_Erosion\", sdeConnect)\n",
    "\n",
    "# local variables sdata is starting data and f data is finishing datatables\n",
    "ffdata = os.path.join(sdeBase, \"sde.SDE.Monitoring\")\n",
    "sdata = os.path.join(sdeCollect, \"sde.SDE.Survey\")\n",
    "fdata = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "\n",
    "##Tables we get the data from in Collect\n",
    "sezsurveytable = os.path.join(sdata, \"sde.SDE.sez_survey\")\n",
    "erosiondata = os.path.join(sdata, \"sde.SDE.Stream_Erosion\")\n",
    "incisiondata = os.path.join(sdata, \"sde.SDE.sez_channel_incision\")\n",
    "invasivedata = os.path.join(sdata, \"sde.SDE.sez_invasive_plant\")\n",
    "headcutdata = os.path.join(sdata, \"sde.SDE.sez_stream_headcut\")\n",
    "streamdata = os.path.join(ffdata, \"sde.SDE.Stream\")\n",
    "\n",
    "\n",
    "#Staging Tables currently living in SEZ_Data.GDB\n",
    "stage_bank_stability = os.path.join(master_path, \"bank_stability\") \n",
    "stage_All_SEZ_Scores = os.path.join(master_path, \"All_SEZ_Scores\")\n",
    "stage_biotic_integrity = os.path.join(master_path, \"biotic_integrity\")\n",
    "stage_headcuts = os.path.join(master_path, \"headcuts_table\")\n",
    "stage_incision = os.path.join(master_path, \"incision\")\n",
    "stage_invasives = os.path.join(master_path, \"invasives\")\n",
    "\n",
    "\n",
    "\n",
    "#Final table to append to\n",
    "#finalSEZtable = os.path.join(fdata, \"sde.SDE.SEZ_Assessment_Unit\")\n",
    "#finalSEZtable = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "\n",
    "\n",
    "# network path to connection files??????\n",
    "#filePath = \"C:\\\\GIS\\\\DB_CONNECT\"\n",
    "# network path to connection files\n",
    "#filePath = r'F:\\Research and Analysis\\Workspace\\Sarah'\n",
    "\n",
    "#--------------------------------------------#\n",
    "#Notes to self\n",
    "#--------------------------------------------#\n",
    "#F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb. \n",
    "###'Assessment_Unit_Master' has all data for SEZ\n",
    "###“Bioassessment Sample Locations” has stream sites in SEZs\n",
    "\n",
    "gdbworking_folder = \"F:\\GIS\\GIS_DATA\\Monitoring\"\n",
    "headcutgdbfolder = os.path.join(gdbworking_folder, \"Stream_Headcut\", \"StreamHeadcut_Survey\")\n",
    "invasivegdbfolder = os.path.join(gdbworking_folder, \"Invasive_Species\", \"Invasive_Species_Survey\")\n",
    "## GDB with Raw Data straight from S123 not in the original folder (that one is not edited)\n",
    "headcut23gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2023.gdb\")\n",
    "headcut22gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2022.gdb\")\n",
    "headcut20gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2020.gdb\")\n",
    "headcut19gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2019.gdb\")\n",
    "#erosion23gdb = os.path.join(gdbworking_folder, \"Stream_Erosion\",\"Stream_Erosion_Survey\", \"Stream_Erosion_Survey_2023.gdb\")\n",
    "#erosion22gdb = os.path.join(working_folder, \"Stream_Erosion\",\"Stream_Erosion_Survey\", \"Stream_Erosion_Survey_2022.gdb\")\n",
    "#erosion20gdb = os.path.join(working_folder, \"Stream_Erosion\",\"Stream_Erosion_Survey\", \"Stream_Erosion_Survey_2020.gdb\")\n",
    "#channelincision23gdb = os.path.join(working_folder,\"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2023.gdb\")\n",
    "#channelincision22gdb = os.path.join(working_folder, \"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2022.gdb\")\n",
    "#channelincision20gdb = os.path.join(working_folder, \"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2020.gdb\")\n",
    "invasiveplant23gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2023.gdb\")\n",
    "invasiveplant22gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2022.gdb\")\n",
    "invasiveplant20gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2020.gdb\")\n",
    "invasiveplant19gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2019.gdb\")\n",
    "\n",
    "#This is thelocatoin for the final SEZ GDB to be updated in the gdb on f drive in the AssessmentUnits Master (polygon) i believe\n",
    "#FinalGDBtoupdate:F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data \\SEZ_Data.gdb\n",
    "\n",
    "#Location of USFS Invasive Species Data\n",
    "#https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer\n",
    "\n",
    "#Monitoring Dashboard location\n",
    "#Finalsdelocation:f'Vector.SDE' Sde.Monitoring Sde. SEZ_Assessment_Unit\n",
    "\n",
    "#Threshold Location? sde.tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup Dictionary for SEZ ID's -Old code uses AssessmentUnit_Master feature class in SEZ_Data.gdb- don't use for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------#\n",
    "#Create look up dictionary for SEZ_ID fill in--currently works for stream erosion code--\n",
    "#-----------------------------------------------------------------------------------#\n",
    "#Add in SEZ_Survey table to match up parentglobalid to assessment unit name so we can match an sez_id\n",
    "#----------------------------------------------------------------##Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "# Set workspace environment\n",
    "arcpy.env.workspace = master_path\n",
    "\n",
    "# Specify the feature class name SEZ_Master\n",
    "#feature_class = \"AssessmentUnits_Master\"\n",
    "# Create a cursor to iterate over the rows in the feature class\n",
    "fields = ['Assessment_Unit_Name', 'SEZ_ID']\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "# Iterate over the rows in the second feature class and append to data list\n",
    "with arcpy.da.SearchCursor(SEZ_Master, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "# Convert the data to a Pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Define function to update SEZ ID lookup dictionary\n",
    "def update_SEZID_lookup_dict(df, lookup_dict):\n",
    "    for index, row in df.iterrows():\n",
    "        # Update SEZ_ID column in DataFrame with data from the lookup dictionary\n",
    "        df.at[index, 'SEZ_ID'] = lookup_dict.get(row['Assessment_Unit_Name'], None)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Assuming 'Assessment_Unit_Name' is the common identifier between DataFrame and lookup dictionary\n",
    "selected_columns = ['Assessment_Unit_Name', 'SEZ_ID']\n",
    "\n",
    "# Drop duplicates based on 'Assessment_Unit_Name' and keep the first occurrence\n",
    "unique_values = df.drop_duplicates(subset='Assessment_Unit_Name', keep='first')\n",
    "\n",
    "# Convert selected columns to dictionary\n",
    "lookup_dict = unique_values[selected_columns].set_index('Assessment_Unit_Name').to_dict(orient='index')\n",
    "\n",
    "# Display the dictionary\n",
    "print(lookup_dict)\n",
    "\n",
    "# Update 'SEZ_ID' in other DataFrame with data from the lookup dictionary do this for each indicator...\n",
    "#Other_df = update_SEZID_lookup_dict(Other_df, lookup_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------#\n",
    "# DONT USE YETT Create look up dictionary for SEZ_ID fill in with Global ID,SEZ ID, and Assessment Unit Name\n",
    "\n",
    "# Set workspace environment\n",
    "arcpy.env.workspace = master_path\n",
    "\n",
    "# Define the fields for SEZ_Master and the additional feature class\n",
    "Masterfields = ['Assessment_Unit_Name', 'SEZ_ID']\n",
    "surveyfields = ['assessment_unit_name', 'GlobalID']  # Add any additional fields you need\n",
    "\n",
    "# Create empty lists to store data\n",
    "Masterdata = []\n",
    "surveydata = []\n",
    "\n",
    "# Iterate over the rows in the SEZ_Master feature class and append to data list\n",
    "with arcpy.da.SearchCursor(SEZ_Master, Masterfields) as cursor:\n",
    "    for row in cursor:\n",
    "        Masterdata.append(row)\n",
    "\n",
    "# Iterate over the rows in the additional feature class and append to data list\n",
    "with arcpy.da.SearchCursor(sezsurveytable, surveyfields) as cursor:\n",
    "    for row in cursor:\n",
    "        surveydata.append(row)\n",
    "\n",
    "# Convert the data to Pandas DataFrames\n",
    "Masterdf = pd.DataFrame(Masterdata, columns=Masterfields)\n",
    "surveydf = pd.DataFrame(surveydata, columns=surveyfields)\n",
    "# Convert the column name in surveydf to match Masterdf\n",
    "surveydf = surveydf.rename(columns={'assessment_unit_name': 'Assessment_Unit_Name'})\n",
    "# Merge the DataFrames on 'Assessment_Unit_Name'\n",
    "SEZIDdf = pd.merge(Masterdf, surveydf, on='Assessment_Unit_Name', how='inner')\n",
    "\n",
    "# Check for missing values in 'GlobalID' or 'SEZ_ID'\n",
    "missing_values = SEZIDdf[SEZIDdf['GlobalID'].isna() | SEZIDdf['SEZ_ID'].isna()]\n",
    "\n",
    "# Replace missing values in 'SEZ_ID' with NaN\n",
    "SEZIDdf.loc[missing_values.index, 'SEZ_ID'] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(SEZIDdf)\n",
    "# Assuming 'Assessment_Unit_Name' is the common identifier between DataFrame and lookup dictionary\n",
    "selected_columns = ['Assessment_Unit_Name', 'SEZ_ID', 'GlobalID']\n",
    "\n",
    "# Drop duplicates based on 'Assessment_Unit_Name' and keep the first occurrence\n",
    "unique_values = SEZIDdf.drop_duplicates(subset='Assessment_Unit_Name', keep='first')\n",
    "\n",
    "# Convert selected columns to dictionary\n",
    "lookup_dict = unique_values[selected_columns].set_index('Assessment_Unit_Name').to_dict(orient='index')\n",
    "\n",
    "# Display the dictionary\n",
    "print(lookup_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New SEZ ID based off of excel file- larger polygons only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Angora Creek - tributary': 519, 'Angora Creek - upper': 520, 'Angora meadows - 1': 87, 'Angora meadows - 2': 90, 'Angora meadows - 3': 91, 'Angora meadows - 4': 143, 'Angora meadows - 5': 144, 'Angora meadows - 6': 142, 'Angora meadows - 7': 89, 'Angora meadows - 8': 88, 'Angora meadows - 9': 92, 'Angora meadows tributary - 1': 217, 'Angora meadows tributary - 2': 99, 'Angora meadows tributary - 3': 97, 'Angora meadows tributary - 4': 94, 'Angora meadows tributary - 5': 214, 'Angora meadows tributary - 6': 146, 'Angora meadows tributary - 7': 93, 'Angora meadows tributary - 8': 95, 'Angora meadows tributary - 9': 96, 'Angora tributary': 446, 'Antone meadows': 187, 'Baldwin marsh - 1': 160, 'Benwood meadows - 1': 129, 'Benwood meadows - 2': 131, 'Big Meadow - 1': 47, 'Big Meadow - 2': 48, 'Big Meadow - 3': 38, 'Big Meadow - 4': 39, 'Big Meadow - 5': 37, 'Big Meadow - 6': 40, 'Big meadow - 7': 126, 'Big Meadow Creek - lower': 521, 'Big Meadow Creek - upper': 491, 'Big Meadow Creek - upper 2': 522, 'Bijou meadows - 1': 115, 'Bijou meadows - 2': 116, 'Bijou meadows - 3': 164, 'Bijou meadows - private': 114, 'Bijou meadows - tributary 1': 226, 'Bijou Park Creek meadows - 1': 219, 'Bijou Park Creek meadows - 2': 166, 'Bijou Park Creek meadows - 3': 167, 'Bijou Park Creek meadows - 4': 163, 'Bijou Park Creek meadows - 5': 489, 'Bijou Park Creek meadows - 6': 488, 'Blackwood Creek - lower 1': 523, 'Blackwood Creek - lower 2': 596, 'Blackwood Creek - middle 1': 597, 'Blackwood Creek - middle 2': 524, 'Blackwood Creek - middle 3': 598, 'Blackwood Creek - middle 4': 525, 'Blackwood Creek - Upper 1': 599, 'Blackwood Creek - Upper 2': 526, 'Blackwood Creek - upper 3': 527, 'Blackwood meadows - 1': 66, 'Blackwood meadows - 3': 65, 'Buck Lake meadows': 176, 'Buddhas meadow': 168, 'Burke Creek - middle': 528, 'Burke Creek - upper': 503, 'Burke Creek meadows - 1': 171, 'Burke Creek meadows - 2': 3, 'Burke Creek tributary': 461, 'Burton Creek - lower': 633, 'Burton Creek - upper': 529, 'Carnelian Canyon Creek - lower': 620, 'Carnelian Canyon Creek - upper': 621, 'Cascade Creek - lower': 501, 'Cascade Creek - upper': 498, 'Casino meadows': 170, 'Christmas Valley meadows - 1': 136, 'Christmas Valley meadows - 2': 134, 'Christmas Valley meadows - 3': 130, 'Christmas Valley meadows - 4': 225, 'Cold Creek - Highland Woods': 110, 'Cold Creek - middle': 530, 'Cold Creek - tributary 1': 532, 'Cold Creek - tributary 2': 533, 'Cold Creek - tributary 3': 531, 'Cold Creek - upper': 534, 'Cold Creek tributary - 4': 465, 'Cold Creek tributary - 5': 466, 'Colony Inn meadows - lower': 169, 'Colony Inn meadows - upper': 485, 'Cookhouse meadow': 128, 'Deer Creek - headwaters': 535, 'Deer Creek - lower': 537, 'Deer Creek - middle': 538, 'Deer Creek - middle 2': 539, 'Deer Creek - upper': 536, 'Deer Creek meadows': 30, 'Dollar Creek - lower': 540, 'Dollar Creek - upper': 512, 'Eagle Creek': 497, 'Echo Creek - below lake': 541, 'Echo Creek - upper': 494, 'Edgewood Creek - middle': 542, 'Edgewood Creek tributary - 2 - headwaters': 462, 'Edgewood Creek tributary - 2 - upper': 635, 'Edgewood Creek tributary - 2 - lower': 476, 'Edgewood Creek tributary - 3 - lower': 628, 'Edgewood Creek tributary - 3 - upper': 629, 'Edgewood meadows': 221, 'Elks Club meadows - 1': 141, 'Elks Club meadows - 2': 85, 'Fallen Leaf meadows - 1': 154, 'Fallen Leaf meadows - 2': 155, 'Fallen Leaf meadows - 3': 147, 'Fallen Leaf meadows - 4': 76, 'First Creek - lower': 543, 'First Creek - upper': 516, 'Freel Meadows - 1': 25, 'Freel Meadows - 2': 24, 'Gardner meadow': 150, 'General Creek - lower': 544, 'General Creek - middle': 506, 'General Creek - upper': 545, 'General Creek meadows': 61, 'Ginny Lake Meadows': 193, 'Glen Alpine Creek - lower': 546, 'Glen Alpine Creek - upper': 606, 'Glenbrook Creek - middle': 510, 'Glenbrook Creek - upper': 547, 'Glenbrook meadows - 1': 177, 'Glenbrook meadows - 2': 178, 'Golden Bear meadows - 1': 212, 'Golden Bear meadows - 2': 196, 'Grass Lake Creek': 609, 'Grass Lake meadow': 127, 'Griff Creek - lower': 514, 'Griff Creek - tributary': 548, 'Griff Creek - upper': 549, 'Griff Creek meadows': 35, 'Haypress Meadows': 50, 'Heavenly Valley Creek - middle': 550, 'Heavenly Valley Creek - upper': 499, 'Heavenly Valley Creek meadows - 1': 111, 'Heavenly Valley Creek meadows - 2': 112, 'Heavenly Valley Creek meadows - 3': 152, 'Heavenly Valley Creek meadows - 4': 113, 'Hell Hole meadows - 1': 230, 'Hell Hole Meadows - 2': 8, 'Hidden Valley Creek - lower': 551, 'Hidden Valley Creek - upper': 552, 'High meadows - 1': 203, 'High meadows - 2': 198, 'High Meadows - 3': 200, 'High meadows - 4': 202, 'High meadows - 6': 437, 'Homewood Canyon Creek - lower': 600, 'Homewood Canyon Creek - upper': 508, 'Incline Creek - lower': 553, 'Incline Creek - middle 1': 554, 'Incline Creek - middle 2': 555, 'Incline Creek - middle 3': 636, 'Incline Creek - ski run': 556, 'Incline Creek - upper': 557, 'Incline Lake meadows - 1': 192, 'Incline Lake meadows - 2': 190, 'Incline Village tributary - 1': 458, 'Incline Village tributary - 2': 459, 'Incline Village tributary - 3': 460, 'Incline Village tributary - 4': 457, 'Kahle meadows - 2': 204, 'Kahle meadows - 3': 172, 'Kahle meadows - 4': 468, 'Kahle meadows - 5': 118, 'Kahle meadows - 6': 469, 'Kahle meadows - 7': 119, 'Kahle meadows - 8': 486, 'Kingsbury meadows': 222, 'Lake Forest meadows - 1': 185, 'Lake Forest meadows - 2': 186, 'Lake Forest meadows - 3': 184, 'Lake Forest meadows - 4': 487, 'Lake Forest meadows - 5': 223, 'Lake Forest meadows - 6': 183, 'Lake Forest tributary': 624, 'Lakeshore meadows': 72, 'Logan House Creek - lower': 558, 'Logan House Creek - upper': 507, 'Logan House meadow': 13, 'Lonely Gulch Creek - lower': 559, 'Lonely Gulch Creek - middle': 560, 'Lonely Gulch Creek - upper': 504, 'Madden Creek': 561, 'Marlette Creek - lower': 562, 'Marlette Creek - old dam site': 564, 'Marlette Creek - south fork (lower)': 642, 'Marlette Creek - south fork (upper)': 563, 'Marlette Creek - upper': 631, 'Marlette Lake meadows': 32, 'McFaul Creek - lower': 565, 'McFual meadow': 71, 'McKinney Creek - lower': 566, 'McKinney Creek - middle': 567, 'McKinney Creek - upper': 505, 'McKinney tributary - 1': 626, 'McKinney tributary - 2': 453, 'Meeks Bay meadows - 1': 205, 'Meeks Bay meadows - 2': 207, 'Meeks Bay meadows - 3': 36, 'Meeks Bay meadows - 4': 206, 'Meeks Creek - upper': 502, 'Meeks Bay Lagoon': 235, 'Meiss meadows - 1': 123, 'Meiss meadows - 2': 122, 'Meiss meadows - 3': 124, 'Meiss meadows - 4': 210, 'Meiss meadows - 5': 209, 'Meyers meadow': 218, 'Meyers tributary - 1': 447, 'Meyers tributary - 2': 467, 'Mill Creek - lower': 568, 'Mill Creek - upper': 515, 'Mill Creek meadows': 234, 'Mount Rainier Drive meadows - 1': 215, 'Mount Rainier Drive meadows - 2': 216, 'Muskawi Drive meadows': 83, 'North Logan House Creek': 509, 'North Logan House meadows': 12, 'North Zephyr Creek - lower': 570, 'North Zephyr Creek - middle': 615, 'North Zephyr Creek - tributary': 569, 'North Zephyr Creek - upper': 571, 'Nottaway Drive meadows': 213, 'Osgood Creek - above road': 572, 'Osgood Creek - below road': 573, 'Osgood Swamp': 482, 'Paige meadows': 182, 'Pope marsh meadow': 483, 'Quail Creek - lower': 574, 'Quail Creek - upper': 575, 'Quail Creek meadow': 26, 'Rosewood Creek - lower': 576, 'Rosewood Creek - middle 1': 586, 'Rosewood Creek - middle 2': 587, 'Rosewood Creek - middle 3': 588, 'Rubicon Creek': 601, 'Rubicon Creek - tributary': 602, 'Rubicon Meadows': 60, 'Saxon Creek - headwaters': 495, 'Saxon Creek - upper': 641, 'Saxon Creek meadows - above Fountain Place 1': 2, 'Saxon Creek meadows - above Fountain Place 2': 102, 'Saxon Creek meadows - below Fountain Place': 1, 'Saxon Creek tributary meadows - 1': 101, 'Saxon Creek tributary meadows - 3': 100, 'Saxon Creek tributary meadows - 4': 140, 'Saxon Creek tributary meadows - 5': 197, 'Saxon Creek tributary meadows - 6': 139, 'Saxon Creek tributary meadows - 7': 231, 'Second Creek - lower': 591, 'Second Creek - lower 2': 592, 'Second Creek - middle': 593, 'Second Creek - upper': 517, 'Secret Harbor Creek - lower': 618, 'Secret Harbor Creek - upper': 619, 'Sierra Tract wetlands': 211, 'Ski Run meadows': 220, 'Sky meadows': 79, 'Skylandia SEZ': 622, 'Slaughterhouse Creek - lower': 614, 'Slaughterhouse Creek - middle': 616, 'Slaughterhouse Creek - upper': 617, 'Slaughterhouse Meadows - 1': 6, 'Slaughterhouse meadows - 2': 180, 'small meadow 1': 11, 'small meadow 10': 64, 'small meadow 100': 77, 'small meadow 105': 125, 'small meadow 111': 132, 'small meadow 112': 133, 'small meadow 113': 135, 'small meadow 116': 181, 'small meadow 13': 20, 'small meadow 14': 19, 'small meadow 15': 18, 'small meadow 16': 21, 'small meadow 17': 28, 'small meadow 19': 63, 'small meadow 2': 10, 'small meadow 20': 623, 'small meadow 21': 67, 'small meadow 22': 53, 'small meadow 23': 54, 'small meadow 24': 56, 'small meadow 25': 55, 'small meadow 26': 57, 'small meadow 27': 58, 'small meadow 28': 59, 'small meadow 29': 174, 'small meadow 3': 70, 'small meadow 30': 175, 'small meadow 32': 51, 'small meadow 35': 49, 'small meadow 36': 52, 'small meadow 40': 74, 'small meadow 5': 29, 'small meadow 50': 23, 'small meadow 51': 42, 'small meadow 52': 41, 'small meadow 54': 22, 'small meadow 56': 46, 'small meadow 57': 73, 'small meadow 58': 173, 'small meadow 59': 14, 'small meadow 6': 17, 'small meadow 7': 27, 'small meadow 8': 69, 'small meadow 82': 232, 'small meadow 9': 68, 'small meadow 92': 31, 'small meadow 93': 33, 'small meadow 95': 43, 'small meadow 96': 44, 'small meadow 98': 62, 'small meadow 99': 75, 'Snow Creek tributary - 1': 451, 'Snow Creek tributary - 2': 452, 'Snow Creek wetlands - 1': 188, 'Snow Creek wetlands - 2': 189, 'South Lake Tahoe - wetland 1': 229, 'South Lake Tahoe airport': 484, 'South Lake Tahoe tributary - 1': 463, 'South Lake Tahoe tributary - 2': 464, 'South Lake Tahoe tributary - 3': 627, 'Spooner meadows - 1': 445, 'Spooner meadows - 2': 431, 'Spooner meadows - 3': 120, 'Spooner Meadows - 4': 5, 'Spooner meadows - 5': 121, 'Star Lake meadows': 45, 'Susquehana meadows - 1': 108, 'Susquehana meadows - 2': 107, 'Tahoe City meadow': 224, 'Tahoe City tributary - 1': 449, 'Tahoe City tributary - 2': 450, 'Tahoe Island meadows - 1': 158, 'Tahoe Island meadows - 2': 156, 'Tahoe Keys': 162, 'Tahoe Paradise golf course': 632, 'Tahoe Valley meadows - 1': 153, 'Tahoe Valley meadows - 2': 228, 'Tahoe Vista meadows': 227, 'Tallac Creek - abv highway - 1': 334, 'Tallac Creek - abv highway - 2': 604, 'Tallac Creek - tributary': 500, 'Tallac marsh': 159, 'Tallac meadows': 157, 'Taylor Creek': 605, 'Taylor Creek marsh': 208, 'Third Creek - headwaters': 585, 'Third Creek - lower': 577, 'Third Creek - lower 2': 578, 'Third Creek - middle': 581, 'Third Creek - middle 1': 579, 'Third Creek - middle 2': 580, 'Third Creek - upper 1': 582, 'Third Creek - upper 2': 584, 'Third Creek - upper 3': 583, 'Third Creek meadows - 1': 191, 'Third Creek meadows - 3': 34, 'Third Creek meadows - 4': 195, 'Third Creek meadows - 6': 194, 'Third Creek meadows - 7': 16, 'Third Creek meadows - 8': 15, 'Trout Creek - Highland Woods': 109, 'Trout Creek - tributary 2': 613, 'Trout Creek - tributary 3': 612, 'Trout Creek - upper': 496, 'Trout Creek above Black Bart': 149, 'Trout Creek below Black Bart': 161, 'Trout Creek headwaters meadows - 1': 137, 'Trout Creek headwaters meadows - 2': 9, 'Trout Creek meadows - above Fountain Place': 103, 'Trout Creek meadows - above Pioneer 1': 148, 'Trout Creek meadows - above Pioneer 2': 106, 'Trout Creek meadows - above Pioneer 3': 105, 'Trout Creek meadows - above Pioneer 4': 104, 'Upper Truckee River - Meyers': 138, 'Upper Truckee River - Tahoe Paradise': 7, 'UTR - Airport reach': 82, 'UTR - Christmas Valley 1': 608, 'UTR - Christmas Valley 3': 493, 'UTR - golf course meadows': 86, 'UTR - Johnson meadows - 2': 151, 'UTR - Johnson meadows - 3': 81, 'UTR - middle': 492, 'UTR - Reach 5': 80, 'UTR - Reach 6': 84, 'UTR - tributary 1': 610, 'UTR - tributary 3': 611, 'UTR - upper': 490, 'UTR - Washoe Meadows': 607, 'UTR marsh - Trout Creek side': 165, 'UTR Marsh - UTR side': 78, 'Van Sickle meadows': 117, 'Ward Creek - lower': 594, 'Ward Creek - middle': 595, 'Ward Creek - upper': 511, 'Ward Creek meadow': 625, 'Washoan Blvd meadows': 145, 'Washoe State Parks meadow - 1': 4, 'Washoe State Parks meadow - 2': 98, 'Watson Creek': 513, 'West Shore tributary - 1': 455, 'West Shore tributary - 2 - lower': 639, 'West Shore tributary - 2 - upper': 448, 'West Shore tributary - 3': 456, 'West Shore tributary - 4': 454, 'Woods Creek - lower': 589, 'Woods Creek - middle': 590, 'Woods Creek - upper': 518, 'High meadows - 5': 201}\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------#\n",
    "#Create look up dictionary for SEZ_ID fill in--currently works for stream erosion code--- change to use excel so its not messy\n",
    "#-----------------------------------------------------------------------------------#\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "excel_data = pd.read_excel(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\SEZID lookup.xlsx\")  # Replace 'your_excel_file.xlsx' with the path to your Excel file\n",
    "\n",
    "#Define Empty look up dataframe\n",
    "lookup_dict = {}\n",
    "\n",
    "for index, row in excel_data.iterrows():\n",
    "    lookup_dict[row['Assessment_Unit_Name']] = row['SEZ_ID']\n",
    "\n",
    "# See dictionary where keys are Assessment Unit Names and values are SEZ IDs\n",
    "print(lookup_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# Display the dictionary using pprint\n",
    "pprint.pprint(lookup_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Grading each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grading for each parameter \n",
    "#Defining Grade for Bank Stability based on Erosiondf[percent_unstable]\n",
    "def categorize_erosion(Percent_Unstable):\n",
    "    if 0 <= Percent_Unstable < 5:\n",
    "        return 'A'\n",
    "    elif 5 <= Percent_Unstable < 20:\n",
    "        return 'B'\n",
    "    elif 20 <= Percent_Unstable < 50:\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'D'\n",
    "    \n",
    "#Scoring based off of grading - check this\n",
    "def score_indicator(Rating):\n",
    "    if  Rating == 'A':\n",
    "        return '12'\n",
    "    elif Rating == 'B':\n",
    "        return '9'\n",
    "    elif Rating == 'C':\n",
    "        return '6'\n",
    "    else:\n",
    "        return '3'\n",
    "\n",
    "#Define Grade for Incision based off of incisino ratio\n",
    "\n",
    "def categorize_incision(bankfull_ratio):\n",
    "    if 0 <= bankfull_ratio < 1.2:\n",
    "        return 'A'\n",
    "    elif 1.2 <= bankfull_ratio < 1.6:\n",
    "        return 'B'\n",
    "    elif 1.6 <= bankfull_ratio < 2.1:\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'D'\n",
    "\n",
    "#Define Grade for Bioassessment Score\n",
    "#def categorize_csci(biotic_integrity)\n",
    " #    if   biotic_integrity > 0.92:\n",
    "  #      return 'A'\n",
    "   # elif 0.79 < biotic_integrity <= 0.92:\n",
    "    #    return 'B'\n",
    "    #elif 0.62 < biotic_integrity <= 0.79:\n",
    "     #   return 'C'\n",
    "    #else:\n",
    "     #   return 'D'\n",
    "\n",
    "#Define Priority List Level of Invasive Plant Species\n",
    "    \n",
    "\n",
    " #Define Grade for Invasive Species based off of Priority List\n",
    "#A=0, B=1 level 2, C 2 level 2 or 1 level 1, D=3+ level 2 or 2+ level 1\n",
    "def rate_invasive(priority):\n",
    "    if priority['Level 2'] == 1:\n",
    "        return 'B'  \n",
    "    elif priority['Level 2'] == 2 or priority['Level 1'] == 1:\n",
    "        return 'C'  \n",
    "    elif priority['Level 2'] >= 3 or priority['Level 1'] >= 2:\n",
    "       return 'D' \n",
    "    else:\n",
    "        return 'A'  # Assign score A (no invasives)     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Define Size for Headcut based off of headcut size\n",
    "##A = 0 headcut, B 1+small headcut\n",
    "def categorize_headcut(headcutdepth):\n",
    "    if pd.isnull(headcutdepth) or headcutdepth == 0:\n",
    "        return np.nan\n",
    "    elif 0.1 <= headcutdepth < 0.5:\n",
    "        return 'small'\n",
    "    elif 0.5 <= headcutdepth < 1:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'large'\n",
    "\n",
    "#define rating for headcut health per sez\n",
    "def rate_headcut(row):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if row['large'] >= 1:\n",
    "        return 'D'  # Assign score D\n",
    "    elif row['medium'] >= 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif row['small'] >= 1:\n",
    "       return 'B'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no headcuts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Erosion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Get Data\n",
    "#----------------------------------------------------------------##Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "erosionfields = ['Assessment_Unit_Name', 'Shape.STLength()', 'Bank_Type', 'Survey_Date']\n",
    "#erosiondf = pd.DataFrame.from_records(data=arcpy.da.SearchCursor(erosiondata, erosionfields), columns=erosionfields)\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    " # Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(erosiondata, erosionfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "erosiondf = pd.DataFrame(data, columns=erosionfields)\n",
    "\n",
    "# Replace NaN values in 'Assessment_Unit_Name' column with 'Skylandia SEZ'\n",
    "#erosiondf['Assessment_Unit_Name'] = erosiondf['Assessment_Unit_Name'].fillna('Skylandia SEZ')\n",
    "# Replace specific values in 'Assessment_Unit_Name' column\n",
    "erosiondf['Assessment_Unit_Name'] = erosiondf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh'})\n",
    "\n",
    "\n",
    "#Add SEZ_ID column using lookup dictionary\n",
    "#erosiondf['SEZ_ID'] = erosiondf['Assessment_Unit_Name'].map(lambda x: lookup_dict.get(x, {}).get('SEZ_ID', None))\n",
    "\n",
    "#This code is for the excel look up dictionary\n",
    "erosiondf['SEZ_ID'] = erosiondf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "erosiondf['SEZ_ID'] = erosiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "erosiondf['SEZ_ID'] = erosiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#erosiondf = erosiondf.merge(df, on='Assessment_Unit_Name', how='left')\n",
    "\n",
    "#calculate year column \n",
    "erosiondf['Year'] = erosiondf['Survey_Date'].dt.year\n",
    "\n",
    "# Replace 'both_banks' with 'Both Banks' in Bank_Type column\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['both_banks', 'Both banks'], 'Both Banks' )\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['one_bank', 'One bank'], 'One Bank')\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['no_bank', 'No bank'], 'No Bank')\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Process Data\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "# Initialize variables\n",
    "erosiondf['bank_multiplier'] = erosiondf['Bank_Type'].apply(lambda x: 2 if x == 'Both Banks' else (1 if x == 'One Bank' else 0))\n",
    "\n",
    "\n",
    "# Calculate the product of 'Shape.STLength()' and 'bank_multiplier' to get the eroded banks per row\n",
    "erosiondf['eroded_banks_per_row'] = erosiondf['Shape.STLength()'] * erosiondf['bank_multiplier']\n",
    "\n",
    "# Group by Assessment_Unit_Name and year and sum the lengths of banks for each unit to get total banks assessed\n",
    "erosiondf['banks_assessed_per_unit'] = erosiondf.groupby(['Assessment_Unit_Name', 'Year'])['Shape.STLength()'].transform('sum') * 2\n",
    "\n",
    "# Group by Assessment_Unit_Name and sum the eroded banks per row for each unit\n",
    "erosiondf['SEZ_total_eroded'] = erosiondf.groupby(['Assessment_Unit_Name', 'Year'])['eroded_banks_per_row'].transform('sum')\n",
    "\n",
    "# Calculate percent unstable Bank Stability per Assessment Unit\n",
    "erosiondf['Bank_Stability_Percent_Unstable'] = (erosiondf['SEZ_total_eroded'] / erosiondf['banks_assessed_per_unit']) * 100\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score\n",
    "#----------------------------------------------------------------#\n",
    "erosiondf['Bank_Stability_Rating']=erosiondf['Bank_Stability_Percent_Unstable'].apply(categorize_erosion)\n",
    "erosiondf['Bank_Stability_Score']= erosiondf['Bank_Stability_Rating'].apply(score_indicator)\n",
    "\n",
    "erosiondf['Bank_Stability_Data_Source'] = 'TRPA'\n",
    "\n",
    "erosiondf.head()\n",
    "\n",
    "print(erosiondf)\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to bank_stability called stage_bank_stability GDB location\n",
    "#----------------------------------------------------------------#\n",
    "# Define the name of the feature class\n",
    "feature_class_name = 'bank_stability'\n",
    "\n",
    "# Define the full path to the feature class\n",
    "feature_class_path = stage_bank_stability \n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Bank_Stability_Data_Source': 'Bank_Stability_Data_Source',\n",
    "    'Bank_Stability_Percent_Unstable': 'Bank_Stability_Percent_Unstable',\n",
    "    'Bank_Stability_Rating': 'Bank_Stability_Rating',\n",
    "    'Bank_Stability_Score': 'Bank_Stability_Score',\n",
    "    'SEZ_ID': 'SEZ_ID'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "bank_stabilitydf = erosiondf.rename(columns=field_mapping).drop(columns=[col for col in erosiondf.columns if col not in field_mapping])\n",
    "\n",
    "readydf = bank_stabilitydf.groupby(['SEZ_ID', 'Year']).first().reset_index()\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "# Fix data type of Year so it writes to table\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y', errors='coerce')\n",
    "\n",
    "# Setting the frequency to 'Y' for year\n",
    "#readydf['Year'] = readydf['Year'].dt.to_period('Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "#Write dataframe to fc in SEZ_Data.GDB\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "with arcpy.da.InsertCursor(stage_bank_stability, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")\n",
    "\n",
    "\n",
    "#Write dataframe to sde.collect.bank.stability eventually, current code write it to GDB in SEZ_Data.GDB\n",
    "# Set environment workspace to your SDE connection file\n",
    "#arcpy.env.workspace = master_path\n",
    "\n",
    "# Convert DataFrame to Feature Class\n",
    "#output_feature_class = \"ErosionUpdate\"  # Name for the output feature class\n",
    "#output_fc_path = os.path.join(arcpy.env.workspace, output_feature_class)\n",
    "\n",
    "# Assuming your DataFrame is already converted to a feature class\n",
    "# Replace \"path_to_your_feature_class\" with the actual path to your feature class\n",
    "#arcpy.conversion.TableToTable(\"path_to_your_feature_class\", arcpy.env.workspace, output_feature_class)\n",
    "\n",
    "# Overwrite Feature Class in SDE\n",
    "# Replace \"path_to_your_dataframe\" with the actual path to your DataFrame\n",
    "#arcpy.management.CopyFeatures(\"path_to_your_dataframe\", output_fc_path)\n",
    "\n",
    "\n",
    "#print(ready_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(readydf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to CSV\n",
    "#csv_path = os.path.join(working_folder, \"Erosiondatamaster.csv\")\n",
    "#ready_df.to_csv(csv_path, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lookup dictionary for parentglobalid,sez_id,Assessment unit\n",
    "\n",
    "#parentglobalid= global id in sez survey and then we can match upa ssessment unit name... do we even need this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Incision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Get Data-- all the data we need is in sde.collect.sde.survey.sde.sez_survey\n",
    "#----------------------------------------------------------------##Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "incisionfields = ['Assessment_Unit_Name', 'incision_ratio', 'survey_date']\n",
    "#incisiondf = pd.DataFrame.from_records(data=arcpy.da.SearchCursor(erosiondata, erosionfields), columns=erosionfields)\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    " # Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(sezsurveytable, incisionfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "incisiondf = pd.DataFrame(data, columns=incisionfields)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Add correct info to dataframe\n",
    "#----------------------------------------------------------------#\n",
    "#use this until we fix the domain\n",
    "incisiondf['Assessment_Unit_Name'] = incisiondf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh'})\n",
    "# Create a new column 'SEZ ID'\n",
    "#This code is for the excel look up dictionary\n",
    "incisiondf['SEZ_ID'] = incisiondf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#calculate year column \n",
    "incisiondf['Year'] = incisiondf['survey_date'].dt.year\n",
    "\n",
    "\n",
    "#incisiondf['SEZ ID'] = np.nan\n",
    "\n",
    "# Iterate through the rows in incisiondf\n",
    "#for index, row in incisiondf.iterrows():\n",
    " #   parentglobalid = row['parentglobalid']\n",
    "    \n",
    "      # Check if the parent_global_id exists in the lookup dictionary\n",
    "  #  if parentglobalid in lookup_dict:\n",
    "        # If it exists, retrieve the corresponding SEZ ID and fill it into SEZ ID column\n",
    "   #     corresponding_entry = lookup_dict[parentglobalid]\n",
    "    #    assert row['GlobalID'] == parentglobalid, \"ParentGlobalID does not match GlobalID in the lookup dictionary\"\n",
    "     #   incisiondf.at[index, 'SEZ_ID'] = corresponding_entry['SEZ_ID']\n",
    "\n",
    "# Display the updated incisiondf\n",
    "print(incisiondf)\n",
    "\n",
    "#Add SEZ_ID column using lookup dictionary\n",
    "#incisiondf['SEZ_ID'] = SEZIDdf['GlobalID'].map(lambda x: lookup_dict.get(x, {}).get('SEZ_ID', None))\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "#incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "#incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#incisiondf = incisiondf.merge(df, on='Assessment_Unit_Name', how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score\n",
    "#----------------------------------------------------------------#\n",
    "incisiondf['Incision_Rating']=incisiondf['incision_ratio'].apply(categorize_incision)\n",
    "incisiondf['Incision_Score']= incisiondf['Incision_Rating'].apply(score_indicator)\n",
    "\n",
    "incisiondf['Incision_Data_Source'] = 'TRPA'\n",
    "\n",
    "incisiondf.head()\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to incision called stage_incision GDB location\n",
    "#----------------------------------------------------------------#\n",
    "# Define the name of the feature class\n",
    "feature_class_name = 'incision'\n",
    "\n",
    "# Define the full path to the feature class\n",
    "feature_class_path = stage_incision \n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Incision_Data_Source': 'Incision_Data_Source',\n",
    "    'incision_ratio': 'Incision_Ratio',\n",
    "    'Incision_Rating': 'Incision_Rating',\n",
    "    'Incision_Score': 'Incision_Score',\n",
    "    'SEZ_ID': 'SEZ_ID'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "incisionfinaldf = incisiondf.rename(columns=field_mapping).drop(columns=[col for col in incisiondf.columns if col not in field_mapping])\n",
    "\n",
    "readydf = incisionfinaldf.groupby(['SEZ_ID', 'Year']).first().reset_index()\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "#Write dataframe to fc in SEZ_Data.GDB\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "with arcpy.da.InsertCursor(stage_incision, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invasive Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Define the USFS REST endpoint\n",
    "usfsrest = \"https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer/0\"\n",
    "usfsfeature_layer = FeatureLayer(usfsrest)\n",
    "\n",
    "# Define the spatial filter condition to retrieve data within the Tahoe area polygon\n",
    "tahoe_geometry = \"POLYGON ((-120.0 39.0, -120.0 38.8, -119.7 38.8, -119.7 39.0, -120.0 39.0))\"\n",
    "\n",
    "# Define the date after which you want the data\n",
    "start_date = datetime(2020, 1, 1)\n",
    "\n",
    "# Convert start date to Unix timestamp (milliseconds since epoch)\n",
    "start_timestamp = int(start_date.timestamp() * 1000)\n",
    "\n",
    "# Define query parameters\n",
    "query_params = {\n",
    "    'geometryType': 'esriGeometryPolygon',\n",
    "    'spatialRel': 'esriSpatialRelWithin',  \n",
    "    'geometry': tahoe_geometry,\n",
    "    'out_fields': '*',  \n",
    "    'returnGeometry': True,  \n",
    "    'returnTrueCurves': False,  \n",
    "    'returnIdsOnly': False,  \n",
    "    'returnCountOnly': False,  \n",
    "    'returnExtentOnly': False,  \n",
    "    'returnDistinctValues': False,  \n",
    "    'orderByFields': '',  \n",
    "    'groupByFieldsForStatistics': '',  \n",
    "    'outStatistics': '',  \n",
    "    'resultOffset': 0,  \n",
    "    'resultRecordCount': 2000,  # Adjust this based on the maximum records per query\n",
    "    'f': 'pjson'  ,\n",
    "    'where': f\"DATE_COLLECTED >= {start_timestamp}\"  # Filter for dates after 2020\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop until all records are retrieved\n",
    "while True:\n",
    "    # Perform query to retrieve data\n",
    "    result = usfsfeature_layer.query(\n",
    "        geometryFilter=query_params['geometry'], \n",
    "        where=query_params['where'],  # Include the where clause\n",
    "        out_fields=query_params['out_fields'], \n",
    "        returnGeometry=query_params['returnGeometry'], \n",
    "        returnTrueCurves=query_params['returnTrueCurves'], \n",
    "        returnIdsOnly=query_params['returnIdsOnly'], \n",
    "        returnCountOnly=query_params['returnCountOnly'], \n",
    "        returnExtentOnly=query_params['returnExtentOnly'], \n",
    "        returnDistinctValues=query_params['returnDistinctValues'], \n",
    "        orderByFields=query_params['orderByFields'], \n",
    "        groupByFieldsForStatistics=query_params['groupByFieldsForStatistics'], \n",
    "        outStatistics=query_params['outStatistics'], \n",
    "        resultOffset=query_params['resultOffset'], \n",
    "        resultRecordCount=query_params['resultRecordCount'], \n",
    "        f=query_params['f']\n",
    "    ).sdf\n",
    "    \n",
    "    # Check if there are records returned\n",
    "    if len(result) == 0:\n",
    "        break  # Exit loop if no more records\n",
    "    \n",
    "    # Append the dataframe to the list\n",
    "    dfs.append(result)\n",
    "    \n",
    "    # Update offset for the next query\n",
    "    query_params['resultOffset'] += query_params['resultRecordCount']\n",
    "\n",
    "# Concatenate all dataframes\n",
    "usfssdf = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Convert the 'DATE_COLLECTED' column to datetime format\n",
    "usfssdf['DATE_COLLECTED'] = pd.to_datetime(usfssdf['DATE_COLLECTED'])\n",
    "\n",
    "# Extract date portion\n",
    "usfssdf['DATE_COLLECTED'] = usfssdf['DATE_COLLECTED'].dt.date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "An unknown error occurred: Traceback (most recent call last):\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\arcgis\\gis\\_impl\\_con\\_connection.py\", line 1466, in post\n    resp = self._session.post(\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\arcgis\\auth\\api.py\", line 525, in post\n    return self._session.post(\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\sessions.py\", line 637, in post\n    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\sessions.py\", line 710, in send\n    r = dispatch_hook(\"response\", hooks, r, **kwargs)\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\hooks.py\", line 30, in dispatch_hook\n    _hook_data = hook(hook_data, **kwargs)\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\arcgis\\auth\\_auth\\_negotiate.py\", line 399, in _response_hook\n    elif r.text.lower().find(\"token required\") > -1:\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\models.py\", line 923, in text\n    if not self.content:\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\models.py\", line 899, in content\n    self._content = b\"\".join(self.iter_content(CONTENT_CHUNK_SIZE)) or b\"\"\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\models.py\", line 816, in generate\n    yield from self.raw.stream(chunk_size, decode_content=True)\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\urllib3\\response.py\", line 624, in stream\n    for line in self.read_chunked(amt, decode_content=decode_content):\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\urllib3\\response.py\", line 831, in read_chunked\n    chunk = self._handle_chunk(amt)\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\urllib3\\response.py\", line 775, in _handle_chunk\n    value = self._fp._safe_read(amt)\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\http\\client.py\", line 626, in _safe_read\n    chunk = self.fp.read(min(amt, MAXAMOUNT))\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\socket.py\", line 704, in readinto\n    return self._sock.recv_into(b)\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\ssl.py\", line 1275, in recv_into\n    return self.read(nbytes, buffer)\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\ssl.py\", line 1133, in read\n    return self._sslobj.read(len, buffer)\nKeyboardInterrupt\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\arcgis\\gis\\_impl\\_con\\_connection.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, path, params, files, **kwargs)\u001b[0m\n\u001b[0;32m   1465\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1466\u001b[1;33m                     resp = self._session.post(\n\u001b[0m\u001b[0;32m   1467\u001b[0m                         \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\arcgis\\auth\\api.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[0mredirects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_redirects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m         return self._session.post(\n\u001b[0m\u001b[0;32m    526\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"POST\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;31m# Response manipulation hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"response\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\hooks.py\u001b[0m in \u001b[0;36mdispatch_hook\u001b[1;34m(key, hooks, hook_data, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0m_hook_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_hook_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\arcgis\\auth\\_auth\\_negotiate.py\u001b[0m in \u001b[0;36m_response_hook\u001b[1;34m(self, r, **kwargs)\u001b[0m\n\u001b[0;32m    398\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_retry_using_http_Negotiate_auth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheme\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"token required\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mscheme\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Negotiate\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"NTLM\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mtext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 923\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    924\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mcontent\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    898\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    815\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 816\u001b[1;33m                     \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    817\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mstream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    623\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunked\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupports_chunked_reads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_chunked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    830\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 831\u001b[1;33m                 \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    832\u001b[0m                 decoded = self._decode(\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36m_handle_chunk\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    774\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    776\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mamt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m             \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1275\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1276\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19624\\3745463678.py\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# Perform query to retrieve data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     result = usfsfeature_layer.query(\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mgeometryFilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'geometry'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mout_fields\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'out_fields'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\arcgis\\features\\layer.py\u001b[0m in \u001b[0;36mquery\u001b[1;34m(self, where, out_fields, time_filter, geometry_filter, return_geometry, return_count_only, return_ids_only, return_distinct_values, return_extent_only, group_by_fields_for_statistics, statistic_filter, result_offset, result_record_count, object_ids, distance, units, max_allowable_offset, out_sr, geometry_precision, gdb_version, order_by_fields, out_statistics, return_z, return_m, multipatch_option, quantization_parameters, return_centroid, return_all_records, result_type, historic_moment, sql_format, return_true_curves, return_exceeded_limit_features, as_df, datum_transformation, **kwargs)\u001b[0m\n\u001b[0;32m   2261\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mas_df\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2262\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_query_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2263\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_raw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2265\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"returnCountOnly\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\arcgis\\features\\layer.py\u001b[0m in \u001b[0;36m_query\u001b[1;34m(self, url, params, raw, **kwargs)\u001b[0m\n\u001b[0;32m   3671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3672\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3673\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mqueryException\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3675\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mis_true\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\arcgis\\features\\layer.py\u001b[0m in \u001b[0;36m_query\u001b[1;34m(self, url, params, raw, **kwargs)\u001b[0m\n\u001b[0;32m   3624\u001b[0m         \u001b[1;34m\"\"\"returns results of query\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3625\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3626\u001b[1;33m             result = self._con.post(\n\u001b[0m\u001b[0;32m   3627\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3628\u001b[0m                 \u001b[0mpostdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\arcgis\\gis\\_impl\\_con\\_connection.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, path, params, files, **kwargs)\u001b[0m\n\u001b[0;32m   1517\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1519\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"An unknown error occurred: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_exc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1520\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbuffer_reader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1521\u001b[0m             \u001b[0mbuffer_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: An unknown error occurred: Traceback (most recent call last):\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\arcgis\\gis\\_impl\\_con\\_connection.py\", line 1466, in post\n    resp = self._session.post(\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\arcgis\\auth\\api.py\", line 525, in post\n    return self._session.post(\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\sessions.py\", line 637, in post\n    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\sessions.py\", line 710, in send\n    r = dispatch_hook(\"response\", hooks, r, **kwargs)\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\hooks.py\", line 30, in dispatch_hook\n    _hook_data = hook(hook_data, **kwargs)\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\arcgis\\auth\\_auth\\_negotiate.py\", line 399, in _response_hook\n    elif r.text.lower().find(\"token required\") > -1:\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\models.py\", line 923, in text\n    if not self.content:\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\models.py\", line 899, in content\n    self._content = b\"\".join(self.iter_content(CONTENT_CHUNK_SIZE)) or b\"\"\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\requests\\models.py\", line 816, in generate\n    yield from self.raw.stream(chunk_size, decode_content=True)\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\urllib3\\response.py\", line 624, in stream\n    for line in self.read_chunked(amt, decode_content=decode_content):\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\urllib3\\response.py\", line 831, in read_chunked\n    chunk = self._handle_chunk(amt)\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\urllib3\\response.py\", line 775, in _handle_chunk\n    value = self._fp._safe_read(amt)\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\http\\client.py\", line 626, in _safe_read\n    chunk = self.fp.read(min(amt, MAXAMOUNT))\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\socket.py\", line 704, in readinto\n    return self._sock.recv_into(b)\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\ssl.py\", line 1275, in recv_into\n    return self.read(nbytes, buffer)\n  File \"c:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\ssl.py\", line 1133, in read\n    return self._sslobj.read(len, buffer)\nKeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#try with looping straight from the gpt, works but takes 95 minutes!\n",
    "import pandas as pd\n",
    "from arcgis.features import FeatureLayer\n",
    "\n",
    "# Define the USFS REST endpoint\n",
    "usfsrest = \"https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer/0\"\n",
    "usfsfeature_layer = FeatureLayer(usfsrest)\n",
    "\n",
    "# Define the spatial filter condition to retrieve data within the Tahoe area polygon\n",
    "tahoe_geometry = \"POLYGON ((-120.0 39.0, -120.0 38.8, -119.7 38.8, -119.7 39.0, -120.0 39.0))\"\n",
    "\n",
    "# Define query parameters\n",
    "query_params = {\n",
    "    'geometryType': 'esriGeometryPolygon',\n",
    "    'spatialRel': 'esriSpatialRelWithin',  \n",
    "    'geometry': tahoe_geometry,\n",
    "    'out_fields': '*',  \n",
    "    'returnGeometry': True,  \n",
    "    'returnTrueCurves': False,  \n",
    "    'returnIdsOnly': False,  \n",
    "    'returnCountOnly': False,  \n",
    "    'returnExtentOnly': False,  \n",
    "    'returnDistinctValues': False,  \n",
    "    'orderByFields': '',  \n",
    "    'groupByFieldsForStatistics': '',  \n",
    "    'outStatistics': '',  \n",
    "    'resultOffset': 0,  \n",
    "    'resultRecordCount': 2000,  # Adjust this based on the maximum records per query\n",
    "    'f': 'pjson'  \n",
    "}\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop until all records are retrieved\n",
    "while True:\n",
    "    # Perform query to retrieve data\n",
    "    result = usfsfeature_layer.query(\n",
    "        geometryFilter=query_params['geometry'], \n",
    "        out_fields=query_params['out_fields'], \n",
    "        returnGeometry=query_params['returnGeometry'], \n",
    "        returnTrueCurves=query_params['returnTrueCurves'], \n",
    "        returnIdsOnly=query_params['returnIdsOnly'], \n",
    "        returnCountOnly=query_params['returnCountOnly'], \n",
    "        returnExtentOnly=query_params['returnExtentOnly'], \n",
    "        returnDistinctValues=query_params['returnDistinctValues'], \n",
    "        orderByFields=query_params['orderByFields'], \n",
    "        groupByFieldsForStatistics=query_params['groupByFieldsForStatistics'], \n",
    "        outStatistics=query_params['outStatistics'], \n",
    "        resultOffset=query_params['resultOffset'], \n",
    "        resultRecordCount=query_params['resultRecordCount'], \n",
    "        f=query_params['f']\n",
    "    ).sdf\n",
    "    \n",
    "    # Check if there are records returned\n",
    "    if len(result) == 0:\n",
    "        break  # Exit loop if no more records\n",
    "    \n",
    "    # Append the dataframe to the list\n",
    "    dfs.append(result)\n",
    "    \n",
    "    # Update offset for the next query\n",
    "    query_params['resultOffset'] += query_params['resultRecordCount']\n",
    "\n",
    "# Concatenate all dataframes\n",
    "usfssdf = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Convert the 'date_field' column to datetime format\n",
    "usfssdf['DATE_COLLECTED'] = pd.to_datetime(usfssdf['DATE_COLLECTED'])\n",
    "\n",
    "# Extract date portion\n",
    "usfssdf['DATE_COLLECTED'] = usfssdf['DATE_COLLECTED'].dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter usfs data down to correct timeframe ... make the data smalller from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2024, 12, 31)\n",
    "\n",
    "# Convert start and end dates to Pandas Timestamp objects\n",
    "start_timestamp = pd.Timestamp(start_date)\n",
    "end_timestamp = pd.Timestamp(end_date)\n",
    "\n",
    "# Convert the 'DATE_COLLECTED' column to datetime format\n",
    "usfssdf['DATE_COLLECTED'] = pd.to_datetime(usfssdf['DATE_COLLECTED'])\n",
    "\n",
    "# Perform the comparison after converting Timestamps to dates\n",
    "query = (usfssdf['DATE_COLLECTED'].dt.date >= start_date.date()) & (usfssdf['DATE_COLLECTED'].dt.date <= end_date.date())\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = usfssdf[query].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEZsdf = pd.DataFrame.spatial.from_featureclass(SEZ_Master)\n",
    "#SEZsdf = Geoaccessor.from_features(SEZ_Master)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Target feature Assessment Unit Polygons\n",
    "#SEZ_Master = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb/AssessmentUnits_Master\"\n",
    "\n",
    "# Create the spatially enabled DataFrame (sdf) for target feature SEZ assessment units\n",
    "#SEZsdf = pd.DataFrame.spatial.from_featureclass(SEZ_Master)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Now you have a Spatially Enabled DataFrame (SEZsdf) from the feature layer in the geodatabase\n",
    "\n",
    "#spatial reference stuff\n",
    "usfssdf.spatial.sr = SEZsdf.spatial.sr\n",
    "\n",
    "#CAsdf.spatial.set_spatial_reference(SEZsdf.spatial.sr)\n",
    "#perform spatial join\n",
    "#joinedsdf = SEZsdf.spatial.join(usfssdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis.features import GeoAccessor\n",
    "#Target feature Assessment Unit Polygons\n",
    "SEZ_Master = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb/AssessmentUnits_Master\"\n",
    "\n",
    "# Create the spatially enabled DataFrame (sdf) for target feature SEZ assessment units\n",
    "SEZsdf = pd.DataFrame.spatial.from_featureclass(SEZ_Master)\n",
    "#Calfora Data points--dates only go to 2012?\n",
    "# dates only go to 2012 hereCArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines07/MapServer/1\"\n",
    "#CArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines27/MapServer/0\"\n",
    "# Create a feature layer object\n",
    "#cafeature_layer = FeatureLayer(CArest)\n",
    "\n",
    "# Query the features and convert them into a spatial DataFrame\n",
    "#CAsdf = cafeature_layer.query().sdf\n",
    "\n",
    "#USFS Data polygons#\n",
    "#usfsrest= \"https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer/0\"\n",
    "#usfsfeature_layer = FeatureLayer(usfsrest)\n",
    "#usfssdf = usfsfeature_layer.query().sdf\n",
    "\n",
    "#Query caflora layer to that it only shows 2020-2023 or only county?\n",
    "#start_year = 'January 1, 2020'\n",
    "#end_year= 'December 31, 2023'\n",
    "# Convert to datetime objects\n",
    "#start_date = pd.to_datetime(start_year)\n",
    "#end_date = pd.to_datetime(end_year)\n",
    "\n",
    "#Query before or after??\n",
    "#query = (CAsdf['Date_']>= start_date) & (CAsdf['Date_']<= end_date)\n",
    "\n",
    "#CA_filtereddf = CAsdf.join[query].copy()\n",
    "#spatial reference stuff\n",
    "usfssdf.spatial.sr = SEZsdf.spatial.sr\n",
    "\n",
    "#CAsdf.spatial.set_spatial_reference(SEZsdf.spatial.sr)\n",
    "#perform spatial join\n",
    "joinedsdf = SEZsdf.spatial.join(usfssdf)\n",
    "\n",
    "#arcpy.env.workspace = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Data Management 2023.gdb\"\n",
    "\n",
    "# Define the target and join layers\n",
    "#target_feature = \"AssessmentUnits_Master\"  # Path to the target feature class or layer\n",
    "#join_feature = \"externalinvasivesez\"  # Path to the join feature class or layer\n",
    "\n",
    "# Define the output feature class\n",
    "#out_feature_class = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Data Management 2023.gdb\\spatial_join_output\"\n",
    "\n",
    "# Perform spatial join\n",
    "#arcpy.analysis.SpatialJoin(\n",
    " #   target_features=target_feature,\n",
    "  #  join_features=join_feature,\n",
    "   # out_feature_class=out_feature_class,\n",
    "    #join_operation=\"JOIN_ONE_TO_ONE\",  # or \"JOIN_ONE_TO_MANY\" based on your requirements\n",
    "    #join_type=\"KEEP_ALL\",  # Keep all target features, even if they don't intersect with any join features\n",
    "    #match_option=\"INTERSECT\",  # Perform the spatial join based on intersection\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Assessment_Unit_Name        plant_type  \\\n",
      "0       Saxon Creek meadows - below Fountain Place              None   \n",
      "1     Saxon Creek meadows - above Fountain Place 1      bull thistle   \n",
      "2                          Burke Creek meadows - 2              None   \n",
      "3                    Washoe State Parks meadow - 1              None   \n",
      "4                              Spooner Meadows - 4              None   \n",
      "...                                            ...               ...   \n",
      "1235                    Blackwood Creek - middle 4     Wooly_mullein   \n",
      "1236                           Saxon Creek - upper      bull_thistle   \n",
      "1237                           Saxon Creek - upper  reed_canarygrass   \n",
      "1238                      UTR - Christmas Valley 3     Wooly_mullein   \n",
      "1239                      UTR - Christmas Valley 3       oxeye_daisy   \n",
      "\n",
      "           SCIENTIFIC               created_date Source  total  percent_cover  \\\n",
      "0                None                        NaT   USFS    NaN            NaN   \n",
      "1     Cirsium vulgare 2023-07-31 00:00:00.000000   USFS    NaN            NaN   \n",
      "2                None                        NaT   USFS    NaN            NaN   \n",
      "3                None                        NaT   USFS    NaN            NaN   \n",
      "4                None                        NaT   USFS    NaN            NaN   \n",
      "...               ...                        ...    ...    ...            ...   \n",
      "1235              NaN 2023-09-27 23:27:42.000000   TRPA    NaN            NaN   \n",
      "1236              NaN 2023-09-27 23:27:53.000001   TRPA    NaN            NaN   \n",
      "1237              NaN 2023-09-27 23:27:53.000001   TRPA    NaN            NaN   \n",
      "1238              NaN 2023-09-27 23:27:59.000000   TRPA    NaN            NaN   \n",
      "1239              NaN 2023-09-27 23:27:59.000000   TRPA    NaN            NaN   \n",
      "\n",
      "     other  Species_Level  \n",
      "0      NaN            NaN  \n",
      "1      NaN            NaN  \n",
      "2      NaN            NaN  \n",
      "3      NaN            NaN  \n",
      "4      NaN            NaN  \n",
      "...    ...            ...  \n",
      "1235  None            4.0  \n",
      "1236  None            3.0  \n",
      "1237  None            2.0  \n",
      "1238  None            4.0  \n",
      "1239  None            4.0  \n",
      "\n",
      "[1240 rows x 9 columns]\n",
      "{'Barb goatgrass': {'Scientific': '(Aegilops triuncialis)', 'Priority': 1, 'S123': nan}, 'Carolina horsenettle ': {'Scientific': '(Solanum carolinense', 'Priority': 1, 'S123': nan}, 'French broom ': {'Scientific': 'Genista monspessulana', 'Priority': 1, 'S123': nan}, 'Garlic mustard ': {'Scientific': 'Alliaria petiolata', 'Priority': 1, 'S123': nan}, 'Jointed goatgrass ': {'Scientific': 'Aegilops cylindrical', 'Priority': 1, 'S123': nan}, 'Leafy spurge ': {'Scientific': 'Euphorbia virgatae', 'Priority': 1, 'S123': nan}, 'Mediterranean sage ': {'Scientific': 'Salvia aethiopis', 'Priority': 1, 'S123': nan}, 'Medusahead ': {'Scientific': 'Taeniatherum caput-medusae', 'Priority': 1, 'S123': 'medusahead'}, 'Myrtle spurge ': {'Scientific': 'Euphorbia myrsinites', 'Priority': 1, 'S123': nan}, 'Oblong spurge ': {'Scientific': 'Euphorbia oblongata', 'Priority': 1, 'S123': nan}, 'Purple starthistle': {'Scientific': 'Centaurea calcitrap', 'Priority': 1, 'S123': 'purple_starthistle'}, 'Spanish broom': {'Scientific': 'Spartium junceum', 'Priority': 1, 'S123': nan}, 'Squarrose knapweed ': {'Scientific': 'Centaurea virgata  ', 'Priority': 1, 'S123': nan}, 'Stinkwort ': {'Scientific': 'Dittrichia graveolens', 'Priority': 1, 'S123': 'stinkwort'}, 'Tamarisk/saltcedar ': {'Scientific': 'Tamarix spp.', 'Priority': 1, 'S123': 'saltcedar'}, 'Black locust ': {'Scientific': 'Robinia pseudoacacia', 'Priority': 2, 'S123': 'black_locust'}, 'Bouncing Bet ': {'Scientific': 'Saponaria officinalis', 'Priority': 2, 'S123': 'bouncing_bet'}, 'Canada thistle ': {'Scientific': 'Cirsium arvense', 'Priority': 2, 'S123': 'canada_thistle'}, 'Diffuse knapweed ': {'Scientific': 'Centaurea diffusa', 'Priority': 2, 'S123': 'diffuse_knapweed'}, 'Dyer’s woad ': {'Scientific': 'Isatis tinctoria', 'Priority': 2, 'S123': 'dyers_woad'}, 'Hairy whitetop ': {'Scientific': 'Lepidium appelianum', 'Priority': 2, 'S123': 'Hairy_whitetop'}, 'Whitetop': {'Scientific': 'Lepidium draba', 'Priority': 2, 'S123': nan}, 'Himalayan blackberry ': {'Scientific': 'Rubus armeniacus', 'Priority': 2, 'S123': nan}, 'Hoary alyssum ': {'Scientific': 'Berteroa incana', 'Priority': 2, 'S123': nan}, 'Hoary cress ': {'Scientific': 'Cardaria species', 'Priority': 2, 'S123': 'hoary_cress'}, 'Musk thistle  ': {'Scientific': 'Carduus nutans', 'Priority': 2, 'S123': 'musk_thistle'}, 'Poison hemlock': {'Scientific': 'Conium maculatum', 'Priority': 2, 'S123': 'Poison_hemlock'}, 'Purple loosestrife ': {'Scientific': 'Lythrum salicaria', 'Priority': 2, 'S123': 'purple_loosestrife'}, 'Reed canary grass ': {'Scientific': 'Phalaris arundinacea', 'Priority': 2, 'S123': 'reed_canarygrass'}, 'Rush skeletonweed ': {'Scientific': 'Chondrilla juncea', 'Priority': 2, 'S123': 'rush_skeletonweed'}, 'Russian knapweed ': {'Scientific': 'Acroptilon repens', 'Priority': 2, 'S123': 'russian_knapweed'}, 'Russian thistle ': {'Scientific': 'Salsola tragus  ', 'Priority': 2, 'S123': nan}, 'Spotted knapweed ': {'Scientific': 'Centaurea stoebe ssp. Micranthos', 'Priority': 2, 'S123': 'spotted_knapweed'}, 'Sulfur cinquefoil ': {'Scientific': 'Potentilla recta', 'Priority': 2, 'S123': 'sulfur_cinquefoil'}, 'Teasel ': {'Scientific': 'Dipsacus fullonum', 'Priority': 2, 'S123': 'teasel'}, 'Tree of Heaven ': {'Scientific': 'Ailanthus altissima  ', 'Priority': 2, 'S123': 'tree_heaven'}, 'Yellow starthistle ': {'Scientific': 'Centaurea solstitialis', 'Priority': 2, 'S123': 'yellow_starthistle'}, 'Puncturevine ': {'Scientific': 'Tribulus terrestris', 'Priority': 2, 'S123': 'Puncturevine'}, 'Scotch thistle ': {'Scientific': 'Onopordum acanthium', 'Priority': 2, 'S123': 'scotch_thistle'}, 'Bull thistle ': {'Scientific': 'Cirsium vulgare', 'Priority': 3, 'S123': 'bull_thistle'}, 'Scotch broom ': {'Scientific': 'Cytisus scoparius', 'Priority': 3, 'S123': 'scotch_broom'}, 'Common bindweed ': {'Scientific': 'Convolvulus arvensis', 'Priority': 3, 'S123': 'common_bindweed'}, 'Dalmatian toadflax ': {'Scientific': 'Linaria dalmatica', 'Priority': 3, 'S123': 'dalmatian_toadflax'}, 'Everlasting peavine ': {'Scientific': 'Lathyrus latifolius', 'Priority': 3, 'S123': 'everlasting_peavine'}, 'Klamathweed ': {'Scientific': 'Hypericum perforatum', 'Priority': 3, 'S123': 'klamathweed'}, 'Perennial pepperweed ': {'Scientific': 'Lepidium latifolium', 'Priority': 3, 'S123': 'perennial_pepperweed'}, 'Yellow toadflax ': {'Scientific': 'Linaria vulgaris', 'Priority': 3, 'S123': 'yellow_toadflax'}, 'Cheatgrass ': {'Scientific': 'Bromus tectorum', 'Priority': 4, 'S123': 'cheatgrass'}, 'False salsify ': {'Scientific': 'Tragopogon dubius', 'Priority': 4, 'S123': 'False_salsify'}, 'Oxeye daisy ': {'Scientific': 'Chrysanthemum leucanthemum', 'Priority': 4, 'S123': 'oxeye_daisy'}, 'Quackgrass ': {'Scientific': 'Elymus repens', 'Priority': 4, 'S123': 'quackgrass'}, 'White sweetclover ': {'Scientific': 'Melilotus alba', 'Priority': 4, 'S123': 'White_sweetclover'}, 'Wooly mullein ': {'Scientific': 'Verbascum thapsus', 'Priority': 4, 'S123': 'Wooly_mullein'}, 'Yellow sweetclover ': {'Scientific': 'Melilotus officinalis', 'Priority': 4, 'S123': 'yellow_sweetclover'}}\n"
     ]
    }
   ],
   "source": [
    "#Invasive Species\n",
    "\n",
    "#--------------------------------#\n",
    "#Get Data from GDB's with assessment unit name \n",
    "#--------------------------------#\n",
    "#Calfora Data\n",
    "#CArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines07/MapServer/1\"\n",
    "\n",
    "#CAsdf = pd.DataFrame.\n",
    "\n",
    "#USFS Data\n",
    "\n",
    "#usfsrest= \"https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer\"\n",
    "\n",
    "#usfssdf = pd.DataFrame.spatial.from_layer(usfsrest)\n",
    "\n",
    "#Spatially join these layers to see if any of the points or polygons intersect with our sez's123\n",
    "#Path to external data usfs \n",
    "usfsdata = os.path.join(master_path, \"usfsinvasivesez24\")\n",
    "\n",
    "\n",
    "# Paths to the feature classes\n",
    "invasive19fc = os.path.join(invasiveplant19gdb, \"Invasive_Species_2019\")\n",
    "invasive20fc = os.path.join(invasiveplant20gdb, \"Invasive_Species_2020\")\n",
    "invasive22fc = os.path.join(invasiveplant22gdb, \"Invasive_Species_2022\")\n",
    "invasive23fc = os.path.join(invasiveplant23gdb, \"sez_invasive_plant\")\n",
    "\n",
    "\n",
    "#Use gdb because they have assessment unit name in them... 2022 i had to manually add in PRO\n",
    "invasive23fields = ['Assessment_Unit', 'invasives_percent_cover', 'invasives_number_of_species','invasives_plant_type', 'Species_Level', 'invasive_type_other', 'created_date']\n",
    "invasive22fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover', 'Invasives_Number_of_Species', 'InvasiveType_Other', 'Survey_Date']\n",
    "invasive20fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover', 'Invasives_Number_of_Species', 'Other', 'Survey_Date']\n",
    "invasive19fields = ['SITE_ID', 'TOTAL_SPECIES', 'SURVEY_DATE', 'INVASIVE_PLANT', 'PERCENT_COVER' ]\n",
    "usfsfields = ['Assessment_Unit_Name', 'PLANT_COMM', 'SCIENTIFIC', 'DATE_COLLE']\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "\n",
    "# Function to read feature class into DataFrame\n",
    "def feature_class_to_dataframe(feature_class, fields):\n",
    "    data = [row for row in arcpy.da.SearchCursor(feature_class, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Read feature classes into DataFrames\n",
    "# Read feature classes into DataFrames\n",
    "invasive19df = feature_class_to_dataframe(invasive19fc, invasive19fields)\n",
    "invasive20df = feature_class_to_dataframe(invasive20fc, invasive20fields)\n",
    "invasive22df = feature_class_to_dataframe(invasive22fc, invasive22fields)\n",
    "invasive23df = feature_class_to_dataframe(invasive23fc, invasive23fields)\n",
    "usfs23df = feature_class_to_dataframe(usfsdata, usfsfields)\n",
    "\n",
    "\n",
    "invasive19df['Source'] = 'TRPA'\n",
    "invasive20df['Source'] = 'TRPA'\n",
    "invasive22df['Source'] = 'TRPA'\n",
    "invasive23df['Source'] = 'TRPA'\n",
    "usfs23df['Source'] = 'USFS'\n",
    "\n",
    "# Rename fields\n",
    "invasive19df.rename(columns={'SITE_ID': 'Assessment_Unit_Name', 'INVASIVE_PLANT': 'plant_type', 'TOTAL_SPECIES': 'total',  'PERCENT_COVER': 'percent_cover', 'SURVEY_DATE': 'created_date','Source':'Source'}, inplace=True)\n",
    "invasive20df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'Other': 'other', 'Invasives_Percent_Cover': 'percent_cover','Invasives_Number_of_Species': 'total', 'Survey_Date': 'created_date', 'Source':'Source'}, inplace=True)\n",
    "invasive22df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'InvasiveType_Other': 'other', 'Invasives_Percent_Cover': 'percent_cover','Invasives_Number_of_Species': 'total', 'Survey_Date': 'created_date','Source':'Source'}, inplace=True)\n",
    "invasive23df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'invasives_percent_cover': 'percent_cover', 'invasives_number_of_species': 'total', 'invasives_plant_type': 'plant_type', 'invasive_type_other': 'other', 'created_date': 'created_date','Source':'Source'}, inplace=True)\n",
    "usfs23df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'PLANT_COMM':'plant_type', 'DATE_COLLE':'created_date', 'Source':'Source'}, inplace=True)\n",
    "\n",
    "\n",
    "# Concatenate DataFrames\n",
    "invasivedf = pd.concat([usfs23df, invasive19df, invasive20df, invasive22df, invasive23df], ignore_index=True)\n",
    "\n",
    "# Display merged DataFrame\n",
    "print(invasivedf)\n",
    "\n",
    "#Define SEZ ID based on Assessment_Unit_Name\n",
    "invasivedf['SEZ_ID'] = invasivedf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].astype(int)\n",
    "#invasivedf['Assessment_Unit_Name'] = invasivedf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky meadows - 1': 'Sky Meadows'})\n",
    "\n",
    "#calculate year column \n",
    "invasivedf['Year'] = invasivedf['created_date'].dt.year\n",
    "\n",
    "#----------------------------------------------------#\n",
    "#Create Look up dictionary to assign priority group for each plant\n",
    "#----------------------------------------------------#\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "excel_data = pd.read_csv(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Invasives Priority lookup.csv\") \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "Invasives_lookup = {}\n",
    "\n",
    "for index, row in excel_data.iterrows():\n",
    "    common_name = row['Common']\n",
    "    scientific_name = row['Scientific']\n",
    "    priority = int(row['Priority'])\n",
    "    s123 = row['S123']\n",
    "    \n",
    "    # Assign values to the dictionary\n",
    "    Invasives_lookup[common_name] = {'Scientific': scientific_name, 'Priority': priority, 'S123': s123}\n",
    "#for index, row in excel_data.iterrows():\n",
    " #   lookup_dict[row['Common']] = row['Priority']\n",
    "# See dictionary where keys are Assessment Unit Names and values are SEZ IDs\n",
    "print(Invasives_lookup)\n",
    "\n",
    "# Capitalize the first word and replace underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.split('_').str[0].str.capitalize() + ' ' + invasivedf['plant_type'].str.split('_').str[1:].str.join(' ')\n",
    "\n",
    "# Define a custom function to map plant types to priorities\n",
    "def map_priority(plant_type):\n",
    "    if pd.isnull(plant_type):\n",
    "        return np.nan  # Return NaN for NaN values\n",
    "    else:\n",
    "        # Extract the priority from the dictionary, or return 'Unknown' if not found\n",
    "        plant_info = Invasives_lookup.get(plant_type)\n",
    "        if plant_info:\n",
    "            return plant_info['Priority']\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "\n",
    "\n",
    "# Create a new column 'Priority' based on the mapping from the dictionary\n",
    "invasivedf['Priority'] = invasivedf['plant_type'].map(map_priority)\n",
    "\n",
    "invasivedf['Priority'] = pd.to_numeric(invasivedf['Priority'], errors='coerce')\n",
    "\n",
    "# Create a new column 'Number_of_species' based on the presence of plant types\n",
    "invasivedf['Number_of_species'] = np.where(invasivedf['plant_type'].notnull(), 1, 0)\n",
    "\n",
    "\n",
    "#Summarize the SEZ Unit for invasives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function map_priority at 0x0000023CF4A63E50>\n"
     ]
    }
   ],
   "source": [
    "print(map_priority)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HEadcuts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Headcuts\n",
    "#--------------------------------#\n",
    "#Get Data from GDB's with assessment unit name \n",
    "#--------------------------------#\n",
    "\n",
    "# Paths to the feature classes\n",
    "headcut19fc = os.path.join(headcut19gdb, \"Stream_Headcut_2019\")\n",
    "headcut20fc = os.path.join(headcut20gdb, \"Stream_Headcut_2020\")\n",
    "headcut22fc = os.path.join(headcut22gdb, \"Stream_Headcut_2022\")\n",
    "headcut23fc = os.path.join(headcut23gdb, \"sez_stream_headcut\")\n",
    "\n",
    "\n",
    "#Use gdb because they have assessment unit name in them... 2022 i had to manually add in PRO\n",
    "headcut23fields = ['Assessment_Unit', 'headcut_depth', 'created_date']\n",
    "headcut22fields = ['Assessment_Unit', 'Headcut_Depth', 'synced_date']\n",
    "headcut20fields = ['Assessment_Unit_Name', 'Headcut_Depth','Survey_Date']\n",
    "headcut19fields = ['SITE_NAME', 'HEADCUT_DEPTH', 'SURVEY_DATE' ]\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "\n",
    "# Function to read feature class into DataFrame\n",
    "def feature_class_to_dataframe(feature_class, fields):\n",
    "    data = [row for row in arcpy.da.SearchCursor(feature_class, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Read feature classes into DataFrames\n",
    "# Read feature classes into DataFrames\n",
    "headcut19df = feature_class_to_dataframe(headcut19fc, headcut19fields)\n",
    "headcut20df = feature_class_to_dataframe(headcut20fc, headcut20fields)\n",
    "headcut22df = feature_class_to_dataframe(headcut22fc, headcut22fields)\n",
    "headcut23df = feature_class_to_dataframe(headcut23fc, headcut23fields)\n",
    "\n",
    "# Rename fields\n",
    "headcut19df.rename(columns={'SITE_NAME': 'Assessment_Unit_Name', 'HEADCUT_DEPTH': 'headcut_depth', 'SURVEY_DATE': 'created_date'}, inplace=True)\n",
    "headcut20df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'Headcut_Depth': 'headcut_depth', 'Survey_Date': 'created_date'}, inplace=True)\n",
    "headcut22df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'Headcut_Depth': 'headcut_depth', 'synced_date': 'created_date'}, inplace=True)\n",
    "headcut23df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'headcut_depth': 'headcut_depth', 'created_date': 'created_date'}, inplace=True)\n",
    "\n",
    "# Concatenate DataFrames\n",
    "headcutdf = pd.concat([headcut19df, headcut20df, headcut22df, headcut23df], ignore_index=True)\n",
    "\n",
    "# Display merged DataFrame\n",
    "print(headcutdf)\n",
    "\n",
    "#hopefully this is just one time\n",
    "\n",
    "\n",
    "#--------------------------------#\n",
    "#Get Data from exported table where Assessment Unit is joined---parentglobalids don't match up for years before 2023. need to redo sde.collect 2019-2022 data append.... \n",
    "#--------------------------------#\n",
    "\n",
    "#--------------------------------#\n",
    "#Get Data from SDE.Collect...instead of GDB\n",
    "#--------------------------------#\n",
    "#----------------------------------------------------------------#\n",
    "#Add correct info to dataframe\n",
    "#----------------------------------------------------------------#\n",
    "# Create a new column 'SEZ ID'\n",
    "#This code is for the excel look up dictionary---ADD last? this is goo dfor QA on assessment unit names because 0's tell you the name is wrong\n",
    "headcutdf['SEZ_ID'] = headcutdf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "headcutdf['SEZ_ID'] = headcutdf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "headcutdf['SEZ_ID'] = headcutdf['SEZ_ID'].astype(int)\n",
    "headcutdf['Assessment_Unit_Name'] = headcutdf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky meadows - 1': 'Sky Meadows'})\n",
    "\n",
    "#calculate year column \n",
    "headcutdf['Year'] = headcutdf['created_date'].dt.year\n",
    "\n",
    "#----------------------------------------------#\n",
    "# Process Data\n",
    "#----------------------------------------------#\n",
    "# assign small, medium, large to headcut\n",
    "headcutdf['Headcut_Size']=headcutdf['headcut_depth'].apply(categorize_headcut)\n",
    "\n",
    "\n",
    "# Group by 'SEZ_ID', 'Year', and 'Headcut_Size', and count the number of occurrences for each group\n",
    "headcut_summary = headcutdf.groupby(['Assessment_Unit_Name', 'Year', 'Headcut_Size']).size().reset_index(name='Count')\n",
    "\n",
    "print(type(headcut_summary))\n",
    "# Pivot the table to have 'Headcut_Size' categories as columns\n",
    "headcut_summary_sml = headcut_summary.pivot_table(index=['Assessment_Unit_Name', 'Year'], columns='Headcut_Size', values='Count', fill_value=0)\n",
    "\n",
    "# Reset the index to flatten the DataFrame\n",
    "headcut_summary_sml.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the summarized DataFrame\n",
    "print(headcut_summary_sml)\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score, # of Headcuts\n",
    "#----------------------------------------------------------------#\n",
    "    \n",
    "    # Process Data\n",
    "def rate_headcut(row):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if row['large'] >= 1:\n",
    "        return 'D'  # Assign score D\n",
    "    elif row['medium'] >= 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif row['small'] >= 1:\n",
    "       return 'B'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no headcuts)\n",
    "\n",
    "# Apply the rating function to the summary DataFrame\n",
    "headcut_summary_sml['Headcuts_Rating'] = headcut_summary_sml.apply(rate_headcut, axis=1)\n",
    "\n",
    "#Calculate total number of headcuts per sez per year\n",
    "headcut_summary_sml['Number_of_Headcuts']= headcut_summary_sml[['large', 'medium', 'small']].sum(axis=1)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "headcut_summary_sml['Headcuts_Score']= headcut_summary_sml['Headcuts_Rating'].apply(score_indicator)\n",
    "\n",
    "#Add Datasource\n",
    "headcut_summary_sml['Headcuts_Data_Source'] = 'TRPA'\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to incision called stage_incision GDB location\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Headcuts_Data_Source': 'Headcuts_Data_Source',\n",
    "    'Number_of_Headcuts': 'Number_of_Headcuts',\n",
    "    'Headcuts_Rating': 'Headcuts_Rating',\n",
    "    'Headcuts_Score': 'Headcuts_Score',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'small': 'small',\n",
    "    'medium': 'medium',\n",
    "    'large': 'large'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = headcut_summary_sml.rename(columns=field_mapping).drop(columns=[col for col in headcut_summary_sml.columns if col not in field_mapping])\n",
    "\n",
    "readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "with arcpy.da.InsertCursor(stage_headcuts, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in External Data from USFS and Calflora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------#\n",
    "#Get Data from external sources and spatially join to our data\n",
    "#--------------------------------#\n",
    "\n",
    "\n",
    "#Calfora Data\n",
    "CArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines07/MapServer/1\"\n",
    "#CAfc = os.path.join(CArest, \"Non-Native and Invasive Plants - Calflora\")\n",
    "CAsdf = pd.DataFrame.spatial.from_layer(CArest)\n",
    "# Delete the existing feature layer if it exists\n",
    "\n",
    "CA_fc = arcpy.MakeFeatureLayer_management(CArest, \"Calfora_invasives\")\n",
    "\n",
    "\n",
    "usfsrest= \"https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer/0\"\n",
    "#usfsfc =os.path.join(usfsrest, \"Current Invasive Plant Locations\") \n",
    "#usfssdf = pd.DataFrame.spatial.from_layer(usfsrest)\n",
    "if arcpy.Exists(\"usfs_layer\"):\n",
    "    arcpy.Delete_management(\"usfs_layer\")\n",
    "usfs_fc = arcpy.MakeFeatureLayer_management(usfsrest, \"usfs_layer\")\n",
    "#Spatially join usfs layers to see if any of the points or polygons intersect with our sez's123\n",
    "\n",
    "# Set the workspace environment\n",
    "arcpy.env.workspace = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Data Management 2023.gdb\"\n",
    "\n",
    "# Define the target and join layers\n",
    "target_feature = SEZ_Master #\"AssessmentUnit_Master\"\n",
    "CAjoin_feature = CA_fc\n",
    "USFSjoin_feature = usfs_fc\n",
    "\n",
    "# Define the output feature class\n",
    "out_feature_class = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Data Management 2023.gdb\\externalinvasivesez\"\n",
    "\n",
    "# Define the fields you want to keep from both layers\n",
    "#field_mappings = arcpy.FieldMappings()\n",
    "#field_mappings.addTable(target_features)\n",
    "#field_mappings.addTable(join_features)\n",
    "\n",
    "# Keep only specific fields from the target layer\n",
    "#keep_fields_target = [\"Assessment_Unit_Name\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_target:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "#Keep only specific fields from the calfora join layer\n",
    "#keep_fields_CA = [\"CName\", \"SName\", \"Source\", \"Obs_Date\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_CA:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "# Keep only specific fields from the usfs join_features\n",
    "#keep_fields_USFS = [\"SCIENTIFIC_NAME\", \"COMMON_NAME\", \"DATE_COLLECTED\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_USFS:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "\n",
    "# Perform spatial join with specified field mappings\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=SEZ_Master,\n",
    "    join_features=[CAjoin_feature],\n",
    "    out_feature_class=out_feature_class,\n",
    "    join_operation=\"JOIN_ONE_TO_MANY\",\n",
    "    join_type=\"KEEP_ALL\"\n",
    "    #field_mapping=field_mappings\n",
    ")\n",
    "\n",
    "#Create Dataframe for spatially joined data\n",
    "# Define the fields you want to retrieve from the output feature class\n",
    "fields = [\"Assessment_Unit_Name\", \"CName\", \"SName\", \"Source\", \"Obs_Date\", \"SCIENTIFIC_NAME\", \"COMMON_NAME\", \"DATE_COLLECTED\"]\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "data = []\n",
    "\n",
    "# Use SearchCursor to read data from the output feature class\n",
    "with arcpy.da.SearchCursor(out_feature_class, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the list of tuples into a pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bioassessment/ Biotic Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bioassessment scores- get all stream data into sde.Stream first... then look at Biotic Integrit Data Source to find which stream was used to evaluate each meadow.. this will help with percent of stream miles I believe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conifer Encroachment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aquatic Organism /Fish Passage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code to make it a spatially enabled dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to conver to spatially enable geodataframe\n",
    "# Create a new DataFrame Final_df with columns based on the field mapping\n",
    "#df = df.rename(columns=field_mapping)\n",
    "\n",
    "# Convert DataFrame to Spatially Enabled DataFrame\n",
    "#sedf = GeoAccessor.from_xy(df, x_column='LONGITUDE', y_column='LATITUDE')\n",
    "\n",
    "# Convert the SEDF to a feature class without sanitizing columns\n",
    "#sedf.spatial.to_featureclass(location=os.path.join(workspace, 'PlanAreaNoise_23_Staging'), sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
