{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install arcgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "import os\n",
    "import sys\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureSet, GeoAccessor, GeoSeriesAccessor, FeatureLayer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.geometry import SpatialReference\n",
    "#import geopandas as gpd to use spatial.reference stuff\n",
    "gis = GIS()\n",
    "# # Set Pandas display options to show all rows and columns\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# in memory output file path\n",
    "wk_memory = \"memory\" + \"\\\\\"\n",
    "\n",
    "\n",
    "# set workspace and sde connections \n",
    "#working_folder = \"C:\\GIS\"\n",
    "\n",
    "#workspace      = \"C:\\GIS\\Scratch.gdb\"\n",
    "#arcpy.env.workspace = \"C:\\GIS\\Scratch.gdb\"\n",
    "#workspace ='F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "#arcpy.env.workspace = 'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "\n",
    "# network path to connection files\n",
    "filePath = \"F:\\\\GIS\\\\DB_CONNECT\"\n",
    "\n",
    "#path to GDB's to update and master data\n",
    "master_path = \"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "SEZ_Master = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "#set workspace for connection to GDB\n",
    "#workspace=master_path\n",
    "# database file paths \n",
    "### SDE Collection New data collected is put into SDE.Survey under the indicator name\n",
    "### SDE Vector is where the data will go \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "#sdeBase    = os.path.join(filePath, \"SarahVector.sde\")\n",
    "#sdeCollect = os.path.join(filepath, \"SarahCollect.sde\")\n",
    "\n",
    "# setup connection string???\n",
    "#connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=sql12;DATABASE=sde;UID=sde;PWD=staff\"\n",
    "#connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "#engine = create_engine(connection_url)\n",
    "\n",
    "#with engine.begin() as sdeConnect:\n",
    " #   erosiondf      = pd.read_sql(\"SELECT * FROM sde.SDE.Stream_Erosion\", sdeConnect)\n",
    "\n",
    "# local variables sdata is starting data and f data is finishing datatables\n",
    "ffdata = os.path.join(sdeBase, \"sde.SDE.Monitoring\")\n",
    "sdata = os.path.join(sdeCollect, \"sde.SDE.Survey\")\n",
    "fdata = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "\n",
    "##Tables we get the data from in Collect 2010-2022 globalids don'tmatch\n",
    "sezsurveytable = os.path.join(sdata, \"sde.SDE.sez_survey\")\n",
    "erosiondata = os.path.join(sdata, \"sde.SDE.Stream_Erosion\")\n",
    "incisiondata = os.path.join(sdata, \"sde.SDE.sez_channel_incision\")\n",
    "invasivedata = os.path.join(sdata, \"sde.SDE.sez_invasive_plant\")\n",
    "headcutdata = os.path.join(sdata, \"sde.SDE.sez_stream_headcut\")\n",
    "\n",
    "#make this a spatial df\n",
    "streamdata = os.path.join(ffdata, \"sde.SDE.Stream\")\n",
    "\n",
    "\n",
    "#Staging Tables currently living in SEZ_Data.GDB\n",
    "stage_bank_stability = os.path.join(master_path, \"bank_stability\") \n",
    "stage_All_SEZ_Scores = os.path.join(master_path, \"All_SEZ_Scores\")\n",
    "stage_biotic_integrity = os.path.join(master_path, \"biotic_integrity\")\n",
    "stage_headcuts = os.path.join(master_path, \"headcuts_table\")\n",
    "stage_incision = os.path.join(master_path, \"incision\")\n",
    "stage_invasives = os.path.join(master_path, \"invasives\")\n",
    "stage_vegetation = os.path.join(master_path, \"vegetation_vigor\")\n",
    "stage_conifer = os.path.join(master_path, \"conifer_encroachment\")\n",
    "stage_aquatic = os.path.join(master_path, \"aquatic_organism_passage_table\")\n",
    "stage_ditches = os.path.join(master_path, \"ditches\")\n",
    "stage_habitat = os.path.join(master_path, \"habitat_fragmentation\")\n",
    "#Final table to append to\n",
    "#finalSEZtable = os.path.join(ffdata, \"sde.SDE.SEZ_Assessment_Unit\")\n",
    "#finalSEZtable = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "\n",
    "\n",
    "# network path to connection files??????\n",
    "#filePath = \"C:\\\\GIS\\\\DB_CONNECT\"\n",
    "# network path to connection files\n",
    "#filePath = r'F:\\Research and Analysis\\Workspace\\Sarah'\n",
    "\n",
    "#--------------------------------------------#\n",
    "#Notes to self\n",
    "#--------------------------------------------#\n",
    "#F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb. \n",
    "###'Assessment_Unit_Master' has all data for SEZ\n",
    "###“Bioassessment Sample Locations” has stream sites in SEZs\n",
    "\n",
    "gdbworking_folder = \"F:\\GIS\\GIS_DATA\\Monitoring\"\n",
    "headcutgdbfolder = os.path.join(gdbworking_folder, \"Stream_Headcut\", \"StreamHeadcut_Survey\")\n",
    "invasivegdbfolder = os.path.join(gdbworking_folder, \"Invasive_Species\", \"Invasive_Species_Survey\")\n",
    "sezgdbfolder= os.path.join(gdbworking_folder, \"SEZ\", \"SEZ_Survey\")\n",
    "## GDB with Raw Data straight from S123 not in the original folder (that one is not edited)\n",
    "headcut23gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2023.gdb\")\n",
    "headcut22gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2022.gdb\")\n",
    "headcut20gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2020.gdb\")\n",
    "headcut19gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2019.gdb\")\n",
    "sez_surveygdb = os.path.join(sezgdbfolder, \"SEZ_Survey_2023.gdb\")\n",
    "\n",
    "#channelincision23gdb = os.path.join(working_folder,\"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2023.gdb\")\n",
    "#channelincision22gdb = os.path.join(working_folder, \"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2022.gdb\")\n",
    "#channelincision20gdb = os.path.join(working_folder, \"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2020.gdb\")\n",
    "invasiveplant23gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2023.gdb\")\n",
    "invasiveplant22gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2022.gdb\")\n",
    "invasiveplant20gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2020.gdb\")\n",
    "invasiveplant19gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2019.gdb\")\n",
    "\n",
    "\n",
    "#This is thelocatoin for the final SEZ GDB to be updated in the gdb on f drive in the AssessmentUnits Master (polygon) i believe\n",
    "#FinalGDBtoupdate:F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data \\SEZ_Data.gdb\n",
    "\n",
    "#Location of USFS Invasive Species Data\n",
    "#https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer\n",
    "\n",
    "#Monitoring Dashboard location\n",
    "#Finalsdelocation:f'Vector.SDE' Sde.Monitoring Sde. SEZ_Assessment_Unit\n",
    "\n",
    "#Threshold Location? sde.tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fs_data_spatial(fs_url):\n",
    "    feature_layer = FeatureLayer(fs_url)\n",
    "    query_result = feature_layer.query().sdf\n",
    "    query_result.spatial.sr = 26910\n",
    "    return query_result\n",
    "\n",
    "SEZ_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/0\"\n",
    "\n",
    "dfSEZ = get_fs_data_spatial(SEZ_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use only if need to make SEZ ID dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE LOOKUP FOR SMALL ACRES SEZ ID AND FEATURE TYPE\n",
    "# Filter Assessment_Unit_Name entries with more than one unique Acres value\n",
    "grouped = dfSEZ.groupby('Assessment_Unit_Name')\n",
    "filtered_df = grouped.filter(lambda x: x['Acres'].nunique() > 1)\n",
    "\n",
    "# Determine the minimum Acres for each Assessment_Unit_Name\n",
    "min_acres_df = filtered_df.groupby('Assessment_Unit_Name', as_index=False)['Acres'].min()\n",
    "\n",
    "# Merge to get the corresponding SEZ_ID and SEZ_Type\n",
    "result_df = pd.merge(min_acres_df, filtered_df, on=['Assessment_Unit_Name', 'Acres'])\n",
    "\n",
    "# Select columns of interest for the CSV\n",
    "columns_of_interest = ['Assessment_Unit_Name', 'SEZ_ID', 'SEZ_Type']\n",
    "\n",
    "# Create a new DataFrame with selected columns\n",
    "df_selected = result_df[columns_of_interest].copy()\n",
    "\n",
    "# Specify the file path where you want to save the CSV file\n",
    "csv_file_path = 'Small_Polygon_Lookup.csv'\n",
    "\n",
    "# Save the selected data to CSV file\n",
    "df_selected.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"CSV file saved successfully to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SEZID and SEZType Lookup Dictionaries Small Polygone, Large Polygon, All Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Angora Creek - tributary': 519, 'Angora Creek - upper': 520, 'Angora meadows - 1': 87, 'Angora meadows - 2': 90, 'Angora meadows - 3': 91, 'Angora meadows - 4': 143, 'Angora meadows - 5': 144, 'Angora meadows - 6': 142, 'Angora meadows - 7': 89, 'Angora meadows - 8': 88, 'Angora meadows - 9': 92, 'Angora meadows tributary - 1': 217, 'Angora meadows tributary - 2': 99, 'Angora meadows tributary - 3': 97, 'Angora meadows tributary - 4': 94, 'Angora meadows tributary - 5': 214, 'Angora meadows tributary - 6': 146, 'Angora meadows tributary - 7': 93, 'Angora meadows tributary - 8': 95, 'Angora meadows tributary - 9': 96, 'Angora tributary': 446, 'Antone meadows': 187, 'Baldwin marsh - 1': 160, 'Benwood meadows - 1': 129, 'Benwood meadows - 2': 131, 'Big Meadow - 1': 47, 'Big Meadow - 2': 48, 'Big Meadow - 3': 38, 'Big Meadow - 4': 39, 'Big Meadow - 5': 37, 'Big Meadow - 6': 40, 'Big meadow - 7': 126, 'Big Meadow Creek - lower': 521, 'Big Meadow Creek - upper': 491, 'Big Meadow Creek - upper 2': 522, 'Bijou meadows - 1': 115, 'Bijou meadows - 2': 116, 'Bijou meadows - 3': 164, 'Bijou meadows - private': 114, 'Bijou meadows - tributary 1': 226, 'Bijou Park Creek meadows - 1': 219, 'Bijou Park Creek meadows - 2': 166, 'Bijou Park Creek meadows - 3': 167, 'Bijou Park Creek meadows - 4': 163, 'Bijou Park Creek meadows - 5': 489, 'Bijou Park Creek meadows - 6': 488, 'Blackwood Creek - lower 1': 523, 'Blackwood Creek - lower 2': 596, 'Blackwood Creek - middle 1': 597, 'Blackwood Creek - middle 2': 524, 'Blackwood Creek - middle 3': 598, 'Blackwood Creek - middle 4': 525, 'Blackwood Creek - Upper 1': 599, 'Blackwood Creek - Upper 2': 526, 'Blackwood Creek - upper 3': 527, 'Blackwood meadows - 1': 66, 'Blackwood meadows - 3': 65, 'Buck Lake meadows': 176, 'Buddhas meadow': 168, 'Burke Creek - middle': 528, 'Burke Creek - upper': 503, 'Burke Creek meadows - 1': 171, 'Burke Creek meadows - 2': 3, 'Burke Creek tributary': 461, 'Burton Creek - lower': 633, 'Burton Creek - upper': 529, 'Carnelian Canyon Creek - lower': 620, 'Carnelian Canyon Creek - upper': 621, 'Cascade Creek - lower': 501, 'Cascade Creek - upper': 498, 'Casino meadows': 170, 'Christmas Valley meadows - 1': 136, 'Christmas Valley meadows - 2': 134, 'Christmas Valley meadows - 3': 130, 'Christmas Valley meadows - 4': 225, 'Cold Creek - Highland Woods': 110, 'Cold Creek - middle': 530, 'Cold Creek - tributary 1': 532, 'Cold Creek - tributary 2': 533, 'Cold Creek - tributary 3': 531, 'Cold Creek - upper': 534, 'Cold Creek tributary - 4': 465, 'Cold Creek tributary - 5': 466, 'Colony Inn meadows - lower': 169, 'Colony Inn meadows - upper': 485, 'Cookhouse meadow': 128, 'Deer Creek - headwaters': 535, 'Deer Creek - lower': 537, 'Deer Creek - middle': 538, 'Deer Creek - middle 2': 539, 'Deer Creek - upper': 536, 'Deer Creek meadows': 30, 'Dollar Creek - lower': 540, 'Dollar Creek - upper': 512, 'Eagle Creek': 497, 'Echo Creek - below lake': 541, 'Echo Creek - upper': 494, 'Edgewood Creek - middle': 542, 'Edgewood Creek tributary - 2 - headwaters': 462, 'Edgewood Creek tributary - 2 - lower': 476, 'Edgewood Creek tributary - 2 - upper': 635, 'Edgewood Creek tributary - 3 - lower': 628, 'Edgewood Creek tributary - 3 - upper': 629, 'Edgewood meadows': 221, 'Elks Club meadows - 1': 141, 'Elks Club meadows - 2': 85, 'Fallen Leaf meadows - 1': 154, 'Fallen Leaf meadows - 2': 155, 'Fallen Leaf meadows - 3': 147, 'Fallen Leaf meadows - 4': 76, 'First Creek - lower': 543, 'First Creek - upper': 516, 'Freel Meadows - 1': 25, 'Freel Meadows - 2': 24, 'Gardner meadow': 150, 'General Creek - lower': 544, 'General Creek - middle': 506, 'General Creek - upper': 545, 'General Creek meadows': 61, 'Ginny Lake Meadows': 193, 'Glen Alpine Creek - lower': 546, 'Glen Alpine Creek - upper': 606, 'Glenbrook Creek - middle': 510, 'Glenbrook Creek - upper': 547, 'Glenbrook meadows - 1': 177, 'Glenbrook meadows - 2': 178, 'Golden Bear meadows - 1': 212, 'Golden Bear meadows - 2': 196, 'Grass Lake Creek': 609, 'Grass Lake meadow': 127, 'Griff Creek - lower': 514, 'Griff Creek - tributary': 548, 'Griff Creek - upper': 549, 'Griff Creek meadows': 35, 'Haypress Meadows': 50, 'Heavenly Valley Creek - middle': 550, 'Heavenly Valley Creek - upper': 499, 'Heavenly Valley Creek meadows - 1': 111, 'Heavenly Valley Creek meadows - 2': 112, 'Heavenly Valley Creek meadows - 3': 152, 'Heavenly Valley Creek meadows - 4': 113, 'Hell Hole meadows - 1': 230, 'Hell Hole Meadows - 2': 8, 'Hidden Valley Creek - lower': 551, 'Hidden Valley Creek - upper': 552, 'High meadows - 1': 203, 'High meadows - 2': 198, 'High Meadows - 3': 200, 'High meadows - 4': 202, 'High meadows - 5': 201, 'High meadows - 6': 199, 'Homewood Canyon Creek - lower': 600, 'Homewood Canyon Creek - upper': 508, 'Incline Creek - lower': 553, 'Incline Creek - middle 1': 554, 'Incline Creek - middle 2': 555, 'Incline Creek - middle 3': 636, 'Incline Creek - ski run': 556, 'Incline Creek - upper': 557, 'Incline Lake meadows - 1': 192, 'Incline Lake meadows - 2': 190, 'Incline Village tributary - 1': 458, 'Incline Village tributary - 2': 459, 'Incline Village tributary - 3': 460, 'Incline Village tributary - 4': 457, 'Kahle meadows - 2': 204, 'Kahle meadows - 3': 172, 'Kahle meadows - 4': 468, 'Kahle meadows - 5': 118, 'Kahle meadows - 6': 469, 'Kahle meadows - 7': 119, 'Kahle meadows - 8': 486, 'Kingsbury meadows': 222, 'Lake Forest meadows - 1': 185, 'Lake Forest meadows - 2': 186, 'Lake Forest meadows - 3': 184, 'Lake Forest meadows - 4': 487, 'Lake Forest meadows - 5': 223, 'Lake Forest meadows - 6': 183, 'Lake Forest tributary': 624, 'Lakeshore meadows': 72, 'Logan House Creek - lower': 558, 'Logan House Creek - upper': 507, 'Logan House meadow': 13, 'Lonely Gulch Creek - lower': 559, 'Lonely Gulch Creek - middle': 560, 'Lonely Gulch Creek - upper': 504, 'Madden Creek': 561, 'Marlette Creek - lower': 562, 'Marlette Creek - old dam site': 564, 'Marlette Creek - south fork (lower)': 642, 'Marlette Creek - south fork (upper)': 563, 'Marlette Creek - upper': 631, 'Marlette Lake meadows': 32, 'McFaul Creek - lower': 565, 'McFual meadow': 71, 'McKinney Creek - lower': 566, 'McKinney Creek - middle': 567, 'McKinney Creek - upper': 505, 'McKinney tributary - 1': 626, 'McKinney tributary - 2': 453, 'Meeks Bay Lagoon': 235, 'Meeks Bay meadows - 1': 205, 'Meeks Bay meadows - 2': 207, 'Meeks Bay meadows - 3': 36, 'Meeks Bay meadows - 4': 206, 'Meeks Creek - upper': 502, 'Meiss meadows - 1': 123, 'Meiss meadows - 2': 122, 'Meiss meadows - 3': 124, 'Meiss meadows - 4': 210, 'Meiss meadows - 5': 209, 'Meyers meadow': 218, 'Meyers tributary - 1': 447, 'Meyers tributary - 2': 467, 'Mill Creek - lower': 568, 'Mill Creek - upper': 515, 'Mill Creek meadows': 234, 'Mount Rainier Drive meadows - 1': 215, 'Mount Rainier Drive meadows - 2': 216, 'Muskawi Drive meadows': 83, 'North Logan House Creek': 509, 'North Logan House meadows': 12, 'North Zephyr Creek - lower': 570, 'North Zephyr Creek - middle': 615, 'North Zephyr Creek - tributary': 569, 'North Zephyr Creek - upper': 571, 'Nottaway Drive meadows': 213, 'Osgood Creek - above road': 572, 'Osgood Creek - below road': 573, 'Osgood Swamp': 482, 'Paige meadows': 182, 'Pope marsh meadow': 483, 'Quail Creek - lower': 574, 'Quail Creek - upper': 575, 'Quail Creek meadow': 26, 'Rosewood Creek - lower': 576, 'Rosewood Creek - middle 1': 586, 'Rosewood Creek - middle 2': 587, 'Rosewood Creek - middle 3': 588, 'Rubicon Creek': 601, 'Rubicon Creek - tributary': 602, 'Rubicon Meadows': 60, 'Saxon Creek - headwaters': 495, 'Saxon Creek - upper': 641, 'Saxon Creek meadows - above Fountain Place 1': 2, 'Saxon Creek meadows - above Fountain Place 2': 102, 'Saxon Creek meadows - below Fountain Place': 1, 'Saxon Creek tributary meadows - 1': 101, 'Saxon Creek tributary meadows - 3': 100, 'Saxon Creek tributary meadows - 4': 140, 'Saxon Creek tributary meadows - 5': 197, 'Saxon Creek tributary meadows - 6': 139, 'Saxon Creek tributary meadows - 7': 231, 'Second Creek - lower': 591, 'Second Creek - lower 2': 592, 'Second Creek - middle': 593, 'Second Creek - upper': 517, 'Secret Harbor Creek - lower': 618, 'Secret Harbor Creek - upper': 619, 'Sierra Tract wetlands': 211, 'Ski Run meadows': 220, 'Sky meadows': 79, 'Skylandia SEZ': 622, 'Slaughterhouse Creek - lower': 614, 'Slaughterhouse Creek - middle': 616, 'Slaughterhouse Creek - upper': 617, 'Slaughterhouse Meadows - 1': 6, 'Slaughterhouse meadows - 2': 180, 'small meadow 1': 11, 'small meadow 10': 64, 'small meadow 100': 77, 'small meadow 105': 125, 'small meadow 111': 132, 'small meadow 112': 133, 'small meadow 113': 135, 'small meadow 116': 181, 'small meadow 13': 20, 'small meadow 14': 19, 'small meadow 15': 18, 'small meadow 16': 21, 'small meadow 17': 28, 'small meadow 19': 63, 'small meadow 2': 10, 'small meadow 20': 623, 'small meadow 21': 67, 'small meadow 22': 53, 'small meadow 23': 54, 'small meadow 24': 56, 'small meadow 25': 55, 'small meadow 26': 57, 'small meadow 27': 58, 'small meadow 28': 59, 'small meadow 29': 174, 'small meadow 3': 70, 'small meadow 30': 175, 'small meadow 32': 51, 'small meadow 35': 49, 'small meadow 36': 52, 'small meadow 40': 74, 'small meadow 5': 29, 'small meadow 50': 23, 'small meadow 51': 42, 'small meadow 52': 41, 'small meadow 54': 22, 'small meadow 56': 46, 'small meadow 57': 73, 'small meadow 58': 173, 'small meadow 59': 14, 'small meadow 6': 17, 'small meadow 7': 27, 'small meadow 8': 69, 'small meadow 82': 232, 'small meadow 9': 68, 'small meadow 92': 31, 'small meadow 93': 33, 'small meadow 95': 43, 'small meadow 96': 44, 'small meadow 98': 62, 'small meadow 99': 75, 'Snow Creek tributary - 1': 451, 'Snow Creek tributary - 2': 452, 'Snow Creek wetlands - 1': 188, 'Snow Creek wetlands - 2': 189, 'South Lake Tahoe - wetland 1': 229, 'South Lake Tahoe airport': 484, 'South Lake Tahoe tributary - 1': 463, 'South Lake Tahoe tributary - 2': 464, 'South Lake Tahoe tributary - 3': 627, 'Spooner meadows - 1': 233, 'Spooner meadows - 2': 179, 'Spooner meadows - 3': 120, 'Spooner meadows - 5': 121, 'Star Lake meadows': 45, 'Susquehana meadows - 1': 108, 'Susquehana meadows - 2': 107, 'Tahoe City meadow': 224, 'Tahoe City tributary - 1': 449, 'Tahoe City tributary - 2': 450, 'Tahoe Island meadows - 1': 158, 'Tahoe Island meadows - 2': 156, 'Tahoe Keys': 162, 'Tahoe Paradise golf course': 632, 'Tahoe Valley meadows - 1': 153, 'Tahoe Valley meadows - 2': 228, 'Tahoe Vista meadows': 227, 'Tallac Creek - abv highway - 1': 334, 'Tallac Creek - abv highway - 2': 604, 'Tallac Creek - tributary': 500, 'Tallac marsh': 159, 'Tallac meadows': 157, 'Taylor Creek': 605, 'Taylor Creek marsh': 208, 'Third Creek - headwaters': 585, 'Third Creek - lower': 577, 'Third Creek - lower 2': 578, 'Third Creek - middle': 581, 'Third Creek - middle 1': 579, 'Third Creek - middle 2': 580, 'Third Creek - upper 1': 582, 'Third Creek - upper 2': 584, 'Third Creek - upper 3': 583, 'Third Creek meadows - 1': 191, 'Third Creek meadows - 3': 34, 'Third Creek meadows - 4': 195, 'Third Creek meadows - 6': 194, 'Third Creek meadows - 7': 16, 'Third Creek meadows - 8': 15, 'Trout Creek - Highland Woods': 109, 'Trout Creek - tributary 2': 613, 'Trout Creek - tributary 3': 612, 'Trout Creek - upper': 496, 'Trout Creek above Black Bart': 149, 'Trout Creek below Black Bart': 161, 'Trout Creek headwaters meadows - 1': 137, 'Trout Creek headwaters meadows - 2': 9, 'Trout Creek meadows - above Fountain Place': 103, 'Trout Creek meadows - above Pioneer 1': 148, 'Trout Creek meadows - above Pioneer 2': 106, 'Trout Creek meadows - above Pioneer 3': 105, 'Trout Creek meadows - above Pioneer 4': 104, 'Upper Truckee River - Meyers': 138, 'Upper Truckee River - Tahoe Paradise': 7, 'UTR - Airport reach': 82, 'UTR - Christmas Valley 1': 608, 'UTR - Christmas Valley 3': 493, 'UTR - golf course meadows': 86, 'UTR - Johnson meadows - 2': 151, 'UTR - Johnson meadows - 3': 81, 'UTR - middle': 492, 'UTR - Reach 5': 80, 'UTR - Reach 6': 84, 'UTR - tributary 1': 610, 'UTR - tributary 3': 611, 'UTR - upper': 490, 'UTR - Washoe Meadows': 607, 'UTR marsh - Trout Creek side': 165, 'UTR Marsh - UTR side': 78, 'Van Sickle meadows': 117, 'Ward Creek - lower': 594, 'Ward Creek - middle': 595, 'Ward Creek - upper': 511, 'Ward Creek meadow': 625, 'Washoan Blvd meadows': 145, 'Washoe State Parks meadow - 1': 4, 'Washoe State Parks meadow - 2': 98, 'Watson Creek': 513, 'West Shore tributary - 1': 455, 'West Shore tributary - 2 - lower': 639, 'West Shore tributary - 2 - upper': 448, 'West Shore tributary - 3': 456, 'West Shore tributary - 4': 454, 'Woods Creek - lower': 589, 'Woods Creek - middle': 590, 'Woods Creek - upper': 518}\n",
      "{'UTR - upper': 236, 'Big Meadow Creek - upper': 237, 'UTR - middle': 238, 'UTR - Christmas Valley 3': 239, 'Echo Creek - upper': 240, 'Saxon Creek - headwaters': 241, 'Trout Creek - upper': 242, 'Eagle Creek': 243, 'Cascade Creek - upper': 244, 'Heavenly Valley Creek - upper': 245, 'Tallac Creek - tributary': 246, 'Cascade Creek - lower': 247, 'Meeks Creek - upper': 248, 'Burke Creek - upper': 249, 'Lonely Gulch Creek - upper': 250, 'McKinney Creek - upper': 251, 'General Creek - middle': 252, 'Logan House Creek - upper': 253, 'Homewood Canyon Creek - upper': 254, 'North Logan House Creek': 255, 'Glenbrook Creek - middle': 256, 'Ward Creek - upper': 257, 'Dollar Creek - upper': 258, 'Watson Creek': 259, 'Griff Creek - lower': 260, 'Mill Creek - upper': 261, 'First Creek - upper': 262, 'Second Creek - upper': 263, 'Woods Creek - upper': 264, 'Angora Creek - tributary': 265, 'Angora Creek - upper': 266, 'Big Meadow Creek - lower': 267, 'Big Meadow Creek - upper 2': 268, 'Blackwood Creek - lower 1': 269, 'Blackwood Creek - middle 2': 270, 'Blackwood Creek - middle 4': 271, 'Blackwood Creek - Upper 2': 272, 'Blackwood Creek - upper 3': 273, 'Burke Creek - middle': 274, 'Burton Creek - upper': 275, 'Cold Creek - middle': 276, 'Cold Creek - tributary 3': 277, 'Cold Creek - tributary 2': 278, 'Cold Creek - upper': 279, 'Deer Creek - headwaters': 280, 'Deer Creek - upper': 281, 'Deer Creek - lower': 282, 'Deer Creek - middle': 283, 'Deer Creek - middle 2': 284, 'Dollar Creek - lower': 285, 'Echo Creek - below lake': 286, 'Edgewood Creek - middle': 287, 'First Creek - lower': 288, 'General Creek - lower': 289, 'General Creek - upper': 290, 'Glen Alpine Creek - lower': 291, 'Glenbrook Creek - upper': 292, 'Griff Creek - tributary': 293, 'Griff Creek - upper': 294, 'Heavenly Valley Creek - middle': 295, 'Hidden Valley Creek - lower': 296, 'Hidden Valley Creek - upper': 297, 'Incline Creek - lower': 298, 'Incline Creek - middle 1': 299, 'Incline Creek - middle 2': 300, 'Incline Creek - ski run': 301, 'Incline Creek - upper': 302, 'Logan House Creek - lower': 303, 'Lonely Gulch Creek - lower': 304, 'Lonely Gulch Creek - middle': 305, 'Madden Creek': 306, 'Marlette Creek - lower': 307, 'Marlette Creek - south fork (upper)': 308, 'McFaul Creek - lower': 309, 'McKinney Creek - lower': 310, 'McKinney Creek - middle': 311, 'Mill Creek - lower': 312, 'North Zephyr Creek - tributary': 313, 'North Zephyr Creek - lower': 314, 'North Zephyr Creek - upper': 315, 'Osgood Creek - above road': 316, 'Osgood Creek - below road': 317, 'Quail Creek - lower': 318, 'Quail Creek - upper': 319, 'Rosewood Creek - lower': 320, 'Rosewood Creek - middle 1': 321, 'Rosewood Creek - middle 2': 322, 'Rosewood Creek - middle 3': 323, 'Rubicon Creek': 324, 'Rubicon Creek - tributary': 325, 'Second Creek - lower': 326, 'Second Creek - lower 2': 327, 'Second Creek - middle': 328, 'Secret Harbor Creek - lower': 329, 'Secret Harbor Creek - upper': 330, 'Slaughterhouse Creek - lower': 331, 'Slaughterhouse Creek - middle': 332, 'Slaughterhouse Creek - upper': 333, 'Taylor Creek': 335, 'Third Creek - headwaters': 336, 'Third Creek - lower': 337, 'Third Creek - lower 2': 338, 'Third Creek - middle 1': 339, 'Third Creek - middle': 340, 'Third Creek - middle 2': 341, 'Third Creek - upper 1': 342, 'Third Creek - upper 2': 343, 'Third Creek - upper 3': 344, 'Trout Creek - tributary 2': 345, 'Trout Creek - tributary 3': 346, 'UTR - Christmas Valley 1': 347, 'UTR - tributary 3': 348, 'UTR - tributary 1': 349, 'UTR - Washoe Meadows': 350, 'Ward Creek - lower': 351, 'Ward Creek - middle': 352, 'Woods Creek - lower': 353, 'Woods Creek - middle': 354, 'Glen Alpine Creek - upper': 355, 'Grass Lake Creek': 356, 'Meiss meadows - 1': 357, 'Christmas Valley meadows - 2': 358, 'Christmas Valley meadows - 1': 359, 'Trout Creek meadows - above Fountain Place': 360, 'Trout Creek meadows - above Pioneer 3': 361, 'Taylor Creek marsh': 362, 'UTR marsh - Trout Creek side': 363, 'small meadow 14': 364, 'Slaughterhouse meadows - 2': 365, 'Antone meadows': 366, 'Snow Creek wetlands - 1': 367, 'Saxon Creek meadows - below Fountain Place': 368, 'Saxon Creek meadows - above Fountain Place 1': 369, 'Hell Hole Meadows - 2': 371, 'Trout Creek headwaters meadows - 2': 372, 'small meadow 1': 373, 'Ginny Lake Meadows': 374, 'Third Creek meadows - 8': 375, 'Third Creek meadows - 7': 376, 'small meadow 15': 377, 'small meadow 13': 378, 'small meadow 16': 379, 'small meadow 50': 380, 'small meadow 111': 381, 'small meadow 7': 382, 'small meadow 17': 383, 'small meadow 92': 384, 'Marlette Lake meadows': 385, 'Third Creek meadows - 3': 386, 'Griff Creek meadows': 387, 'Meeks Bay meadows - 3': 388, 'Big Meadow - 4': 389, 'small meadow 52': 390, 'small meadow 95': 391, 'Star Lake meadows': 392, 'Big Meadow - 1': 393, 'Rubicon Meadows': 394, 'McFual meadow': 395, 'UTR Marsh - UTR side': 396, 'UTR - Reach 5': 397, 'UTR - Johnson meadows - 2': 398, 'UTR - Johnson meadows - 3': 399, 'UTR - Airport reach': 400, 'UTR - Reach 6': 401, 'UTR - golf course meadows': 402, 'Angora meadows - 1': 403, 'Angora meadows - 8': 404, 'Angora meadows - 2': 405, 'Angora meadows - 3': 406, 'Saxon Creek tributary meadows - 1': 407, 'Saxon Creek meadows - above Fountain Place 2': 408, 'Trout Creek meadows - above Pioneer 4': 409, 'Trout Creek meadows - above Pioneer 2': 410, 'Trout Creek meadows - above Pioneer 1': 411, 'Trout Creek - Highland Woods': 412, 'Cold Creek - Highland Woods': 413, 'Heavenly Valley Creek meadows - 1': 414, 'Heavenly Valley Creek meadows - 2': 415, 'Heavenly Valley Creek meadows - 3': 416, 'Heavenly Valley Creek meadows - 4': 417, 'Kahle meadows - 5': 418, 'Meiss meadows - 3': 419, 'Benwood meadows - 1': 420, 'Christmas Valley meadows - 3': 421, 'Benwood meadows - 2': 422, 'small meadow 113': 423, 'Angora meadows - 6': 424, 'Trout Creek above Black Bart': 425, 'Baldwin marsh - 1': 426, 'Trout Creek below Black Bart': 427, 'Burke Creek meadows - 1': 428, 'Buck Lake meadows': 429, 'Glenbrook meadows - 1': 430, 'Spooner meadows - 2': 431, 'Lake Forest meadows - 3': 432, 'Snow Creek wetlands - 2': 433, 'Incline Lake meadows - 2': 434, 'Third Creek meadows - 1': 435, 'High meadows - 2': 436, 'High meadows - 6': 437, 'High meadows - 4': 438, 'High meadows - 1': 439, 'Meeks Bay meadows - 2': 440, 'Meeks Bay meadows - 1': 441, 'Meiss meadows - 4': 442, 'Meiss meadows - 5': 443, 'Hell Hole meadows - 1': 444, 'Spooner meadows - 1': 445, 'Blackwood Creek - lower 2': 470, 'Blackwood Creek - middle 3': 471, 'Kahle meadows - 3': 472, 'Meeks Bay Lagoon': 473, 'Kahle meadows - 6': 474, 'North Zephyr Creek - middle': 475, 'Marlette Creek - old dam site': 477, 'Tallac Creek - abv highway - 2': 478, 'Blackwood Creek - middle 1': 479, 'Homewood Canyon Creek - lower': 480, 'Blackwood Creek - Upper 1': 481, 'Tallac Creek - abv highway - 1': 603, 'Marlette Creek - upper': 630, 'Burton Creek - lower': 634, 'Incline Creek - middle 3': 637, 'Lake Forest meadows - 4': 638, 'Saxon Creek - upper': 640, 'Marlette Creek - south fork (lower)': 643, 'Upper Truckee River - Tahoe Paradise': 370}\n",
      "{1: {'SEZ_Type': 'Channeled Meadow'}, 2: {'SEZ_Type': 'Channeled Meadow'}, 3: {'SEZ_Type': 'Non-Channeled Meadow'}, 4: {'SEZ_Type': 'Non-Channeled Meadow'}, 5: {'SEZ_Type': 'Non-Channeled Meadow'}, 6: {'SEZ_Type': 'Non-Channeled Meadow'}, 7: {'SEZ_Type': 'Channeled Meadow'}, 8: {'SEZ_Type': 'Channeled Meadow'}, 9: {'SEZ_Type': 'Channeled Meadow'}, 10: {'SEZ_Type': 'Non-Channeled Meadow'}, 11: {'SEZ_Type': 'Channeled Meadow'}, 12: {'SEZ_Type': 'Non-Channeled Meadow'}, 13: {'SEZ_Type': 'Non-Channeled Meadow'}, 14: {'SEZ_Type': 'Non-Channeled Meadow'}, 15: {'SEZ_Type': 'Channeled Meadow'}, 16: {'SEZ_Type': 'Channeled Meadow'}, 17: {'SEZ_Type': 'Non-Channeled Meadow'}, 18: {'SEZ_Type': 'Channeled Meadow'}, 19: {'SEZ_Type': 'Channeled Meadow'}, 20: {'SEZ_Type': 'Channeled Meadow'}, 21: {'SEZ_Type': 'Channeled Meadow'}, 22: {'SEZ_Type': 'Non-Channeled Meadow'}, 23: {'SEZ_Type': 'Channeled Meadow'}, 24: {'SEZ_Type': 'Non-Channeled Meadow'}, 25: {'SEZ_Type': 'Non-Channeled Meadow'}, 26: {'SEZ_Type': 'Non-Channeled Meadow'}, 27: {'SEZ_Type': 'Channeled Meadow'}, 28: {'SEZ_Type': 'Channeled Meadow'}, 29: {'SEZ_Type': 'Non-Channeled Meadow'}, 30: {'SEZ_Type': 'Non-Channeled Meadow'}, 31: {'SEZ_Type': 'Channeled Meadow'}, 32: {'SEZ_Type': 'Channeled Meadow'}, 33: {'SEZ_Type': 'Non-Channeled Meadow'}, 34: {'SEZ_Type': 'Channeled Meadow'}, 35: {'SEZ_Type': 'Channeled Meadow'}, 36: {'SEZ_Type': 'Channeled Meadow'}, 37: {'SEZ_Type': 'Non-Channeled Meadow'}, 38: {'SEZ_Type': 'Non-Channeled Meadow'}, 39: {'SEZ_Type': 'Channeled Meadow'}, 40: {'SEZ_Type': 'Non-Channeled Meadow'}, 41: {'SEZ_Type': 'Channeled Meadow'}, 42: {'SEZ_Type': 'Channeled Meadow'}, 43: {'SEZ_Type': 'Channeled Meadow'}, 44: {'SEZ_Type': 'Non-Channeled Meadow'}, 45: {'SEZ_Type': 'Channeled Meadow'}, 46: {'SEZ_Type': 'Non-Channeled Meadow'}, 47: {'SEZ_Type': 'Channeled Meadow'}, 48: {'SEZ_Type': 'Non-Channeled Meadow'}, 49: {'SEZ_Type': 'Non-Channeled Meadow'}, 50: {'SEZ_Type': 'Non-Channeled Meadow'}, 51: {'SEZ_Type': 'Non-Channeled Meadow'}, 52: {'SEZ_Type': 'Non-Channeled Meadow'}, 53: {'SEZ_Type': 'Non-Channeled Meadow'}, 54: {'SEZ_Type': 'Non-Channeled Meadow'}, 55: {'SEZ_Type': 'Channeled Meadow'}, 56: {'SEZ_Type': 'Non-Channeled Meadow'}, 57: {'SEZ_Type': 'Non-Channeled Meadow'}, 58: {'SEZ_Type': 'Non-Channeled Meadow'}, 59: {'SEZ_Type': 'Channeled Meadow'}, 60: {'SEZ_Type': 'Channeled Meadow'}, 61: {'SEZ_Type': 'Non-Channeled Meadow'}, 62: {'SEZ_Type': 'Non-Channeled Meadow'}, 63: {'SEZ_Type': 'Non-Channeled Meadow'}, 64: {'SEZ_Type': 'Non-Channeled Meadow'}, 65: {'SEZ_Type': 'Non-Channeled Meadow'}, 66: {'SEZ_Type': 'Non-Channeled Meadow'}, 67: {'SEZ_Type': 'Non-Channeled Meadow'}, 68: {'SEZ_Type': 'Channeled Meadow'}, 69: {'SEZ_Type': 'Non-Channeled Meadow'}, 70: {'SEZ_Type': 'Non-Channeled Meadow'}, 71: {'SEZ_Type': 'Channeled Meadow'}, 72: {'SEZ_Type': 'Non-Channeled Meadow'}, 73: {'SEZ_Type': 'Non-Channeled Meadow'}, 74: {'SEZ_Type': 'Non-Channeled Meadow'}, 75: {'SEZ_Type': 'Non-Channeled Meadow'}, 76: {'SEZ_Type': 'Non-Channeled Meadow'}, 77: {'SEZ_Type': 'Non-Channeled Meadow'}, 78: {'SEZ_Type': 'Channeled Meadow'}, 79: {'SEZ_Type': 'Non-Channeled Meadow'}, 80: {'SEZ_Type': 'Channeled Meadow'}, 81: {'SEZ_Type': 'Channeled Meadow'}, 82: {'SEZ_Type': 'Channeled Meadow'}, 83: {'SEZ_Type': 'Non-Channeled Meadow'}, 84: {'SEZ_Type': 'Channeled Meadow'}, 85: {'SEZ_Type': 'Non-Channeled Meadow'}, 86: {'SEZ_Type': 'Channeled Meadow'}, 87: {'SEZ_Type': 'Channeled Meadow'}, 88: {'SEZ_Type': 'Channeled Meadow'}, 89: {'SEZ_Type': 'Non-Channeled Meadow'}, 90: {'SEZ_Type': 'Channeled Meadow'}, 91: {'SEZ_Type': 'Channeled Meadow'}, 92: {'SEZ_Type': 'Non-Channeled Meadow'}, 93: {'SEZ_Type': 'Non-Channeled Meadow'}, 94: {'SEZ_Type': 'Non-Channeled Meadow'}, 95: {'SEZ_Type': 'Non-Channeled Meadow'}, 96: {'SEZ_Type': 'Non-Channeled Meadow'}, 97: {'SEZ_Type': 'Non-Channeled Meadow'}, 98: {'SEZ_Type': 'Non-Channeled Meadow'}, 99: {'SEZ_Type': 'Non-Channeled Meadow'}, 100: {'SEZ_Type': 'Non-Channeled Meadow'}, 101: {'SEZ_Type': 'Channeled Meadow'}, 102: {'SEZ_Type': 'Channeled Meadow'}, 103: {'SEZ_Type': 'Channeled Meadow'}, 104: {'SEZ_Type': 'Channeled Meadow'}, 105: {'SEZ_Type': 'Channeled Meadow'}, 106: {'SEZ_Type': 'Channeled Meadow'}, 107: {'SEZ_Type': 'Non-Channeled Meadow'}, 108: {'SEZ_Type': 'Non-Channeled Meadow'}, 109: {'SEZ_Type': 'Channeled Meadow'}, 110: {'SEZ_Type': 'Channeled Meadow'}, 111: {'SEZ_Type': 'Channeled Meadow'}, 112: {'SEZ_Type': 'Channeled Meadow'}, 113: {'SEZ_Type': 'Channeled Meadow'}, 114: {'SEZ_Type': 'Non-Channeled Meadow'}, 115: {'SEZ_Type': 'Non-Channeled Meadow'}, 116: {'SEZ_Type': 'Non-Channeled Meadow'}, 117: {'SEZ_Type': 'Non-Channeled Meadow'}, 118: {'SEZ_Type': 'Channeled Meadow'}, 119: {'SEZ_Type': 'Non-Channeled Meadow'}, 120: {'SEZ_Type': 'Non-Channeled Meadow'}, 121: {'SEZ_Type': 'Non-Channeled Meadow'}, 122: {'SEZ_Type': 'Non-Channeled Meadow'}, 123: {'SEZ_Type': 'Channeled Meadow'}, 124: {'SEZ_Type': 'Channeled Meadow'}, 125: {'SEZ_Type': 'Non-Channeled Meadow'}, 126: {'SEZ_Type': 'Non-Channeled Meadow'}, 127: {'SEZ_Type': 'Non-Channeled Meadow'}, 128: {'SEZ_Type': 'Channeled Meadow'}, 129: {'SEZ_Type': 'Channeled Meadow'}, 130: {'SEZ_Type': 'Channeled Meadow'}, 131: {'SEZ_Type': 'Channeled Meadow'}, 132: {'SEZ_Type': 'Channeled Meadow'}, 133: {'SEZ_Type': 'Non-Channeled Meadow'}, 134: {'SEZ_Type': 'Channeled Meadow'}, 135: {'SEZ_Type': 'Channeled Meadow'}, 136: {'SEZ_Type': 'Channeled Meadow'}, 137: {'SEZ_Type': 'Non-Channeled Meadow'}, 138: {'SEZ_Type': 'Non-Channeled Meadow'}, 139: {'SEZ_Type': 'Non-Channeled Meadow'}, 140: {'SEZ_Type': 'Non-Channeled Meadow'}, 141: {'SEZ_Type': 'Non-Channeled Meadow'}, 142: {'SEZ_Type': 'Channeled Meadow'}, 143: {'SEZ_Type': 'Non-Channeled Meadow'}, 144: {'SEZ_Type': 'Non-Channeled Meadow'}, 145: {'SEZ_Type': 'Non-Channeled Meadow'}, 146: {'SEZ_Type': 'Non-Channeled Meadow'}, 147: {'SEZ_Type': 'Non-Channeled Meadow'}, 148: {'SEZ_Type': 'Channeled Meadow'}, 149: {'SEZ_Type': 'Channeled Meadow'}, 150: {'SEZ_Type': 'Non-Channeled Meadow'}, 151: {'SEZ_Type': 'Channeled Meadow'}, 152: {'SEZ_Type': 'Channeled Meadow'}, 153: {'SEZ_Type': 'Non-Channeled Meadow'}, 154: {'SEZ_Type': 'Non-Channeled Meadow'}, 155: {'SEZ_Type': 'Non-Channeled Meadow'}, 156: {'SEZ_Type': 'Non-Channeled Meadow'}, 157: {'SEZ_Type': 'Non-Channeled Meadow'}, 158: {'SEZ_Type': 'Non-Channeled Meadow'}, 159: {'SEZ_Type': 'Non-Channeled Meadow'}, 160: {'SEZ_Type': 'Channeled Meadow'}, 161: {'SEZ_Type': 'Channeled Meadow'}, 162: {'SEZ_Type': 'Non-Channeled Meadow'}, 163: {'SEZ_Type': 'Non-Channeled Meadow'}, 164: {'SEZ_Type': 'Non-Channeled Meadow'}, 165: {'SEZ_Type': 'Channeled Meadow'}, 166: {'SEZ_Type': 'Non-Channeled Meadow'}, 167: {'SEZ_Type': 'Non-Channeled Meadow'}, 168: {'SEZ_Type': 'Non-Channeled Meadow'}, 169: {'SEZ_Type': 'Non-Channeled Meadow'}, 170: {'SEZ_Type': 'Non-Channeled Meadow'}, 171: {'SEZ_Type': 'Channeled Meadow'}, 172: {'SEZ_Type': 'Channeled Meadow'}, 173: {'SEZ_Type': 'Non-Channeled Meadow'}, 174: {'SEZ_Type': 'Non-Channeled Meadow'}, 175: {'SEZ_Type': 'Non-Channeled Meadow'}, 176: {'SEZ_Type': 'Channeled Meadow'}, 177: {'SEZ_Type': 'Channeled Meadow'}, 178: {'SEZ_Type': 'Non-Channeled Meadow'}, 179: {'SEZ_Type': 'Channeled Meadow'}, 180: {'SEZ_Type': 'Channeled Meadow'}, 181: {'SEZ_Type': 'Non-Channeled Meadow'}, 182: {'SEZ_Type': 'Non-Channeled Meadow'}, 183: {'SEZ_Type': 'Channeled Meadow'}, 184: {'SEZ_Type': 'Channeled Meadow'}, 185: {'SEZ_Type': 'Non-Channeled Meadow'}, 186: {'SEZ_Type': 'Non-Channeled Meadow'}, 187: {'SEZ_Type': 'Channeled Meadow'}, 188: {'SEZ_Type': 'Channeled Meadow'}, 189: {'SEZ_Type': 'Channeled Meadow'}, 190: {'SEZ_Type': 'Channeled Meadow'}, 191: {'SEZ_Type': 'Channeled Meadow'}, 192: {'SEZ_Type': 'Non-Channeled Meadow'}, 193: {'SEZ_Type': 'Channeled Meadow'}, 194: {'SEZ_Type': 'Non-Channeled Meadow'}, 195: {'SEZ_Type': 'Non-Channeled Meadow'}, 196: {'SEZ_Type': 'Non-Channeled Meadow'}, 197: {'SEZ_Type': 'Non-Channeled Meadow'}, 198: {'SEZ_Type': 'Channeled Meadow'}, 199: {'SEZ_Type': 'Channeled Meadow'}, 200: {'SEZ_Type': 'Non-Channeled Meadow'}, 201: {'SEZ_Type': 'Non-Channeled Meadow'}, 202: {'SEZ_Type': 'Channeled Meadow'}, 203: {'SEZ_Type': 'Channeled Meadow'}, 204: {'SEZ_Type': 'Non-Channeled Meadow'}, 205: {'SEZ_Type': 'Channeled Meadow'}, 206: {'SEZ_Type': 'Non-Channeled Meadow'}, 207: {'SEZ_Type': 'Channeled Meadow'}, 208: {'SEZ_Type': 'Channeled Meadow'}, 209: {'SEZ_Type': 'Channeled Meadow'}, 210: {'SEZ_Type': 'Channeled Meadow'}, 211: {'SEZ_Type': 'Non-Channeled Meadow'}, 212: {'SEZ_Type': 'Non-Channeled Meadow'}, 213: {'SEZ_Type': 'Non-Channeled Meadow'}, 214: {'SEZ_Type': 'Non-Channeled Meadow'}, 215: {'SEZ_Type': 'Non-Channeled Meadow'}, 216: {'SEZ_Type': 'Non-Channeled Meadow'}, 217: {'SEZ_Type': 'Non-Channeled Meadow'}, 218: {'SEZ_Type': 'Non-Channeled Meadow'}, 219: {'SEZ_Type': 'Non-Channeled Meadow'}, 220: {'SEZ_Type': 'Non-Channeled Meadow'}, 221: {'SEZ_Type': 'Non-Channeled Meadow'}, 222: {'SEZ_Type': 'Non-Channeled Meadow'}, 223: {'SEZ_Type': 'Non-Channeled Meadow'}, 224: {'SEZ_Type': 'Non-Channeled Meadow'}, 225: {'SEZ_Type': 'Non-Channeled Meadow'}, 226: {'SEZ_Type': 'Non-Channeled Meadow'}, 227: {'SEZ_Type': 'Non-Channeled Meadow'}, 228: {'SEZ_Type': 'Non-Channeled Meadow'}, 229: {'SEZ_Type': 'Non-Channeled Meadow'}, 230: {'SEZ_Type': 'Channeled Meadow'}, 231: {'SEZ_Type': 'Non-Channeled Meadow'}, 232: {'SEZ_Type': 'Non-Channeled Meadow'}, 233: {'SEZ_Type': 'Channeled Meadow'}, 234: {'SEZ_Type': 'Non-Channeled Meadow'}, 235: {'SEZ_Type': 'Channeled Meadow'}, 236: {'SEZ_Type': 'Riverine (Perennial)'}, 237: {'SEZ_Type': 'Riverine (Perennial)'}, 238: {'SEZ_Type': 'Riverine (Perennial)'}, 239: {'SEZ_Type': 'Riverine (Perennial)'}, 240: {'SEZ_Type': 'Riverine (Perennial)'}, 241: {'SEZ_Type': 'Riverine (Perennial)'}, 242: {'SEZ_Type': 'Riverine (Perennial)'}, 243: {'SEZ_Type': 'Riverine (Perennial)'}, 244: {'SEZ_Type': 'Riverine (Perennial)'}, 245: {'SEZ_Type': 'Riverine (Perennial)'}, 246: {'SEZ_Type': 'Riverine (Perennial)'}, 247: {'SEZ_Type': 'Riverine (Perennial)'}, 248: {'SEZ_Type': 'Riverine (Perennial)'}, 249: {'SEZ_Type': 'Riverine (Perennial)'}, 250: {'SEZ_Type': 'Riverine (Perennial)'}, 251: {'SEZ_Type': 'Riverine (Perennial)'}, 252: {'SEZ_Type': 'Riverine (Perennial)'}, 253: {'SEZ_Type': 'Riverine (Perennial)'}, 254: {'SEZ_Type': 'Riverine (Perennial)'}, 255: {'SEZ_Type': 'Riverine (Perennial)'}, 256: {'SEZ_Type': 'Riverine (Perennial)'}, 257: {'SEZ_Type': 'Riverine (Perennial)'}, 258: {'SEZ_Type': 'Riverine (Perennial)'}, 259: {'SEZ_Type': 'Riverine (Perennial)'}, 260: {'SEZ_Type': 'Riverine (Perennial)'}, 261: {'SEZ_Type': 'Riverine (Perennial)'}, 262: {'SEZ_Type': 'Riverine (Perennial)'}, 263: {'SEZ_Type': 'Riverine (Perennial)'}, 264: {'SEZ_Type': 'Riverine (Perennial)'}, 265: {'SEZ_Type': 'Riverine (Perennial)'}, 266: {'SEZ_Type': 'Riverine (Perennial)'}, 267: {'SEZ_Type': 'Riverine (Perennial)'}, 268: {'SEZ_Type': 'Riverine (Perennial)'}, 269: {'SEZ_Type': 'Riverine (Perennial)'}, 270: {'SEZ_Type': 'Riverine (Perennial)'}, 271: {'SEZ_Type': 'Riverine (Perennial)'}, 272: {'SEZ_Type': 'Riverine (Perennial)'}, 273: {'SEZ_Type': 'Riverine (Perennial)'}, 274: {'SEZ_Type': 'Riverine (Perennial)'}, 275: {'SEZ_Type': 'Riverine (Perennial)'}, 276: {'SEZ_Type': 'Riverine (Perennial)'}, 277: {'SEZ_Type': 'Riverine (Perennial)'}, 278: {'SEZ_Type': 'Riverine (Perennial)'}, 279: {'SEZ_Type': 'Riverine (Perennial)'}, 280: {'SEZ_Type': 'Riverine (Perennial)'}, 281: {'SEZ_Type': 'Riverine (Perennial)'}, 282: {'SEZ_Type': 'Riverine (Perennial)'}, 283: {'SEZ_Type': 'Riverine (Perennial)'}, 284: {'SEZ_Type': 'Riverine (Perennial)'}, 285: {'SEZ_Type': 'Riverine (Perennial)'}, 286: {'SEZ_Type': 'Riverine (Perennial)'}, 287: {'SEZ_Type': 'Riverine (Perennial)'}, 288: {'SEZ_Type': 'Riverine (Perennial)'}, 289: {'SEZ_Type': 'Riverine (Perennial)'}, 290: {'SEZ_Type': 'Riverine (Perennial)'}, 291: {'SEZ_Type': 'Riverine (Perennial)'}, 292: {'SEZ_Type': 'Riverine (Perennial)'}, 293: {'SEZ_Type': 'Riverine (Perennial)'}, 294: {'SEZ_Type': 'Riverine (Perennial)'}, 295: {'SEZ_Type': 'Riverine (Perennial)'}, 296: {'SEZ_Type': 'Riverine (Perennial)'}, 297: {'SEZ_Type': 'Riverine (Perennial)'}, 298: {'SEZ_Type': 'Riverine (Perennial)'}, 299: {'SEZ_Type': 'Riverine (Perennial)'}, 300: {'SEZ_Type': 'Riverine (Perennial)'}, 301: {'SEZ_Type': 'Riverine (Perennial)'}, 302: {'SEZ_Type': 'Riverine (Perennial)'}, 303: {'SEZ_Type': 'Riverine (Perennial)'}, 304: {'SEZ_Type': 'Riverine (Perennial)'}, 305: {'SEZ_Type': 'Riverine (Perennial)'}, 306: {'SEZ_Type': 'Riverine (Perennial)'}, 307: {'SEZ_Type': 'Riverine (Perennial)'}, 308: {'SEZ_Type': 'Riverine (Perennial)'}, 309: {'SEZ_Type': 'Riverine (Perennial)'}, 310: {'SEZ_Type': 'Riverine (Perennial)'}, 311: {'SEZ_Type': 'Riverine (Perennial)'}, 312: {'SEZ_Type': 'Riverine (Perennial)'}, 313: {'SEZ_Type': 'Riverine (Perennial)'}, 314: {'SEZ_Type': 'Riverine (Perennial)'}, 315: {'SEZ_Type': 'Riverine (Perennial)'}, 316: {'SEZ_Type': 'Riverine (Perennial)'}, 317: {'SEZ_Type': 'Riverine (Perennial)'}, 318: {'SEZ_Type': 'Riverine (Perennial)'}, 319: {'SEZ_Type': 'Riverine (Perennial)'}, 320: {'SEZ_Type': 'Riverine (Perennial)'}, 321: {'SEZ_Type': 'Riverine (Perennial)'}, 322: {'SEZ_Type': 'Riverine (Perennial)'}, 323: {'SEZ_Type': 'Riverine (Perennial)'}, 324: {'SEZ_Type': 'Riverine (Perennial)'}, 325: {'SEZ_Type': 'Riverine (Perennial)'}, 326: {'SEZ_Type': 'Riverine (Perennial)'}, 327: {'SEZ_Type': 'Riverine (Perennial)'}, 328: {'SEZ_Type': 'Riverine (Perennial)'}, 329: {'SEZ_Type': 'Riverine (Perennial)'}, 330: {'SEZ_Type': 'Riverine (Perennial)'}, 331: {'SEZ_Type': 'Riverine (Perennial)'}, 332: {'SEZ_Type': 'Riverine (Perennial)'}, 333: {'SEZ_Type': 'Riverine (Perennial)'}, 334: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 335: {'SEZ_Type': 'Riverine (Perennial)'}, 336: {'SEZ_Type': 'Riverine (Perennial)'}, 337: {'SEZ_Type': 'Riverine (Perennial)'}, 338: {'SEZ_Type': 'Riverine (Perennial)'}, 339: {'SEZ_Type': 'Riverine (Perennial)'}, 340: {'SEZ_Type': 'Riverine (Perennial)'}, 341: {'SEZ_Type': 'Riverine (Perennial)'}, 342: {'SEZ_Type': 'Riverine (Perennial)'}, 343: {'SEZ_Type': 'Riverine (Perennial)'}, 344: {'SEZ_Type': 'Riverine (Perennial)'}, 345: {'SEZ_Type': 'Riverine (Perennial)'}, 346: {'SEZ_Type': 'Riverine (Perennial)'}, 347: {'SEZ_Type': 'Riverine (Perennial)'}, 348: {'SEZ_Type': 'Riverine (Perennial)'}, 349: {'SEZ_Type': 'Riverine (Perennial)'}, 350: {'SEZ_Type': 'Riverine (Perennial)'}, 351: {'SEZ_Type': 'Riverine (Perennial)'}, 352: {'SEZ_Type': 'Riverine (Perennial)'}, 353: {'SEZ_Type': 'Riverine (Perennial)'}, 354: {'SEZ_Type': 'Riverine (Perennial)'}, 355: {'SEZ_Type': 'Riverine (Perennial)'}, 356: {'SEZ_Type': 'Riverine (Perennial)'}, 357: {'SEZ_Type': 'Riverine (Perennial)'}, 358: {'SEZ_Type': 'Riverine (Perennial)'}, 359: {'SEZ_Type': 'Riverine (Perennial)'}, 360: {'SEZ_Type': 'Riverine (Perennial)'}, 361: {'SEZ_Type': 'Riverine (Perennial)'}, 362: {'SEZ_Type': 'Riverine (Perennial)'}, 363: {'SEZ_Type': 'Riverine (Perennial)'}, 364: {'SEZ_Type': 'Riverine (Perennial)'}, 365: {'SEZ_Type': 'Riverine (Perennial)'}, 366: {'SEZ_Type': 'Riverine (Perennial)'}, 367: {'SEZ_Type': 'Riverine (Perennial)'}, 368: {'SEZ_Type': 'Riverine (Perennial)'}, 369: {'SEZ_Type': 'Riverine (Perennial)'}, 645: {'SEZ_Type': 'Riverine (Perennial)'}, 371: {'SEZ_Type': 'Riverine (Perennial)'}, 372: {'SEZ_Type': 'Riverine (Perennial)'}, 373: {'SEZ_Type': 'Riverine (Perennial)'}, 374: {'SEZ_Type': 'Riverine (Perennial)'}, 375: {'SEZ_Type': 'Riverine (Perennial)'}, 376: {'SEZ_Type': 'Riverine (Perennial)'}, 377: {'SEZ_Type': 'Riverine (Perennial)'}, 378: {'SEZ_Type': 'Riverine (Perennial)'}, 379: {'SEZ_Type': 'Riverine (Perennial)'}, 380: {'SEZ_Type': 'Riverine (Perennial)'}, 381: {'SEZ_Type': 'Riverine (Perennial)'}, 382: {'SEZ_Type': 'Riverine (Perennial)'}, 383: {'SEZ_Type': 'Riverine (Perennial)'}, 384: {'SEZ_Type': 'Riverine (Perennial)'}, 385: {'SEZ_Type': 'Riverine (Perennial)'}, 386: {'SEZ_Type': 'Riverine (Perennial)'}, 387: {'SEZ_Type': 'Riverine (Perennial)'}, 388: {'SEZ_Type': 'Riverine (Perennial)'}, 389: {'SEZ_Type': 'Riverine (Perennial)'}, 390: {'SEZ_Type': 'Riverine (Perennial)'}, 391: {'SEZ_Type': 'Riverine (Perennial)'}, 392: {'SEZ_Type': 'Riverine (Perennial)'}, 393: {'SEZ_Type': 'Riverine (Perennial)'}, 394: {'SEZ_Type': 'Riverine (Perennial)'}, 395: {'SEZ_Type': 'Riverine (Perennial)'}, 396: {'SEZ_Type': 'Riverine (Perennial)'}, 397: {'SEZ_Type': 'Riverine (Perennial)'}, 398: {'SEZ_Type': 'Riverine (Perennial)'}, 399: {'SEZ_Type': 'Riverine (Perennial)'}, 400: {'SEZ_Type': 'Riverine (Perennial)'}, 401: {'SEZ_Type': 'Riverine (Perennial)'}, 402: {'SEZ_Type': 'Riverine (Perennial)'}, 403: {'SEZ_Type': 'Riverine (Perennial)'}, 404: {'SEZ_Type': 'Riverine (Perennial)'}, 405: {'SEZ_Type': 'Riverine (Perennial)'}, 406: {'SEZ_Type': 'Riverine (Perennial)'}, 407: {'SEZ_Type': 'Riverine (Perennial)'}, 408: {'SEZ_Type': 'Riverine (Perennial)'}, 409: {'SEZ_Type': 'Riverine (Perennial)'}, 410: {'SEZ_Type': 'Riverine (Perennial)'}, 411: {'SEZ_Type': 'Riverine (Perennial)'}, 412: {'SEZ_Type': 'Riverine (Perennial)'}, 413: {'SEZ_Type': 'Riverine (Perennial)'}, 414: {'SEZ_Type': 'Riverine (Perennial)'}, 415: {'SEZ_Type': 'Riverine (Perennial)'}, 416: {'SEZ_Type': 'Riverine (Perennial)'}, 417: {'SEZ_Type': 'Riverine (Perennial)'}, 418: {'SEZ_Type': 'Riverine (Perennial)'}, 419: {'SEZ_Type': 'Riverine (Perennial)'}, 420: {'SEZ_Type': 'Riverine (Perennial)'}, 421: {'SEZ_Type': 'Riverine (Perennial)'}, 422: {'SEZ_Type': 'Riverine (Perennial)'}, 423: {'SEZ_Type': 'Riverine (Perennial)'}, 424: {'SEZ_Type': 'Riverine (Perennial)'}, 425: {'SEZ_Type': 'Riverine (Perennial)'}, 426: {'SEZ_Type': 'Riverine (Perennial)'}, 427: {'SEZ_Type': 'Riverine (Perennial)'}, 428: {'SEZ_Type': 'Riverine (Perennial)'}, 429: {'SEZ_Type': 'Riverine (Perennial)'}, 430: {'SEZ_Type': 'Riverine (Perennial)'}, 431: {'SEZ_Type': 'Riverine (Perennial)'}, 432: {'SEZ_Type': 'Riverine (Perennial)'}, 433: {'SEZ_Type': 'Riverine (Perennial)'}, 434: {'SEZ_Type': 'Riverine (Perennial)'}, 435: {'SEZ_Type': 'Riverine (Perennial)'}, 436: {'SEZ_Type': 'Riverine (Perennial)'}, 437: {'SEZ_Type': 'Riverine (Perennial)'}, 438: {'SEZ_Type': 'Riverine (Perennial)'}, 439: {'SEZ_Type': 'Riverine (Perennial)'}, 440: {'SEZ_Type': 'Riverine (Perennial)'}, 441: {'SEZ_Type': 'Riverine (Perennial)'}, 442: {'SEZ_Type': 'Riverine (Perennial)'}, 443: {'SEZ_Type': 'Riverine (Perennial)'}, 444: {'SEZ_Type': 'Riverine (Perennial)'}, 445: {'SEZ_Type': 'Riverine (Perennial)'}, 446: {'SEZ_Type': 'Forested'}, 447: {'SEZ_Type': 'Forested'}, 448: {'SEZ_Type': 'Forested'}, 449: {'SEZ_Type': 'Forested'}, 450: {'SEZ_Type': 'Riverine (Perennial)'}, 451: {'SEZ_Type': 'Forested'}, 452: {'SEZ_Type': 'Forested'}, 453: {'SEZ_Type': 'Forested'}, 454: {'SEZ_Type': 'Forested'}, 455: {'SEZ_Type': 'Forested'}, 456: {'SEZ_Type': 'Forested'}, 457: {'SEZ_Type': 'Forested'}, 458: {'SEZ_Type': 'Forested'}, 459: {'SEZ_Type': 'Forested'}, 460: {'SEZ_Type': 'Forested'}, 461: {'SEZ_Type': 'Forested'}, 462: {'SEZ_Type': 'Forested'}, 463: {'SEZ_Type': 'Forested'}, 464: {'SEZ_Type': 'Forested'}, 465: {'SEZ_Type': 'Forested'}, 466: {'SEZ_Type': 'Forested'}, 467: {'SEZ_Type': 'Riverine (Perennial)'}, 468: {'SEZ_Type': 'Non-Channeled Meadow'}, 469: {'SEZ_Type': 'Channeled Meadow'}, 470: {'SEZ_Type': 'Riverine (Perennial)'}, 471: {'SEZ_Type': 'Riverine (Perennial)'}, 472: {'SEZ_Type': 'Riverine (Perennial)'}, 473: {'SEZ_Type': 'Riverine (Perennial)'}, 474: {'SEZ_Type': 'Riverine (Perennial)'}, 475: {'SEZ_Type': 'Riverine (Perennial)'}, 476: {'SEZ_Type': 'Riverine (Perennial)'}, 477: {'SEZ_Type': 'Riverine (Perennial)'}, 478: {'SEZ_Type': 'Riverine (Perennial)'}, 479: {'SEZ_Type': 'Riverine (Perennial)'}, 480: {'SEZ_Type': 'Riverine (Perennial)'}, 481: {'SEZ_Type': 'Riverine (Perennial)'}, 482: {'SEZ_Type': 'Non-Channeled Meadow'}, 483: {'SEZ_Type': 'Non-Channeled Meadow'}, 484: {'SEZ_Type': 'Channeled Meadow'}, 485: {'SEZ_Type': 'Non-Channeled Meadow'}, 486: {'SEZ_Type': 'Channeled Meadow'}, 487: {'SEZ_Type': 'Channeled Meadow'}, 488: {'SEZ_Type': 'Non-Channeled Meadow'}, 489: {'SEZ_Type': 'Non-Channeled Meadow'}, 490: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 491: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 492: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 493: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 494: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 495: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 496: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 497: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 498: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 499: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 500: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 501: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 502: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 503: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 504: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 505: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 506: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 507: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 508: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 509: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 510: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 511: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 512: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 513: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 514: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 515: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 516: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 517: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 518: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 519: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 520: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 521: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 522: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 523: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 524: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 525: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 526: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 527: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 528: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 529: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 530: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 531: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 532: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 533: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 534: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 535: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 536: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 537: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 538: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 539: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 540: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 541: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 542: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 543: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 544: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 545: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 546: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 547: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 548: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 549: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 550: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 551: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 552: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 553: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 554: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 555: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 556: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 557: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 558: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 559: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 560: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 561: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 562: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 563: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 564: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 565: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 566: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 567: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 568: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 569: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 570: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 571: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 572: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 573: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 574: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 575: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 576: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 577: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 578: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 579: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 580: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 581: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 582: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 583: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 584: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 585: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 586: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 587: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 588: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 589: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 590: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 591: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 592: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 593: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 594: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 595: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 596: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 597: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 598: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 599: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 600: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 601: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 602: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 603: {'SEZ_Type': 'Riverine (Perennial)'}, 604: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 605: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 606: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 607: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 608: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 609: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 610: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 611: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 612: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 613: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 614: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 615: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 616: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 617: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 618: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 619: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 620: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 621: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 622: {'SEZ_Type': 'Forested'}, 623: {'SEZ_Type': 'Channeled Meadow'}, 624: {'SEZ_Type': 'Riverine (Perennial)'}, 625: {'SEZ_Type': 'Non-Channeled Meadow'}, 626: {'SEZ_Type': 'Forested'}, 627: {'SEZ_Type': 'Forested'}, 628: {'SEZ_Type': 'Forested'}, 629: {'SEZ_Type': 'Forested'}, 630: {'SEZ_Type': 'Riverine (Perennial)'}, 631: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 632: {'SEZ_Type': 'Non-Channeled Meadow'}, 633: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 634: {'SEZ_Type': 'Riverine (Perennial)'}, 635: {'SEZ_Type': 'Forested'}, 636: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 637: {'SEZ_Type': 'Riverine (Perennial)'}, 638: {'SEZ_Type': 'Riverine (Perennial)'}, 639: {'SEZ_Type': 'Forested'}, 640: {'SEZ_Type': 'Riverine (Perennial)'}, 641: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 642: {'SEZ_Type': 'Riverine (Perennial) + Forested'}, 643: {'SEZ_Type': 'Riverine (Perennial)'}}\n"
     ]
    }
   ],
   "source": [
    "#Large Polygons or only polygon shapes lookup dictionary for Assessment Units with lerger values of acreage\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "excel_data = pd.read_csv(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Large_Polygon_Lookup.csv\")  \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "lookup_dict = {}\n",
    "\n",
    "for index, row in excel_data.iterrows():\n",
    "    lookup_dict[row['Assessment_Unit_Name']] = row['SEZ_ID']\n",
    "\n",
    "# See dictionary where keys are Assessment Unit Names and values are SEZ IDs\n",
    "print(lookup_dict)\n",
    "\n",
    "#Small Polygon if there are two acres for an SEZ\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "excel_data = pd.read_csv(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Small_Polygon_Lookup.csv\")  \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "lookup_riverine = {}\n",
    "\n",
    "for index, row in excel_data.iterrows():\n",
    "    lookup_riverine[row['Assessment_Unit_Name']] = row['SEZ_ID']\n",
    "\n",
    "# See dictionary where keys are Assessment Unit Names and values are SEZ IDs\n",
    "print(lookup_riverine)\n",
    "\n",
    "#All Polygons\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "excel_data = pd.read_csv(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\All_SEZID_Lookup.csv\")  \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "lookup_all = {}\n",
    "\n",
    "for index, row in excel_data.iterrows():\n",
    "    lookup_all[row['SEZ_ID']] = {'SEZ_Type': row['SEZ_Type']}\n",
    "\n",
    "# See dictionary where keys are Assessment Unit Names and values are SEZ IDs\n",
    "print(lookup_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Grading each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grading for each parameter \n",
    "#Defining Grade for Bank Stability based on Erosiondf[percent_unstable]\n",
    "def categorize_erosion(Percent_Unstable):\n",
    "    if pd.isna(Percent_Unstable):\n",
    "        return np.nan\n",
    "    elif 0 <= Percent_Unstable < 5:\n",
    "        return 'A'\n",
    "    elif 5 <= Percent_Unstable < 20:\n",
    "        return 'B'\n",
    "    elif 20 <= Percent_Unstable < 50:\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'D'\n",
    "    \n",
    "#Scoring based off of grading - check this\n",
    "def score_indicator(Rating):\n",
    "    if pd.isna(Rating):\n",
    "        return np.nan\n",
    "    elif  Rating == 'A':\n",
    "        return '12'\n",
    "    elif Rating == 'B':\n",
    "        return '9'\n",
    "    elif Rating == 'C':\n",
    "        return '6'\n",
    "    else:\n",
    "        return '3'\n",
    "\n",
    "#Define Grade for Incision based off of incisino ratio\n",
    "\n",
    "def categorize_incision(bankfull_ratio):\n",
    "    if pd.isna(bankfull_ratio):\n",
    "        return np.nan\n",
    "    elif 0 <= bankfull_ratio < 1.2:\n",
    "        return 'A'\n",
    "    elif 1.2 <= bankfull_ratio < 1.6:\n",
    "        return 'B'\n",
    "    elif 1.6 <= bankfull_ratio < 2.1:\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'D'\n",
    "\n",
    "#Define Grade for Bioassessment Score\n",
    "def categorize_csci(biotic_integrity):\n",
    "     if pd.isna(biotic_integrity):\n",
    "        return np.nan\n",
    "     elif   biotic_integrity > 0.92:\n",
    "        return 'A'\n",
    "     elif 0.79 < biotic_integrity <= 0.92:\n",
    "        return 'B'\n",
    "     elif 0.62 < biotic_integrity <= 0.79:\n",
    "        return 'C'\n",
    "     else:\n",
    "        return 'D'\n",
    "\n",
    "#Define Priority List Level of Invasive Plant Species\n",
    "    \n",
    "\n",
    "# Process Data\n",
    "def rate_invasive(priority):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if (priority[3] + priority[4] == 1):\n",
    "        return 'B'  # Assign score D\n",
    "    elif (priority[3] + priority[4] == 2) or priority[1] == 1 or priority[2] == 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif (priority[3] + priority[4] >= 3)or priority[1] >= 2 or priority[2] >= 2 or (priority[1] + priority[2] >= 2):  \n",
    "       return 'D'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no invasives of any priority are present)\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Define Size for Headcut based off of headcut size\n",
    "##A = 0 headcut, B 1+small headcut\n",
    "def categorize_headcut(headcutdepth):\n",
    "    if pd.isnull(headcutdepth) or headcutdepth == 0:\n",
    "        return 'None'\n",
    "    elif 0.1 <= headcutdepth < 0.5:\n",
    "        return 'small'\n",
    "    elif 0.5 <= headcutdepth < 1:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'large'\n",
    "\n",
    "#define rating for headcut health per sez\n",
    "def rate_headcut(row):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if row['large'] >= 1:\n",
    "        return 'D'  # Assign score D\n",
    "    elif row['medium'] >= 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif row['small'] >= 1:\n",
    "       return 'B'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no headcuts)\n",
    "\n",
    "\n",
    "#define rating SEZ Rating\n",
    "def rate_SEZ(percent):\n",
    "    if 0 <= percent < .70:\n",
    "        return 'D'\n",
    "    elif .7 <= percent < .80:\n",
    "        return 'C'\n",
    "    elif .80 <= percent < .90:\n",
    "        return 'B'\n",
    "    else:\n",
    "        return 'A'\n",
    "    \n",
    "    #Define Grade for IPI Score - Used only for Stream HAbitat Condition\n",
    "def categorize_phab(IPI):\n",
    "     if   IPI >= 0.94:\n",
    "        return 'A'\n",
    "     elif 0.83 < IPI < 0.94:\n",
    "        return 'B'\n",
    "     elif 0.7 < IPI <= 0.83:\n",
    "        return 'C'\n",
    "     else:\n",
    "        return 'D'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Erosion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Get Data\n",
    "#----------------------------------------------------------------#\n",
    "#Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "erosionfields = ['Assessment_Unit_Name', 'Shape.STLength()', 'Bank_Type', 'Survey_Date']\n",
    "#erosiondf = pd.DataFrame.from_records(data=arcpy.da.SearchCursor(erosiondata, erosionfields), columns=erosionfields)\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    " # Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(erosiondata, erosionfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "erosiondf = pd.DataFrame(data, columns=erosionfields)\n",
    "\n",
    "# Replace NaN values in 'Assessment_Unit_Name' column with 'Skylandia SEZ'\n",
    "#erosiondf['Assessment_Unit_Name'] = erosiondf['Assessment_Unit_Name'].fillna('Skylandia SEZ')\n",
    "# Replace specific values in 'Assessment_Unit_Name' column\n",
    "erosiondf['Assessment_Unit_Name'] = erosiondf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh'})\n",
    "\n",
    "\n",
    "#Add SEZ_ID column using lookup dictionary\n",
    "#erosiondf['SEZ_ID'] = erosiondf['Assessment_Unit_Name'].map(lambda x: lookup_dict.get(x, {}).get('SEZ_ID', None))\n",
    "\n",
    "#This code is for the excel look up dictionary\n",
    "erosiondf['SEZ_ID'] = erosiondf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "erosiondf['SEZ_ID'] = erosiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "erosiondf['SEZ_ID'] = erosiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#erosiondf = erosiondf.merge(df, on='Assessment_Unit_Name', how='left')\n",
    "\n",
    "#calculate year column \n",
    "erosiondf['Year'] = erosiondf['Survey_Date'].dt.year\n",
    "\n",
    "# Replace 'both_banks' with 'Both Banks' in Bank_Type column\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['both_banks', 'Both banks'], 'Both Banks' )\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['one_bank', 'One bank'], 'One Bank')\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['no_bank', 'No bank'], 'No Bank')\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Process Data\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "# Initialize variables\n",
    "erosiondf['bank_multiplier'] = erosiondf['Bank_Type'].apply(lambda x: 2 if x == 'Both Banks' else (1 if x == 'One Bank' else 0))\n",
    "\n",
    "\n",
    "# Calculate the product of 'Shape.STLength()' and 'bank_multiplier' to get the eroded banks per row\n",
    "erosiondf['eroded_banks_per_row'] = erosiondf['Shape.STLength()'] * erosiondf['bank_multiplier']\n",
    "\n",
    "# Group by Assessment_Unit_Name and year and sum the lengths of banks for each unit to get total banks assessed\n",
    "erosiondf['banks_assessed_per_unit'] = erosiondf.groupby(['Assessment_Unit_Name', 'Year'])['Shape.STLength()'].transform('sum') * 2\n",
    "\n",
    "# Group by Assessment_Unit_Name and sum the eroded banks per row for each unit\n",
    "erosiondf['SEZ_total_eroded'] = erosiondf.groupby(['Assessment_Unit_Name', 'Year'])['eroded_banks_per_row'].transform('sum')\n",
    "\n",
    "# Calculate percent unstable Bank Stability per Assessment Unit\n",
    "erosiondf['Bank_Stability_Percent_Unstable'] = (erosiondf['SEZ_total_eroded'] / erosiondf['banks_assessed_per_unit']) * 100\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score\n",
    "#----------------------------------------------------------------#\n",
    "erosiondf['Bank_Stability_Rating']=erosiondf['Bank_Stability_Percent_Unstable'].apply(categorize_erosion)\n",
    "erosiondf['Bank_Stability_Score']= erosiondf['Bank_Stability_Rating'].apply(score_indicator)\n",
    "\n",
    "erosiondf['Bank_Stability_Data_Source'] = 'TRPA' #baseline condition assessment?'\n",
    "\n",
    "erosiondf.head()\n",
    "\n",
    "print(erosiondf)\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to bank_stability called stage_bank_stability GDB location\n",
    "#----------------------------------------------------------------#\n",
    "# Define the name of the feature class\n",
    "feature_class_name = 'bank_stability'\n",
    "\n",
    "# Define the full path to the feature class\n",
    "feature_class_path = stage_bank_stability \n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Bank_Stability_Data_Source': 'Bank_Stability_Data_Source',\n",
    "    'Bank_Stability_Percent_Unstable': 'Bank_Stability_Percent_Unstable',\n",
    "    'Bank_Stability_Rating': 'Bank_Stability_Rating',\n",
    "    'Bank_Stability_Score': 'Bank_Stability_Score',\n",
    "    'SEZ_ID': 'SEZ_ID'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "bank_stabilitydf = erosiondf.rename(columns=field_mapping).drop(columns=[col for col in erosiondf.columns if col not in field_mapping])\n",
    "\n",
    "readybankdf = bank_stabilitydf.groupby(['SEZ_ID', 'Year']).first().reset_index()\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "# Fix data type of Year so it writes to table\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y', errors='coerce')\n",
    "\n",
    "# Setting the frequency to 'Y' for year\n",
    "#readydf['Year'] = readydf['Year'].dt.to_period('Y')\n",
    "\n",
    "print(readybankdf)\n",
    "\n",
    "#Write dataframe to fc in SEZ_Data.GDB\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readybankdf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readybankdf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(stage_bank_stability, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")\n",
    "\n",
    "\n",
    "#Write dataframe to sde.collect.bank.stability eventually, current code write it to GDB in SEZ_Data.GDB\n",
    "# Set environment workspace to your SDE connection file\n",
    "#arcpy.env.workspace = master_path\n",
    "\n",
    "# Convert DataFrame to Feature Class\n",
    "#output_feature_class = \"ErosionUpdate\"  # Name for the output feature class\n",
    "#output_fc_path = os.path.join(arcpy.env.workspace, output_feature_class)\n",
    "\n",
    "# Assuming your DataFrame is already converted to a feature class\n",
    "# Replace \"path_to_your_feature_class\" with the actual path to your feature class\n",
    "#arcpy.conversion.TableToTable(\"path_to_your_feature_class\", arcpy.env.workspace, output_feature_class)\n",
    "\n",
    "# Overwrite Feature Class in SDE\n",
    "# Replace \"path_to_your_dataframe\" with the actual path to your DataFrame\n",
    "#arcpy.management.CopyFeatures(\"path_to_your_dataframe\", output_fc_path)\n",
    "\n",
    "\n",
    "#print(ready_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to CSV\n",
    "#csv_path = os.path.join(working_folder, \"Erosiondatamaster.csv\")\n",
    "#ready_df.to_csv(csv_path, index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Incision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Get Data-- all the data we need is in sde.collect.sde.survey.sde.sez_survey\n",
    "#----------------------------------------------------------------##Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "incisionfields = ['Assessment_Unit_Name', 'incision_ratio', 'survey_date']\n",
    "#incisiondf = pd.DataFrame.from_records(data=arcpy.da.SearchCursor(erosiondata, erosionfields), columns=erosionfields)\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    " # Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(sezsurveytable, incisionfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "incisiondf = pd.DataFrame(data, columns=incisionfields)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Add correct info to dataframe\n",
    "#----------------------------------------------------------------#\n",
    "#use this until we fix the domain\n",
    "incisiondf['Assessment_Unit_Name'] = incisiondf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh'})\n",
    "# Create a new column 'SEZ ID'\n",
    "#This code is for the excel look up dictionary\n",
    "incisiondf['SEZ_ID'] = incisiondf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#calculate year column \n",
    "incisiondf['Year'] = incisiondf['survey_date'].dt.year\n",
    "\n",
    "incisiondf\n",
    "\n",
    "# Display the updated incisiondf\n",
    "print(incisiondf)\n",
    "\n",
    "#Add SEZ_ID column using lookup dictionary\n",
    "#incisiondf['SEZ_ID'] = SEZIDdf['GlobalID'].map(lambda x: lookup_dict.get(x, {}).get('SEZ_ID', None))\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "#incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "#incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#incisiondf = incisiondf.merge(df, on='Assessment_Unit_Name', how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score\n",
    "#----------------------------------------------------------------#\n",
    "incisiondf['Incision_Rating']=incisiondf['incision_ratio'].apply(categorize_incision)\n",
    "incisiondf['Incision_Score']= incisiondf['Incision_Rating'].apply(score_indicator)\n",
    "\n",
    "incisiondf['Incision_Data_Source'] = 'TRPA' #baseline condition assessment?'\n",
    "\n",
    "incisiondf.head()\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to incision called stage_incision GDB location\n",
    "#----------------------------------------------------------------#\n",
    "# Define the name of the feature class\n",
    "feature_class_name = 'incision'\n",
    "\n",
    "# Define the full path to the feature class\n",
    "feature_class_path = stage_incision \n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Incision_Data_Source': 'Incision_Data_Source',\n",
    "    'incision_ratio': 'Incision_Ratio',\n",
    "    'Incision_Rating': 'Incision_Rating',\n",
    "    'Incision_Score': 'Incision_Score',\n",
    "    'SEZ_ID': 'SEZ_ID'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "incisionfinaldf = incisiondf.rename(columns=field_mapping).drop(columns=[col for col in incisiondf.columns if col not in field_mapping])\n",
    "\n",
    "readyincisiondf = incisionfinaldf.groupby(['SEZ_ID', 'Year']).first().reset_index()\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readyincisiondf)\n",
    "\n",
    "#Write dataframe to fc in SEZ_Data.GDB\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readyincisiondf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readyincisiondf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(stage_incision, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge readyincisiondf with SEZinfo_df to add the SEZ_Type\n",
    "readyincisiondf = readyincisiondf.merge(dfSEZ[['SEZ_ID', 'SEZ_Type']], on='SEZ_ID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invasive Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calfora Data points--dates only go to 2012?\n",
    "# dates only go to 2012 hereCArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines07/MapServer/1\"\n",
    "#CArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines27/MapServer/0\"\n",
    "# Create a feature layer object\n",
    "#cafeature_layer = FeatureLayer(CArest)\n",
    "#Query caflora layer to that it only shows 2020-2023 or only county?\n",
    "#start_year = 'January 1, 2020'\n",
    "#end_year= 'December 31, 2023'\n",
    "# Convert to datetime objects\n",
    "#start_date = pd.to_datetime(start_year)\n",
    "#end_date = pd.to_datetime(end_year)\n",
    "\n",
    "#Query before or after??\n",
    "#query = (CAsdf['Date_']>= start_date) & (CAsdf['Date_']<= end_date)\n",
    "\n",
    "#CA_filtereddf = CAsdf.join[query].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code using REST Service- most likely will reuse this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from arcgis.features import FeatureLayer\n",
    "from utils import get_fs_data_spatial_query\n",
    "\n",
    "# Define the USFS REST endpoint\n",
    "usfsrest = \"https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer/0\"\n",
    "where    = \"FS_UNIT_ID = '0519'\"\n",
    "\n",
    "# Query the feature layer\n",
    "sdfUSFS = get_fs_data_spatial_query(usfsrest, where)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spatial join of sdf and sez master\n",
    "\n",
    "\n",
    "# Create the spatially enabled DataFrame (sdf) for target feature SEZ assessment units\n",
    "SEZsdf = pd.DataFrame.spatial.from_featureclass(SEZ_Master)\n",
    "#spatial reference stuff\n",
    "sdfUSFS.spatial.sr = SEZsdf.spatial.sr\n",
    "\n",
    "#CAsdf.spatial.set_spatial_reference(SEZsdf.spatial.sr)\n",
    "#perform spatial join\n",
    "usfsdata = SEZsdf.spatial.join(sdfUSFS, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "#SET UP DATA WRANGLE\n",
    "#-----------------------------------------\n",
    "\n",
    "#Path to external data usfs with rest service--This assumes rest service is up to date to 2023\n",
    "\n",
    "# Paths to the feature classes\n",
    "invasive19fc = os.path.join(invasiveplant19gdb, \"Invasive_Species_2019\")\n",
    "invasive20fc = os.path.join(invasiveplant20gdb, \"Invasive_Species_2020\")\n",
    "invasive22fc = os.path.join(invasiveplant22gdb, \"Invasive_Species_2022\")\n",
    "invasive23fc = os.path.join(invasiveplant23gdb, \"sez_invasive_plant\")\n",
    "sez_surveyfc = os.path.join(sez_surveygdb, \"sez_survey\")\n",
    "\n",
    "#Use gdb because they have assessment unit name in them... 2022 i had to manually add in PRO\n",
    "invasive23fields = ['ParentGlobalID', 'invasives_percent_cover','invasives_plant_type', 'invasive_type_other']\n",
    "invasive22fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover', 'InvasiveType_Other', 'Survey_Date']\n",
    "invasive20fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover',  'Other', 'Survey_Date']\n",
    "invasive19fields = ['SITE_ID', 'SURVEY_DATE', 'INVASIVE_PLANT', 'PERCENT_COVER' ]\n",
    "usfsfields = ['Assessment_Unit_Name', 'COMMON_NAME', 'SCIENTIFIC_NAME']\n",
    "sez_surveyfields = ['GlobalID', 'invasives_percent_cover', 'Assessment_Unit_Name', 'invasives_number_of_species', 'survey_date']\n",
    "\n",
    "\n",
    "# Function to read feature class into DataFrame\n",
    "def feature_class_to_dataframe(feature_class, fields):\n",
    "    data = [row for row in arcpy.da.SearchCursor(feature_class, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "\n",
    "# Read feature classes into DataFrames\n",
    "invasive19df = feature_class_to_dataframe(invasive19fc, invasive19fields)\n",
    "invasive20df = feature_class_to_dataframe(invasive20fc, invasive20fields)\n",
    "invasive22df = feature_class_to_dataframe(invasive22fc, invasive22fields)\n",
    "invasivemeasurements23df = feature_class_to_dataframe(invasive23fc, invasive23fields)\n",
    "sez_surveyinvasivedf = feature_class_to_dataframe(sez_surveyfc, sez_surveyfields)\n",
    "\n",
    "\n",
    "#Set invasives_plant_type to NaN if invasives_number_of_species is 0 or NaN\n",
    "#sez_survey- groupby\n",
    "\n",
    "#sez_surveyinvasivedf.loc[(sez_surveyinvasivedf['invasives_number_of_species'] == 0) | (sez_surveyinvasivedf['invasives_number_of_species'].isna()), 'invasives_plant_type'] = np.nan\n",
    "sez_surveyinvasivedf['created_date']= sez_surveyinvasivedf['survey_date']\n",
    "#Join sez_survey and headcut23\n",
    "# Perform the join\n",
    "invasive23df = invasivemeasurements23df.merge(sez_surveyinvasivedf, left_on='ParentGlobalID', right_on='GlobalID', how='right')\n",
    "\n",
    "invasive23df.drop('invasives_percent_cover_y', axis=1, inplace=True)\n",
    "\n",
    "invasive23df.rename(columns={'invasives_percent_cover_x': 'invasives_percent_cover'}, inplace=True)\n",
    "\n",
    "invasive23df['invasives_percent_cover'].fillna(0, inplace=True)\n",
    "invasive23df['invasives_plant_type'].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(invasivedf['plant_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Capture data if there were no invasives observed\n",
    "\n",
    "#invasive23df.loc[invasive23df['invasives_number_of_species'] == 0, 'invasives_plant_type'] = None\n",
    "# Convert SpatialDataFrame to DataFrame\n",
    "\n",
    "usfsdf = usfsdata[usfsfields]\n",
    "#usfsdf = usfsdata.drop(columns='SHAPE')\n",
    "\n",
    "#print(usfsdf)\n",
    "#usfs23df = feature_class_to_dataframe(usfsdata, usfsfields)\n",
    "# Rename fields for consistency\n",
    "invasive19df.rename(columns={'SITE_ID': 'Assessment_Unit_Name', 'INVASIVE_PLANT': 'plant_type', 'PERCENT_COVER': 'percent_cover', 'SURVEY_DATE': 'created_date'}, inplace=True)\n",
    "invasive20df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date'}, inplace=True)\n",
    "invasive22df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'InvasiveType_Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date'}, inplace=True)\n",
    "invasive23df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'invasives_percent_cover': 'percent_cover', 'invasives_plant_type': 'plant_type', 'invasive_type_other': 'other', 'created_date': 'created_date'}, inplace=True)\n",
    "usfsdf.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'COMMON_NAME': 'plant_type'}, inplace=True)\n",
    "\n",
    "required_columns = ['Assessment_Unit_Name', 'plant_type', 'percent_cover', 'other', 'created_date', 'Source']\n",
    "\n",
    "# Define a function to add missing columns and keep only required columns\n",
    "def add_and_keep_columns(df, required_columns):\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "    return df[required_columns]\n",
    "\n",
    "# Add missing columns to each dataframe\n",
    "invasive19df = add_and_keep_columns(invasive19df, required_columns)\n",
    "invasive20df = add_and_keep_columns(invasive20df, required_columns)\n",
    "invasive22df = add_and_keep_columns(invasive22df, required_columns)\n",
    "invasive23df = add_and_keep_columns(invasive23df, required_columns)\n",
    "usfsdf = add_and_keep_columns(usfsdf, required_columns)\n",
    "\n",
    "\n",
    "#Remove null plant types for usfs data\n",
    "usfsdf = usfsdf[~usfsdf['plant_type'].isna()]\n",
    "# Remove records where plant_type is 'Eurasian watermilfoil'\n",
    "usfsdf = usfsdf[usfsdf['plant_type'] != 'Eurasian watermilfoil']\n",
    "\n",
    "\n",
    "#Add Source\n",
    "invasive19df['Source'] = 'TRPA'\n",
    "invasive20df['Source'] = 'TRPA'\n",
    "invasive22df['Source'] = 'TRPA'\n",
    "invasive23df['Source'] = 'TRPA'\n",
    "usfsdf['Source'] = 'USFS'\n",
    "\n",
    "# Concatenate DataFrames\n",
    "invasivedf = pd.concat([usfsdf, invasive19df, invasive20df, invasive22df, invasive23df], ignore_index=True)\n",
    "\n",
    "#Define SEZ ID based on Assessment_Unit_Name for QA on SEZ Name \n",
    "invasivedf['SEZ_ID'] = invasivedf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].fillna(0)\n",
    "\n",
    "#remove meadow from 2019 test period that are not actually meadows- \n",
    "# first run code without this  in case new data has wrongly spelled assessmne tunit name\n",
    "invasivedf = invasivedf[invasivedf['SEZ_ID'] != 0]\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "#invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].astype(int)\n",
    "#invasivedf['Assessment_Unit_Name'] = invasivedf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky Meadows meadow - 1': 'Sky meadows'})\n",
    "\n",
    "\n",
    "# Set 'Year' column based on data source\n",
    "invasivedf['Year'] = invasivedf['created_date'].dt.year\n",
    "\n",
    "\n",
    "invasivedf.loc[invasivedf['Source'] == 'USFS', 'Year'] = '2023'\n",
    "invasivedf.loc[invasivedf['Source'] == 'TRPA', 'Year'] = invasivedf['created_date'].dt.year\n",
    "\n",
    "#invasivedf.reset_index(drop=True, inplace=True)\n",
    "invasivedf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#invasivedf = invasivedf.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip this\n",
    "invasivedf_values_unique = invasivedf.values.flatten()\n",
    "is_unique = len(invasivedf_values_unique) == len(set(invasivedf_values_unique))\n",
    "print(is_unique)\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = invasivedf[invasivedf.duplicated(keep=False)]\n",
    "\n",
    "# Display duplicate rows\n",
    "print(duplicate_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------#\n",
    "    #Prep Plant_type Data\n",
    "#---------------------------#\n",
    "#Make a dataframe to capture 'other' plants in trpa data and then add it to invasive df\n",
    "other_plants_df = invasivedf[['Source', 'Year', 'SEZ_ID', 'Assessment_Unit_Name', 'other']].copy()\n",
    "\n",
    "#Get rid of Null values\n",
    "other_plants_df = other_plants_df[~other_plants_df['other'].isna()]\n",
    "\n",
    "#invasivedf.reset_index(drop=True, inplace=True)\n",
    "other_plants_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "#Rename 'other to plant_type\n",
    "other_plants_df.rename(columns={'other': 'plant_type'}, inplace=True)\n",
    "\n",
    "#invasivedf = invasivedf.drop_duplicates()\n",
    "\n",
    "\n",
    "# Concatenate other_plants_df with invasivedf JUST DO THIS MANUALLY \n",
    "#invasivedf = pd.concat([invasivedf, other_plants_df], ignore_index=True)\n",
    "\n",
    "#Append\n",
    "#invasivesdf=invasivedf.append(other_plants_df)\n",
    "#Concatenate the new DataFrame with the existing invasivedf\n",
    "#invasivedf = pd.concat([invasivedf, other_plants_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Replace various representations of null values with 'none'\n",
    "null_representations = ['<null>', '<Null>', '', 'NA', 'N/A', 'nan', 'NaN', 'None', 'NULL', None]\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace(null_representations, 'none')\n",
    "\n",
    "# Split plant types by comma and create new rows\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.split(pat=',')\n",
    "invasivedf = invasivedf.explode('plant_type')\n",
    "\n",
    "# Capitalize the first word and replace underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.split('_').str[0].str.capitalize() + ' ' + invasivedf['plant_type'].str.split('_').str[1:].str.join(' ')\n",
    "\n",
    "# Remove spaces after the words and capitalize the first word while replacing underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].astype(str)\n",
    "\n",
    "\n",
    "# Remove any spaces at the beginning or end of a word\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.strip()\n",
    "\n",
    "\n",
    "#---------------------#\n",
    "#These replacements are so the same weed with different common name doesn't get counted twice toward score MAY NEED TO ADD MORE DESCREPENCIES in coming years if more pop up or find more elopquent solution\n",
    "#---------------------#\n",
    "# Replace 'Common mullein' with 'Wooly mullein' in the 'plant_type' column \n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common mullein', 'Wooly mullein')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Nodding plumeless thistle', 'Musk thistle')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Field bindweed', 'Common bindweed')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common st. johnswort', 'Klamathweed')\n",
    "# Replace 'Broadleaf and leaved' with 'Perennial' in the 'plant_type' column\n",
    "# Define the replacements dictionary\n",
    "replacements = {'Broadleaf Pepperweed': 'Perennial pepperweed', 'Broadleaved pepperweed': 'Perennial pepperweed'}\n",
    "\n",
    "# Replace values in the 'plant_type' column using the dictionary\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace(replacements)\n",
    "\n",
    "#Replace Sulphur Cinquefoil with sulfur\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sulphur cinquefoil', 'Sulfur cinquefoil')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sweetclover', 'White sweetclover')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Reed canary grass', 'Reed canarygrass')\n",
    "#Replace\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Salt cedar', 'Tamarisk')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Butter and eggs', 'Yellow toadflax')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Canada cottonthistle', 'Canada thistle')\n",
    "# Replace empty strings or other placeholders with NaN\n",
    "#invasivedf['plant_type'] = invasivedf['plant_type'].replace('', np.nan)\n",
    "\n",
    "# Drop duplicates based on 'Assessment_Unit_Name' and 'Year' in the remaining DataFrame\n",
    "invasivedf = invasivedf.drop_duplicates(subset=['Assessment_Unit_Name', 'Year', 'plant_type'], keep='first')\n",
    "\n",
    "\n",
    "grouped_df = invasivedf.groupby(['Assessment_Unit_Name', 'Year'])['plant_type']\n",
    "\n",
    "# Aggregate the plant types into one column separated by commas\n",
    "combined_plant_types = grouped_df.apply(lambda x: ', '.join(x)).reset_index(name='all_plant_types')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group, data in invasivedf.groupby(['Assessment_Unit_Name', 'Year']):\n",
    "    print(group)\n",
    "    print(data)\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------#\n",
    "#Create Look up dictionary to assign priority group for each plant and then use it\n",
    "#----------------------------------------------------#\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "csv_data = pd.read_csv(r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Invasives Priority lookup.csv\") \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "Invasives_lookup = {}\n",
    "\n",
    "key = 'Common'\n",
    "values = ['Scientific', 'Priority'] \n",
    "\n",
    "Invasives_lookup= csv_data.set_index(key)[values].to_dict(orient='index')\n",
    "\n",
    "# Define a custom function to map plant types to priorities\n",
    "def map_priority(plant_type):\n",
    "    if pd.isnull(plant_type):\n",
    "        return 'None' # Return NaN for NaN values\n",
    "    else:\n",
    "        # Extract the priority from the dictionary, or return 'Unknown' if not found\n",
    "        plant_info = Invasives_lookup.get(plant_type)\n",
    "        if plant_info:\n",
    "            return plant_info['Priority']\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "\n",
    "\n",
    "# Create a new column 'Priority' based on the mapping from the dictionary\n",
    "invasivedf['Priority'] = invasivedf['plant_type'].map(map_priority)\n",
    "\n",
    "print(invasivedf.columns)\n",
    "#invasivedf['Priority'] = pd.to_numeric(invasivedf['Priority'], errors='coerce')\n",
    "\n",
    "# group by assessment unit and year and summarize the priority level of plants in each unit\n",
    "invasive_summary = invasivedf.groupby(['Assessment_Unit_Name', 'Year', 'Priority','Source'], dropna=False).size().reset_index(name='Count')\n",
    "\n",
    "invasive_summary_priority = invasive_summary.pivot_table(index=['Assessment_Unit_Name', 'Year','Source'], columns='Priority', values='Count', fill_value=0)\n",
    "\n",
    "\n",
    "# Reset the index to flatten the DataFrame\n",
    "invasive_summary_priority.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Grade, Score, # of Invasives\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "# Process Data\n",
    "def rate_invasive(priority):\n",
    "    if (priority[3] + priority[4] == 1):\n",
    "        return 'B'  # Assign score D\n",
    "    elif (priority[3] + priority[4] == 2) or priority[1] == 1 or priority[2] == 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif (priority[3] + priority[4] >= 3)or priority[1] >= 2 or priority[2] >= 2 or (priority[1] + priority[2] >= 2):  \n",
    "       return 'D'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no invasives of any priority are present)\n",
    "\n",
    "# Apply the rating function to the summary DataFrame\n",
    "invasive_summary_priority['Invasives_Rating'] = invasive_summary_priority[[1, 2, 3, 4]].apply(rate_invasive, axis=1)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "invasive_summary_priority['Invasives_Score']= invasive_summary_priority['Invasives_Rating'].apply(score_indicator) \n",
    "\n",
    "#Calculate total number of invasives per sez per year\n",
    "invasive_summary_priority['Number_of_Invasives']= invasive_summary_priority[[1, 2, 3, 4]].sum(axis=1)\n",
    "\n",
    "#Define SEZ ID based on Assessment_Unit_Name for QA on SEZ Name \n",
    "invasive_summary_priority['SEZ_ID'] = invasive_summary_priority['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "invasive_summary_priority['all_plants']= combined_plant_types['all_plant_types']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Prep and post ending dataframe to invasives table in SEZ_Data.GDB\n",
    "#----------------------------------------------------------------#\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Source': 'Invasives_Data_Source',\n",
    "    'Number_of_Invasives': 'Invasives_Number_of_Invasives',\n",
    "    'Invasives_Rating': 'Invasives_Rating',\n",
    "    'Invasives_Score': 'Invasives_Scores',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'percent_cover': 'Invasives_Percent_Cover',\n",
    "    'all_plants': 'Invasives_Plant_Types',\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readyinvasivedf = invasive_summary_priority.rename(columns=field_mapping).drop(columns=[col for col in invasive_summary_priority.columns if col not in field_mapping])\n",
    "\n",
    "readyinvasivedf['SEZ_ID'] = readyinvasivedf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readyinvasivedf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readyinvasivedf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readyinvasivedf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_invasives, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invasive with gdb and usfs pre joined layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invasive Species Use if need to use GDB to import data--shouldn't have to\n",
    "\n",
    "\n",
    "#Path to external data usfs \n",
    "usfsdata = os.path.join(master_path, \"usfsinvasivesez24\")\n",
    "#Path to external data Calflora aka State Park Data\n",
    "#calfloradata = os.path.join(master_path, \"\")\n",
    "\n",
    "# Paths to the feature classes\n",
    "invasive19fc = os.path.join(invasiveplant19gdb, \"Invasive_Species_2019\")\n",
    "invasive20fc = os.path.join(invasiveplant20gdb, \"Invasive_Species_2020\")\n",
    "invasive22fc = os.path.join(invasiveplant22gdb, \"Invasive_Species_2022\")\n",
    "invasive23fc = os.path.join(invasiveplant23gdb, \"sez_invasive_plant\")\n",
    "\n",
    "\n",
    "#Use gdb because they have assessment unit name in them... 2022 i had to manually add in PRO\n",
    "invasive23fields = ['Assessment_Unit', 'invasives_percent_cover','invasives_plant_type', 'Species_Level', 'invasive_type_other', 'created_date']\n",
    "invasive22fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover', 'InvasiveType_Other', 'Survey_Date']\n",
    "invasive20fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover',  'Other', 'Survey_Date']\n",
    "invasive19fields = ['SITE_ID', 'SURVEY_DATE', 'INVASIVE_PLANT', 'PERCENT_COVER' ]\n",
    "usfsfields = ['Assessment_Unit_Name', 'PLANT_COMM', 'SCIENTIFIC', 'DATE_COLLE', 'Eradicated']\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "\n",
    "# Function to read feature class into DataFrame\n",
    "def feature_class_to_dataframe(feature_class, fields):\n",
    "    data = [row for row in arcpy.da.SearchCursor(feature_class, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Read feature classes into DataFrames\n",
    "# Read feature classes into DataFrames\n",
    "invasive19df = feature_class_to_dataframe(invasive19fc, invasive19fields)\n",
    "invasive20df = feature_class_to_dataframe(invasive20fc, invasive20fields)\n",
    "invasive22df = feature_class_to_dataframe(invasive22fc, invasive22fields)\n",
    "invasive23df = feature_class_to_dataframe(invasive23fc, invasive23fields)\n",
    "usfs23df = feature_class_to_dataframe(usfsdata, usfsfields)\n",
    "\n",
    "\n",
    "invasive19df['Source'] = 'TRPA'\n",
    "invasive20df['Source'] = 'TRPA'\n",
    "invasive22df['Source'] = 'TRPA'\n",
    "invasive23df['Source'] = 'TRPA'\n",
    "usfs23df['Source'] = 'USFS'\n",
    "\n",
    "# Rename fields\n",
    "invasive19df.rename(columns={'SITE_ID': 'Assessment_Unit_Name', 'INVASIVE_PLANT': 'plant_type', 'PERCENT_COVER': 'percent_cover', 'SURVEY_DATE': 'created_date','Source':'Source'}, inplace=True)\n",
    "invasive20df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date', 'Source':'Source1'}, inplace=True)\n",
    "invasive22df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'InvasiveType_Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date','Source':'Source1'}, inplace=True)\n",
    "invasive23df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'invasives_percent_cover': 'percent_cover', 'invasives_plant_type': 'plant_type', 'invasive_type_other': 'other', 'created_date': 'created_date','Source':'Source1'}, inplace=True)\n",
    "usfs23df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'PLANT_COMM':'plant_type', 'DATE_COLLE':'created_date', 'Source':'Source2'}, inplace=True)\n",
    "\n",
    "\n",
    "# Concatenate DataFrames\n",
    "invasivedf = pd.concat([usfs23df, invasive19df, invasive20df, invasive22df, invasive23df], ignore_index=True)\n",
    "\n",
    "# Display merged DataFrame\n",
    "print(invasivedf)\n",
    "\n",
    "#Define SEZ ID based on Assessment_Unit_Name\n",
    "invasivedf['SEZ_ID'] = invasivedf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].astype(int)\n",
    "invasivedf['Assessment_Unit_Name'] = invasivedf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky Meadows meadow - 1': 'Sky meadows'})\n",
    "\n",
    "#calculate year column \n",
    "invasivedf['Year'] = invasivedf['created_date'].dt.year\n",
    "\n",
    "\n",
    "#---------------------------#\n",
    "    #Prep Data\n",
    "#---------------------------#\n",
    "# Replace 'other' or 'Other' in 'plant_type' column with values from 'other' column\n",
    "invasivedf['plant_type'] = invasivedf.apply(lambda row: row['other'] if pd.notna(row['plant_type']) and row['plant_type'].lower() in ['other', 'Other'] else row['plant_type'], axis=1)\n",
    "\n",
    "# Drop the 'other' column\n",
    "invasivedf.drop(columns=['other'], inplace=True)\n",
    "\n",
    "# Function to separate plant types and create new rows\n",
    "def separate_species(df):\n",
    "    # Split plant types by comma and create new rows\n",
    "    df['plant_type'] = df['plant_type'].str.split(',')\n",
    "    df = df.explode('plant_type')\n",
    "    return df\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "invasivedf = separate_species(invasivedf)\n",
    "\n",
    "# Remove rows where data source is USFS and plant type is NaN\n",
    "invasivedf = invasivedf[~((invasivedf['Source'] == 'USFS') & invasivedf['plant_type'].isna())]\n",
    "\n",
    "# Capitalize the first word and replace underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.split('_').str[0].str.capitalize() + ' ' + invasivedf['plant_type'].str.split('_').str[1:].str.join(' ')\n",
    "\n",
    "# Remove spaces after the words and capitalize the first word while replacing underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].astype(str)\n",
    "\n",
    "\n",
    "# Remove any spaces at the beginning or end of a word\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.strip()\n",
    "\n",
    "\n",
    "# Remove rows where data source is USFS and plant type is NaN\n",
    "invasivedf = invasivedf[~((invasivedf['Source'] == 'USFS') & invasivedf['plant_type'].isna())]\n",
    "#Remove Eradicated \n",
    "\n",
    "# Filter out rows where 'eradicated' column is 'Yes'\n",
    "invasivedf = invasivedf[invasivedf['Eradicated'] != 'Yes']\n",
    "\n",
    "#These replacements are so the same weed with different common name doesn't get counted twice toward score MAY NEED TO ADD MORE DESCREPENCIES in coming years if more pop up or find more elopquent solution\n",
    "# Replace 'Common mullein' with 'Wooly mullein' in the 'plant_type' column \n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common mullein', 'Wooly mullein')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Nodding plumeless thistle', 'Musk thistle')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Field bindweed', 'Common bindweed')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common st. johnswort', 'Klamathweed')\n",
    "# Replace 'Broadleaf and leaved' with 'Perennial' in the 'plant_type' column\n",
    "# Define the replacements dictionary\n",
    "replacements = {'Broadleaf Pepperweed': 'Perennial pepperweed', 'Broadleaved pepperweed': 'Perennial pepperweed'}\n",
    "\n",
    "# Replace values in the 'plant_type' column using the dictionary\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace(replacements)\n",
    "\n",
    "#Replace Sulphur Cinquefoil with sulfur\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sulphur cinquefoil', 'Sulfur cinquefoil')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sweetclover', 'White sweetclover')\n",
    "\n",
    "#Replace\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Salt cedar', 'Tamarisk')\n",
    "\n",
    "# Now, drop duplicates based on the specified subset of columns\n",
    "invasivedf = invasivedf.drop_duplicates(subset=['SEZ_ID', 'Year', 'plant_type'])\n",
    "\n",
    "# Reset index if needed\n",
    "invasivedf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Remove duplicates based on SEZ, Year, and plant_type\n",
    "invasivedf = invasivedf.drop_duplicates(subset=['SEZ_ID', 'Year', 'plant_type'])\n",
    "#----------------------------------------------------#\n",
    "#Create Look up dictionary to assign priority group for each plant and then use it\n",
    "#----------------------------------------------------#\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "csv_data = pd.read_csv(r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Invasives Priority lookup.csv\") \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "Invasives_lookup = {}\n",
    "\n",
    "key = 'Common'\n",
    "values = ['Scientific', 'Priority'] \n",
    "\n",
    "Invasives_lookup= csv_data.set_index(key)[values].to_dict(orient='index')\n",
    "\n",
    "# Define a custom function to map plant types to priorities\n",
    "def map_priority(plant_type):\n",
    "    if pd.isnull(plant_type):\n",
    "        return np.nan  # Return NaN for NaN values\n",
    "    else:\n",
    "        # Extract the priority from the dictionary, or return 'Unknown' if not found\n",
    "        plant_info = Invasives_lookup.get(plant_type)\n",
    "        if plant_info:\n",
    "            return plant_info['Priority']\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "\n",
    "#Create a new column [Scientific based on look up dictionary\n",
    "#invasivedf['Scientific']=invasivedf['plant_type'].map(map_priority)\n",
    "\n",
    "# Create a new column 'Priority' based on the mapping from the dictionary\n",
    "invasivedf['Priority'] = invasivedf['plant_type'].map(map_priority)\n",
    "\n",
    "#invasivedf['Priority'] = pd.to_numeric(invasivedf['Priority'], errors='coerce')\n",
    "# group by assessment unit and year and summarize the priority level of plants in each unit\n",
    "invasive_summary = invasivedf.groupby(['Assessment_Unit_Name', 'Year', 'Priority']).size().reset_index(name='Count')\n",
    "\n",
    "invasive_summary_priority2 = invasive_summary.pivot_table(index=['Assessment_Unit_Name', 'Year'], columns='Priority', values='Count', fill_value=0)\n",
    "\n",
    "# Reset the index to flatten the DataFrame\n",
    "invasive_summary_priority2.reset_index(inplace=True)\n",
    "\n",
    "#invasive_summary_priority['Source'] = invasivedf['Source']\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score, # of Headcuts\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "# Process Data\n",
    "def rate_invasive(priority):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if (priority[3] + priority[4] == 1):\n",
    "        return 'B'  # Assign score D\n",
    "    elif (priority[3] + priority[4] == 2) or priority[1] == 1 or priority[2] == 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif (priority[3] + priority[4] >= 3)or priority[1] >= 2 or priority[2] >= 2 or (priority[1] + priority[2] >= 2):  \n",
    "       return 'B'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no invasives of any priority are present)\n",
    "\n",
    "# Apply the rating function to the summary DataFrame\n",
    "invasive_summary_priority2['Invasives_Rating'] = invasive_summary_priority2.apply(rate_invasive, axis=1)\n",
    "\n",
    "#Calculate total number of invasives per sez per year\n",
    "invasive_summary_priority2['Number_of_Invasives']= invasive_summary_priority2[[1, 2, 3, 4,'Unknown']].sum(axis=1)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "invasive_summary_priority2['Invasives_Score']= invasive_summary_priority2['Invasives_Rating'].apply(score_indicator)    \n",
    "\n",
    "# make a columns in invasive summary that totals up percent cover per sez/year\n",
    "invasive_summary_priority2['Invasives_Percent_Cover'] = invasivedf.groupby(['SEZ_ID', 'Year'])['percent_cover'].sum().reset_index(drop=True)\n",
    "\n",
    "# combine the source column so that it shows all data sources that contributed to the data\n",
    "# Group invasivedf by 'SEZ_ID' and 'Year' and concatenate 'Data Source 1' values\n",
    "#data_source_1_combined = invasivedf.groupby(['SEZ_ID', 'Year'])['Source 1'].apply(lambda x: ', '.join(x.dropna())).reset_index()\n",
    "\n",
    "# Group invasivedf by 'SEZ_ID' and 'Year' and concatenate 'Data Source 2' values\n",
    "#data_source_2_combined = invasivedf.groupby(['SEZ_ID', 'Year'])['Source 2'].apply(lambda x: ', '.join(x.dropna())).reset_index()\n",
    "\n",
    "# Merge data_source_1_combined and data_source_2_combined on 'SEZ_ID' and 'Year'\n",
    "#merged_data_sources = pd.merge(data_source_1_combined, data_source_2_combined, on=['SEZ_ID', 'Year'], how='outer')\n",
    "\n",
    "# Combine 'Data Source 1' and 'Data Source 2' values with a comma separator\n",
    "#merged_data_sources['Data_Sources'] = merged_data_sources.apply(lambda row: ', '.join(filter(None, [row['Source 1'], row['Source 2']])), axis=1)\n",
    "\n",
    "# Drop the individual 'Data Source 1' and 'Data Source 2' columns\n",
    "#merged_data_sources.drop(columns=['Source 1', 'Source 2'], inplace=True)\n",
    "\n",
    "# Merge merged_data_sources with invasive_summary_priority on 'SEZ_ID' and 'Year'\n",
    "#invasive_summary_priority = pd.merge(invasive_summary_priority, merged_data_sources, on=['SEZ_ID', 'Year'], how='left')\n",
    "\n",
    "#invasive_summary_priority['Data_Sources']= invasivedfinvasivedf.groupby(['SED_ID', 'Year'])[Data Source 1 ] merge with DataSource 1 separate with comma if there are both \n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to incision called stage_incision GDB location\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Data_Sources': 'Invasives_Data_Source',\n",
    "    'Number_of_Invasives': 'Invasives_Number_of_Invasives',\n",
    "    'Invasives_Rating': 'Invasives_Rating',\n",
    "    'Invasives_Score': 'Invasives_Scores',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'percent_cover': 'Invasives_Percent_Cover',\n",
    "    'plant_type': 'Invasives_Plant_Type',\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "#readydf = invasive_summary_priority.rename(columns=field_mapping).drop(columns=[col for col in headcut_summary_sml.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "#print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "#data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "#field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(stage_headcuts, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data type of 'plant_type' column in the DataFrame\n",
    "print(\"Data type of 'plant_type' column in DataFrame:\", invasivedf['plant_type'].dtype)\n",
    "\n",
    "# Check the data type of values in the lookup dictionary\n",
    "for key, value in Invasives_lookup.items():\n",
    "    print(\"Data type of value for key\", key, \"in lookup dictionary:\", type(value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headcuts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sez_stream_headcuts doesn't capture the sez_survey data when there is 0!\n",
    "# Paths to the feature classes in GIS/GIS/DATA/Monitoring\n",
    "headcut19fc = os.path.join(headcut19gdb, \"Stream_Headcut_2019\")\n",
    "headcut20fc = os.path.join(headcut20gdb, \"Stream_Headcut_2020\")\n",
    "headcut22fc = os.path.join(headcut22gdb, \"Stream_Headcut_2022\")\n",
    "headcut23fc = os.path.join(headcut23gdb, \"sez_stream_headcut\")\n",
    "sez_surveyfc = os.path.join(sez_surveygdb, \"sez_survey\")\n",
    "\n",
    "headcut23fields = ['ParentGlobalID', 'headcut_depth']\n",
    "headcut22fields = ['Assessment_Unit', 'Headcut_Depth', 'synced_date']\n",
    "headcut20fields = ['Assessment_Unit_Name', 'Headcut_Depth','Survey_Date']\n",
    "headcut19fields = ['SITE_NAME', 'HEADCUT_DEPTH', 'SURVEY_DATE' ]\n",
    "sez_surveyfields = ['GlobalID', 'Assessment_Unit_Name', 'headcuts_number_of_headcuts', 'survey_date']\n",
    "\n",
    "# Function to read feature class into DataFrame\n",
    "def feature_class_to_dataframe(feature_class, fields):\n",
    "    data = [row for row in arcpy.da.SearchCursor(feature_class, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Read feature classes into DataFrames\n",
    "\n",
    "headcut19df = feature_class_to_dataframe(headcut19fc, headcut19fields)\n",
    "headcut20df = feature_class_to_dataframe(headcut20fc, headcut20fields)\n",
    "headcut22df = feature_class_to_dataframe(headcut22fc, headcut22fields)\n",
    "headcut23df = feature_class_to_dataframe(headcut23fc, headcut23fields)\n",
    "sez_surveyheadcutdf = feature_class_to_dataframe(sez_surveyfc, sez_surveyfields)\n",
    "\n",
    "#Join sez_survey and headcut23\n",
    "# Perform the join\n",
    "joined2023_df = headcut23df.merge(sez_surveyheadcutdf, left_on='ParentGlobalID', right_on='GlobalID', how='right')\n",
    "\n",
    "#joined2023_df.drop(columns=['Assessment_Unit'], inplace=True)\n",
    "\n",
    "# Rename fields\n",
    "headcut19df.rename(columns={'SITE_NAME': 'Assessment_Unit_Name', 'HEADCUT_DEPTH': 'headcut_depth', 'SURVEY_DATE': 'created_date'}, inplace=True)\n",
    "headcut20df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'Headcut_Depth': 'headcut_depth', 'Survey_Date': 'created_date'}, inplace=True)\n",
    "headcut22df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'Headcut_Depth': 'headcut_depth', 'synced_date': 'created_date'}, inplace=True)\n",
    "#joined2023_df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'headcut_depth': 'headcut_depth', 'created_date': 'created_date'}, inplace=True)\n",
    "joined2023_df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'survey_date': 'created_date', 'headcuts_number_of_headcuts': 'Count', 'headcut_depth': 'headcut_depth'}, inplace=True)\n",
    "# Concatenate DataFrames\n",
    "headcutdf = pd.concat([headcut19df, headcut20df, headcut22df, joined2023_df], ignore_index=True)\n",
    "\n",
    "#----------------------------------------------#\n",
    "# Process Data\n",
    "#----------------------------------------------#\n",
    "#calculate year column \n",
    "headcutdf['Year'] = headcutdf['created_date'].dt.year\n",
    "# assign small, medium, large to headcut\n",
    "headcutdf['Headcut_Size']=headcutdf['headcut_depth'].apply(categorize_headcut)\n",
    "\n",
    "\n",
    "# Group by 'SEZ_ID', 'Year', and 'Headcut_Size', and count the number of occurrences for each group\n",
    "headcut_summary = headcutdf.groupby(['Assessment_Unit_Name', 'Year', 'Headcut_Size']).size().reset_index(name='Count')\n",
    "\n",
    "#print(type(headcut_summary))\n",
    "\n",
    "#Attempt to add data from sez_survey that was not captured in sez_stream_headcut\n",
    "#allheadcutdata=pd.concat([headcut_summary, moreheadcut23df], ignore_index=True)\n",
    "\n",
    "headcut_summary_sml = headcut_summary.pivot_table(index=['Assessment_Unit_Name', 'Year'], columns='Headcut_Size', values='Count', fill_value=0)\n",
    "\n",
    "# Reset the index to flatten the DataFrame\n",
    "headcut_summary_sml.reset_index(inplace=True)\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score, # of Headcuts\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "# Apply the rating function to the summary DataFrame\n",
    "headcut_summary_sml['Headcuts_Rating'] = headcut_summary_sml.apply(rate_headcut, axis=1)\n",
    "\n",
    "#Calculate total number of headcuts per sez per year\n",
    "headcut_summary_sml['Number_of_Headcuts']= headcut_summary_sml[['large', 'medium', 'small']].sum(axis=1)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "headcut_summary_sml['Headcuts_Score']= headcut_summary_sml['Headcuts_Rating'].apply(score_indicator)\n",
    "\n",
    "#Add Datasource\n",
    "headcut_summary_sml['Headcuts_Data_Source'] = 'TRPA' #baseline condition assessment?'\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to incision called stage_incision GDB location\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Headcuts_Data_Source': 'Headcuts_Data_Source',\n",
    "    'Number_of_Headcuts': 'Number_of_Headcuts',\n",
    "    'Headcuts_Rating': 'Headcuts_Rating',\n",
    "    'Headcuts_Score': 'Headcuts_Score',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'small': 'small',\n",
    "    'medium': 'medium',\n",
    "    'large': 'large'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "#readydf = headcut_summary_sml.rename(columns=field_mapping).drop(columns=[col for col in headcut_summary_sml.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "#print(readydf)\n",
    "\n",
    "#readydf.to_csv(r\"C:\\Users\\snewsome\\Documents\\SEZ\\fullheadcutdata2023.csv\", index=False)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "#data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "#field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(stage_headcuts, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------#\n",
    "#Get Data from external sources and spatially join to our data\n",
    "#--------------------------------#\n",
    "\n",
    "\n",
    "#Calfora Data\n",
    "CArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines07/MapServer/1\"\n",
    "#CAfc = os.path.join(CArest, \"Non-Native and Invasive Plants - Calflora\")\n",
    "CAsdf = pd.DataFrame.spatial.from_layer(CArest)\n",
    "# Delete the existing feature layer if it exists\n",
    "\n",
    "CA_fc = arcpy.MakeFeatureLayer_management(CArest, \"Calfora_invasives\")\n",
    "\n",
    "\n",
    "usfsrest= \"https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer/0\"\n",
    "#usfsfc =os.path.join(usfsrest, \"Current Invasive Plant Locations\") \n",
    "#usfssdf = pd.DataFrame.spatial.from_layer(usfsrest)\n",
    "if arcpy.Exists(\"usfs_layer\"):\n",
    "    arcpy.Delete_management(\"usfs_layer\")\n",
    "usfs_fc = arcpy.MakeFeatureLayer_management(usfsrest, \"usfs_layer\")\n",
    "#Spatially join usfs layers to see if any of the points or polygons intersect with our sez's123\n",
    "\n",
    "# Set the workspace environment\n",
    "arcpy.env.workspace = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Data Management 2023.gdb\"\n",
    "\n",
    "# Define the target and join layers\n",
    "target_feature = SEZ_Master #\"AssessmentUnit_Master\"\n",
    "CAjoin_feature = CA_fc\n",
    "USFSjoin_feature = usfs_fc\n",
    "\n",
    "# Define the output feature class\n",
    "out_feature_class = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Data Management 2023.gdb\\externalinvasivesez\"\n",
    "\n",
    "# Define the fields you want to keep from both layers\n",
    "#field_mappings = arcpy.FieldMappings()\n",
    "#field_mappings.addTable(target_features)\n",
    "#field_mappings.addTable(join_features)\n",
    "\n",
    "# Keep only specific fields from the target layer\n",
    "#keep_fields_target = [\"Assessment_Unit_Name\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_target:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "#Keep only specific fields from the calfora join layer\n",
    "#keep_fields_CA = [\"CName\", \"SName\", \"Source\", \"Obs_Date\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_CA:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "# Keep only specific fields from the usfs join_features\n",
    "#keep_fields_USFS = [\"SCIENTIFIC_NAME\", \"COMMON_NAME\", \"DATE_COLLECTED\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_USFS:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "\n",
    "# Perform spatial join with specified field mappings\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=SEZ_Master,\n",
    "    join_features=[CAjoin_feature],\n",
    "    out_feature_class=out_feature_class,\n",
    "    join_operation=\"JOIN_ONE_TO_MANY\",\n",
    "    join_type=\"KEEP_ALL\"\n",
    "    #field_mapping=field_mappings\n",
    ")\n",
    "\n",
    "#Create Dataframe for spatially joined data\n",
    "# Define the fields you want to retrieve from the output feature class\n",
    "fields = [\"Assessment_Unit_Name\", \"CName\", \"SName\", \"Source\", \"Obs_Date\", \"SCIENTIFIC_NAME\", \"COMMON_NAME\", \"DATE_COLLECTED\"]\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "data = []\n",
    "\n",
    "# Use SearchCursor to read data from the output feature class\n",
    "with arcpy.da.SearchCursor(out_feature_class, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the list of tuples into a pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bioassessment/ Biotic Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bioassessment scores- get all stream data into sde.Stream first... then look at Biotic Integrit Data Source to find which stream was used to evaluate each meadow.. this will help with percent of stream miles I believe\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Get Data-- all the data we need is in sde.collect.sde.survey.sde.sez_survey\n",
    "#----------------------------------------------------------------##Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "\n",
    "#Create SEDF setup\n",
    "#streamdata is the path to the feature class in sde\n",
    "# Set the workspace to your SDE connection file\n",
    "arcpy.env.workspace = streamdata\n",
    "feature_class= \"Stream\"\n",
    "\n",
    "# Convert feature class to a pandas DataFrame\n",
    "fields = [field.name for field in arcpy.ListFields(feature_class)]\n",
    "\n",
    "# Create DataFrame\n",
    "streamsdf = pd.DataFrame.spatial.from_featureclass(feature_class, columns=fields)\n",
    "\n",
    "# Create the spatially enabled DataFrame (sdf) for target feature SEZ assessment units\n",
    "SEZsdf = pd.DataFrame.spatial.from_featureclass(SEZ_Master)\n",
    "#spatial reference stuff\n",
    "streamsdf.spatial.sr = SEZsdf.spatial.sr\n",
    "\n",
    "#perform spatial join of sde.stream and sez units\n",
    "thesdf = SEZsdf.spatial.join(streamsdf, how='inner')\n",
    "\n",
    "#Notes to self, Stream Miles?\n",
    "#Keep only Riverine?, this may be what the smaller polygons are for \n",
    "# Filter for SEZ type 'Riverine'\n",
    "#riverine_df = bioticsdf[bioticsdf['Feature_Type'] == 'Riverine']\n",
    "\n",
    "#if the layer contains Riverine? or just for any spatial join.. see what it does\n",
    "#spatial join this layer to asessment unit master layer\n",
    "#ASsessment unit master layer is called SEZ_Master\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------#\n",
    "#DATA PREP\n",
    "#----------------------#\n",
    "# List of columns to keep\n",
    "columns_to_keep = ['Assessment_Unit_Name', 'SEZ_Type', 'Feature_Type', 'SEZ_ID','SITE_NAME', 'COUNT_VALUE', 'YEAR_OF_COUNT', 'STATION_TYPE', 'LONGITUDE', 'LATITUDE', ]\n",
    "##Try this instead\n",
    "# Select only the desired columns\n",
    "bioticdf = thesdf.loc[:, columns_to_keep].copy()  \n",
    "\n",
    "#DATA PREP\n",
    "# Filter for years 2020 to 2023\n",
    "filtered_df = bioticdf.loc[(bioticdf['YEAR_OF_COUNT'] >= 2020) & (bioticdf['YEAR_OF_COUNT'] <= 2023)].copy()\n",
    "\n",
    "# Define SEZ ID based on Assessment_Unit_Name for QA on SEZ Name THIS METHOD USES LARGER POLYGONES \n",
    "filtered_df.loc[:, 'SEZ_ID'] = filtered_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "filtered_df.loc[:, 'SEZ_ID'] = filtered_df['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "filtered_df.loc[:, 'SEZ_ID'] = filtered_df['SEZ_ID'].astype(int)\n",
    "\n",
    "# Replace values in the 'Assessment_Unit_Name' column\n",
    "filtered_df.loc[:, 'Assessment_Unit_Name'] = filtered_df['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky Meadows meadow - 1': 'Sky meadows'})\n",
    "\n",
    "# Add data source information\n",
    "filtered_df['Source'] = 'TRPA, ' + filtered_df['SITE_NAME'].astype(str) + ', ' + filtered_df['YEAR_OF_COUNT'].astype(str)\n",
    "\n",
    "#Group by Year and Assessment Unit and Site NAME and remove duplicates\n",
    "\n",
    "filtered_df['SITE_NAME'] = filtered_df['SITE_NAME'].str.strip()\n",
    "filtered_df['YEAR_OF_COUNT'] = filtered_df['YEAR_OF_COUNT'].astype(str).str.strip().astype(int)\n",
    "filtered_df['YEAR_OF_COUNT'] = pd.to_numeric(filtered_df['YEAR_OF_COUNT'], errors='coerce')\n",
    "\n",
    "\n",
    "# Group by Assessment_Unit_Name, SITE_NAME, and YEAR_OF_COUNT and drop duplicates\n",
    "BIdf = filtered_df.groupby(['SEZ_ID', 'SITE_NAME', 'YEAR_OF_COUNT', 'COUNT_VALUE']).apply(lambda x: x.drop_duplicates()).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_df['YEAR_OF_COUNT'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------#\n",
    "#Grade and Score biotic integrity\n",
    "#----------------------#\n",
    "\n",
    "#Rate the score\n",
    "#ef categorize_csci(biotic_integrity):\n",
    "# Apply the rating function to the summary DataFrame\n",
    "BIdf['Biotic_Rating'] = BIdf['COUNT_VALUE'].apply(categorize_csci)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "BIdf['Biotic_Score']= BIdf['Biotic_Rating'].apply(score_indicator) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Prep and post ending dataframe to invasives table in SEZ_Data.GDB\n",
    "#----------------------------------------------------------------#\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'YEAR_OF_COUNT': 'Year',\n",
    "    'Source': 'Biotic_Integrity_Data_Source',\n",
    "    'COUNT_VALUE': 'Biotic_Integrity_CSCI',\n",
    "    'Biotic_Rating': 'Biotic_Integrity_Rating',\n",
    "    'Biotic_Score': 'Biotic_Integrity_Score',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = BIdf.rename(columns=field_mapping).drop(columns=[col for col in BIdf.columns if col not in field_mapping])\n",
    "\n",
    "readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_biotic_integrity, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")\n",
    "\n",
    "#Delete duplicates yourself.. not that much data to go through, can't figure out why it won't remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conifer Encroachment Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import arcpy\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Conifer_Encroachment_Data_Sourc',\n",
    "                        'Conifer_Encroachment_Rating',                    \n",
    "                        'Conifer_Encroachment_Percent_En',\n",
    "                        'Conifer_Encroachment_Score',\n",
    "                        'SEZ_ID',\n",
    "                        'Assessment_Unit_Name',\n",
    "                        'ConiferEncroachment_Comments']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    conifer_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "conifer_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Conifer_Encroachment_Data_Sourc': 'Conifer_Encroachment_Data_Source',\n",
    "                'Conifer_Encroachment_Rating': 'Conifer_Encroachment_Rating',                    \n",
    "                'Conifer_Encroachment_Percent_En': 'Conifer_Percent_Encroached',\n",
    "                'Conifer_Encroachment_Score': 'Conifer_Encroachment_Score',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'ConiferEncroachment_Comments': 'ConiferEncroachment_Comments'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = conifer_df.rename(columns=field_mapping).drop(columns=[col for col in conifer_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_conifer, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aquatic Organism Passage STagin table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SEZ_Master.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aquatic Organism /Fish Passage- only old data for now\n",
    "import os\n",
    "import pandas as pd\n",
    "import arcpy\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['AquaticOrganismPassage_Barriers',\n",
    "                        'AquaticOrganismPassage_DataSour',                    \n",
    "                        'AquaticOrganismPassage_NumberOf',\n",
    "                        'AquaticOrganismPassage_Rating',\n",
    "                        'SEZ_ID',\n",
    "                        'Assessment_Unit_Name',\n",
    "                        'AquaticOrganismPassage_Score',\n",
    "                        'AquaticOrganismPassage_StreamMi']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    AOP_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "AOP_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'AquaticOrganismPassage_Barriers': 'AOP_BarriersPerMile',\n",
    "                'AquaticOrganismPassage_DataSour': 'AOP_DataSource',                    \n",
    "                'AquaticOrganismPassage_NumberOf': 'AOP_NumberofBarriers',\n",
    "                'AquaticOrganismPassage_Score': 'AOP_Score',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'AquaticOrganismPassage_StreamMi': 'AOP_StreamMiles',\n",
    "                'AquaticOrganismPassage_Rating': 'AOP_Rating'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = AOP_df.rename(columns=field_mapping).drop(columns=[col for col in AOP_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_aquatic, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Habitat Fragmentation Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create STaging Tables for External Data Put in STagin Table\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Habitat_Fragmentation_Data_Sour',\n",
    "                        'Habitat_Fragmentation_Imperviou',\n",
    "                        'Habitat_Fragmentation_Percent_I',\n",
    "                        'Habitat_Fragmentation_Rating',\n",
    "                        'Habitat_Fragmentation_Score',\n",
    "                        'SEZ_ID', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    HabFrag_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "HabFrag_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Habitat_Fragmentation_Data_Sour': 'Habitat_Frag_Data_Source',\n",
    "                'Habitat_Fragmentation_Percent_I': 'HAbitat_Frag_Percent_Impervious',                    \n",
    "                'Habitat_Fragmentation_Score': 'Habitat_Frag_Score',\n",
    "                'Habitat_Fragmentation_Imperviou': 'Habitat_Frag_Impervious_Acres',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'Habitat_Fragmentation_Rating': 'Habitat_Frag_Rating'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = HabFrag_df.rename(columns=field_mapping).drop(columns=[col for col in HabFrag_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_habitat, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ditches Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create STaging Tables for External Data Put in STagin Table\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Ditches_Data_Source',\n",
    "                        'Ditches_Length',\n",
    "                        'Ditches_Meadow_Length',\n",
    "                        'Ditches_Percent',\n",
    "                        'Ditches_Rating',\n",
    "                        'Ditches_Score',\n",
    "                        'SEZ_ID', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    Ditch_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "Ditch_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Ditches_Data_Source': 'Ditches_Data_Source',\n",
    "                'Ditches_Length': 'Ditches_Length',                    \n",
    "                'Ditches_Meadow_Length': 'Ditches_Meadow_Length',\n",
    "                'Ditches_Percent': 'Ditches_Percent',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'Ditches_Rating': 'Ditches_Rating',\n",
    "                'Ditches_Score': 'Ditches_Score'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = Ditch_df.rename(columns=field_mapping).drop(columns=[col for col in Ditch_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_ditches, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vegetation Vigor- old data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create STaging Tables for External Data Put in STagin Table\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['VegetationVigor_DataSource',\n",
    "                        'NDVI_ID',\n",
    "                        'VegetationVigor_Raw',\n",
    "                        'VegetationVigor_Rating',\n",
    "                        'VegetationVigor_Score',\n",
    "                        'SEZ_ID', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    vegetation_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "vegetation_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'VegetationVigor_DataSource': 'VegetationVigor_DataSource',\n",
    "                'NDVI_ID': 'NDVI_ID',                    \n",
    "                'VegetationVigor_Raw': 'VegetationVigor_Raw',\n",
    "                'VegetationVigor_Rating': 'VegetationVigor_Rating',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'VegetationVigor_Score': 'VegetationVigor_Score'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = vegetation_df.rename(columns=field_mapping).drop(columns=[col for col in vegetation_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_vegetation, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEZ data from 2020 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All SEZ Scores from current Data\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Acres',\n",
    "                        'Final_Percent',\n",
    "                        'Final_Points_Possible',\n",
    "                        'Final_Rating',\n",
    "                        'Final_Total_Points',\n",
    "                        'SEZ_ID',\n",
    "                        'Comments',\n",
    "                        'SEZ_Type', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    SEZ20_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "SEZ20_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Acres': 'Acres',\n",
    "                'SEZ_Type': 'SEZ_Type',                    \n",
    "                'Final_Percent': 'Final_Percent',\n",
    "                'Final_Total_Points': 'Final_Total_Points',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'Final_Points_Possible': 'Final_Points_Possible',\n",
    "                'Final_Rating': 'Final_Rating',\n",
    "                'Comments': 'Comments'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = SEZ20_df.rename(columns=field_mapping).drop(columns=[col for col in SEZ20_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_All_SEZ_Scores, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final SEZ Scores Calculations for SEZ Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Data with REST SERVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Paths to Staging tables in SDE... via REST service\n",
    "# Use rest service to get data \n",
    "#Setup\n",
    "import pandas as pd\n",
    "import os\n",
    "from arcgis.features import FeatureLayer\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def get_fs_data(fs_url):\n",
    "    feature_layer = FeatureLayer(fs_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "def get_fs_data_spatial(fs_url):\n",
    "    feature_layer = FeatureLayer(fs_url)\n",
    "    query_result = feature_layer.query().sdf\n",
    "    query_result.spatial.sr = 26910\n",
    "    return query_result\n",
    "\n",
    "\n",
    "\n",
    "bank_stability_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/4\"\n",
    "biotic_integrity_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/5\"\n",
    "conifer_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/6\"\n",
    "ditches_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/7\"\n",
    "invasives_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/11\"\n",
    "Hab_Frag_url = 'https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/8'\n",
    "vegetation_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/12\"\n",
    "incision_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/10\"\n",
    "headcuts_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/9\"\n",
    "AOP_url= \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/3\"\n",
    "SEZ_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/0\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframes from Rest Services\n",
    "\n",
    "dfbanks = get_fs_data(bank_stability_url)\n",
    "dfbiotic = get_fs_data(biotic_integrity_url)\n",
    "dfconifer = get_fs_data(conifer_url)\n",
    "dfditch = get_fs_data(ditches_url)\n",
    "dfinvasive = get_fs_data(invasives_url)\n",
    "dfhabitat = get_fs_data(Hab_Frag_url)\n",
    "dfvegetation = get_fs_data(vegetation_url)\n",
    "dfincision = get_fs_data(incision_url)\n",
    "dfheadcuts = get_fs_data(headcuts_url)\n",
    "dfAOP = get_fs_data(AOP_url)\n",
    "dfSEZ = get_fs_data_spatial(SEZ_url)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data in staging tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Assessment_Unit_Name  Year  Biotic_Integrity_CSCI  \\\n",
      "0    Angora Creek - tributary  2013               0.996000   \n",
      "1        Angora Creek - upper  2013               0.996000   \n",
      "2          Angora meadows - 1  2019               0.690000   \n",
      "3          Angora meadows - 2  2019               0.820000   \n",
      "4          Angora meadows - 3  2017               0.940000   \n",
      "..                        ...   ...                    ...   \n",
      "226       Woods Creek - lower  2018               0.874000   \n",
      "227      Woods Creek - middle  2015               1.014000   \n",
      "228       Woods Creek - upper  2010               1.101000   \n",
      "229            small meadow 1  2018               1.010000   \n",
      "230           small meadow 57  2022               0.564053   \n",
      "\n",
      "    Biotic_Integrity_Data_Source Biotic_Integrity_Rating  \\\n",
      "0          TRPA, 634S13217, 2013                       A   \n",
      "1          TRPA, 634S13217, 2013                       A   \n",
      "2          TRPA, 634S19606, 2019                       C   \n",
      "3          TRPA, 634S19498, 2019                       B   \n",
      "4          TRPA, 634S17345, 2017                       A   \n",
      "..                           ...                     ...   \n",
      "226        TRPA, 634TPB112, 2018                       B   \n",
      "227        TRPA, 634S15348, 2015                       A   \n",
      "228        TRPA, 634S10092, 2010                       A   \n",
      "229        TRPA, 634REFNLH, 2018                       A   \n",
      "230         TRPA, 634HVC-1, 2022                       D   \n",
      "\n",
      "     Biotic_Integrity_Score  \n",
      "0                        12  \n",
      "1                        12  \n",
      "2                         6  \n",
      "3                         9  \n",
      "4                        12  \n",
      "..                      ...  \n",
      "226                       9  \n",
      "227                      12  \n",
      "228                      12  \n",
      "229                      12  \n",
      "230                       3  \n",
      "\n",
      "[231 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# #------------------#\n",
    "#Biotic Integrity\n",
    "#------------------#\n",
    "#Prep data- Add any scores and find average oif there are two stream sites for one sez. Also rename data source so it includes are streams that were averaged\n",
    "# Function to average scores and concatenate data sources for each Year and Assessment_Unit_Name\n",
    "def average_biotic_scores(dfbiotic, unit_col='Assessment_Unit_Name', year_col='Year', score='Biotic_Integrity_CSCI', source_col='Biotic_Integrity_Data_Source'):\n",
    "    # Group by Assessment Unit and Year\n",
    "    group = dfbiotic.groupby([unit_col, year_col])\n",
    "    \n",
    "    # Calculate the mean of the scores\n",
    "    averaged_scores = group[score].mean().reset_index()\n",
    "    \n",
    "    # Concatenate the data sources with specific formatting\n",
    "    def concatenate_sources(x, year):\n",
    "        formatted_sources = []\n",
    "        for entry in x:\n",
    "            parts = entry.split(\",\")\n",
    "            if len(parts) >= 3:\n",
    "                formatted_sources.append(f'TRPA, {parts[1].strip()}, {parts[-1].strip()}')  # Extract station code and year\n",
    "        if formatted_sources:\n",
    "            return '/ '.join(formatted_sources)\n",
    "        else:\n",
    "            return None  # Return None if all entries are invalid\n",
    "    \n",
    "    # Apply concatenate_sources to each group\n",
    "    concatenated_sources = group.apply(lambda grp: concatenate_sources(grp[source_col], grp[year_col])).reset_index(name=source_col)\n",
    "    \n",
    "    # Merge the averaged scores with concatenated sources\n",
    "    averaged_df = pd.merge(averaged_scores, concatenated_sources, on=[unit_col, year_col], how='left')\n",
    "    \n",
    "    return averaged_df\n",
    "\n",
    "\n",
    "# Drop duplicates based on 'Assessment_Unit_Name' and 'Year'\n",
    "dfbiotic = dfbiotic.drop_duplicates(subset=['Assessment_Unit_Name', 'Year', 'Biotic_Integrity_CSCI'])\n",
    "\n",
    "# Apply the function to dfbiotic\n",
    "averaged_biotic_df = average_biotic_scores(dfbiotic)\n",
    "\n",
    "# Apply the rating function to the averaged biotic integrity scores\n",
    "averaged_biotic_df['Biotic_Integrity_Rating'] = averaged_biotic_df['Biotic_Integrity_CSCI'].apply(categorize_csci)\n",
    "\n",
    "# Calculate the biotic score for each SEZ\n",
    "averaged_biotic_df['Biotic_Integrity_Score'] = averaged_biotic_df['Biotic_Integrity_Rating'].apply(score_indicator)\n",
    "\n",
    "averaged_biotic_df['Biotic_Integrity_Score']=averaged_biotic_df['Biotic_Integrity_Score'].astype(int)\n",
    "\n",
    "# Output the resulting DataFrame\n",
    "print(averaged_biotic_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     OBJECTID  SEZ_ID      Assessment_Unit_Name  Year  Number_of_Headcuts  \\\n",
      "0         449   519.0  Angora Creek - tributary  2020                   0   \n",
      "1         450   519.0  Angora Creek - tributary  2022                   0   \n",
      "2         451   519.0  Angora Creek - tributary  2023                   4   \n",
      "3         452   519.0  Angora Creek - tributary  2023                   0   \n",
      "4         453    87.0        Angora meadows - 1  2019                   0   \n",
      "..        ...     ...                       ...   ...                 ...   \n",
      "377      2631    44.0           small meadow 96  2023                   0   \n",
      "378      2632    62.0           small meadow 98  2019                   0   \n",
      "379      2633    75.0           small meadow 99  2020                   0   \n",
      "380      2634    75.0           small meadow 99  2022                   0   \n",
      "381      2635     NaN            unnamed meadow  2019                   0   \n",
      "\n",
      "    Headcuts_Data_Source  Headcuts_Score Headcuts_Rating  \\\n",
      "0                   TRPA            12.0               A   \n",
      "1                   TRPA            12.0               A   \n",
      "2                   TRPA             9.0               B   \n",
      "3                   TRPA            12.0               A   \n",
      "4                   TRPA            12.0               A   \n",
      "..                   ...             ...             ...   \n",
      "377                 TRPA            12.0               A   \n",
      "378                 TRPA            12.0               A   \n",
      "379                 TRPA            12.0               A   \n",
      "380                 TRPA            12.0               A   \n",
      "381                 TRPA            12.0               A   \n",
      "\n",
      "                                   GlobalID created_user   created_date  \\\n",
      "0    {B791D11F-A207-4BC2-BA77-6A76EAD19115}     SNEWSOME  1720715810000   \n",
      "1    {B21B47EF-207C-4AA5-A8E7-9AB5251E460F}     SNEWSOME  1720715810000   \n",
      "2    {98D76551-DBB4-467C-92B8-A6EADEC0CB24}     SNEWSOME  1720715810000   \n",
      "3    {7184D8C1-428A-479D-B08E-4CD4AFB8B7D0}     SNEWSOME  1720715810000   \n",
      "4    {EFBCE2A2-3F62-47A7-8CD3-6536E5CC18AA}     SNEWSOME  1720715810000   \n",
      "..                                      ...          ...            ...   \n",
      "377  {01430875-7EB8-44F1-9B8D-EE0E2711E682}     SNEWSOME  1720715810000   \n",
      "378  {1894CDC9-5519-4A84-8D4D-2B1BA17A7594}     SNEWSOME  1720715810000   \n",
      "379  {39EEB79D-A118-4E62-B3BB-3F9CF2A05277}     SNEWSOME  1720715810000   \n",
      "380  {A1A5D8F6-D4A5-4C23-9D20-A794F5CDC601}     SNEWSOME  1720715810000   \n",
      "381  {4C00C623-EF20-47E3-9B31-03A8BC96F80E}     SNEWSOME  1720715810000   \n",
      "\n",
      "    last_edited_user  last_edited_date  \n",
      "0           SNEWSOME     1720715810000  \n",
      "1           SNEWSOME     1720715810000  \n",
      "2           SNEWSOME     1720715810000  \n",
      "3           SNEWSOME     1720715810000  \n",
      "4           SNEWSOME     1720715810000  \n",
      "..               ...               ...  \n",
      "377         SNEWSOME     1720715810000  \n",
      "378         SNEWSOME     1720715810000  \n",
      "379         SNEWSOME     1720715810000  \n",
      "380         SNEWSOME     1720715810000  \n",
      "381         SNEWSOME     1720715810000  \n",
      "\n",
      "[382 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "#-------------------\n",
    "#Headcuts \n",
    "#------------------\n",
    "#Reorganize dfHeadcuts to drop small medium large headcut columns\n",
    "# Drop the columns 'small', 'medium', and 'large'\n",
    "dfheadcuts = dfheadcuts.drop(columns=['small', 'medium', 'large'])\n",
    "\n",
    "# Print the DataFrame to see the changes\n",
    "print(dfheadcuts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Prep SEZ Baseline Data for assessment unit...will need to rethink if acreage changes.. or just manually change in sde\n",
    "keep_columns = ['SHAPE', 'SEZ_ID', 'Feature_Type', 'SEZ_Type', 'Ownership_Primary', 'Ownership_Secondary', 'Ownership_Secondary_2', 'Ownership_Secondary_3', 'Acres', 'Comments']\n",
    "\n",
    "dfSEZinfo=dfSEZ.loc[:,keep_columns].copy()\n",
    "\n",
    "dfSEZinfo['SEZ_ID']= dfSEZinfo['SEZ_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add year to data source so we can drop the year column later (Dont double run this)\n",
    "\n",
    "#Create Dictionary of Dataframes to adjust year to be in datashource column and not its own column\n",
    "yeartodatasource = {\n",
    "    'dfbanks': dfbanks,\n",
    "    'dfheadcuts': dfheadcuts,\n",
    "    'dfincision': dfincision,\n",
    "    'dfinvasive': dfinvasive\n",
    "}\n",
    "\n",
    "# Iterate over each DataFrame in meadowdata\n",
    "for name, df in yeartodatasource.items():\n",
    "    # Iterate over columns in the DataFrame\n",
    "    for col in df.columns:\n",
    "        # Check if the column name contains 'Data'\n",
    "        if 'Data_' in col:\n",
    "            # Add Year to the column if it contains 'Data'\n",
    "            df[col] = df[col] + ', ' + df['Year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Large Polygon and Small Polygon Data frames called meadow and riverine for now so we can assign the correct SEZ_ID\n",
    "#Same for meadow(large polygon) and riverine(small polygon) data drop these columns because not needed in final merge, will assign SEZ ID later\n",
    "columns_to_drop = {'Year', 'SEZ_ID', 'GlobalID', 'last_edited_user', 'created_date', 'OBJECTID', 'created_user', 'last_edited_date'}\n",
    "\n",
    "#Name dataframes so we can reference later\n",
    "largepolygondata= {'dfbanks': dfbanks, \n",
    "             'dfaveraged_biotic':averaged_biotic_df,\n",
    "                'dfconifer': dfconifer,\n",
    "                'dfditch': dfditch,\n",
    "                'dfinvasive': dfinvasive,\n",
    "                'dfhabitat': dfhabitat,\n",
    "                'dfvegetation': dfvegetation,\n",
    "                'dfincision': dfincision,\n",
    "                'dfheadcuts': dfheadcuts,\n",
    "                'dfAOP': dfAOP\n",
    "}\n",
    "\n",
    "\n",
    "#Staging Tables Riverine/ small polygons\n",
    "smallpolygondata = {'dfbanks': dfbanks, \n",
    "                'dfaveraged_biotic':averaged_biotic_df,\n",
    "                'dfconifer': dfconifer,\n",
    "                'dfditch': dfditch,\n",
    "                'dfinvasive': dfinvasive,\n",
    "                'dfhabitat': dfhabitat,\n",
    "                'dfvegetation': dfvegetation,\n",
    "                'dfincision': dfincision,\n",
    "                'dfheadcuts': dfheadcuts,\n",
    "                'dfAOP': dfAOP\n",
    "}\n",
    "\n",
    "#Get most recent year of data for each Assessment Unit NAme\n",
    "# Function to get the most recent year of data\n",
    "# Function to get the most recent year of data\n",
    "def get_most_recent_scores(df, groupfield):\n",
    "    return df.loc[df.groupby(groupfield)['Year'].idxmax()]\n",
    "\n",
    "#most_recent_small = get_most_recent_scores(smallpolygondata, 'Assessment_Unit_Name')\n",
    "#mosrecent_large = get_most_recent_scores(largepolygondata, 'Assessment_Unit_Name')\n",
    "\n",
    "# Function to drop unnecessary columns from DataFrames\n",
    "def drop_columns(df, columns_to_drop):\n",
    "    return df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "\n",
    "# Function to assign SEZ_ID to each DataFrame using the provided lookup dictionary\n",
    "def assign_sez_ids(df, sezid_dict):\n",
    "    df['SEZ_ID'] = df['Assessment_Unit_Name'].map(sezid_dict)\n",
    "    df = df.dropna(subset=['SEZ_ID'])\n",
    "    \n",
    "    # Use .loc to modify SEZ_ID safely\n",
    "    df.loc[:, 'SEZ_ID'] = df['SEZ_ID'].astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process data for large and small polygons\n",
    "def process_data(data_dict, sezid_dict, columns_to_drop):\n",
    "    processed_data = {}\n",
    "    for key, df in data_dict.items():\n",
    "        # Step 1: Get most recent scores\n",
    "        df_most_recent = get_most_recent_scores(df, 'Assessment_Unit_Name')\n",
    "        \n",
    "        # Step 2: Drop unnecessary columns\n",
    "        df_cleaned = drop_columns(df_most_recent, columns_to_drop)\n",
    "        \n",
    "        # Step 3: Assign SEZ_ID\n",
    "        df_with_sez_id = assign_sez_ids(df_cleaned, sezid_dict)\n",
    "        \n",
    "        # Store the processed DataFrame\n",
    "        processed_data[key] = df_with_sez_id\n",
    "    return processed_data\n",
    "\n",
    "# Process large polygon (meadow) and small polygon (riverine) data\n",
    "processed_largepolygon_data = process_data(largepolygondata, lookup_dict, columns_to_drop)\n",
    "processed_smallpolygon_data = process_data(smallpolygondata, lookup_riverine, columns_to_drop)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Create Final Dataframe for all SEZ_ID's\n",
    "# Combine small polygon  and large polygon  DataFrames into a single DataFrame\n",
    "#combined_df = pd.concat([smallpolygon_df, largepolygon_df], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Combined DataFrame:\n",
      "             Assessment_Unit_Name Bank_Stability_Data_Source  \\\n",
      "0        Angora Creek - tributary                 TRPA, 2023   \n",
      "1              Angora meadows - 1                 TRPA, 2019   \n",
      "2              Angora meadows - 2                 TRPA, 2019   \n",
      "3              Angora meadows - 3                 TRPA, 2022   \n",
      "4              Angora meadows - 6                 TRPA, 2019   \n",
      "..                            ...                        ...   \n",
      "637   Secret Harbor Creek - lower                        NaN   \n",
      "638  Slaughterhouse Creek - upper                        NaN   \n",
      "639         Third Creek - upper 3                        NaN   \n",
      "640       Third Creek meadows - 3                        NaN   \n",
      "641               small meadow 92                        NaN   \n",
      "\n",
      "     Bank_Stability_Percent_Unstable Bank_Stability_Rating  \\\n",
      "0                           0.859340                     A   \n",
      "1                           0.000000                     A   \n",
      "2                           0.000000                     A   \n",
      "3                           5.327053                     B   \n",
      "4                           1.368514                     A   \n",
      "..                               ...                   ...   \n",
      "637                              NaN                   NaN   \n",
      "638                              NaN                   NaN   \n",
      "639                              NaN                   NaN   \n",
      "640                              NaN                   NaN   \n",
      "641                              NaN                   NaN   \n",
      "\n",
      "     Bank_Stability_Score  SEZ_ID  Biotic_Integrity_CSCI  \\\n",
      "0                    12.0   519.0               0.996000   \n",
      "1                    12.0    87.0               0.690000   \n",
      "2                    12.0    90.0               0.820000   \n",
      "3                     9.0    91.0               0.940000   \n",
      "4                    12.0   142.0               0.812284   \n",
      "..                    ...     ...                    ...   \n",
      "637                   NaN   329.0                    NaN   \n",
      "638                   NaN   333.0                    NaN   \n",
      "639                   NaN   344.0                    NaN   \n",
      "640                   NaN   386.0                    NaN   \n",
      "641                   NaN   384.0                    NaN   \n",
      "\n",
      "    Biotic_Integrity_Data_Source Biotic_Integrity_Rating  \\\n",
      "0          TRPA, 634S13217, 2013                       A   \n",
      "1          TRPA, 634S19606, 2019                       C   \n",
      "2          TRPA, 634S19498, 2019                       B   \n",
      "3          TRPA, 634S17345, 2017                       A   \n",
      "4          TRPA, 634TPB153, 2020                       B   \n",
      "..                           ...                     ...   \n",
      "637                          NaN                     NaN   \n",
      "638                          NaN                     NaN   \n",
      "639                          NaN                     NaN   \n",
      "640                          NaN                     NaN   \n",
      "641                          NaN                     NaN   \n",
      "\n",
      "     Biotic_Integrity_Score  ... Number_of_Headcuts  Headcuts_Data_Source  \\\n",
      "0                      12.0  ...                4.0            TRPA, 2023   \n",
      "1                       6.0  ...                0.0            TRPA, 2019   \n",
      "2                       9.0  ...                0.0            TRPA, 2022   \n",
      "3                      12.0  ...                0.0            TRPA, 2019   \n",
      "4                       9.0  ...                NaN                   NaN   \n",
      "..                      ...  ...                ...                   ...   \n",
      "637                     NaN  ...                NaN                   NaN   \n",
      "638                     NaN  ...                NaN                   NaN   \n",
      "639                     NaN  ...                NaN                   NaN   \n",
      "640                     NaN  ...                0.0            TRPA, 2019   \n",
      "641                     NaN  ...                0.0            TRPA, 2020   \n",
      "\n",
      "    Headcuts_Score  Headcuts_Rating AOP_BarriersPerMile  \\\n",
      "0              9.0                B            0.000000   \n",
      "1             12.0                A            0.000000   \n",
      "2             12.0                A            0.000000   \n",
      "3             12.0                A            0.000000   \n",
      "4              NaN              NaN            0.000000   \n",
      "..             ...              ...                 ...   \n",
      "637            NaN              NaN            1.408451   \n",
      "638            NaN              NaN            0.000000   \n",
      "639            NaN              NaN            0.000000   \n",
      "640           12.0                A            0.000000   \n",
      "641           12.0                A            1.560000   \n",
      "\n",
      "             AOP_DataSource  AOP_NumberofBarriers  AOP_Rating  AOP_Score  \\\n",
      "0    USFS / TRPA 2009, 2019                   0.0           A       12.0   \n",
      "1    USFS / TRPA 2009, 2019                   0.0           A       12.0   \n",
      "2    USFS / TRPA 2009, 2019                   0.0           A       12.0   \n",
      "3    USFS / TRPA 2009, 2019                   0.0           A       12.0   \n",
      "4    USFS / TRPA 2009, 2019                   0.0           A       12.0   \n",
      "..                      ...                   ...         ...        ...   \n",
      "637  USFS / TRPA 2009, 2019                   1.0           C        6.0   \n",
      "638  USFS / TRPA 2009, 2019                   0.0           A       12.0   \n",
      "639  USFS / TRPA 2009, 2019                   0.0           A       12.0   \n",
      "640  USFS / TRPA 2009, 2019                   0.0           A       12.0   \n",
      "641  USFS / TRPA 2009, 2019                   1.0           C        6.0   \n",
      "\n",
      "    AOP_StreamMiles  \n",
      "0               NaN  \n",
      "1               NaN  \n",
      "2               NaN  \n",
      "3               NaN  \n",
      "4               NaN  \n",
      "..              ...  \n",
      "637            0.71  \n",
      "638             NaN  \n",
      "639             NaN  \n",
      "640             NaN  \n",
      "641             NaN  \n",
      "\n",
      "[642 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to merge all DataFrames on multiple keys\n",
    "def merge_dataframes(data_dict, keys):\n",
    "    return reduce(lambda left, right: pd.merge(left, right, on=keys, how='outer'), data_dict.values())\n",
    "\n",
    "# Merge small polygon DataFrames\n",
    "smallpolygon_df = merge_dataframes(processed_smallpolygon_data, ['SEZ_ID', 'Assessment_Unit_Name'])\n",
    "\n",
    "# Merge large polygon DataFrames\n",
    "largepolygon_df = merge_dataframes(processed_largepolygon_data, ['SEZ_ID', 'Assessment_Unit_Name'])\n",
    "\n",
    "# Append smallpolygon_df to largepolygon_df\n",
    "final_combined_df = pd.concat([largepolygon_df, smallpolygon_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Print the final combined DataFrame to check\n",
    "print(\"Final Combined DataFrame:\")\n",
    "print(final_combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Assessment_Unit_Name Bank_Stability_Data_Source  \\\n",
      "0        Angora Creek - tributary                 TRPA, 2023   \n",
      "1              Angora meadows - 1                 TRPA, 2019   \n",
      "2              Angora meadows - 2                 TRPA, 2019   \n",
      "3              Angora meadows - 3                 TRPA, 2022   \n",
      "4              Angora meadows - 6                 TRPA, 2019   \n",
      "..                            ...                        ...   \n",
      "636   Secret Harbor Creek - lower                        NaN   \n",
      "637  Slaughterhouse Creek - upper                        NaN   \n",
      "638         Third Creek - upper 3                        NaN   \n",
      "639       Third Creek meadows - 3                        NaN   \n",
      "640               small meadow 92                        NaN   \n",
      "\n",
      "     Bank_Stability_Percent_Unstable Bank_Stability_Rating  \\\n",
      "0                           0.859340                     A   \n",
      "1                           0.000000                     A   \n",
      "2                           0.000000                     A   \n",
      "3                           5.327053                     B   \n",
      "4                           1.368514                     A   \n",
      "..                               ...                   ...   \n",
      "636                              NaN                   NaN   \n",
      "637                              NaN                   NaN   \n",
      "638                              NaN                   NaN   \n",
      "639                              NaN                   NaN   \n",
      "640                              NaN                   NaN   \n",
      "\n",
      "     Bank_Stability_Score  SEZ_ID  Biotic_Integrity_CSCI  \\\n",
      "0                    12.0   519.0               0.996000   \n",
      "1                    12.0    87.0               0.690000   \n",
      "2                    12.0    90.0               0.820000   \n",
      "3                     9.0    91.0               0.940000   \n",
      "4                    12.0   142.0               0.812284   \n",
      "..                    ...     ...                    ...   \n",
      "636                   NaN   329.0                    NaN   \n",
      "637                   NaN   333.0                    NaN   \n",
      "638                   NaN   344.0                    NaN   \n",
      "639                   NaN   386.0                    NaN   \n",
      "640                   NaN   384.0                    NaN   \n",
      "\n",
      "    Biotic_Integrity_Data_Source Biotic_Integrity_Rating  \\\n",
      "0          TRPA, 634S13217, 2013                       A   \n",
      "1          TRPA, 634S19606, 2019                       C   \n",
      "2          TRPA, 634S19498, 2019                       B   \n",
      "3          TRPA, 634S17345, 2017                       A   \n",
      "4          TRPA, 634TPB153, 2020                       B   \n",
      "..                           ...                     ...   \n",
      "636                          NaN                     NaN   \n",
      "637                          NaN                     NaN   \n",
      "638                          NaN                     NaN   \n",
      "639                          NaN                     NaN   \n",
      "640                          NaN                     NaN   \n",
      "\n",
      "     Biotic_Integrity_Score  ...  \\\n",
      "0                      12.0  ...   \n",
      "1                       6.0  ...   \n",
      "2                       9.0  ...   \n",
      "3                      12.0  ...   \n",
      "4                       9.0  ...   \n",
      "..                      ...  ...   \n",
      "636                     NaN  ...   \n",
      "637                     NaN  ...   \n",
      "638                     NaN  ...   \n",
      "639                     NaN  ...   \n",
      "640                     NaN  ...   \n",
      "\n",
      "                                                 SHAPE  Feature_Type  \\\n",
      "0    {\"rings\": [[[756244.4929999998, 4306482.355], ...      Riverine   \n",
      "1    {\"rings\": [[[758658.7938000001, 4307035.7872],...        Meadow   \n",
      "2    {\"rings\": [[[757832.7397999996, 4307609.7169],...        Meadow   \n",
      "3    {\"rings\": [[[757531.2456999999, 4307388.3127],...        Meadow   \n",
      "4    {\"rings\": [[[757257.6279999996, 4307817.941600...        Meadow   \n",
      "..                                                 ...           ...   \n",
      "636  {\"rings\": [[[765189.9900000002, 4337750.09], [...      Riverine   \n",
      "637  {\"rings\": [[[767541.5099999998, 4336220.59], [...      Riverine   \n",
      "638  {\"rings\": [[[764407.0700000003, 4353039.109999...      Riverine   \n",
      "639  {\"rings\": [[[764355.1425000001, 4356962.147399...      Riverine   \n",
      "640  {\"rings\": [[[768161.1299999999, 4337332.41], [...      Riverine   \n",
      "\n",
      "                            SEZ_Type    Ownership_Primary Ownership_Secondary  \\\n",
      "0    Riverine (Perennial) + Forested                 USFS                <NA>   \n",
      "1                   Channeled Meadow  State of California                USFS   \n",
      "2                   Channeled Meadow  State of California             private   \n",
      "3                   Channeled Meadow  State of California             private   \n",
      "4                   Channeled Meadow                 USFS               local   \n",
      "..                               ...                  ...                 ...   \n",
      "636             Riverine (Perennial)                 USFS                <NA>   \n",
      "637             Riverine (Perennial)      State of Nevada                <NA>   \n",
      "638             Riverine (Perennial)                 USFS                <NA>   \n",
      "639             Riverine (Perennial)                 USFS                <NA>   \n",
      "640             Riverine (Perennial)      State of Nevada                USFS   \n",
      "\n",
      "    Ownership_Secondary_2  Ownership_Secondary_3      Acres  \\\n",
      "0                    <NA>                   <NA>  71.030026   \n",
      "1                 private                   <NA>  23.009646   \n",
      "2                    <NA>                   <NA>  51.464984   \n",
      "3                    USFS                   <NA>   36.74867   \n",
      "4                 private                   <NA>  16.357205   \n",
      "..                    ...                    ...        ...   \n",
      "636                  <NA>                   <NA>   0.916305   \n",
      "637                  <NA>                   <NA>   1.567064   \n",
      "638                  <NA>                   <NA>   0.676143   \n",
      "639                  <NA>                   <NA>    0.19251   \n",
      "640                  <NA>                   <NA>   0.891974   \n",
      "\n",
      "                                              Comments Threshold_Year  \n",
      "0    Mostly stable and healthy after restoration pr...           2023  \n",
      "1    No indications of degradation. Restoration pro...           2023  \n",
      "2    No indications of degradation. Restoration pro...           2023  \n",
      "3    Lots of former meadow developed with ditching ...           2023  \n",
      "4    No indications of degradation. Restoration pro...           2023  \n",
      "..                                                 ...            ...  \n",
      "636                     No indications of degradation.           2023  \n",
      "637                                      Not assessed.           2023  \n",
      "638                     No indications of degradation.           2023  \n",
      "639                     No indications of degradation.           2023  \n",
      "640                     No indications of degradation.           2023  \n",
      "\n",
      "[641 rows x 61 columns]\n"
     ]
    }
   ],
   "source": [
    "# Join SEZinfo to combined df to get a dataframe with more info about each Assessment Unit\n",
    "\n",
    "# Join SEZinfo to the combined_df using SEZ_ID\n",
    "final_df = pd.merge(final_combined_df, dfSEZinfo, on='SEZ_ID')\n",
    "\n",
    "#Assign Threshold Calculations which is the Threshold Year--> is just the most recent data within the past 4 years\n",
    "final_df['Threshold_Year'] = '2023'\n",
    "\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't USE This is for All SEZ Scores Only no othe rdata.. DONT USE unless need all _sez_Score table with just scores of Indicator and SEZ no floof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT USECreate RiverineIndicators list containing specific dataframes\n",
    "RiverineIndicators = ['Assessment_Unit_Name', 'AOP_Score', 'Bank_Stability_Score', 'Biotic_Integrity_Score', 'Habitat_Frag_Score', 'Incision_Score', 'Headcuts_Score']\n",
    "\n",
    "mergedriverine_df= mergedmeadow_df[RiverineIndicators]\n",
    "\n",
    "print(mergedriverine_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT USECalculate Scores for Each SEZ This will go into table All_SEZ_Scores\n",
    "\n",
    "#FOR MEADOWS\n",
    "\n",
    "\n",
    "#mergedmeadow_df['SEZ_ID']=mergedmeadow_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "# Identify columns that contain 'score' in their names\n",
    "score_columns = [col for col in mergedmeadow_df.columns if 'Score' in col]\n",
    "\n",
    "# Sum the values across identified score columns to get the final points for the SEZ\n",
    "mergedmeadow_df['Final_Total_Points'] = mergedmeadow_df[score_columns].sum(axis=1, skipna=True)\n",
    "\n",
    "#POints Possible per SEZ\n",
    "\n",
    "mergedmeadow_df['Final_Points_Possible'] = mergedmeadow_df[score_columns].notna().sum(axis=1)*12\n",
    "\n",
    "\n",
    "#ASsign Meadow SEZ_ID to each Assessment_Unit\n",
    "\n",
    "mergedmeadow_df['SEZ_ID']=mergedmeadow_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "#FOR QA purposes add SEZ_type so we can make sure lal data is available\n",
    "mergedmeadow_df['SEZ_Type'] = dfSEZ['SEZ_Type']\n",
    "#Would it be better to just add up how many indicators have a score? 12 x5?\n",
    "#mergedmeadow_df['Final_Points_Possible']= dfSEZ['Final_Points_Possible']\n",
    "\n",
    "# Merge based on 'SEZID'\n",
    "#mergedmeadow_df = pd.merge(mergedmeadow_df, dfSEZ[['SEZ_ID', 'Final_Points_Possible']], on='SEZ_ID', how='left')\n",
    "\n",
    "# Assign 'Final_Points_Possible' from dfSEZ to mergedmeadow_df\n",
    "#mergedmeadow_df['Final_Points_Possible'] = mergedmeadow_df['Final_Points_Possible']\n",
    "#or? just base of of how many indicators are not null for each row?\n",
    "#mergedmeadow_df['Final_Points_Possible2']= (12 x number of columns per row that say score have data?) \n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------\n",
    "# Calculate the final rating\n",
    "#------------------------------------\n",
    "mergedmeadow_df['Final_Percent'] = mergedmeadow_df['Final_Total_Points'] / mergedmeadow_df['Final_Points_Possible']\n",
    "\n",
    "mergedmeadow_df['Final_Rating']= mergedmeadow_df['Final_Percent'].apply(rate_SEZ)\n",
    "\n",
    "mergedmeadow_df['Final_Score']= mergedmeadow_df['Final_Rating'].apply(score_indicator)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT USECalculate Scores for Each SEZ This will go into table All_SEZ_Scores\n",
    "\n",
    "#FOR RIVERINE SEZ\n",
    "# Identify columns that contain 'score' in their names\n",
    "score_columns = [col for col in mergedriverine_df.columns if 'Score' in col]\n",
    "\n",
    "# Sum the values across identified score columns to get the final points for the SEZ\n",
    "mergedriverine_df['Final_Total_Points'] = mergedriverine_df[score_columns].sum(axis=1, skipna=True)\n",
    "\n",
    "#POints Possible per SEZ\n",
    "\n",
    "mergedriverine_df['Final_Points_Possible'] = mergedriverine_df[score_columns].notna().sum(axis=1)*12\n",
    "\n",
    "\n",
    "#ASsigndriverine SEZ_ID to each Assessment_Unit\n",
    "\n",
    "mergedriverine_df['SEZ_ID']=mergedriverine_df['Assessment_Unit_Name'].map(lookup_riverine)\n",
    "\n",
    "#FOR QA purposes add SEZ_type so we can make sure lal data is available\n",
    "mergedriverine_df['SEZ_Type'] = dfSEZ['SEZ_Type']\n",
    "\n",
    "#--------------------------------------\n",
    "# Calculate the final rating\n",
    "#------------------------------------\n",
    "mergedriverine_df['Final_Percent'] = mergedriverine_df['Final_Total_Points'] / mergedriverine_df['Final_Points_Possible']\n",
    "\n",
    "mergedriverine_df['Final_Rating']= mergedriverine_df['Final_Percent'].apply(rate_SEZ)\n",
    "\n",
    "mergedriverine_df['Final_Score']= mergedriverine_df['Final_Rating'].apply(score_indicator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT USEMerge Riverine and Meadow dataframes for a final dataframe\n",
    "\n",
    "# Concatenate DataFrames doesn't work-try merging--Do we want to make an All_sez_scores\n",
    "both_df = pd.concat([mergedriverine_df, mergedmeadow_df], axis=0, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTS WORKINNG... WILL NEED THIS for FINAL TABLEsezsurveytable grab comments to add to dataframe--MAYBE JUST DO THIS MANUALLY\n",
    "#Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "sezsurveyfields = ['Assessment_Unit_Name', 'Comments', 'Survey_Date']\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "# Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(sezsurveytable, sezsurveyfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "sezsurvey_df= pd.DataFrame(data, columns=sezsurveyfields)\n",
    "\n",
    "sezsurvey_df['Year'] = sezsurvey_df['Survey_Date'].dt.year\n",
    "print(sezsurvey_df)\n",
    "#add comments to dataframe\n",
    "# Create a dictionary to map Assessment_Unit_Name to Comments\n",
    "comments_map = dict(zip(sezsurvey_df['Assessment_Unit_Name'], sezsurvey_df['Comments']))\n",
    "\n",
    "# Add the Comments column to both_df using the map function\n",
    "both_df['Comments'] = both_df['Assessment_Unit_Name'].map(comments_map)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#DONT USE\n",
    "# both_df['Year']= '2024'\n",
    "\n",
    "both_df = both_df.dropna(subset='SEZ_ID')\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Prep and post ending dataframe to All Scores table in SEZ_Data.GDB\n",
    "#----------------------------------------------------------------#\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Comments': 'Comments',\n",
    "    'SEZ_Type': 'SEZ_Type',\n",
    "    'Final_Rating': 'Final_Rating',\n",
    "    'Final_Percent': 'Final_Percent',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'Final_Points_Possible': 'Final_Points_Possible',\n",
    "    'Final_Total_Points': 'Final_Total_Points'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = both_df.rename(columns=field_mapping).drop(columns=[col for col in both_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Assuming ready_df is the DataFrame you want to write to CSV\n",
    "readydf.to_csv('ready_df.csv', index=False)\n",
    "\n",
    "allscoresdf=readydf\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "#data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "#field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(allsezscores, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDE.SEZ Assessment_Unit final table with all info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DONT USEProblem with Riverine Data and this code--August 1, 2024\n",
    "# #Clean Data, Assign SEZ's based on polygon size, get most recent\n",
    "#Take df's and create a riverine and meadow df using small polygon size and large polygon size\n",
    "\n",
    "\n",
    "# Function to Get most recent year of data from each DataFrame\n",
    "def get_most_recent_scores(df, groupfield):\n",
    "    df = df.loc[df.groupby(groupfield)['Year'].idxmax()]\n",
    "    return df\n",
    "\n",
    "# Function to merge DataFrames and assign SEZ IDs\n",
    "def merge_and_assign_sez_ids(cleaned_data, lookup_dict):\n",
    "    # Merge all DataFrames on 'Assessment_Unit_Name' using reduce\n",
    "    merged_df = reduce(lambda left, right: pd.merge(left, right, on='Assessment_Unit_Name', how='outer'), cleaned_data.values())\n",
    "    \n",
    "    # Assign SEZ_ID based on 'Assessment_Unit_Name' using lookup_dict\n",
    "    merged_df['SEZ_ID'] = merged_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "    \n",
    "    # Drop rows with missing SEZ_ID\n",
    "    merged_df = merged_df.dropna(subset=['SEZ_ID'])\n",
    "    \n",
    "    merged_df['SEZ_ID'] = merged_df['SEZ_ID'].astype(int)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Main function to process data frames, get most recent data, clean, and merge\n",
    "def get_most_recent_and_clean(meadowdata, riverinedata, lookup_dict, lookup_riverine, columns_to_drop):\n",
    "    most_recent_data_meadow = {}\n",
    "    cleaned_data_meadow = {}\n",
    "    \n",
    "    most_recent_data_riverine = {}\n",
    "    cleaned_data_riverine = {}\n",
    "    \n",
    "    # Process meadowdata\n",
    "    for key, df in meadowdata.items():\n",
    "        processed_df = get_most_recent_scores(df, 'Assessment_Unit_Name')\n",
    "        most_recent_data_meadow[key] = processed_df\n",
    "        \n",
    "        # Drop specified columns and remove duplicate rows\n",
    "        df_cleaned = processed_df.drop(columns=[col for col in columns_to_drop if col in processed_df.columns])\n",
    "        df_cleaned = df_cleaned.drop_duplicates()\n",
    "        cleaned_data_meadow[key] = df_cleaned\n",
    "    \n",
    "    # Process riverinedata\n",
    "    for key, df in riverinedata.items():\n",
    "        processed_df = get_most_recent_scores(df, 'Assessment_Unit_Name')\n",
    "        most_recent_data_riverine[key] = processed_df\n",
    "        \n",
    "        # Drop specified columns and remove duplicate rows\n",
    "        df_cleaned = processed_df.drop(columns=[col for col in columns_to_drop if col in processed_df.columns])\n",
    "        df_cleaned = df_cleaned.drop_duplicates()\n",
    "        cleaned_data_riverine[key] = df_cleaned\n",
    "    \n",
    "    # Merge and assign SEZ IDs for meadowdata\n",
    "    merged_meadow_df = merge_and_assign_sez_ids(cleaned_data_meadow, lookup_dict)\n",
    "    \n",
    "    # Merge and assign SEZ IDs for riverinedata\n",
    "    merged_riverine_df = merge_and_assign_sez_ids(cleaned_data_riverine, lookup_riverine)\n",
    "    \n",
    "    # Combine merged_meadow_df and merged_riverine_df\n",
    "    combined_df = pd.concat([merged_meadow_df, merged_riverine_df], axis=0, join='outer')\n",
    "\n",
    "    return {\n",
    "        'merged_meadow_df': merged_meadow_df,\n",
    "        'merged_riverine_df': merged_riverine_df,\n",
    "        'combined_df': combined_df,\n",
    "        'cleaned_data_meadow': cleaned_data_meadow,\n",
    "        'cleaned_data_riverine': cleaned_data_riverine,\n",
    "        'most_recent_data_meadow': most_recent_data_meadow,\n",
    "        'most_recent_data_riverine': most_recent_data_riverine\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DONT USESame for meadow(large polygon) and riverine(small polygon) data drop these columns because not needed in final merge, will assign SEZ ID later\n",
    "columns_to_drop = {'Year', 'SEZ_ID', 'GlobalID', 'last_edited_user', 'created_date', 'OBJECTID', 'created_user', 'last_edited_date'}\n",
    "\n",
    "#Name dataframes so we can reference later meadow data is large polygone\n",
    "meadowdata= {'dfbanks': dfbanks, \n",
    "             'dfaveraged_biotic':averaged_biotic_df,\n",
    "                'dfconifer': dfconifer,\n",
    "                'dfditch': dfditch,\n",
    "                'dfinvasive': dfinvasive,\n",
    "                'dfhabitat': dfhabitat,\n",
    "                'dfvegetation': dfvegetation,\n",
    "                'dfincision': dfincision,\n",
    "                'dfheadcuts': dfheadcuts,\n",
    "                'dfAOP': dfAOP\n",
    "}\n",
    "\n",
    "\n",
    "#Staging Tables Riverine/ small polygons\n",
    "riverinedata = {'dfheadcuts': dfbanks,\n",
    "                'dfbiotic': averaged_biotic_df,\n",
    "                'dfhabitat': dfhabitat,\n",
    "                'dfincision': dfincision,\n",
    "                'dfheadcuts': dfheadcuts,\n",
    "                'dfAOP': dfAOP\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DONT USE Call the function with all the required arguments\n",
    "result = get_most_recent_and_clean(meadowdata, riverinedata, lookup_dict, lookup_riverine, columns_to_drop)\n",
    "\n",
    "# Access the results\n",
    "merged_meadow_df = result['merged_meadow_df']\n",
    "merged_riverine_df = result['merged_riverine_df']\n",
    "combined_df=result['combined_df']\n",
    "cleaned_data_meadow = result['cleaned_data_meadow']\n",
    "cleaned_data_riverine = result['cleaned_data_riverine']\n",
    "most_recent_data_meadow = result['most_recent_data_meadow']\n",
    "most_recent_data_riverine = result['most_recent_data_riverine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join SEZinfo to combined df to get a dataframe with more info about each Assessment Unit\n",
    "\n",
    "# Join SEZinfo to the combined_df using SEZ_ID\n",
    "final_df = pd.merge(final_combined_df, dfSEZinfo, on='SEZ_ID')\n",
    "\n",
    "#Assign Threshold Calculations which is the Threshold Year--> is just the most recent data within the past 4 years\n",
    "final_df['Threshold_Year'] = '2023'\n",
    "\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SEZ_ID  AOP_BarriersPerMile          AOP_DataSource  \\\n",
      "0       1.0                  0.0  USFS / TRPA 2009, 2019   \n",
      "1       2.0                  0.0  USFS / TRPA 2009, 2019   \n",
      "2       3.0                 <NA>  USFS / TRPA 2009, 2019   \n",
      "3       4.0                 <NA>  USFS / TRPA 2009, 2019   \n",
      "4       6.0                 <NA>  USFS / TRPA 2009, 2019   \n",
      "..      ...                  ...                     ...   \n",
      "636   639.0                 <NA>  USFS / TRPA 2009, 2019   \n",
      "637   640.0                  0.0  USFS / TRPA 2009, 2019   \n",
      "638   641.0                  0.0  USFS / TRPA 2009, 2019   \n",
      "639   642.0                 10.0  USFS / TRPA 2009, 2019   \n",
      "640   643.0                 10.0  USFS / TRPA 2009, 2019   \n",
      "\n",
      "     AOP_NumberofBarriers AOP_Rating  AOP_Score  AOP_StreamMiles      Acres  \\\n",
      "0                     0.0          A       12.0             <NA>  16.661797   \n",
      "1                     0.0          A       12.0             <NA>  12.805647   \n",
      "2                    <NA>       <NA>       <NA>             <NA>   4.919027   \n",
      "3                    <NA>       <NA>       <NA>             <NA>  35.718239   \n",
      "4                    <NA>       <NA>       <NA>             <NA>  21.172427   \n",
      "..                    ...        ...        ...              ...        ...   \n",
      "636                  <NA>       <NA>       <NA>             <NA>   0.566975   \n",
      "637                   0.0          A       12.0             <NA>   1.350239   \n",
      "638                   0.0          A       12.0             <NA>  20.885109   \n",
      "639                   2.0          D        3.0              0.2   2.533502   \n",
      "640                   2.0          D        3.0              0.2   0.268518   \n",
      "\n",
      "                             Assessment_Unit_Name  \\\n",
      "0      Saxon Creek meadows - below Fountain Place   \n",
      "1    Saxon Creek meadows - above Fountain Place 1   \n",
      "2                         Burke Creek meadows - 2   \n",
      "3                   Washoe State Parks meadow - 1   \n",
      "4                      Slaughterhouse Meadows - 1   \n",
      "..                                            ...   \n",
      "636              West Shore tributary - 2 - lower   \n",
      "637                           Saxon Creek - upper   \n",
      "638                           Saxon Creek - upper   \n",
      "639           Marlette Creek - south fork (lower)   \n",
      "640           Marlette Creek - south fork (lower)   \n",
      "\n",
      "                   Bank_Stability_Data_Source  ...  Ownership_Secondary  \\\n",
      "0                                  TRPA, 2022  ...                 <NA>   \n",
      "1                                  TRPA, 2022  ...                 <NA>   \n",
      "2                                        <NA>  ...              private   \n",
      "3                                        <NA>  ...               <Null>   \n",
      "4                                        <NA>  ...                 <NA>   \n",
      "..                                        ...  ...                  ...   \n",
      "636  TRPA baseline condition assessment, 2019  ...                        \n",
      "637                                TRPA, 2023  ...                 <NA>   \n",
      "638                                TRPA, 2023  ...                 <NA>   \n",
      "639       TRPA baseline condition assessment.  ...                 <NA>   \n",
      "640       TRPA baseline condition assessment.  ...                 <NA>   \n",
      "\n",
      "    Ownership_Secondary_2  Ownership_Secondary_3  \\\n",
      "0                    <NA>                   <NA>   \n",
      "1                    <NA>                   <NA>   \n",
      "2                    <NA>                   <NA>   \n",
      "3                    <NA>                   <NA>   \n",
      "4                    <NA>                   <NA>   \n",
      "..                    ...                    ...   \n",
      "636                                         <NA>   \n",
      "637                  <NA>                   <NA>   \n",
      "638                  <NA>                   <NA>   \n",
      "639                  <NA>                   <NA>   \n",
      "640                  <NA>                   <NA>   \n",
      "\n",
      "                            SEZ_Type  \\\n",
      "0                   Channeled Meadow   \n",
      "1                   Channeled Meadow   \n",
      "2               Non-Channeled Meadow   \n",
      "3               Non-Channeled Meadow   \n",
      "4               Non-Channeled Meadow   \n",
      "..                               ...   \n",
      "636                         Forested   \n",
      "637             Riverine (Perennial)   \n",
      "638  Riverine (Perennial) + Forested   \n",
      "639  Riverine (Perennial) + Forested   \n",
      "640             Riverine (Perennial)   \n",
      "\n",
      "                                                 SHAPE Threshold_Year  \\\n",
      "0    {\"rings\": [[[761982.5646000002, 4308181.1055],...           2023   \n",
      "1    {\"rings\": [[[761850.1218999997, 4306652.6184],...           2023   \n",
      "2    {\"rings\": [[[767001.017, 4319248.6436], [76700...           2023   \n",
      "3    {\"rings\": [[[758100.4269000003, 4306113.9651],...           2023   \n",
      "4    {\"rings\": [[[764325.8827999998, 4333153.6456],...           2023   \n",
      "..                                                 ...            ...   \n",
      "636  {\"rings\": [[[745029.2533999998, 4332303.546599...           2023   \n",
      "637  {\"rings\": [[[761563.8099999996, 4305361.949999...           2023   \n",
      "638  {\"rings\": [[[761250.0661000004, 4304817.177100...           2023   \n",
      "639  {\"rings\": [[[765700.0664999997, 4339900.082800...           2023   \n",
      "640  {\"rings\": [[[765692.8995000003, 4339889.4464],...           2023   \n",
      "\n",
      "     VegetationVigor_DataSource VegetationVigor_Rating VegetationVigor_Raw  \\\n",
      "0                      DRI 2020                     NA                <NA>   \n",
      "1                      DRI 2020                     NA                <NA>   \n",
      "2                      DRI 2020                      A                <NA>   \n",
      "3                      DRI 2020                     NA                <NA>   \n",
      "4                      DRI 2020                      A                <NA>   \n",
      "..                          ...                    ...                 ...   \n",
      "636                    DRI 2020                    NaN                <NA>   \n",
      "637                    DRI 2020                    NaN                <NA>   \n",
      "638                    DRI 2020                    NaN                <NA>   \n",
      "639                    DRI 2020                    NaN                <NA>   \n",
      "640                    DRI 2020                    NaN                <NA>   \n",
      "\n",
      "    VegetationVigor_Score  \n",
      "0                    <NA>  \n",
      "1                    <NA>  \n",
      "2                    12.0  \n",
      "3                    <NA>  \n",
      "4                    12.0  \n",
      "..                    ...  \n",
      "636                  <NA>  \n",
      "637                  <NA>  \n",
      "638                  <NA>  \n",
      "639                  <NA>  \n",
      "640                  <NA>  \n",
      "\n",
      "[641 rows x 66 columns]\n"
     ]
    }
   ],
   "source": [
    "#Bring in Last year threshold scores for any indicators thataren't in our raw data- some bank stability came from our stream surveys but aren't in the data\n",
    "#Fill in any indicators that have missing data with data from 2019\n",
    "# Field Mapping so 2019 threshol data renamed so it can be joined to new data\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Threshold_Year': 'Threshold Year',\n",
    "    'Comments': 'Comments',\n",
    "    'SEZ_Type': 'SEZ_Type',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'Acres': 'Acres',\n",
    "    'AquaticOrganismPassage_Barriers': 'AOP_BarriersPerMile',\n",
    "    'AquaticOrganismPassage_NumberOf': 'AOP_NumberofBarriers',\n",
    "    'AquaticOrganismPassage_Score': 'AOP_Score',\n",
    "    'AquaticOrganismPassage_Rating': 'AOP_Rating',\n",
    "    'AquaticOrganismPassage_StreamMi': 'AOP_StreamMiles',\n",
    "    'AquaticOrganismPassage_DataSour': 'AOP_DataSource',\n",
    "    'Bank_Stability_Data_Source': 'Bank_Stability_Data_Source',\n",
    "    'Bank_Stability_Percent_Unstable': 'Bank_Stability_Percent_Unstable',\n",
    "    'Bank_Stability_Rating': 'Bank_Stability_Rating',\n",
    "    'Bank_Stability_Score': 'Bank_Stability_Score',\n",
    "    'Biotic_Integrity_Rating': 'Biotic_Integrity_Rating',\n",
    "    'Biotic_Integrity_CSCI': 'Biotic_Integrity_CSCI',\n",
    "    'Biotic_Integrity_Data_Source': 'Biotic_Integrity_Data_Source',\n",
    "    'Biotic_Integrity_Score': 'Biotic_Integrity_Score',\n",
    "    'Conifer_Encroachment_Percent_En': 'Conifer_Percent_Encroached',\n",
    "    'Conifer_Encroachment_Data_Sourc': 'Conifer_Encroachment_Data_Sourc',\n",
    "    'Conifer_Encroachment_Rating': 'Conifer_Encroachment_Rating',\n",
    "    'Conifer_Encroachment_Score': 'Conifer_Encroachment_Score',\n",
    "    'ConiferEncroachment_Comments': 'ConiferEncroachment_Comments',\n",
    "    'Ditches_Data_Source': 'Ditches_Data_Source',\n",
    "    'Ditches_Length': 'Ditches_Length',\n",
    "    'Ditches_Meadow_Length': 'Ditches_Meadow_Length',\n",
    "    'Ditches_Percent': 'Ditches_Percent',\n",
    "    'Ditches_Rating': 'Ditches_Rating',\n",
    "    'Ditches_Score': 'Ditches_Score',\n",
    "    'Feature_Type': 'Feature_Type',\n",
    "    'Habitat_Fragmentation_Data_Sour': 'Habitat_Frag_Data_Source',\n",
    "    'Habitat_Fragmentation_Imperviou': 'Habitat_Frag_Impervious_Acres',\n",
    "    'Habitat_Fragmentation_Percent_I': 'Habitat_Frag_Percent_Impervious',\n",
    "    'Habitat_Fragmentation_Rating': 'Habitat_Frag_Rating',\n",
    "    'Habitat_Fragmentation_Score': 'Habitat_Frag_Score',\n",
    "    'Headcuts_Data_Source': 'Headcuts_Data_Source',\n",
    "    'Headcuts_Number_of_Headcuts':'Number_of_Headcuts',\n",
    "    'Headcuts_Rating': 'Headcuts_Rating',\n",
    "    'Headcuts_Score': 'Headcuts_Score',\n",
    "    'Incision_Data_Source': 'Incision_Data_Source',\n",
    "    'Incision_Rating': 'Incision_Rating',\n",
    "    'Incision_Score': 'Incision_Score',\n",
    "    'Incision_Ratio': 'Incision_Ratio',\n",
    "    'Invasive_Percent_Cover': 'Invasive_Percent_Cover',\n",
    "    'Invasive_Rating': 'Invasives_Rating',\n",
    "    'Invasives_Data_Source': 'Invasives_Data_Source',\n",
    "    'Invasives_Number_of_Invasives': 'Invasives_Number_of_Invasives',\n",
    "    'Invasives_Plant_Types': 'Invasives_Plant_Types',\n",
    "    'Invasives_Scores': 'Invasives_Scores',\n",
    "    'NDVI_ID': 'NDVI_ID',\n",
    "    'Ownership_Primary': 'Ownership_Primary',\n",
    "    'Ownership_Secondary': 'Ownership_Secondary',\n",
    "    'Ownership_Secondary_2': 'Ownership_Secondary_2',\n",
    "    'Ownership_Secondary_3': 'Ownership_Secondary_3',\n",
    "    'VegetationVigor_DataSource': 'VegetationVigor_DataSource',\n",
    "    'VegetationVigor_Rating': 'VegetationVigor_Rating',\n",
    "    'VegetationVigor_Raw': 'VegetationVigor_Raw',\n",
    "    'VegetationVigor_Score': 'VegetationVigor_Score'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "dfSEZprep = dfSEZ.rename(columns=field_mapping)\n",
    "# Filter out columns in dfSEZprep that are not in final_df\n",
    "dfSEZprep = dfSEZprep[[col for col in final_df.columns if col in dfSEZprep.columns]]\n",
    "# # Ensure dfSEZprep has the same columns as final_df\n",
    "#dfSEZprep = dfSEZprep[final_df.columns]\n",
    "\n",
    "# Set SEZ_ID as index for both dataframes\n",
    "final_df.set_index('SEZ_ID', inplace=True)\n",
    "dfSEZprep.set_index('SEZ_ID', inplace=True)\n",
    "\n",
    "# Update missing values in final_df using values from dfSEZprep\n",
    "#final_df.update(dfSEZprep)\n",
    "#final_df = final_df.fillna(dfSEZprep)\n",
    "# Reset the index to get SEZ_ID back as a column\n",
    "#final_df.reset_index(inplace=True)\n",
    "\n",
    " # Combine DataFrames to fill missing values\n",
    "final_df = final_df.combine_first(dfSEZprep)\n",
    "\n",
    "# Reset the index to get SEZ_ID back as a column\n",
    "final_df.reset_index(inplace=True)\n",
    "\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_df columns: Index(['SEZ_ID', 'AOP_BarriersPerMile', 'AOP_DataSource',\n",
      "       'AOP_NumberofBarriers', 'AOP_Rating', 'AOP_Score', 'AOP_StreamMiles',\n",
      "       'Acres', 'Assessment_Unit_Name', 'Bank_Stability_Data_Source',\n",
      "       'Bank_Stability_Percent_Unstable', 'Bank_Stability_Rating',\n",
      "       'Bank_Stability_Score', 'Biotic_Integrity_CSCI',\n",
      "       'Biotic_Integrity_Data_Source', 'Biotic_Integrity_Rating',\n",
      "       'Biotic_Integrity_Score', 'Comments', 'ConiferEncroachment_Comments',\n",
      "       'Conifer_Encroachment_Data_Sourc', 'Conifer_Encroachment_Rating',\n",
      "       'Conifer_Encroachment_Score', 'Conifer_Percent_Encroached',\n",
      "       'Ditches_Data_Source', 'Ditches_Length', 'Ditches_Meadow_Length',\n",
      "       'Ditches_Percent', 'Ditches_Rating', 'Ditches_Score', 'Feature_Type',\n",
      "       'Habitat_Frag_Data_Source', 'Habitat_Frag_Impervious_Acres',\n",
      "       'Habitat_Frag_Percent_Impervious', 'Habitat_Frag_Rating',\n",
      "       'Habitat_Frag_Score', 'Headcuts_Data_Source', 'Headcuts_Rating',\n",
      "       'Headcuts_Score', 'Incision_Data_Source', 'Incision_Rating',\n",
      "       'Incision_Ratio', 'Incision_Score', 'Invasive_Percent_Cover',\n",
      "       'Invasives_Data_Source', 'Invasives_Number_of_Invasives',\n",
      "       'Invasives_Plant_Types', 'Invasives_Rating', 'Invasives_Scores',\n",
      "       'NDVI_ID', 'Number_of_Headcuts', 'Ownership_Primary',\n",
      "       'Ownership_Secondary', 'Ownership_Secondary_2', 'Ownership_Secondary_3',\n",
      "       'SEZ_Type', 'SHAPE', 'Threshold_Year', 'VegetationVigor_DataSource',\n",
      "       'VegetationVigor_Rating', 'VegetationVigor_Raw',\n",
      "       'VegetationVigor_Score'],\n",
      "      dtype='object')\n",
      "dfSEZprep columns: Index(['AOP_BarriersPerMile', 'AOP_DataSource', 'AOP_NumberofBarriers',\n",
      "       'AOP_Rating', 'AOP_Score', 'AOP_StreamMiles', 'Acres',\n",
      "       'Assessment_Unit_Name', 'Bank_Stability_Data_Source',\n",
      "       'Bank_Stability_Percent_Unstable', 'Bank_Stability_Rating',\n",
      "       'Bank_Stability_Score', 'Biotic_Integrity_CSCI',\n",
      "       'Biotic_Integrity_Data_Source', 'Biotic_Integrity_Rating',\n",
      "       'Biotic_Integrity_Score', 'Comments', 'ConiferEncroachment_Comments',\n",
      "       'Conifer_Encroachment_Data_Sourc', 'Conifer_Encroachment_Rating',\n",
      "       'Conifer_Encroachment_Score', 'Conifer_Percent_Encroached',\n",
      "       'Ditches_Data_Source', 'Ditches_Length', 'Ditches_Meadow_Length',\n",
      "       'Ditches_Percent', 'Ditches_Rating', 'Ditches_Score', 'Feature_Type',\n",
      "       'Habitat_Frag_Data_Source', 'Habitat_Frag_Impervious_Acres',\n",
      "       'Habitat_Frag_Percent_Impervious', 'Habitat_Frag_Rating',\n",
      "       'Habitat_Frag_Score', 'Headcuts_Data_Source', 'Headcuts_Rating',\n",
      "       'Headcuts_Score', 'Incision_Data_Source', 'Incision_Rating',\n",
      "       'Incision_Ratio', 'Incision_Score', 'Invasive_Percent_Cover',\n",
      "       'Invasives_Data_Source', 'Invasives_Number_of_Invasives',\n",
      "       'Invasives_Plant_Types', 'Invasives_Rating', 'Invasives_Scores',\n",
      "       'NDVI_ID', 'Number_of_Headcuts', 'Ownership_Primary',\n",
      "       'Ownership_Secondary', 'Ownership_Secondary_2', 'Ownership_Secondary_3',\n",
      "       'SEZ_Type', 'SHAPE', 'VegetationVigor_DataSource',\n",
      "       'VegetationVigor_Rating', 'VegetationVigor_Raw',\n",
      "       'VegetationVigor_Score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Verify column names\n",
    "print(\"final_df columns:\", final_df.columns)\n",
    "print(\"dfSEZprep columns:\", dfSEZprep.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data type of the SEZ_ID column in combined_df\n",
    "print(\"Data type of SEZ_ID in combined_df:\", combined_df['SEZ_ID'].dtype)\n",
    "\n",
    "# Check the data type of the SEZ_ID column in dfSEZinfo\n",
    "print(\"Data type of SEZ_ID in dfSEZinfo:\", dfSEZinfo['SEZ_ID'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----\n",
    "# #QAOnly\n",
    "#------------\n",
    "score_columns = [col for col in final_df.columns if 'Score' in col]\n",
    "print(\"score columns in final_df:\", final_df[score_columns].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------\n",
    "#QAOnly\n",
    "#------------\n",
    "# #RUN FOR QA so you can see the resulting dataframe/scores/formatting etc.\n",
    "#Will need to rerun to fix dropping the shape column\n",
    "# List of columns to export (excluding 'geometry' or 'Shape')\n",
    "columns_to_export = [col for col in final_df.columns if col != 'SHAPE']\n",
    "final_df[columns_to_export].to_csv('ready_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Score based on SEZ_Type  #Future- make it so any indicator that isn't in the lists below for SEZ Type says NA\n",
    "#Use SEZ_Type to select only needed indicators for SEZ Type\n",
    "ReadytoScore= final_df\n",
    "# Define the score columns needed for each SEZ Type\n",
    "RiverineIndicators = ['AOP_Score', 'Bank_Stability_Score', 'Biotic_Integrity_Score', 'Habitat_Frag_Score', 'Incision_Score', 'Headcuts_Score']\n",
    "ForestRiverineIndicators =['AOP_Score', 'Bank_Stability_Score', 'Biotic_Integrity_Score', 'Ditches_Score', 'Habitat_Frag_Score', 'Incision_Score', 'Headcuts_Score']\n",
    "NonChanneledIndicators=['Invasives_Scores', 'Conifer_Encroachment_Score', 'Ditches_Score', 'Habitat_Frag_Score', 'Headcuts_Score', 'VegetationVigor_Score']\n",
    "ChanneledIndicators= ['AOP_Score', 'Bank_Stability_Score', 'Biotic_Integrity_Score', 'Invasives_Scores', 'Conifer_Encroachment_Score', 'Ditches_Score', 'Habitat_Frag_Score', 'Incision_Score', 'Headcuts_Score', 'VegetationVigor_Score']\n",
    "ForestIndicators= ['Bank_Stability_Score', 'Ditches_Score', 'Habitat_Frag_Score', 'Headcuts_Score']\n",
    "\n",
    "# Function to get the score columns based on SEZ_Type\n",
    "def get_score_columns(sez_type):\n",
    "    if sez_type == 'Riverine (Perennial)':\n",
    "        return RiverineIndicators\n",
    "    elif sez_type == 'Riverine (Perennial) + Forested':\n",
    "        return ForestRiverineIndicators\n",
    "    elif sez_type == 'Non-Channeled Meadow':\n",
    "        return NonChanneledIndicators\n",
    "    elif sez_type == 'Channeled Meadow':\n",
    "        return ChanneledIndicators\n",
    "    elif sez_type == 'Forested':\n",
    "        return ForestIndicators\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Apply the appropriate score columns based on SEZ_Type\n",
    "final_df['Score_Columns'] = final_df['SEZ_Type'].apply(get_score_columns)\n",
    "# Function to calculate the final points and points possible\n",
    "\n",
    "# Function to calculate the final points and points possible\n",
    "def calculate_scores(row):\n",
    "    score_columns = row['Score_Columns']\n",
    "    if not score_columns:\n",
    "        return pd.Series([None, None])\n",
    "    total_points = row[score_columns].sum(skipna=True)\n",
    "    points_possible = row[score_columns].notna().sum() * 12\n",
    "    return pd.Series([total_points, points_possible])\n",
    "\n",
    "# Apply the score calculation to each row\n",
    "final_df[['Final_Total_Points', 'Final_Points_Possible']] = final_df.apply(calculate_scores, axis=1)\n",
    "\n",
    "# Calculate the final percent\n",
    "final_df['Final_Percent'] = final_df['Final_Total_Points'] / final_df['Final_Points_Possible']\n",
    "\n",
    "# Calculate the final rating and score\n",
    "final_df['Final_Rating'] = final_df['Final_Percent'].apply(rate_SEZ)\n",
    "final_df['Final_Score'] = final_df['Final_Rating'].apply(score_indicator)\n",
    "\n",
    "# Drop the temporary 'Score_Columns' column\n",
    "final_df = final_df.drop(columns=['Score_Columns'])\n",
    "\n",
    "# Convert SEZ_ID to string\n",
    "#final_df['SEZ_ID'] = final_df['SEZ_ID'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##RUN FOR QA so you can see the resulting dataframe/scores/formatting etc.\n",
    "#Will need to rerun to fix dropping the shape column\n",
    "# List of columns to export (excluding 'geometry' or 'Shape')\n",
    "#columns_to_export = [col for col in final_df.columns if col != 'SHAPE']\n",
    "#final_df[columns_to_export].to_csv('ready_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SEZ_ID  AquaticOrganismPassage_Barriers AquaticOrganismPassage_DataSour  \\\n",
      "0       1.0                              0.0          USFS / TRPA 2009, 2019   \n",
      "1       2.0                              0.0          USFS / TRPA 2009, 2019   \n",
      "2       3.0                             <NA>          USFS / TRPA 2009, 2019   \n",
      "3       4.0                             <NA>          USFS / TRPA 2009, 2019   \n",
      "4       6.0                             <NA>          USFS / TRPA 2009, 2019   \n",
      "..      ...                              ...                             ...   \n",
      "636   639.0                             <NA>          USFS / TRPA 2009, 2019   \n",
      "637   640.0                              0.0          USFS / TRPA 2009, 2019   \n",
      "638   641.0                              0.0          USFS / TRPA 2009, 2019   \n",
      "639   642.0                             10.0          USFS / TRPA 2009, 2019   \n",
      "640   643.0                             10.0          USFS / TRPA 2009, 2019   \n",
      "\n",
      "     AquaticOrganismPassage_NumberOf AquaticOrganismPassage_Rating  \\\n",
      "0                                0.0                             A   \n",
      "1                                0.0                             A   \n",
      "2                               <NA>                          <NA>   \n",
      "3                               <NA>                          <NA>   \n",
      "4                               <NA>                          <NA>   \n",
      "..                               ...                           ...   \n",
      "636                             <NA>                          <NA>   \n",
      "637                              0.0                             A   \n",
      "638                              0.0                             A   \n",
      "639                              2.0                             D   \n",
      "640                              2.0                             D   \n",
      "\n",
      "     AquaticOrganismPassage_Score  AquaticOrganismPassage_StreamMiles  \\\n",
      "0                            12.0                                <NA>   \n",
      "1                            12.0                                <NA>   \n",
      "2                            <NA>                                <NA>   \n",
      "3                            <NA>                                <NA>   \n",
      "4                            <NA>                                <NA>   \n",
      "..                            ...                                 ...   \n",
      "636                          <NA>                                <NA>   \n",
      "637                          12.0                                <NA>   \n",
      "638                          12.0                                <NA>   \n",
      "639                           3.0                                 0.2   \n",
      "640                           3.0                                 0.2   \n",
      "\n",
      "         Acres                          Assessment_Unit_Name  \\\n",
      "0    16.661797    Saxon Creek meadows - below Fountain Place   \n",
      "1    12.805647  Saxon Creek meadows - above Fountain Place 1   \n",
      "2     4.919027                       Burke Creek meadows - 2   \n",
      "3    35.718239                 Washoe State Parks meadow - 1   \n",
      "4    21.172427                    Slaughterhouse Meadows - 1   \n",
      "..         ...                                           ...   \n",
      "636   0.566975              West Shore tributary - 2 - lower   \n",
      "637   1.350239                           Saxon Creek - upper   \n",
      "638  20.885109                           Saxon Creek - upper   \n",
      "639   2.533502           Marlette Creek - south fork (lower)   \n",
      "640   0.268518           Marlette Creek - south fork (lower)   \n",
      "\n",
      "                   Bank_Stability_Data_Source  ...  Ownership_Secondary  \\\n",
      "0                                  TRPA, 2022  ...                 <NA>   \n",
      "1                                  TRPA, 2022  ...                 <NA>   \n",
      "2                                        <NA>  ...              private   \n",
      "3                                        <NA>  ...               <Null>   \n",
      "4                                        <NA>  ...                 <NA>   \n",
      "..                                        ...  ...                  ...   \n",
      "636  TRPA baseline condition assessment, 2019  ...                        \n",
      "637                                TRPA, 2023  ...                 <NA>   \n",
      "638                                TRPA, 2023  ...                 <NA>   \n",
      "639       TRPA baseline condition assessment.  ...                 <NA>   \n",
      "640       TRPA baseline condition assessment.  ...                 <NA>   \n",
      "\n",
      "    Ownership_Secondary_2  Ownership_Secondary_3  \\\n",
      "0                    <NA>                   <NA>   \n",
      "1                    <NA>                   <NA>   \n",
      "2                    <NA>                   <NA>   \n",
      "3                    <NA>                   <NA>   \n",
      "4                    <NA>                   <NA>   \n",
      "..                    ...                    ...   \n",
      "636                                         <NA>   \n",
      "637                  <NA>                   <NA>   \n",
      "638                  <NA>                   <NA>   \n",
      "639                  <NA>                   <NA>   \n",
      "640                  <NA>                   <NA>   \n",
      "\n",
      "                            SEZ_Type  \\\n",
      "0                   Channeled Meadow   \n",
      "1                   Channeled Meadow   \n",
      "2               Non-Channeled Meadow   \n",
      "3               Non-Channeled Meadow   \n",
      "4               Non-Channeled Meadow   \n",
      "..                               ...   \n",
      "636                         Forested   \n",
      "637             Riverine (Perennial)   \n",
      "638  Riverine (Perennial) + Forested   \n",
      "639  Riverine (Perennial) + Forested   \n",
      "640             Riverine (Perennial)   \n",
      "\n",
      "                                                 SHAPE Threshold Year  \\\n",
      "0    {\"rings\": [[[761982.5646000002, 4308181.1055],...           2023   \n",
      "1    {\"rings\": [[[761850.1218999997, 4306652.6184],...           2023   \n",
      "2    {\"rings\": [[[767001.017, 4319248.6436], [76700...           2023   \n",
      "3    {\"rings\": [[[758100.4269000003, 4306113.9651],...           2023   \n",
      "4    {\"rings\": [[[764325.8827999998, 4333153.6456],...           2023   \n",
      "..                                                 ...            ...   \n",
      "636  {\"rings\": [[[745029.2533999998, 4332303.546599...           2023   \n",
      "637  {\"rings\": [[[761563.8099999996, 4305361.949999...           2023   \n",
      "638  {\"rings\": [[[761250.0661000004, 4304817.177100...           2023   \n",
      "639  {\"rings\": [[[765700.0664999997, 4339900.082800...           2023   \n",
      "640  {\"rings\": [[[765692.8995000003, 4339889.4464],...           2023   \n",
      "\n",
      "     VegetationVigor_DataSource VegetationVigor_Rating VegetationVigor_Raw  \\\n",
      "0                      DRI 2020                     NA                <NA>   \n",
      "1                      DRI 2020                     NA                <NA>   \n",
      "2                      DRI 2020                      A                <NA>   \n",
      "3                      DRI 2020                     NA                <NA>   \n",
      "4                      DRI 2020                      A                <NA>   \n",
      "..                          ...                    ...                 ...   \n",
      "636                    DRI 2020                   <NA>                <NA>   \n",
      "637                    DRI 2020                   <NA>                <NA>   \n",
      "638                    DRI 2020                   <NA>                <NA>   \n",
      "639                    DRI 2020                   <NA>                <NA>   \n",
      "640                    DRI 2020                   <NA>                <NA>   \n",
      "\n",
      "    VegetationVigor_Score  \n",
      "0                    <NA>  \n",
      "1                    <NA>  \n",
      "2                    12.0  \n",
      "3                    <NA>  \n",
      "4                    12.0  \n",
      "..                    ...  \n",
      "636                  <NA>  \n",
      "637                  <NA>  \n",
      "638                  <NA>  \n",
      "639                  <NA>  \n",
      "640                  <NA>  \n",
      "\n",
      "[641 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "#Final set up\n",
    "\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'SHAPE': 'SHAPE',\n",
    "    'Threshold_Year': 'Threshold Year',\n",
    "    'Comments': 'Comments',\n",
    "    'SEZ_Type': 'SEZ_Type',\n",
    "    'Final_Rating': 'Final_Rating',\n",
    "    'Final_Percent': 'Final_Percent',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'Final_Points_Possible': 'Final_Points_Possible',\n",
    "    'Final_Total_Points': 'Final_Total_Points',\n",
    "    'Acres': 'Acres',\n",
    "    'AOP_BarriersPerMile':'AquaticOrganismPassage_Barriers',\n",
    "    'AOP_NumberofBarriers': 'AquaticOrganismPassage_NumberOf',\n",
    "    'AOP_Score': 'AquaticOrganismPassage_Score',\n",
    "    'AOP_Rating': 'AquaticOrganismPassage_Rating',\n",
    "    'AOP_StreamMiles': 'AquaticOrganismPassage_StreamMiles',\n",
    "    'AOP_DataSource': 'AquaticOrganismPassage_DataSour',\n",
    "    'Bank_Stability_Data_Source': 'Bank_Stability_Data_Source',\n",
    "    'Bank_Stability_Percent_Unstable': 'Bank_Stability_Percent_Unstable',\n",
    "    'Bank_Stability_Rating': 'Bank_Stability_Rating',\n",
    "    'Bank_Stability_Score': 'Bank_Stability_Score',\n",
    "    'Biotic_Integrity_Rating': 'Biotic_Integrity_Rating',\n",
    "    'Biotic_Integrity_CSCI': 'Biotic_Integrity_CSCI',\n",
    "    'Biotic_Integrity_Data_Source': 'Biotic_Integrity_Data_Source',\n",
    "    'Biotic_Integrity_Score': 'Biotic_Integrity_Score',\n",
    "    'Comments': 'Comments',\n",
    "    'Conifer_Percent_Encroached': 'Conifer_Encroachment_Percent_En',\n",
    "    'Conifer_Encroachment_Data_Sourc': 'Conifer_Encroachment_Data_Sourc',\n",
    "    'Conifer_Encroachment_Rating': 'Conifer_Encroachment_Rating',\n",
    "    'Conifer_Encroachment_Score': 'Conifer_Encroachment_Score',\n",
    "    'ConiferEncroachment_Comments': 'Conifer_Encroachment_Comments',\n",
    "    'Ditches_Data_Source': 'Ditches_Data_Source',\n",
    "    'Ditches_Length': 'Ditches_Length',\n",
    "    'Ditches_Meadow_Length': 'Ditches_Meadow_Length',\n",
    "    'Ditches_Percent': 'Ditches_Percent',\n",
    "    'Ditches_Rating': 'Ditches_Rating',\n",
    "    'Ditches_Score': 'Ditches_Score',\n",
    "    'Feature_Type': 'Feature_Type',\n",
    "    'Habitat_Frag_Data_Source': 'Habitat_Fragmentation_Data_Sour',\n",
    "    'Habitat_Frag_Impervious_Acres': 'Habitat_Fragmentation_Imperviou',\n",
    "    'Habitat_Frag_Percent_Impervious': 'Habitat_Fragmentation_Percent_I',\n",
    "    'Habitat_Frag_Rating': 'Habitat_Fragmentation_Rating',\n",
    "    'Habitat_Frag_Score': 'Habitat_Fragmentation_Score',\n",
    "    'Headcuts_Data_Source': 'Headcuts_Data_Source',\n",
    "    'Number_of_Headcuts': 'Headcuts_Number_of_Headcuts',\n",
    "    'Headcuts_Rating': 'Headcuts_Rating',\n",
    "    'Headcuts_Score': 'Headcuts_Score',\n",
    "    'Incision_Data_Source': 'Incision_Data_Source',\n",
    "    'Incision_Rating': 'Incision_Rating',\n",
    "    'Incision_Score': 'Incision_Score',\n",
    "    'Incision_Ratio': 'Incision_Ratio',\n",
    "    'Invasive_Percent_Cover': 'Invasive_Percent_Cover',\n",
    "    'Invasives_Rating': 'Invasive_Rating',\n",
    "    'Invasives_Data_Source': 'Invasives_Data_Source',\n",
    "    'Invasives_Number_of_Invasives': 'Invasives_Number_of_Invasives',\n",
    "    'Invasives_Plant_Types': 'Invasives_Plant_Types',\n",
    "    'Invasives_Scores': 'Invasives_Scores',\n",
    "    'NDVI_ID': 'NDVI_ID',\n",
    "    'Ownership_Primary': 'Ownership_Primary',\n",
    "    'Ownership_Secondary': 'Ownership_Secondary',\n",
    "    'Ownership_Secondary_2': 'Ownership_Secondary_2',\n",
    "    'Ownership_Secondary_3': 'Ownership_Secondary_3',\n",
    "    'VegetationVigor_DataSource': 'VegetationVigor_DataSource',\n",
    "    'VegetationVigor_Rating': 'VegetationVigor_Rating',\n",
    "    'VegetationVigor_Raw': 'VegetationVigor_Raw',\n",
    "    'VegetationVigor_Score': 'VegetationVigor_Score'\n",
    "}\n",
    "# Rename fields based on field mappings\n",
    "SEZscores_readydf = final_df.rename(columns=field_mapping).drop(columns=[col for col in final_df.columns if col not in field_mapping])\n",
    "\n",
    "\n",
    "print(SEZscores_readydf)\n",
    "#----------------------------------------------------\n",
    "#Post results to CSV in gis/projects/Researchanalysis/SEZ for further QA\n",
    "#----------------------------------------------------\n",
    " \n",
    "#export all columns but shape\n",
    "columns_to_export = [col for col in SEZscores_readydf.columns if col != 'SHAPE']\n",
    "#Store csv on F drive for QA/add comments manually on F drive and to change up comments later based on SEZ's that scores changed\n",
    "#final_results = \"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Threshold24_SEZScores.csv\"\n",
    "\n",
    "# Write to excel\n",
    "#SEZscores_readydf[columns_to_export].to_excel(final_results, index=False)\n",
    "\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "#data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "#field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(allsezscores, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Assessment_Unit_Name  SEZ_ID Final_Rating_2023  \\\n",
      "1    Saxon Creek meadows - above Fountain Place 1     2.0                 C   \n",
      "4                      Slaughterhouse Meadows - 1     6.0                 B   \n",
      "5            Upper Truckee River - Tahoe Paradise     7.0                 C   \n",
      "6                           Hell Hole Meadows - 2     8.0                 B   \n",
      "25                                 small meadow 7    27.0                 B   \n",
      "..                                            ...     ...               ...   \n",
      "630                          Burton Creek - lower   633.0                 C   \n",
      "631                          Burton Creek - lower   634.0                 C   \n",
      "632          Edgewood Creek tributary - 2 - upper   635.0                 D   \n",
      "637                           Saxon Creek - upper   640.0                 C   \n",
      "638                           Saxon Creek - upper   641.0                 C   \n",
      "\n",
      "                       SEZ_Type_2023 Final_Rating_2019  \\\n",
      "1                   Channeled Meadow                 B   \n",
      "4               Non-Channeled Meadow                 A   \n",
      "5                   Channeled Meadow                 B   \n",
      "6                   Channeled Meadow                 A   \n",
      "25                  Channeled Meadow                 A   \n",
      "..                               ...               ...   \n",
      "630  Riverine (Perennial) + Forested                 B   \n",
      "631             Riverine (Perennial)                 B   \n",
      "632                         Forested                 C   \n",
      "637             Riverine (Perennial)                 B   \n",
      "638  Riverine (Perennial) + Forested                 B   \n",
      "\n",
      "                       SEZ_Type_2019  \n",
      "1                   Channeled Meadow  \n",
      "4               Non-Channeled Meadow  \n",
      "5                   Channeled Meadow  \n",
      "6                   Channeled Meadow  \n",
      "25                  Channeled Meadow  \n",
      "..                               ...  \n",
      "630  Riverine (Perennial) + Forested  \n",
      "631             Riverine (Perennial)  \n",
      "632                         Forested  \n",
      "637             Riverine (Perennial)  \n",
      "638  Riverine (Perennial) + Forested  \n",
      "\n",
      "[142 rows x 6 columns]\n",
      "                             Assessment_Unit_Name  SEZ_ID Final_Rating_2023  \\\n",
      "1    Saxon Creek meadows - above Fountain Place 1     2.0                 C   \n",
      "4                      Slaughterhouse Meadows - 1     6.0                 B   \n",
      "5            Upper Truckee River - Tahoe Paradise     7.0                 C   \n",
      "6                           Hell Hole Meadows - 2     8.0                 B   \n",
      "25                                 small meadow 7    27.0                 B   \n",
      "..                                            ...     ...               ...   \n",
      "630                          Burton Creek - lower   633.0                 C   \n",
      "631                          Burton Creek - lower   634.0                 C   \n",
      "632          Edgewood Creek tributary - 2 - upper   635.0                 D   \n",
      "637                           Saxon Creek - upper   640.0                 C   \n",
      "638                           Saxon Creek - upper   641.0                 C   \n",
      "\n",
      "    Final_Rating_2019  \n",
      "1                   B  \n",
      "4                   A  \n",
      "5                   B  \n",
      "6                   A  \n",
      "25                  A  \n",
      "..                ...  \n",
      "630                 B  \n",
      "631                 B  \n",
      "632                 C  \n",
      "637                 B  \n",
      "638                 B  \n",
      "\n",
      "[142 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#This creates a csv of score changes and all information in row \n",
    "\n",
    "#Before Final comparison do more QA\n",
    "    # Check to make sure there are 641 ASsessment Units\n",
    "    #Check Final Points and make sure there are no really low numbers could be missing data\n",
    "    #Check all staging tables for completeness- There are ratings A-D, check on null data for that SEZ\n",
    "    #other QA methods can be added to this list\n",
    "\n",
    "#Check to see which SEZ Scores Changed\n",
    "Threshold23sezdata= SEZscores_readydf\n",
    "Threshold19sezdata = dfSEZ\n",
    "\n",
    "# Merge the datasets on Assessment_Unit_Name and SEZ_ID\n",
    "merged_df = pd.merge(\n",
    "    Threshold23sezdata[['Assessment_Unit_Name', 'SEZ_ID', 'Final_Rating', 'SEZ_Type']],\n",
    "    Threshold19sezdata[['Assessment_Unit_Name', 'SEZ_ID', 'Final_Rating', 'SEZ_Type']],\n",
    "    on=['Assessment_Unit_Name', 'SEZ_ID'],\n",
    "    suffixes=('_2023', '_2019')\n",
    ")\n",
    "\n",
    "# Identify Assessment Unit Names with a change in Final Rating\n",
    "changed_scores_df = merged_df[merged_df['Final_Rating_2023'] != merged_df['Final_Rating_2019']]\n",
    "\n",
    "# List of Assessment Unit Names with a change in score\n",
    "changed_assessment_units = changed_scores_df[['Assessment_Unit_Name', 'SEZ_ID', 'Final_Rating_2023', 'Final_Rating_2019']]\n",
    "\n",
    "# Output the DataFrame with changed scores\n",
    "print(changed_scores_df)\n",
    "\n",
    "# Output the list of Assessment Unit Names with a change in score\n",
    "print(changed_assessment_units)\n",
    "\n",
    "#Post results to CSV in gis/projects/Researchanalysis/SEZ for further QA\n",
    "columns_to_export = [col for col in changed_assessment_units.columns if col != 'SHAPE']\n",
    "#Store csv on F drive for QA/add comments manually on F drive and to change up comments later based on SEZ's that scores changed\n",
    "#Changed_Scores_List = \"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Threshold24_SEZScoreChanges.csv\"\n",
    "# Assuming ready_df is the DataFrame you want to write to CSV\n",
    "#changed_scores_df.to_csv(Changed_Scores_List, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Assessment_Unit_Name  SEZ_ID  \\\n",
      "0      Saxon Creek meadows - below Fountain Place     1.0   \n",
      "1    Saxon Creek meadows - above Fountain Place 1     2.0   \n",
      "4                      Slaughterhouse Meadows - 1     6.0   \n",
      "5            Upper Truckee River - Tahoe Paradise     7.0   \n",
      "6                           Hell Hole Meadows - 2     8.0   \n",
      "..                                            ...     ...   \n",
      "630                          Burton Creek - lower   633.0   \n",
      "631                          Burton Creek - lower   634.0   \n",
      "632          Edgewood Creek tributary - 2 - upper   635.0   \n",
      "637                           Saxon Creek - upper   640.0   \n",
      "638                           Saxon Creek - upper   641.0   \n",
      "\n",
      "    AquaticOrganismPassage_Rating_2023 Bank_Stability_Rating_2023  \\\n",
      "0                                    A                          C   \n",
      "1                                    A                          B   \n",
      "4                                 <NA>                         NA   \n",
      "5                                    A                          B   \n",
      "6                                    A                          A   \n",
      "..                                 ...                        ...   \n",
      "630                                  A                          B   \n",
      "631                                  A                          B   \n",
      "632                               <NA>                          A   \n",
      "637                                  A                          B   \n",
      "638                                  A                          B   \n",
      "\n",
      "    Biotic_Integrity_Rating_2023 Conifer_Encroachment_Rating_2023  \\\n",
      "0                              A                                D   \n",
      "1                              A                                D   \n",
      "4                         <Null>                                C   \n",
      "5                              B                                D   \n",
      "6                             NA                                C   \n",
      "..                           ...                              ...   \n",
      "630                            D                             <NA>   \n",
      "631                            D                             <NA>   \n",
      "632                         <NA>                             <NA>   \n",
      "637                            C                             <NA>   \n",
      "638                            C                             <NA>   \n",
      "\n",
      "    Ditches_Rating_2023 Habitat_Fragmentation_Rating_2023  \\\n",
      "0                     A                                 A   \n",
      "1                     A                                 A   \n",
      "4                     A                                 A   \n",
      "5                     C                                 A   \n",
      "6                     A                                 A   \n",
      "..                  ...                               ...   \n",
      "630                <NA>                                 A   \n",
      "631                <NA>                                 A   \n",
      "632                   A                                 C   \n",
      "637                <NA>                                 A   \n",
      "638                <NA>                                 A   \n",
      "\n",
      "    Headcuts_Rating_2023 Incision_Rating_2023  ... Bank_Stability_Rating_2019  \\\n",
      "0                      A                    C  ...                          A   \n",
      "1                      A                    D  ...                          B   \n",
      "4                      A                   NA  ...                         NA   \n",
      "5                      A                    C  ...                          B   \n",
      "6                      A                    D  ...                          A   \n",
      "..                   ...                  ...  ...                        ...   \n",
      "630                    D                    A  ...                          B   \n",
      "631                    D                    A  ...                          B   \n",
      "632                    D               <Null>  ...                          A   \n",
      "637                    A                    C  ...                          B   \n",
      "638                    A                    C  ...                          B   \n",
      "\n",
      "    Biotic_Integrity_Rating_2019 Conifer_Encroachment_Rating_2019  \\\n",
      "0                              A                                D   \n",
      "1                              A                                D   \n",
      "4                         <Null>                                C   \n",
      "5                              B                                D   \n",
      "6                             NA                                C   \n",
      "..                           ...                              ...   \n",
      "630                           NA                             <NA>   \n",
      "631                           NA                             <NA>   \n",
      "632                         <NA>                             <NA>   \n",
      "637                            A                             <NA>   \n",
      "638                            A                             <NA>   \n",
      "\n",
      "    Ditches_Rating_2019 Habitat_Fragmentation_Rating_2019  \\\n",
      "0                     A                                 A   \n",
      "1                     A                                 A   \n",
      "4                     A                                 A   \n",
      "5                     C                                 A   \n",
      "6                     A                                 A   \n",
      "..                  ...                               ...   \n",
      "630                <NA>                                 A   \n",
      "631                <NA>                                 A   \n",
      "632                   A                                 C   \n",
      "637                <NA>                                 A   \n",
      "638                <NA>                                 A   \n",
      "\n",
      "    Headcuts_Rating_2019 Incision_Rating_2019 Invasive_Rating_2019  \\\n",
      "0                      A                    C                    A   \n",
      "1                      A                    D                    A   \n",
      "4                     A                    NA                    A   \n",
      "5                      A                    C                    B   \n",
      "6                      A                    B                    A   \n",
      "..                   ...                  ...                  ...   \n",
      "630                    D                    A                 <NA>   \n",
      "631                    D                    A                 <NA>   \n",
      "632                    C               <Null>                 <NA>   \n",
      "637                    A                    D                 <NA>   \n",
      "638                    A                    D                 <NA>   \n",
      "\n",
      "    VegetationVigor_Rating_2019 Final_Rating_2019  \n",
      "0                            NA                 B  \n",
      "1                            NA                 B  \n",
      "4                             A                 A  \n",
      "5                             A                 B  \n",
      "6                            NA                 A  \n",
      "..                          ...               ...  \n",
      "630                        <NA>                 B  \n",
      "631                        <NA>                 B  \n",
      "632                        <NA>                 C  \n",
      "637                        <NA>                 B  \n",
      "638                        <NA>                 B  \n",
      "\n",
      "[158 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "#Otion for different CSV Look at only changed scores so we can update comments and make sure data is good, need to run this to get long format csv\n",
    "# List of all columns with 'Rating' in their name for both datasets\n",
    "rating_columns_2023 = [col for col in Threshold23sezdata.columns if 'Rating' in col]\n",
    "rating_columns_2019 = [col for col in Threshold19sezdata.columns if 'Rating' in col]\n",
    "\n",
    "# Ensure both DataFrames have the same Assessment_Unit_Name and SEZ_ID columns\n",
    "Threshold23sezdata = Threshold23sezdata[['Assessment_Unit_Name', 'SEZ_ID'] + rating_columns_2023]\n",
    "Threshold19sezdata = Threshold19sezdata[['Assessment_Unit_Name', 'SEZ_ID'] + rating_columns_2019]\n",
    "\n",
    "# Merge the datasets on Assessment_Unit_Name and SEZ_ID, for units that have changed scores\n",
    "final_merged_df = pd.merge(\n",
    "    Threshold23sezdata,\n",
    "    Threshold19sezdata,\n",
    "    on=['Assessment_Unit_Name', 'SEZ_ID'],\n",
    "    suffixes=('_2023', '_2019')\n",
    ")\n",
    "\n",
    "# Filter the final merged DataFrame to only include units that changed scores\n",
    "final_changed_scores_df = final_merged_df[final_merged_df['Assessment_Unit_Name'].isin(changed_assessment_units['Assessment_Unit_Name'])]\n",
    "\n",
    "# Print the final DataFrame with all rating columns and changed scores\n",
    "print(final_changed_scores_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "#final_changed_scores_df.to_csv(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Threshold24_SEZScoreChanges_indicators.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEST QA csv.. shows indicators scores and final score changes by year of threshold\n",
    "#futureadd in final points and points possible and comments\n",
    "# Used changed changed_assessment_units to get Rating scores from df's \n",
    "#Create CSV with assessment units that have changed , all indicator scores and final rating by year and SEZID\n",
    "\n",
    "# List of all columns with 'Rating' in their name for both datasets\n",
    "rating_columns_2023 = [col for col in Threshold23sezdata.columns if 'Rating' in col]\n",
    "rating_columns_2019 = [col for col in Threshold19sezdata.columns if 'Rating' in col]\n",
    "\n",
    "# Ensure both DataFrames have the same Assessment_Unit_Name and SEZ_ID columns\n",
    "Threshold23sezdata = Threshold23sezdata[['Assessment_Unit_Name', 'SEZ_ID'] + rating_columns_2023]\n",
    "Threshold19sezdata = Threshold19sezdata[['Assessment_Unit_Name', 'SEZ_ID'] + rating_columns_2019]\n",
    "\n",
    "# Merge changed_assessment_units with Threshold23sezdata and Threshold19sezdata to get ratings for changed units\n",
    "merged_23 = pd.merge(changed_assessment_units, Threshold23sezdata, on=['Assessment_Unit_Name', 'SEZ_ID'])\n",
    "merged_19 = pd.merge(changed_assessment_units, Threshold19sezdata, on=['Assessment_Unit_Name', 'SEZ_ID'])\n",
    "\n",
    "# Extract columns with 'Rating' for both datasets\n",
    "rating_columns_2023 = [col for col in merged_23.columns if 'Rating' in col]\n",
    "rating_columns_2019 = [col for col in merged_19.columns if 'Rating' in col]\n",
    "\n",
    "# Filtered DataFrames with only relevant columns\n",
    "filtered_23 = merged_23[['Assessment_Unit_Name', 'SEZ_ID'] + rating_columns_2023]\n",
    "filtered_19 = merged_19[['Assessment_Unit_Name', 'SEZ_ID'] + rating_columns_2019]\n",
    "\n",
    "# Add a 'Year' column to each DataFrame\n",
    "filtered_23['Year'] = '2023'\n",
    "filtered_19['Year'] = '2019'\n",
    "\n",
    "# Concatenate filtered_23 and filtered_19 DataFrames\n",
    "allratings = pd.concat([filtered_23, filtered_19], ignore_index=True)\n",
    "\n",
    "\n",
    "#Function to map SEZ_Type based on Assessment_Unit_Name\n",
    "def get_sez_type(row):\n",
    "    return lookup_all.get(row['SEZ_ID'], {}).get('SEZ_Type', None)\n",
    "\n",
    "# Apply the function to add SEZ_Type column\n",
    "allratings['SEZ_Type'] = allratings.apply(get_sez_type, axis=1)\n",
    "\n",
    "# Apply the function to add SEZ_Type column\n",
    "allratings['SEZ_Type'] = allratings.apply(get_sez_type, axis=1)\n",
    "\n",
    "# Drop the specified columns from merged_df\n",
    "columns_to_drop = ['Final_Rating_2019', 'Final_Rating_2023']\n",
    "allratings.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Save the final long format DataFrame to a CSV file\n",
    "#allratings.to_csv(\"F:\\\\GIS\\\\PROJECTS\\\\ResearchAnalysis\\\\SEZ\\\\allchangedscores_longformat.csv\", index=False)\n",
    "\n",
    "allratings.to_csv(\"C:\\\\Users\\\\snewsome\\\\Documents\\\\SEZallchangedscores_longformat.csv\", index=False)\n",
    "\n",
    "# Note to SELF - when group editing csv and fixing comments for changed scores this must be in an excel and is easiest if put online into google office sheets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Final_Rating' column is in both DataFrames\n",
    "if 'Final_Rating' not in Threshold23sezdata.columns or 'Final_Rating' not in Threshold19sezdata.columns:\n",
    "    raise ValueError(\"Both DataFrames must contain 'Final_Rating' column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QA only\n",
    "# DONT USE unless want to use for QA Make a column for QA for SEZ_Type/final points possible\n",
    "def categorize_finalpoints(SEZ_Type):\n",
    "     if pd.isna(SEZ_Type):\n",
    "        return np.nan\n",
    "     elif   SEZ_Type == 'Non-Channeled Meadow':\n",
    "        return '72'\n",
    "     elif SEZ_Type == 'Channeled Meadow':\n",
    "        return '120'\n",
    "     elif SEZ_Type == 'Riverine (Perennial)':\n",
    "        return 'C'\n",
    "     elif SEZ_Type == 'Forested':\n",
    "        return '48'\n",
    "     elif SEZ_Type == 'Riverine (Perennial) + Forested':\n",
    "        return '96'\n",
    "     \n",
    "final_df['TESTFinalPointsPossible']= final_df['SEZ_Type'].apply(categorize_finalpoints)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Basin wide SEZ Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total SEZ Condition Index: 991980.5835202622\n",
      "Total Acres: 11908.518083750001\n",
      "Final Number: 83.30008625287209\n",
      "Data has been saved to 'BasinwideSEZscores.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snewsome\\AppData\\Local\\Temp\\ipykernel_22384\\2663739531.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  BasinwideScore.loc[:, 'SEZ_Quality'] = BasinwideScore['Final_Percent'] * 100\n",
      "C:\\Users\\snewsome\\AppData\\Local\\Temp\\ipykernel_22384\\2663739531.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  BasinwideScore.loc[:, 'SEZ_Condition_Index'] = BasinwideScore['Acres'] * BasinwideScore['SEZ_Quality']\n"
     ]
    }
   ],
   "source": [
    "#Use this dataframe---SEZscores_readydf for 2023 data\n",
    "#or import from rest service... going to use dataframe right no\n",
    "\n",
    "BasinwideScore= SEZscores_readydf[['Acres', 'SEZ_ID','Assessment_Unit_Name','Threshold Year', 'Final_Percent', 'SEZ_Type']]\n",
    "\n",
    "# Calculate SEZ_Quality\n",
    "BasinwideScore['SEZ_Quality'] = BasinwideScore['Final_Percent'] * 100\n",
    "\n",
    "# Calculate SEZ_Condition Index\n",
    "BasinwideScore[:, 'SEZ_Condition_Index'] = BasinwideScore['Acres'] * BasinwideScore['SEZ_Quality']\n",
    "\n",
    "# Calculate the sums\n",
    "total_sez_ci = BasinwideScore['SEZ_Condition_Index'].sum()\n",
    "total_acres = BasinwideScore['Acres'].sum()\n",
    "\n",
    "# Calculate the final number\n",
    "final_number = total_sez_ci / total_acres\n",
    "\n",
    "\n",
    "print(f\"Total SEZ Condition Index: {total_sez_ci}\")\n",
    "print(f\"Total Acres: {total_acres}\")\n",
    "print(f\"Final Number: {final_number}\")\n",
    "\n",
    "# Create a new DataFrame with Threshold Year and final number\n",
    "final_data = BasinwideScore[['Threshold Year']].drop_duplicates().copy()\n",
    "final_data['Acre-weighted average SEZ quality'] = final_number\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "final_data.to_csv('F:\\Research and Analysis\\Threshold reporting\\ThresholdData\\SEZ\\BasinwideSEZscores.csv', index=False, mode=a, Header=False)\n",
    "\n",
    "print(\"Data has been saved to 'BasinwideSEZscores.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total SEZ Condition Index: 936261.9071145033\n",
      "Total Acres: 11908.518083750001\n",
      "Final Number: 78.62119371444695\n",
      "Data has been saved to 'BasinwideSEZscores.csv'\n"
     ]
    }
   ],
   "source": [
    "#2019 threshold data\n",
    "\n",
    "Basinwide2019 = dfSEZ[['Acres', 'SEZ_ID', 'Assessment_Unit_Name', 'Final_Percent', 'SEZ_Type']].copy()\n",
    "Basinwide2019['SEZ_Quality'] = Basinwide2019['Final_Percent'] \n",
    "Basinwide2019['SEZ_Condition_Index'] = Basinwide2019['Acres'] * Basinwide2019['SEZ_Quality']\n",
    "\n",
    "\n",
    "# Calculate the sums\n",
    "total_sez_ci = Basinwide2019['SEZ_Condition_Index'].sum()\n",
    "total_acres = Basinwide2019['Acres'].sum()\n",
    "\n",
    "# Calculate the final number\n",
    "final_number2019 = total_sez_ci / total_acres\n",
    "\n",
    "print(f\"Total SEZ Condition Index: {total_sez_ci}\")\n",
    "print(f\"Total Acres: {total_acres}\")\n",
    "print(f\"Final Number: {final_number2019}\")\n",
    "\n",
    "# Create and save the DataFrame with the result\n",
    "result2019 = pd.DataFrame({\n",
    "    'Threshold Year': ['2019'],\n",
    "    'Acre-weighted average SEZ quality': [final_number2019]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "result2019.to_csv('F:\\Research and Analysis\\Threshold reporting\\ThresholdData\\SEZ\\BasinwideSEZscores.csv', index=False, mode=a, header=False)\n",
    "\n",
    "print(\"Data has been saved to 'BasinwideSEZscores.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orginal code in case we need it--will probably delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Don't USeoriginal code.. merge needs work it seems like it is using lookup dict for both\n",
    "\n",
    "from functools import reduce\n",
    "# Functino to Get most recent year of data from each Dataframe\n",
    "def get_most_recent_scores(df,groupfield):\n",
    "    df = df.loc[df.groupby(groupfield)['Year'].idxmax()]\n",
    "    return df\n",
    "\n",
    "def get_most_recent_and_clean(data_frames, lookup_dict, columns_to_drop):\n",
    "    most_recent_data = {}\n",
    "\n",
    "    # Iterate over the items in the original 'meadowdata' dictionary\n",
    "    for key, df in data_frames.items():\n",
    "        # Apply the 'get_most_recent_scores' function to each DataFrame\n",
    "        processed_df = get_most_recent_scores(df, 'Assessment_Unit_Name')\n",
    "        \n",
    "        # Store the processed DataFrame in the new dictionary using the same key\n",
    "        most_recent_data[key] = processed_df\n",
    "    \n",
    "# Columns to drop\n",
    "\n",
    "    # Drop specified columns and remove duplicate rows\n",
    "    cleaned_data = {}\n",
    "    for key, df in most_recent_data.items():\n",
    "        # Drop specified columns if they exist\n",
    "        df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "        \n",
    "        # Remove duplicate rows\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        # Append the cleaned DataFrame to the list\n",
    "        cleaned_data[key] = df\n",
    "    \n",
    "    # Merge all DataFrames on 'Assessment_Unit_Name' using reduce\n",
    "    merged_df = {}\n",
    "    for key, df in cleaned_data.items():\n",
    "        merged_df = reduce(lambda left, right: pd.merge(left, right, on='Assessment_Unit_Name', how='outer'), cleaned_data.values())\n",
    "\n",
    "\n",
    "    # Meadowdata gets the lookup_dict\n",
    "    # Riverinedatagets lookup_riverine dictionary how the hell do i do this \n",
    "\n",
    "    #Add large polygon/meadow feature type sez id\n",
    "    merged_df['SEZ_ID']=merged_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "    merged_df = merged_df.dropna(subset=['SEZ_ID'])\n",
    "\n",
    "    merged_df[key] = df\n",
    "    \n",
    "    return {'merged_df':merged_df,\n",
    "            'cleaned_data':cleaned_data,\n",
    "            'most_recent_data':most_recent_data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DONT USE FOR NOW --maybe for final points.. Try SEZ_Type to say what indicators are.. so it would be Foresteddata, Forested+ Riverine(perennial), Riverine(Perennial), Channeled MEadow, Non_Channeled meadow\n",
    "#MIGHT NOT USE\n",
    "#Forest and Riverine Indicators\n",
    "#Name dataframes so we can reference later\n",
    "forestriverine= {'dfbanks': dfbanks, \n",
    "                    'dfaveraged_biotic':averaged_biotic_df,\n",
    "                    'dfditch': dfditch,\n",
    "                    'dfinvasive': dfinvasive,\n",
    "                    'dfhabitat': dfhabitat,\n",
    "                    'dfincision': dfincision,\n",
    "                    'dfheadcuts': dfheadcuts,\n",
    "                    'dfAOP': dfAOP\n",
    "}\n",
    "                \n",
    "#Forest Indicators\n",
    "forest= {'dfbanks': dfbanks, \n",
    "            'dfditch': dfditch,\n",
    "            'dfhabitat': dfhabitat,\n",
    "            'dfheadcuts': dfheadcuts,\n",
    "}\n",
    "#Channeled Meadow Indicators\n",
    "channeled= {'dfbanks': dfbanks, \n",
    "             'dfaveraged_biotic':averaged_biotic_df,\n",
    "                'dfconifer': dfconifer,\n",
    "                'dfditch': dfditch,\n",
    "                'dfinvasive': dfinvasive,\n",
    "                'dfhabitat': dfhabitat,\n",
    "                'dfvegetation': dfvegetation,\n",
    "                'dfincision': dfincision,\n",
    "                'dfheadcuts': dfheadcuts,\n",
    "                'dfAOP': dfAOP\n",
    "}\n",
    "\n",
    "\n",
    "#Non-Channeled Meadow Indicators\n",
    "NonChanneled= {'dfconifer': dfconifer,\n",
    "                'dfditch': dfditch,\n",
    "                'dfinvasive': dfinvasive,\n",
    "                'dfhabitat': dfhabitat,\n",
    "                'dfvegetation': dfvegetation,\n",
    "                'dfheadcuts': dfheadcuts\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Calculate Scores for Each SEZ This will go into table All_SEZ_Scores\n",
    "\n",
    "#FOR MEADOWS\n",
    "\n",
    "\n",
    "#mergedmeadow_df['SEZ_ID']=mergedmeadow_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "# Identify columns that contain 'score' in their names\n",
    "mergedmeadow_df = meadowdata_processed['merged_df']\n",
    "score_columns = [col for col in mergedmeadow_df.columns if 'Score' in col]\n",
    "\n",
    "# Sum the values across identified score columns to get the final points for the SEZ\n",
    "mergedmeadow_df['Final_Total_Points'] = mergedmeadow_df[score_columns].sum(axis=1, skipna=True)\n",
    "\n",
    "#POints Possible per SEZ\n",
    "\n",
    "mergedmeadow_df['Final_Points_Possible'] = mergedmeadow_df[score_columns].notna().sum(axis=1)*12\n",
    "\n",
    "\n",
    "#ASsign Meadow SEZ_ID to each Assessment_Unit\n",
    "\n",
    "mergedmeadow_df['SEZ_ID']=mergedmeadow_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "mergedmeadow_df['SEZ_ID'] = mergedmeadow_df['SEZ_ID'].astype(str)\n",
    "\n",
    "#--------------------------------------\n",
    "# Calculate the final rating\n",
    "#------------------------------------\n",
    "mergedmeadow_df['Final_Percent'] = mergedmeadow_df['Final_Total_Points'] / mergedmeadow_df['Final_Points_Possible']\n",
    "\n",
    "mergedmeadow_df['Final_Rating']= mergedmeadow_df['Final_Percent'].apply(rate_SEZ)\n",
    "\n",
    "mergedmeadow_df['Final_Score']= mergedmeadow_df['Final_Rating'].apply(score_indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Scores for Each SEZ This will go into final table\n",
    "\n",
    "#FOR RIVERINE SEZ\n",
    "# Identify columns that contain 'score' in their names\n",
    "mergedriverine_df = riverinedata_processed['merged_df']\n",
    "score_columns = [col for col in mergedriverine_df.columns if 'Score' in col]\n",
    "\n",
    "# Sum the values across identified score columns to get the final points for the SEZ\n",
    "mergedriverine_df['Final_Total_Points'] = mergedriverine_df[score_columns].sum(axis=1, skipna=True)\n",
    "\n",
    "#POints Possible per SEZ\n",
    "\n",
    "mergedriverine_df['Final_Points_Possible'] = mergedriverine_df[score_columns].notna().sum(axis=1)*12\n",
    "\n",
    "\n",
    "#ASsigndriverine SEZ_ID to each Assessment_Unit\n",
    "\n",
    "mergedriverine_df['SEZ_ID']=mergedriverine_df['Assessment_Unit_Name'].map(lookup_riverine)\n",
    "\n",
    "mergedriverine_df['SEZ_ID'] = mergedriverine_df['SEZ_ID'].astype(int)\n",
    "\n",
    "#FOR QA purposes add SEZ_type so we can make sure lal data is available\n",
    "#mergedriverine_df['SEZ_Type'] = dfSEZ['SEZ_Type']\n",
    "\n",
    "#--------------------------------------\n",
    "# Calculate the final rating\n",
    "#------------------------------------\n",
    "mergedriverine_df['Final_Percent'] = mergedriverine_df['Final_Total_Points'] / mergedriverine_df['Final_Points_Possible']\n",
    "\n",
    "mergedriverine_df['Final_Rating']= mergedriverine_df['Final_Percent'].apply(rate_SEZ)\n",
    "\n",
    "mergedriverine_df['Final_Score']= mergedriverine_df['Final_Rating'].apply(score_indicator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(dfSEZ.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate mergedmeadow_df and mergedriverine_df\n",
    "riverinemeadowdf = pd.concat([mergedmeadow_df, mergedriverine_df], axis=0, ignore_index=True)\n",
    "\n",
    "#riverinemeadowdf['SEZ_ID']= riverinemeadowdf['SEZ_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "riverinemeadowdf_duplicate_sez = riverinemeadowdf[riverinemeadowdf.duplicated(subset='SEZ_ID', keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NOTS WORKINNG... WILL NEED THIS for FINAL TABLEsezsurveytable grab comments to add to dataframe--MAYBE JUST DO THIS MANUALLY\n",
    "#Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "sezsurveyfields = ['Assessment_Unit_Name', 'Comments', 'Survey_Date']\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "# Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(sezsurveytable, sezsurveyfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "sezsurvey_df= pd.DataFrame(data, columns=sezsurveyfields)\n",
    "\n",
    "sezsurvey_df['Year'] = sezsurvey_df['Survey_Date'].dt.year\n",
    "print(sezsurvey_df)\n",
    "#add comments to dataframe\n",
    "# Create a dictionary to map Assessment_Unit_Name to Comments\n",
    "comments_map = dict(zip(sezsurvey_df['Assessment_Unit_Name'], sezsurvey_df['Comments']))\n",
    "\n",
    "# Add the Comments column to both_df using the map function\n",
    "riverinemeadowdf['Comments'] = riverinemeadowdf['Assessment_Unit_Name'].map(comments_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Join Base data to riverine and meadow data\n",
    "\n",
    "Final_mergeddf =pd.merge(dfSEZinfo,riverinemeadowdf, on='SEZ_ID', how='outer', indicator= True)\n",
    "\n",
    "Final_mergeddf['Year'] = '2024'\n",
    "Final_mergeddf.to_csv('ready_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_mergeddf['_merge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final set up\n",
    "\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Threshold Year',\n",
    "    'Comments': 'Comments',\n",
    "    'SEZ_Type': 'SEZ_Type',\n",
    "    'Final_Rating': 'Final_Rating',\n",
    "    'Final_Percent': 'Final_Percent',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'Final_Points_Possible': 'Final_Points_Possible',\n",
    "    'Final_Total_Points': 'Final_Total_Points',\n",
    "    'Acres': 'Acres',\n",
    "    '':'AquaticOrganismPassage_Barriers',\n",
    "    '': 'AquaticOrganismPassage_NumberOf',\n",
    "    '': 'AquaticOrganismPassage_Score',\n",
    "    '': 'AquaticOrganismPassage_StreamMiles',\n",
    "    '': 'Bank_Stability_Data_Source',\n",
    "    '': 'Bank_Stability_Percent_Unstable',\n",
    "    '': 'Bank_Stability_Rating',\n",
    "    '': 'Bank_Stability_Score',\n",
    "    '': 'Biotic_Integrity_Rating',\n",
    "    '': 'Biotic_Integrity_CSCI',\n",
    "    '': 'Biotic_Integrity_Data_Source',\n",
    "    '': 'Biotic_Integrity_Score',\n",
    "    '': 'Comments',\n",
    "    '': 'Conifer_Encroachment_Percent_En',\n",
    "    '': 'Conifer_Encroachment_Data_Sourc',\n",
    "    '': 'Conifer_Encroachment_Rating',\n",
    "    '': 'Conifer_Encroachment_Score',\n",
    "    '': 'Conifer_Encroachment_Comments',\n",
    "    '': 'CountAttachments',\n",
    "    '': 'Ditches_Data_Source',\n",
    "    '': 'Ditches_Length',\n",
    "    '': 'Ditches_Meadow_Length',\n",
    "    '': 'Ditches_Percent',\n",
    "    '': 'Ditches_Rating',\n",
    "    '': 'Ditches_Score',\n",
    "    '': 'Feature_Type',\n",
    "    '': 'Habitat_Fragmentation_Data_Sour',\n",
    "    '': 'Habitat_Fragmentation_Imperviou',\n",
    "    '': 'Habitat_Fragmentation_Percent_I',\n",
    "    '': 'Habitat_Fragmentation_Rating',\n",
    "    '': 'Habitat_Fragmentation_Score',\n",
    "    '': 'Headcuts_Data_Source',\n",
    "    '': 'Headcuts_Number_of_Headcuts',\n",
    "    '': 'Headcuts_Rating',\n",
    "    '': 'Headcuts_Score',\n",
    "    '': 'Incision_Data_Source',\n",
    "    '': 'Incision_Rating',\n",
    "    '': 'Incision_Score',\n",
    "    '': 'Incision_Ratio',\n",
    "    '': 'Invasive_Percent_Cover',\n",
    "    '': 'Invasive_Rating',\n",
    "    '': 'Invasives_Data_Source',\n",
    "    '': 'Invasives_Invasives_Number_of_Invasives',\n",
    "    '': 'Invasives_Plant_Types',\n",
    "    '': 'Invasives_Scores',\n",
    "    '': 'NDVI_ID',\n",
    "    '': 'Ownership_Primary',\n",
    "    '': 'Ownership_Secondary',\n",
    "    '': 'Ownership_Secondary_2',\n",
    "    '': 'Ownership_Secondary_3',\n",
    "    '': 'VegetationVigor_DataSource',\n",
    "    '': 'VegetationVigor_Rating',\n",
    "    '': 'VegetationVigor_Raw',\n",
    "    '': 'VegetationVigor_Score',\n",
    "}\n",
    "# Rename fields based on field mappings\n",
    "readydf = both_df.rename(columns=field_mapping).drop(columns=[col for col in both_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Assuming ready_df is the DataFrame you want to write to CSV\n",
    "readydf.to_csv('ready_df.csv', index=False)\n",
    "\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "#data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "#field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(allsezscores, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the current working directory\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Habitat Condition use mergedriverine_df but add IPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import IPI Data and add it to mergedriverine_df\n",
    "IPIfolder = \"F:\\Research and Analysis\\Fisheries\\Streams\\Bioassessment\"\n",
    "IPI22 = os.path.join(IPIfolder, \"2022\", \"IPI_22.csv\")\n",
    "IPI20 = os.path.join(IPIfolder, \"2020\", \"IPI_20.csv\")\n",
    "\n",
    "#Create IPI Dataframes\n",
    "IPI22df = pd.read_csv(IPI22)\n",
    "IPI20df = pd.read_csv(IPI20)\n",
    "\n",
    "IPI22df['IPIYear']= '2022'\n",
    "IPI20df['IPIYear']= '2020'\n",
    "#Merge dataframes into one \n",
    "# Concatenate IPIdf1 and IPIdf2 along rows (axis=0)\n",
    "concatIPI_df = pd.concat([IPI22df, IPI20df], axis=0, ignore_index=True)\n",
    "\n",
    "#Calculate Scores in IPI\n",
    "#Code for Grading IPI\n",
    "#Define Grade for IPI Score - Used only for Stream HAbitat Condition\n",
    "def categorize_phab(IPI):\n",
    "     if   IPI >= 0.94:\n",
    "        return 'A'\n",
    "     elif 0.83 < IPI < 0.94:\n",
    "        return 'B'\n",
    "     elif 0.7 < IPI <= 0.83:\n",
    "        return 'C'\n",
    "     else:\n",
    "        return 'D'\n",
    "\n",
    "concatIPI_df['IPI_Rating']=concatIPI_df['IPI'].apply(categorize_phab)\n",
    "concatIPI_df['IPI_Score']= concatIPI_df['IPI_Rating'].apply(score_indicator)\n",
    "\n",
    "\n",
    "\n",
    "concatIPI_df.head()\n",
    "\n",
    "columns_to_keep = ['StationCode', 'IPI', 'IPI_Rating', 'IPI_Score', 'IPIYear']\n",
    "\n",
    "concatIPI_df = concatIPI_df[columns_to_keep]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS MATCHES THE BIOTIC INTEGRITY SCORES, next code block will do a spatial join with the data\n",
    "\n",
    "#prep biotic data so that we have a station code to match to so we have an sez assessment unit\n",
    "dfbiotic['StationCode'] = dfbiotic['Biotic_Integrity_Data_Source'].str.split(',').str[1].str.strip()\n",
    "\n",
    "sezipi_df = pd.merge(dfbiotic, concatIPI_df, on='StationCode', how='outer')\n",
    "\n",
    "\n",
    "sezipi_df = sezipi_df.dropna(subset=['IPI'])\n",
    "\n",
    "# Define a function to get the most recent score for each assessment unit\n",
    "def get_most_recent_scores(reduced_dfs):\n",
    "    \n",
    "# Group by 'Assessment_Unit_Name' and find the row with the maximum 'Year' or do i want an average?\n",
    "most_recent_IPI = df.loc[df.groupby('Assessment_Unit_Name')['Year'].idxmax()]\n",
    "\n",
    "#AVerage IPI for sez assessment unit\n",
    "\n",
    "print(most_recent_IPI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep mergedriverine_df calculate stream  maybe match scores to biotic integrity since those will all get phab?\n",
    "#mergedriverine_df = StationCode so I can match data from IPI df to merged riverine\n",
    "#mergedmeadow_df = pd.merge(mergedmeadow_df, df, on='Assessment_Unit_Name', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code to make it a spatially enabled dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to conver to spatially enable geodataframe\n",
    "# Create a new DataFrame Final_df with columns based on the field mapping\n",
    "#df = df.rename(columns=field_mapping)\n",
    "\n",
    "# Convert DataFrame to Spatially Enabled DataFrame\n",
    "#sedf = GeoAccessor.from_xy(df, x_column='LONGITUDE', y_column='LATITUDE')\n",
    "\n",
    "# Convert the SEDF to a feature class without sanitizing columns\n",
    "#sedf.spatial.to_featureclass(location=os.path.join(workspace, 'PlanAreaNoise_23_Staging'), sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
