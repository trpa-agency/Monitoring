{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install arcgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureSet, GeoAccessor, GeoSeriesAccessor, FeatureLayer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.geometry import SpatialReference\n",
    "#import geopandas as gpd to use spatial.reference stuff\n",
    "gis = GIS()\n",
    "# # Set Pandas display options to show all rows and columns\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# in memory output file path\n",
    "wk_memory = \"memory\" + \"\\\\\"\n",
    "\n",
    "\n",
    "# set workspace and sde connections \n",
    "#working_folder = \"C:\\GIS\"\n",
    "\n",
    "#workspace      = \"C:\\GIS\\Scratch.gdb\"\n",
    "#arcpy.env.workspace = \"C:\\GIS\\Scratch.gdb\"\n",
    "#workspace ='F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "#arcpy.env.workspace = 'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "\n",
    "# network path to connection files\n",
    "filePath = \"F:\\\\GIS\\\\DB_CONNECT\"\n",
    "\n",
    "#path to GDB's to update and master data\n",
    "master_path = \"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "SEZ_Master = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "#set workspace for connection to GDB\n",
    "#workspace=master_path\n",
    "# database file paths \n",
    "### SDE Collection New data collected is put into SDE.Survey under the indicator name\n",
    "### SDE Vector is where the data will go \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "#sdeBase    = os.path.join(filePath, \"SarahVector.sde\")\n",
    "#sdeCollect = os.path.join(filepath, \"SarahCollect.sde\")\n",
    "\n",
    "# setup connection string???\n",
    "#connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=sql12;DATABASE=sde;UID=sde;PWD=staff\"\n",
    "#connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "#engine = create_engine(connection_url)\n",
    "\n",
    "#with engine.begin() as sdeConnect:\n",
    " #   erosiondf      = pd.read_sql(\"SELECT * FROM sde.SDE.Stream_Erosion\", sdeConnect)\n",
    "\n",
    "# local variables sdata is starting data and f data is finishing datatables\n",
    "ffdata = os.path.join(sdeBase, \"sde.SDE.Monitoring\")\n",
    "sdata = os.path.join(sdeCollect, \"sde.SDE.Survey\")\n",
    "fdata = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "\n",
    "##Tables we get the data from in Collect 2010-2022 globalids don'tmatch\n",
    "sezsurveytable = os.path.join(sdata, \"sde.SDE.sez_survey\")\n",
    "erosiondata = os.path.join(sdata, \"sde.SDE.Stream_Erosion\")\n",
    "incisiondata = os.path.join(sdata, \"sde.SDE.sez_channel_incision\")\n",
    "invasivedata = os.path.join(sdata, \"sde.SDE.sez_invasive_plant\")\n",
    "headcutdata = os.path.join(sdata, \"sde.SDE.sez_stream_headcut\")\n",
    "\n",
    "#make this a spatial df\n",
    "streamdata = os.path.join(ffdata, \"sde.SDE.Stream\")\n",
    "\n",
    "\n",
    "#Staging Tables currently living in SEZ_Data.GDB\n",
    "stage_bank_stability = os.path.join(master_path, \"bank_stability\") \n",
    "stage_All_SEZ_Scores = os.path.join(master_path, \"All_SEZ_Scores\")\n",
    "stage_biotic_integrity = os.path.join(master_path, \"biotic_integrity\")\n",
    "stage_headcuts = os.path.join(master_path, \"headcuts_table\")\n",
    "stage_incision = os.path.join(master_path, \"incision\")\n",
    "stage_invasives = os.path.join(master_path, \"invasives\")\n",
    "stage_vegetation = os.path.join(master_path, \"vegetation_vigor\")\n",
    "stage_conifer = os.path.join(master_path, \"conifer_encroachment\")\n",
    "stage_aquatic = os.path.join(master_path, \"aquatic_organism_passage_table\")\n",
    "stage_ditches = os.path.join(master_path, \"ditches\")\n",
    "stage_habitat = os.path.join(master_path, \"habitat_fragmentation\")\n",
    "#Final table to append to\n",
    "#finalSEZtable = os.path.join(ffdata, \"sde.SDE.SEZ_Assessment_Unit\")\n",
    "#finalSEZtable = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "\n",
    "\n",
    "# network path to connection files??????\n",
    "#filePath = \"C:\\\\GIS\\\\DB_CONNECT\"\n",
    "# network path to connection files\n",
    "#filePath = r'F:\\Research and Analysis\\Workspace\\Sarah'\n",
    "\n",
    "#--------------------------------------------#\n",
    "#Notes to self\n",
    "#--------------------------------------------#\n",
    "#F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb. \n",
    "###'Assessment_Unit_Master' has all data for SEZ\n",
    "###“Bioassessment Sample Locations” has stream sites in SEZs\n",
    "\n",
    "gdbworking_folder = \"F:\\GIS\\GIS_DATA\\Monitoring\"\n",
    "headcutgdbfolder = os.path.join(gdbworking_folder, \"Stream_Headcut\", \"StreamHeadcut_Survey\")\n",
    "invasivegdbfolder = os.path.join(gdbworking_folder, \"Invasive_Species\", \"Invasive_Species_Survey\")\n",
    "## GDB with Raw Data straight from S123 not in the original folder (that one is not edited)\n",
    "headcut23gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2023.gdb\")\n",
    "headcut22gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2022.gdb\")\n",
    "headcut20gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2020.gdb\")\n",
    "headcut19gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2019.gdb\")\n",
    "#erosion23gdb = os.path.join(gdbworking_folder, \"Stream_Erosion\",\"Stream_Erosion_Survey\", \"Stream_Erosion_Survey_2023.gdb\")\n",
    "#erosion22gdb = os.path.join(working_folder, \"Stream_Erosion\",\"Stream_Erosion_Survey\", \"Stream_Erosion_Survey_2022.gdb\")\n",
    "#erosion20gdb = os.path.join(working_folder, \"Stream_Erosion\",\"Stream_Erosion_Survey\", \"Stream_Erosion_Survey_2020.gdb\")\n",
    "#channelincision23gdb = os.path.join(working_folder,\"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2023.gdb\")\n",
    "#channelincision22gdb = os.path.join(working_folder, \"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2022.gdb\")\n",
    "#channelincision20gdb = os.path.join(working_folder, \"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2020.gdb\")\n",
    "invasiveplant23gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2023.gdb\")\n",
    "invasiveplant22gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2022.gdb\")\n",
    "invasiveplant20gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2020.gdb\")\n",
    "invasiveplant19gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2019.gdb\")\n",
    "\n",
    "#This is thelocatoin for the final SEZ GDB to be updated in the gdb on f drive in the AssessmentUnits Master (polygon) i believe\n",
    "#FinalGDBtoupdate:F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data \\SEZ_Data.gdb\n",
    "\n",
    "#Location of USFS Invasive Species Data\n",
    "#https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer\n",
    "\n",
    "#Monitoring Dashboard location\n",
    "#Finalsdelocation:f'Vector.SDE' Sde.Monitoring Sde. SEZ_Assessment_Unit\n",
    "\n",
    "#Threshold Location? sde.tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't Use-Lookup Dictionary for SEZ ID's -Old code uses AssessmentUnit_Master feature class in SEZ_Data.gdb- don't use for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------#\n",
    "#Create look up dictionary for SEZ_ID fill in--currently works for stream erosion code--\n",
    "#-----------------------------------------------------------------------------------#\n",
    "#Add in SEZ_Survey table to match up parentglobalid to assessment unit name so we can match an sez_id\n",
    "#----------------------------------------------------------------##Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "# Set workspace environment\n",
    "arcpy.env.workspace = master_path\n",
    "\n",
    "# Specify the feature class name SEZ_Master\n",
    "#feature_class = \"AssessmentUnits_Master\"\n",
    "# Create a cursor to iterate over the rows in the feature class\n",
    "fields = ['Assessment_Unit_Name', 'SEZ_ID']\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "# Iterate over the rows in the second feature class and append to data list\n",
    "with arcpy.da.SearchCursor(SEZ_Master, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "# Convert the data to a Pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Define function to update SEZ ID lookup dictionary\n",
    "def update_SEZID_lookup_dict(df, lookup_dict):\n",
    "    for index, row in df.iterrows():\n",
    "        # Update SEZ_ID column in DataFrame with data from the lookup dictionary\n",
    "        df.at[index, 'SEZ_ID'] = lookup_dict.get(row['Assessment_Unit_Name'], None)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Assuming 'Assessment_Unit_Name' is the common identifier between DataFrame and lookup dictionary\n",
    "selected_columns = ['Assessment_Unit_Name', 'SEZ_ID']\n",
    "\n",
    "# Drop duplicates based on 'Assessment_Unit_Name' and keep the first occurrence\n",
    "unique_values = df.drop_duplicates(subset='Assessment_Unit_Name', keep='first')\n",
    "\n",
    "# Convert selected columns to dictionary\n",
    "lookup_dict = unique_values[selected_columns].set_index('Assessment_Unit_Name').to_dict(orient='index')\n",
    "\n",
    "# Display the dictionary\n",
    "print(lookup_dict)\n",
    "\n",
    "# Update 'SEZ_ID' in other DataFrame with data from the lookup dictionary do this for each indicator...\n",
    "#Other_df = update_SEZID_lookup_dict(Other_df, lookup_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEZ ID based off of excel lookup dictionary larger polygons only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------#\n",
    "#Create look up dictionary for SEZ_ID fill in for Meadows--currently works for stream erosion code--- change to use excel so its not messy\n",
    "#-----------------------------------------------------------------------------------#\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "excel_data = pd.read_excel(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\SEZID lookup Meadow.xlsx\")  \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "lookup_dict = {}\n",
    "\n",
    "for index, row in excel_data.iterrows():\n",
    "    lookup_dict[row['Assessment_Unit_Name']] = row['SEZ_ID']\n",
    "\n",
    "# See dictionary where keys are Assessment Unit Names and values are SEZ IDs\n",
    "print(lookup_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEZ ID Riverine feature type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------#\n",
    "#Create look up dictionary for SEZ_ID for Riverine feature type\n",
    "#-----------------------------------------------------------------------------------#\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "excel_data_riverine = pd.read_excel(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\SEZID lookup Riverine.xlsx\")  # Replace 'your_excel_file.xlsx' with the path to your Excel file\n",
    "\n",
    "#Define Empty look up dataframe\n",
    "lookup_riverine = {}\n",
    "\n",
    "for index, row in excel_data_riverine.iterrows():\n",
    "    lookup_riverine[row['Assessment_Unit_Name']] = row['SEZ_ID']\n",
    "\n",
    "print(lookup_riverine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# Display the dictionary using pprint\n",
    "pprint.pprint(lookup_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Grading each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grading for each parameter \n",
    "#Defining Grade for Bank Stability based on Erosiondf[percent_unstable]\n",
    "def categorize_erosion(Percent_Unstable):\n",
    "    if pd.isna(Percent_Unstable):\n",
    "        return np.nan\n",
    "    elif 0 <= Percent_Unstable < 5:\n",
    "        return 'A'\n",
    "    elif 5 <= Percent_Unstable < 20:\n",
    "        return 'B'\n",
    "    elif 20 <= Percent_Unstable < 50:\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'D'\n",
    "    \n",
    "#Scoring based off of grading - check this\n",
    "def score_indicator(Rating):\n",
    "    if pd.isna(Rating):\n",
    "        return np.nan\n",
    "    elif  Rating == 'A':\n",
    "        return '12'\n",
    "    elif Rating == 'B':\n",
    "        return '9'\n",
    "    elif Rating == 'C':\n",
    "        return '6'\n",
    "    else:\n",
    "        return '3'\n",
    "\n",
    "#Define Grade for Incision based off of incisino ratio\n",
    "\n",
    "def categorize_incision(bankfull_ratio):\n",
    "    if pd.isna(bankfull_ratio):\n",
    "        return np.nan\n",
    "    elif 0 <= bankfull_ratio < 1.2:\n",
    "        return 'A'\n",
    "    elif 1.2 <= bankfull_ratio < 1.6:\n",
    "        return 'B'\n",
    "    elif 1.6 <= bankfull_ratio < 2.1:\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'D'\n",
    "\n",
    "#Define Grade for Bioassessment Score\n",
    "def categorize_csci(biotic_integrity):\n",
    "     if pd.isna(biotic_integrity):\n",
    "        return np.nan\n",
    "     elif   biotic_integrity > 0.92:\n",
    "        return 'A'\n",
    "     elif 0.79 < biotic_integrity <= 0.92:\n",
    "        return 'B'\n",
    "     elif 0.62 < biotic_integrity <= 0.79:\n",
    "        return 'C'\n",
    "     else:\n",
    "        return 'D'\n",
    "\n",
    "#Define Priority List Level of Invasive Plant Species\n",
    "    \n",
    "\n",
    "# Process Data\n",
    "def rate_invasive(priority):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if (priority[3] + priority[4] == 1):\n",
    "        return 'B'  # Assign score D\n",
    "    elif (priority[3] + priority[4] == 2) or priority[1] == 1 or priority[2] == 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif (priority[3] + priority[4] >= 3)or priority[1] >= 2 or priority[2] >= 2 or (priority[1] + priority[2] >= 2):  \n",
    "       return 'D'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no invasives of any priority are present)\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Define Size for Headcut based off of headcut size\n",
    "##A = 0 headcut, B 1+small headcut\n",
    "def categorize_headcut(headcutdepth):\n",
    "    if pd.isnull(headcutdepth) or headcutdepth == 0:\n",
    "        return 'None'\n",
    "    elif 0.1 <= headcutdepth < 0.5:\n",
    "        return 'small'\n",
    "    elif 0.5 <= headcutdepth < 1:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'large'\n",
    "\n",
    "#define rating for headcut health per sez\n",
    "def rate_headcut(row):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if row['large'] >= 1:\n",
    "        return 'D'  # Assign score D\n",
    "    elif row['medium'] >= 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif row['small'] >= 1:\n",
    "       return 'B'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no headcuts)\n",
    "\n",
    "\n",
    "#define rating SEZ Rating\n",
    "def rate_SEZ(percent):\n",
    "    if 0 <= percent < .69:\n",
    "        return 'D'\n",
    "    elif .7 <= percent < .79:\n",
    "        return 'C'\n",
    "    elif .80 <= percent <= .89:\n",
    "        return 'B'\n",
    "    else:\n",
    "        return 'A'\n",
    "    \n",
    "    #Define Grade for IPI Score - Used only for Stream HAbitat Condition\n",
    "def categorize_phab(IPI):\n",
    "     if   IPI >= 0.94:\n",
    "        return 'A'\n",
    "     elif 0.83 < IPI < 0.94:\n",
    "        return 'B'\n",
    "     elif 0.7 < IPI <= 0.83:\n",
    "        return 'C'\n",
    "     else:\n",
    "        return 'D'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Erosion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Get Data\n",
    "#----------------------------------------------------------------#\n",
    "#Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "erosionfields = ['Assessment_Unit_Name', 'Shape.STLength()', 'Bank_Type', 'Survey_Date']\n",
    "#erosiondf = pd.DataFrame.from_records(data=arcpy.da.SearchCursor(erosiondata, erosionfields), columns=erosionfields)\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    " # Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(erosiondata, erosionfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "erosiondf = pd.DataFrame(data, columns=erosionfields)\n",
    "\n",
    "# Replace NaN values in 'Assessment_Unit_Name' column with 'Skylandia SEZ'\n",
    "#erosiondf['Assessment_Unit_Name'] = erosiondf['Assessment_Unit_Name'].fillna('Skylandia SEZ')\n",
    "# Replace specific values in 'Assessment_Unit_Name' column\n",
    "erosiondf['Assessment_Unit_Name'] = erosiondf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh'})\n",
    "\n",
    "\n",
    "#Add SEZ_ID column using lookup dictionary\n",
    "#erosiondf['SEZ_ID'] = erosiondf['Assessment_Unit_Name'].map(lambda x: lookup_dict.get(x, {}).get('SEZ_ID', None))\n",
    "\n",
    "#This code is for the excel look up dictionary\n",
    "erosiondf['SEZ_ID'] = erosiondf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "erosiondf['SEZ_ID'] = erosiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "erosiondf['SEZ_ID'] = erosiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#erosiondf = erosiondf.merge(df, on='Assessment_Unit_Name', how='left')\n",
    "\n",
    "#calculate year column \n",
    "erosiondf['Year'] = erosiondf['Survey_Date'].dt.year\n",
    "\n",
    "# Replace 'both_banks' with 'Both Banks' in Bank_Type column\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['both_banks', 'Both banks'], 'Both Banks' )\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['one_bank', 'One bank'], 'One Bank')\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['no_bank', 'No bank'], 'No Bank')\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Process Data\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "# Initialize variables\n",
    "erosiondf['bank_multiplier'] = erosiondf['Bank_Type'].apply(lambda x: 2 if x == 'Both Banks' else (1 if x == 'One Bank' else 0))\n",
    "\n",
    "\n",
    "# Calculate the product of 'Shape.STLength()' and 'bank_multiplier' to get the eroded banks per row\n",
    "erosiondf['eroded_banks_per_row'] = erosiondf['Shape.STLength()'] * erosiondf['bank_multiplier']\n",
    "\n",
    "# Group by Assessment_Unit_Name and year and sum the lengths of banks for each unit to get total banks assessed\n",
    "erosiondf['banks_assessed_per_unit'] = erosiondf.groupby(['Assessment_Unit_Name', 'Year'])['Shape.STLength()'].transform('sum') * 2\n",
    "\n",
    "# Group by Assessment_Unit_Name and sum the eroded banks per row for each unit\n",
    "erosiondf['SEZ_total_eroded'] = erosiondf.groupby(['Assessment_Unit_Name', 'Year'])['eroded_banks_per_row'].transform('sum')\n",
    "\n",
    "# Calculate percent unstable Bank Stability per Assessment Unit\n",
    "erosiondf['Bank_Stability_Percent_Unstable'] = (erosiondf['SEZ_total_eroded'] / erosiondf['banks_assessed_per_unit']) * 100\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score\n",
    "#----------------------------------------------------------------#\n",
    "erosiondf['Bank_Stability_Rating']=erosiondf['Bank_Stability_Percent_Unstable'].apply(categorize_erosion)\n",
    "erosiondf['Bank_Stability_Score']= erosiondf['Bank_Stability_Rating'].apply(score_indicator)\n",
    "\n",
    "erosiondf['Bank_Stability_Data_Source'] = 'TRPA'\n",
    "\n",
    "erosiondf.head()\n",
    "\n",
    "print(erosiondf)\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to bank_stability called stage_bank_stability GDB location\n",
    "#----------------------------------------------------------------#\n",
    "# Define the name of the feature class\n",
    "feature_class_name = 'bank_stability'\n",
    "\n",
    "# Define the full path to the feature class\n",
    "feature_class_path = stage_bank_stability \n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Bank_Stability_Data_Source': 'Bank_Stability_Data_Source',\n",
    "    'Bank_Stability_Percent_Unstable': 'Bank_Stability_Percent_Unstable',\n",
    "    'Bank_Stability_Rating': 'Bank_Stability_Rating',\n",
    "    'Bank_Stability_Score': 'Bank_Stability_Score',\n",
    "    'SEZ_ID': 'SEZ_ID'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "bank_stabilitydf = erosiondf.rename(columns=field_mapping).drop(columns=[col for col in erosiondf.columns if col not in field_mapping])\n",
    "\n",
    "readybankdf = bank_stabilitydf.groupby(['SEZ_ID', 'Year']).first().reset_index()\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "# Fix data type of Year so it writes to table\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y', errors='coerce')\n",
    "\n",
    "# Setting the frequency to 'Y' for year\n",
    "#readydf['Year'] = readydf['Year'].dt.to_period('Y')\n",
    "\n",
    "print(readybankdf)\n",
    "\n",
    "#Write dataframe to fc in SEZ_Data.GDB\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readybankdf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readybankdf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(stage_bank_stability, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")\n",
    "\n",
    "\n",
    "#Write dataframe to sde.collect.bank.stability eventually, current code write it to GDB in SEZ_Data.GDB\n",
    "# Set environment workspace to your SDE connection file\n",
    "#arcpy.env.workspace = master_path\n",
    "\n",
    "# Convert DataFrame to Feature Class\n",
    "#output_feature_class = \"ErosionUpdate\"  # Name for the output feature class\n",
    "#output_fc_path = os.path.join(arcpy.env.workspace, output_feature_class)\n",
    "\n",
    "# Assuming your DataFrame is already converted to a feature class\n",
    "# Replace \"path_to_your_feature_class\" with the actual path to your feature class\n",
    "#arcpy.conversion.TableToTable(\"path_to_your_feature_class\", arcpy.env.workspace, output_feature_class)\n",
    "\n",
    "# Overwrite Feature Class in SDE\n",
    "# Replace \"path_to_your_dataframe\" with the actual path to your DataFrame\n",
    "#arcpy.management.CopyFeatures(\"path_to_your_dataframe\", output_fc_path)\n",
    "\n",
    "\n",
    "#print(ready_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to CSV\n",
    "#csv_path = os.path.join(working_folder, \"Erosiondatamaster.csv\")\n",
    "#ready_df.to_csv(csv_path, index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Incision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Get Data-- all the data we need is in sde.collect.sde.survey.sde.sez_survey\n",
    "#----------------------------------------------------------------##Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "incisionfields = ['Assessment_Unit_Name', 'incision_ratio', 'survey_date']\n",
    "#incisiondf = pd.DataFrame.from_records(data=arcpy.da.SearchCursor(erosiondata, erosionfields), columns=erosionfields)\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    " # Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(sezsurveytable, incisionfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "incisiondf = pd.DataFrame(data, columns=incisionfields)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Add correct info to dataframe\n",
    "#----------------------------------------------------------------#\n",
    "#use this until we fix the domain\n",
    "incisiondf['Assessment_Unit_Name'] = incisiondf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh'})\n",
    "# Create a new column 'SEZ ID'\n",
    "#This code is for the excel look up dictionary\n",
    "incisiondf['SEZ_ID'] = incisiondf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#calculate year column \n",
    "incisiondf['Year'] = incisiondf['survey_date'].dt.year\n",
    "\n",
    "\n",
    "#incisiondf['SEZ ID'] = np.nan\n",
    "\n",
    "# Iterate through the rows in incisiondf\n",
    "#for index, row in incisiondf.iterrows():\n",
    " #   parentglobalid = row['parentglobalid']\n",
    "    \n",
    "      # Check if the parent_global_id exists in the lookup dictionary\n",
    "  #  if parentglobalid in lookup_dict:\n",
    "        # If it exists, retrieve the corresponding SEZ ID and fill it into SEZ ID column\n",
    "   #     corresponding_entry = lookup_dict[parentglobalid]\n",
    "    #    assert row['GlobalID'] == parentglobalid, \"ParentGlobalID does not match GlobalID in the lookup dictionary\"\n",
    "     #   incisiondf.at[index, 'SEZ_ID'] = corresponding_entry['SEZ_ID']\n",
    "\n",
    "# Display the updated incisiondf\n",
    "print(incisiondf)\n",
    "\n",
    "#Add SEZ_ID column using lookup dictionary\n",
    "#incisiondf['SEZ_ID'] = SEZIDdf['GlobalID'].map(lambda x: lookup_dict.get(x, {}).get('SEZ_ID', None))\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "#incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "#incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#incisiondf = incisiondf.merge(df, on='Assessment_Unit_Name', how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score\n",
    "#----------------------------------------------------------------#\n",
    "incisiondf['Incision_Rating']=incisiondf['incision_ratio'].apply(categorize_incision)\n",
    "incisiondf['Incision_Score']= incisiondf['Incision_Rating'].apply(score_indicator)\n",
    "\n",
    "incisiondf['Incision_Data_Source'] = 'TRPA'\n",
    "\n",
    "incisiondf.head()\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to incision called stage_incision GDB location\n",
    "#----------------------------------------------------------------#\n",
    "# Define the name of the feature class\n",
    "feature_class_name = 'incision'\n",
    "\n",
    "# Define the full path to the feature class\n",
    "feature_class_path = stage_incision \n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Incision_Data_Source': 'Incision_Data_Source',\n",
    "    'incision_ratio': 'Incision_Ratio',\n",
    "    'Incision_Rating': 'Incision_Rating',\n",
    "    'Incision_Score': 'Incision_Score',\n",
    "    'SEZ_ID': 'SEZ_ID'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "incisionfinaldf = incisiondf.rename(columns=field_mapping).drop(columns=[col for col in incisiondf.columns if col not in field_mapping])\n",
    "\n",
    "readyincisiondf = incisionfinaldf.groupby(['SEZ_ID', 'Year']).first().reset_index()\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readyincisiondf)\n",
    "\n",
    "#Write dataframe to fc in SEZ_Data.GDB\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readyincisiondf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readyincisiondf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(stage_incision, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invasive Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calfora Data points--dates only go to 2012?\n",
    "# dates only go to 2012 hereCArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines07/MapServer/1\"\n",
    "#CArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines27/MapServer/0\"\n",
    "# Create a feature layer object\n",
    "#cafeature_layer = FeatureLayer(CArest)\n",
    "#Query caflora layer to that it only shows 2020-2023 or only county?\n",
    "#start_year = 'January 1, 2020'\n",
    "#end_year= 'December 31, 2023'\n",
    "# Convert to datetime objects\n",
    "#start_date = pd.to_datetime(start_year)\n",
    "#end_date = pd.to_datetime(end_year)\n",
    "\n",
    "#Query before or after??\n",
    "#query = (CAsdf['Date_']>= start_date) & (CAsdf['Date_']<= end_date)\n",
    "\n",
    "#CA_filtereddf = CAsdf.join[query].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code using REST Service- most likely will reuse this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from arcgis.features import FeatureLayer\n",
    "from utils import get_fs_data_spatial_query\n",
    "\n",
    "# Define the USFS REST endpoint\n",
    "usfsrest = \"https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer/0\"\n",
    "where    = \"FS_UNIT_ID = '0519'\"\n",
    "\n",
    "# Query the feature layer\n",
    "sdfUSFS = get_fs_data_spatial_query(usfsrest, where)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spatial join of sdf and sez master\n",
    "\n",
    "\n",
    "# Create the spatially enabled DataFrame (sdf) for target feature SEZ assessment units\n",
    "SEZsdf = pd.DataFrame.spatial.from_featureclass(SEZ_Master)\n",
    "#spatial reference stuff\n",
    "sdfUSFS.spatial.sr = SEZsdf.spatial.sr\n",
    "\n",
    "#CAsdf.spatial.set_spatial_reference(SEZsdf.spatial.sr)\n",
    "#perform spatial join\n",
    "usfsdata = SEZsdf.spatial.join(sdfUSFS, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(invasivedf['plant_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "#SET UP DATA WRANGLE\n",
    "#-----------------------------------------\n",
    "\n",
    "#Path to external data usfs with rest service--This assumes rest service is up to date to 2023\n",
    "\n",
    "# Paths to the feature classes\n",
    "invasive19fc = os.path.join(invasiveplant19gdb, \"Invasive_Species_2019\")\n",
    "invasive20fc = os.path.join(invasiveplant20gdb, \"Invasive_Species_2020\")\n",
    "invasive22fc = os.path.join(invasiveplant22gdb, \"Invasive_Species_2022\")\n",
    "invasive23fc = os.path.join(invasiveplant23gdb, \"sez_invasive_plant\")\n",
    "\n",
    "\n",
    "#Use gdb because they have assessment unit name in them... 2022 i had to manually add in PRO\n",
    "invasive23fields = ['Assessment_Unit', 'invasives_percent_cover','invasives_plant_type', 'Species_Level', 'invasive_type_other', 'created_date']\n",
    "invasive22fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover', 'InvasiveType_Other', 'Survey_Date']\n",
    "invasive20fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover',  'Other', 'Survey_Date']\n",
    "invasive19fields = ['SITE_ID', 'SURVEY_DATE', 'INVASIVE_PLANT', 'PERCENT_COVER' ]\n",
    "usfsfields = ['Assessment_Unit_Name', 'COMMON_NAME', 'SCIENTIFIC_NAME']\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "\n",
    "# Function to read feature class into DataFrame\n",
    "def feature_class_to_dataframe(feature_class, fields):\n",
    "    data = [row for row in arcpy.da.SearchCursor(feature_class, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Read feature classes into DataFrames\n",
    "# Read feature classes into DataFrames\n",
    "invasive19df = feature_class_to_dataframe(invasive19fc, invasive19fields)\n",
    "invasive20df = feature_class_to_dataframe(invasive20fc, invasive20fields)\n",
    "invasive22df = feature_class_to_dataframe(invasive22fc, invasive22fields)\n",
    "invasive23df = feature_class_to_dataframe(invasive23fc, invasive23fields)\n",
    "# Create a new DataFrame with only the specified columns\n",
    "# Convert SpatialDataFrame to DataFrame\n",
    "\n",
    "usfsdf = usfsdata[usfsfields]\n",
    "#usfsdf = usfsdata.drop(columns='SHAPE')\n",
    "\n",
    "print(usfsdf)\n",
    "#usfs23df = feature_class_to_dataframe(usfsdata, usfsfields)\n",
    "\n",
    "\n",
    "invasive19df['Source'] = 'TRPA'\n",
    "invasive20df['Source'] = 'TRPA'\n",
    "invasive22df['Source'] = 'TRPA'\n",
    "invasive23df['Source'] = 'TRPA'\n",
    "usfsdf['Source'] = 'USFS'\n",
    "\n",
    "#usfsdf['Year'] = '2023'\n",
    "\n",
    "\n",
    "# Rename fields\n",
    "invasive19df.rename(columns={'SITE_ID': 'Assessment_Unit_Name', 'INVASIVE_PLANT': 'plant_type', 'PERCENT_COVER': 'percent_cover', 'SURVEY_DATE': 'created_date','Source':'Source'}, inplace=True)\n",
    "invasive20df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date', 'Source':'Source'}, inplace=True)\n",
    "invasive22df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'InvasiveType_Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date','Source':'Source'}, inplace=True)\n",
    "invasive23df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'invasives_percent_cover': 'percent_cover', 'invasives_plant_type': 'plant_type', 'invasive_type_other': 'other', 'created_date': 'created_date','Source':'Source'}, inplace=True)\n",
    "usfsdf.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'COMMON_NAME': 'plant_type', 'Source':'Source'}, inplace=True)\n",
    "#Remove null plant types for usfs data\n",
    "usfsdf = usfsdf[~usfsdf['plant_type'].isna()]\n",
    "\n",
    "\n",
    "# Concatenate DataFrames\n",
    "invasivedf = pd.concat([usfsdf, invasive19df, invasive20df, invasive22df, invasive23df], ignore_index=True)\n",
    "# Display merged DataFrame\n",
    "print(invasivedf)\n",
    "\n",
    "\n",
    "#Define SEZ ID based on Assessment_Unit_Name for QA on SEZ Name \n",
    "invasivedf['SEZ_ID'] = invasivedf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "#invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].astype(int)\n",
    "invasivedf['Assessment_Unit_Name'] = invasivedf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky Meadows meadow - 1': 'Sky meadows'})\n",
    "# Remove rows where data source is USFS and plant type is NaN\n",
    "#invasivedf = invasivedf[~((invasivedf['Source'] == 'USFS') & invasivedf['plant_type'].isna())]\n",
    "\n",
    "# Set 'Year' column based on data source\n",
    "\n",
    "invasivedf.loc[invasivedf['Source'] == 'USFS', 'Year'] = '2023'\n",
    "invasivedf.loc[invasivedf['Source'] == 'TRPA', 'Year'] = invasivedf['created_date'].dt.year\n",
    "\n",
    "#invasivedf.reset_index(drop=True, inplace=True)\n",
    "invasivedf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#invasivedf = invasivedf.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip this\n",
    "invasivedf_values_unique = invasivedf.values.flatten()\n",
    "is_unique = len(invasivedf_values_unique) == len(set(invasivedf_values_unique))\n",
    "print(is_unique)\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = invasivedf[invasivedf.duplicated(keep=False)]\n",
    "\n",
    "# Display duplicate rows\n",
    "print(duplicate_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------#\n",
    "    #Prep Plant_type Data\n",
    "#---------------------------#\n",
    "#Make a dataframe to capture 'other' plants in trpa data and then add it to invasive df\n",
    "other_plants_df = invasivedf[['Source', 'Year', 'SEZ_ID', 'Assessment_Unit_Name', 'Year', 'other']].copy()\n",
    "\n",
    "#Get rid of Null values\n",
    "other_plants_df = other_plants_df[~other_plants_df['other'].isna()]\n",
    "\n",
    "#invasivedf.reset_index(drop=True, inplace=True)\n",
    "other_plants_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "#Rename 'other to plant_type\n",
    "other_plants_df.rename(columns={'other': 'plant_type'}, inplace=True)\n",
    "\n",
    "#invasivedf = invasivedf.drop_duplicates()\n",
    "\n",
    "\n",
    "# Concatenate other_plants_df with invasivedf JUST DO THIS MANUALLY \n",
    "#invasivedf = pd.concat([invasivedf, other_plants_df], ignore_index=True)\n",
    "\n",
    "#Append\n",
    "#invasivesdf=invasivedf.append(other_plants_df)\n",
    "#Concatenate the new DataFrame with the existing invasivedf\n",
    "#invasivedf = pd.concat([invasivedf, other_plants_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Replace various representations of null values with 'none'\n",
    "null_representations = ['<null>', '<Null>', '', 'NA', 'N/A', 'nan', 'NaN', 'None', 'NULL', None]\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace(null_representations, 'none')\n",
    "\n",
    "# Split plant types by comma and create new rows\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.split(pat=',')\n",
    "invasivedf = invasivedf.explode('plant_type')\n",
    "\n",
    "# Capitalize the first word and replace underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.split('_').str[0].str.capitalize() + ' ' + invasivedf['plant_type'].str.split('_').str[1:].str.join(' ')\n",
    "\n",
    "# Remove spaces after the words and capitalize the first word while replacing underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].astype(str)\n",
    "\n",
    "\n",
    "# Remove any spaces at the beginning or end of a word\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.strip()\n",
    "\n",
    "\n",
    "#---------------------#\n",
    "#These replacements are so the same weed with different common name doesn't get counted twice toward score MAY NEED TO ADD MORE DESCREPENCIES in coming years if more pop up or find more elopquent solution\n",
    "#---------------------#\n",
    "# Replace 'Common mullein' with 'Wooly mullein' in the 'plant_type' column \n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common mullein', 'Wooly mullein')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Nodding plumeless thistle', 'Musk thistle')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Field bindweed', 'Common bindweed')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common st. johnswort', 'Klamathweed')\n",
    "# Replace 'Broadleaf and leaved' with 'Perennial' in the 'plant_type' column\n",
    "# Define the replacements dictionary\n",
    "replacements = {'Broadleaf Pepperweed': 'Perennial pepperweed', 'Broadleaved pepperweed': 'Perennial pepperweed'}\n",
    "\n",
    "# Replace values in the 'plant_type' column using the dictionary\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace(replacements)\n",
    "\n",
    "#Replace Sulphur Cinquefoil with sulfur\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sulphur cinquefoil', 'Sulfur cinquefoil')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sweetclover', 'White sweetclover')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Reed canary grass', 'Reed canarygrass')\n",
    "#Replace\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Salt cedar', 'Tamarisk')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Butter and eggs', 'Yellow toadflax')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Canada cottonthistle', 'Canada thistle')\n",
    "# Replace empty strings or other placeholders with NaN\n",
    "#invasivedf['plant_type'] = invasivedf['plant_type'].replace('', np.nan)\n",
    "\n",
    "# Drop duplicates based on 'Assessment_Unit_Name' and 'Year' in the remaining DataFrame\n",
    "invasivedf = invasivedf.drop_duplicates(subset=['Assessment_Unit_Name', 'Year', 'plant_type'], keep='first')\n",
    "\n",
    "\n",
    "grouped_df = invasivedf.groupby(['Assessment_Unit_Name', 'Year'])['plant_type']\n",
    "\n",
    "# Aggregate the plant types into one column separated by commas\n",
    "combined_plant_types = grouped_df.apply(lambda x: ', '.join(x)).reset_index(name='all_plant_types')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group, data in invasivedf.groupby(['Assessment_Unit_Name', 'Year']):\n",
    "    print(group)\n",
    "    print(data)\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------#\n",
    "#Create Look up dictionary to assign priority group for each plant and then use it\n",
    "#----------------------------------------------------#\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "csv_data = pd.read_csv(r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Invasives Priority lookup.csv\") \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "Invasives_lookup = {}\n",
    "\n",
    "key = 'Common'\n",
    "values = ['Scientific', 'Priority'] \n",
    "\n",
    "Invasives_lookup= csv_data.set_index(key)[values].to_dict(orient='index')\n",
    "\n",
    "# Define a custom function to map plant types to priorities\n",
    "def map_priority(plant_type):\n",
    "    if pd.isnull(plant_type):\n",
    "        return 'None' # Return NaN for NaN values\n",
    "    else:\n",
    "        # Extract the priority from the dictionary, or return 'Unknown' if not found\n",
    "        plant_info = Invasives_lookup.get(plant_type)\n",
    "        if plant_info:\n",
    "            return plant_info['Priority']\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "\n",
    "\n",
    "# Create a new column 'Priority' based on the mapping from the dictionary\n",
    "invasivedf['Priority'] = invasivedf['plant_type'].map(map_priority)\n",
    "\n",
    "print(invasivedf.columns)\n",
    "#invasivedf['Priority'] = pd.to_numeric(invasivedf['Priority'], errors='coerce')\n",
    "\n",
    "# group by assessment unit and year and summarize the priority level of plants in each unit\n",
    "invasive_summary = invasivedf.groupby(['Assessment_Unit_Name', 'Year', 'Priority','Source'], dropna=False).size().reset_index(name='Count')\n",
    "\n",
    "invasive_summary_priority = invasive_summary.pivot_table(index=['Assessment_Unit_Name', 'Year','Source'], columns='Priority', values='Count', fill_value=0)\n",
    "\n",
    "\n",
    "# Reset the index to flatten the DataFrame\n",
    "invasive_summary_priority.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Grade, Score, # of Invasives\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "# Process Data\n",
    "def rate_invasive(priority):\n",
    "    if (priority[3] + priority[4] == 1):\n",
    "        return 'B'  # Assign score D\n",
    "    elif (priority[3] + priority[4] == 2) or priority[1] == 1 or priority[2] == 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif (priority[3] + priority[4] >= 3)or priority[1] >= 2 or priority[2] >= 2 or (priority[1] + priority[2] >= 2):  \n",
    "       return 'D'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no invasives of any priority are present)\n",
    "\n",
    "# Apply the rating function to the summary DataFrame\n",
    "invasive_summary_priority['Invasives_Rating'] = invasive_summary_priority[[1, 2, 3, 4]].apply(rate_invasive, axis=1)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "invasive_summary_priority['Invasives_Score']= invasive_summary_priority['Invasives_Rating'].apply(score_indicator) \n",
    "\n",
    "#Calculate total number of invasives per sez per year\n",
    "invasive_summary_priority['Number_of_Invasives']= invasive_summary_priority[[1, 2, 3, 4]].sum(axis=1)\n",
    "\n",
    "#Define SEZ ID based on Assessment_Unit_Name for QA on SEZ Name \n",
    "invasive_summary_priority['SEZ_ID'] = invasive_summary_priority['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "invasive_summary_priority['all_plants']= combined_plant_types['all_plant_types']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Prep and post ending dataframe to invasives table in SEZ_Data.GDB\n",
    "#----------------------------------------------------------------#\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Source': 'Invasives_Data_Source',\n",
    "    'Number_of_Invasives': 'Invasives_Number_of_Invasives',\n",
    "    'Invasives_Rating': 'Invasives_Rating',\n",
    "    'Invasives_Score': 'Invasives_Scores',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'percent_cover': 'Invasives_Percent_Cover',\n",
    "    'all_plants': 'Invasives_Plant_Types',\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readyinvasivedf = invasive_summary_priority.rename(columns=field_mapping).drop(columns=[col for col in invasive_summary_priority.columns if col not in field_mapping])\n",
    "\n",
    "readyinvasivedf['SEZ_ID'] = readyinvasivedf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readyinvasivedf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readyinvasivedf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readyinvasivedf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "#with arcpy.da.InsertCursor(stage_invasives, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invasive with gdb and usfs pre joined layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invasive Species Use if need to use GDB to import data--shouldn't have to\n",
    "\n",
    "\n",
    "#Path to external data usfs \n",
    "usfsdata = os.path.join(master_path, \"usfsinvasivesez24\")\n",
    "#Path to external data Calflora aka State Park Data\n",
    "#calfloradata = os.path.join(master_path, \"\")\n",
    "\n",
    "# Paths to the feature classes\n",
    "invasive19fc = os.path.join(invasiveplant19gdb, \"Invasive_Species_2019\")\n",
    "invasive20fc = os.path.join(invasiveplant20gdb, \"Invasive_Species_2020\")\n",
    "invasive22fc = os.path.join(invasiveplant22gdb, \"Invasive_Species_2022\")\n",
    "invasive23fc = os.path.join(invasiveplant23gdb, \"sez_invasive_plant\")\n",
    "\n",
    "\n",
    "#Use gdb because they have assessment unit name in them... 2022 i had to manually add in PRO\n",
    "invasive23fields = ['Assessment_Unit', 'invasives_percent_cover','invasives_plant_type', 'Species_Level', 'invasive_type_other', 'created_date']\n",
    "invasive22fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover', 'InvasiveType_Other', 'Survey_Date']\n",
    "invasive20fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover',  'Other', 'Survey_Date']\n",
    "invasive19fields = ['SITE_ID', 'SURVEY_DATE', 'INVASIVE_PLANT', 'PERCENT_COVER' ]\n",
    "usfsfields = ['Assessment_Unit_Name', 'PLANT_COMM', 'SCIENTIFIC', 'DATE_COLLE', 'Eradicated']\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "\n",
    "# Function to read feature class into DataFrame\n",
    "def feature_class_to_dataframe(feature_class, fields):\n",
    "    data = [row for row in arcpy.da.SearchCursor(feature_class, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Read feature classes into DataFrames\n",
    "# Read feature classes into DataFrames\n",
    "invasive19df = feature_class_to_dataframe(invasive19fc, invasive19fields)\n",
    "invasive20df = feature_class_to_dataframe(invasive20fc, invasive20fields)\n",
    "invasive22df = feature_class_to_dataframe(invasive22fc, invasive22fields)\n",
    "invasive23df = feature_class_to_dataframe(invasive23fc, invasive23fields)\n",
    "usfs23df = feature_class_to_dataframe(usfsdata, usfsfields)\n",
    "\n",
    "\n",
    "invasive19df['Source'] = 'TRPA'\n",
    "invasive20df['Source'] = 'TRPA'\n",
    "invasive22df['Source'] = 'TRPA'\n",
    "invasive23df['Source'] = 'TRPA'\n",
    "usfs23df['Source'] = 'USFS'\n",
    "\n",
    "# Rename fields\n",
    "invasive19df.rename(columns={'SITE_ID': 'Assessment_Unit_Name', 'INVASIVE_PLANT': 'plant_type', 'PERCENT_COVER': 'percent_cover', 'SURVEY_DATE': 'created_date','Source':'Source'}, inplace=True)\n",
    "invasive20df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date', 'Source':'Source1'}, inplace=True)\n",
    "invasive22df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'InvasiveType_Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date','Source':'Source1'}, inplace=True)\n",
    "invasive23df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'invasives_percent_cover': 'percent_cover', 'invasives_plant_type': 'plant_type', 'invasive_type_other': 'other', 'created_date': 'created_date','Source':'Source1'}, inplace=True)\n",
    "usfs23df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'PLANT_COMM':'plant_type', 'DATE_COLLE':'created_date', 'Source':'Source2'}, inplace=True)\n",
    "\n",
    "\n",
    "# Concatenate DataFrames\n",
    "invasivedf = pd.concat([usfs23df, invasive19df, invasive20df, invasive22df, invasive23df], ignore_index=True)\n",
    "\n",
    "# Display merged DataFrame\n",
    "print(invasivedf)\n",
    "\n",
    "#Define SEZ ID based on Assessment_Unit_Name\n",
    "invasivedf['SEZ_ID'] = invasivedf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].astype(int)\n",
    "invasivedf['Assessment_Unit_Name'] = invasivedf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky Meadows meadow - 1': 'Sky meadows'})\n",
    "\n",
    "#calculate year column \n",
    "invasivedf['Year'] = invasivedf['created_date'].dt.year\n",
    "\n",
    "\n",
    "#---------------------------#\n",
    "    #Prep Data\n",
    "#---------------------------#\n",
    "# Replace 'other' or 'Other' in 'plant_type' column with values from 'other' column\n",
    "invasivedf['plant_type'] = invasivedf.apply(lambda row: row['other'] if pd.notna(row['plant_type']) and row['plant_type'].lower() in ['other', 'Other'] else row['plant_type'], axis=1)\n",
    "\n",
    "# Drop the 'other' column\n",
    "invasivedf.drop(columns=['other'], inplace=True)\n",
    "\n",
    "# Function to separate plant types and create new rows\n",
    "def separate_species(df):\n",
    "    # Split plant types by comma and create new rows\n",
    "    df['plant_type'] = df['plant_type'].str.split(',')\n",
    "    df = df.explode('plant_type')\n",
    "    return df\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "invasivedf = separate_species(invasivedf)\n",
    "\n",
    "# Remove rows where data source is USFS and plant type is NaN\n",
    "invasivedf = invasivedf[~((invasivedf['Source'] == 'USFS') & invasivedf['plant_type'].isna())]\n",
    "\n",
    "# Capitalize the first word and replace underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.split('_').str[0].str.capitalize() + ' ' + invasivedf['plant_type'].str.split('_').str[1:].str.join(' ')\n",
    "\n",
    "# Remove spaces after the words and capitalize the first word while replacing underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].astype(str)\n",
    "\n",
    "\n",
    "# Remove any spaces at the beginning or end of a word\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.strip()\n",
    "\n",
    "\n",
    "# Remove rows where data source is USFS and plant type is NaN\n",
    "invasivedf = invasivedf[~((invasivedf['Source'] == 'USFS') & invasivedf['plant_type'].isna())]\n",
    "#Remove Eradicated \n",
    "\n",
    "# Filter out rows where 'eradicated' column is 'Yes'\n",
    "invasivedf = invasivedf[invasivedf['Eradicated'] != 'Yes']\n",
    "\n",
    "#These replacements are so the same weed with different common name doesn't get counted twice toward score MAY NEED TO ADD MORE DESCREPENCIES in coming years if more pop up or find more elopquent solution\n",
    "# Replace 'Common mullein' with 'Wooly mullein' in the 'plant_type' column \n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common mullein', 'Wooly mullein')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Nodding plumeless thistle', 'Musk thistle')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Field bindweed', 'Common bindweed')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common st. johnswort', 'Klamathweed')\n",
    "# Replace 'Broadleaf and leaved' with 'Perennial' in the 'plant_type' column\n",
    "# Define the replacements dictionary\n",
    "replacements = {'Broadleaf Pepperweed': 'Perennial pepperweed', 'Broadleaved pepperweed': 'Perennial pepperweed'}\n",
    "\n",
    "# Replace values in the 'plant_type' column using the dictionary\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace(replacements)\n",
    "\n",
    "#Replace Sulphur Cinquefoil with sulfur\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sulphur cinquefoil', 'Sulfur cinquefoil')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sweetclover', 'White sweetclover')\n",
    "\n",
    "#Replace\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Salt cedar', 'Tamarisk')\n",
    "\n",
    "# Now, drop duplicates based on the specified subset of columns\n",
    "invasivedf = invasivedf.drop_duplicates(subset=['SEZ_ID', 'Year', 'plant_type'])\n",
    "\n",
    "# Reset index if needed\n",
    "invasivedf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Remove duplicates based on SEZ, Year, and plant_type\n",
    "invasivedf = invasivedf.drop_duplicates(subset=['SEZ_ID', 'Year', 'plant_type'])\n",
    "#----------------------------------------------------#\n",
    "#Create Look up dictionary to assign priority group for each plant and then use it\n",
    "#----------------------------------------------------#\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "csv_data = pd.read_csv(r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Invasives Priority lookup.csv\") \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "Invasives_lookup = {}\n",
    "\n",
    "key = 'Common'\n",
    "values = ['Scientific', 'Priority'] \n",
    "\n",
    "Invasives_lookup= csv_data.set_index(key)[values].to_dict(orient='index')\n",
    "\n",
    "# Define a custom function to map plant types to priorities\n",
    "def map_priority(plant_type):\n",
    "    if pd.isnull(plant_type):\n",
    "        return np.nan  # Return NaN for NaN values\n",
    "    else:\n",
    "        # Extract the priority from the dictionary, or return 'Unknown' if not found\n",
    "        plant_info = Invasives_lookup.get(plant_type)\n",
    "        if plant_info:\n",
    "            return plant_info['Priority']\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "\n",
    "#Create a new column [Scientific based on look up dictionary\n",
    "#invasivedf['Scientific']=invasivedf['plant_type'].map(map_priority)\n",
    "\n",
    "# Create a new column 'Priority' based on the mapping from the dictionary\n",
    "invasivedf['Priority'] = invasivedf['plant_type'].map(map_priority)\n",
    "\n",
    "#invasivedf['Priority'] = pd.to_numeric(invasivedf['Priority'], errors='coerce')\n",
    "# group by assessment unit and year and summarize the priority level of plants in each unit\n",
    "invasive_summary = invasivedf.groupby(['Assessment_Unit_Name', 'Year', 'Priority']).size().reset_index(name='Count')\n",
    "\n",
    "invasive_summary_priority2 = invasive_summary.pivot_table(index=['Assessment_Unit_Name', 'Year'], columns='Priority', values='Count', fill_value=0)\n",
    "\n",
    "# Reset the index to flatten the DataFrame\n",
    "invasive_summary_priority2.reset_index(inplace=True)\n",
    "\n",
    "#invasive_summary_priority['Source'] = invasivedf['Source']\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score, # of Headcuts\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "# Process Data\n",
    "def rate_invasive(priority):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if (priority[3] + priority[4] == 1):\n",
    "        return 'B'  # Assign score D\n",
    "    elif (priority[3] + priority[4] == 2) or priority[1] == 1 or priority[2] == 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif (priority[3] + priority[4] >= 3)or priority[1] >= 2 or priority[2] >= 2 or (priority[1] + priority[2] >= 2):  \n",
    "       return 'B'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no invasives of any priority are present)\n",
    "\n",
    "# Apply the rating function to the summary DataFrame\n",
    "invasive_summary_priority2['Invasives_Rating'] = invasive_summary_priority2.apply(rate_invasive, axis=1)\n",
    "\n",
    "#Calculate total number of invasives per sez per year\n",
    "invasive_summary_priority2['Number_of_Invasives']= invasive_summary_priority2[[1, 2, 3, 4,'Unknown']].sum(axis=1)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "invasive_summary_priority2['Invasives_Score']= invasive_summary_priority2['Invasives_Rating'].apply(score_indicator)    \n",
    "\n",
    "# make a columns in invasive summary that totals up percent cover per sez/year\n",
    "invasive_summary_priority2['Invasives_Percent_Cover'] = invasivedf.groupby(['SEZ_ID', 'Year'])['percent_cover'].sum().reset_index(drop=True)\n",
    "\n",
    "# combine the source column so that it shows all data sources that contributed to the data\n",
    "# Group invasivedf by 'SEZ_ID' and 'Year' and concatenate 'Data Source 1' values\n",
    "#data_source_1_combined = invasivedf.groupby(['SEZ_ID', 'Year'])['Source 1'].apply(lambda x: ', '.join(x.dropna())).reset_index()\n",
    "\n",
    "# Group invasivedf by 'SEZ_ID' and 'Year' and concatenate 'Data Source 2' values\n",
    "#data_source_2_combined = invasivedf.groupby(['SEZ_ID', 'Year'])['Source 2'].apply(lambda x: ', '.join(x.dropna())).reset_index()\n",
    "\n",
    "# Merge data_source_1_combined and data_source_2_combined on 'SEZ_ID' and 'Year'\n",
    "#merged_data_sources = pd.merge(data_source_1_combined, data_source_2_combined, on=['SEZ_ID', 'Year'], how='outer')\n",
    "\n",
    "# Combine 'Data Source 1' and 'Data Source 2' values with a comma separator\n",
    "#merged_data_sources['Data_Sources'] = merged_data_sources.apply(lambda row: ', '.join(filter(None, [row['Source 1'], row['Source 2']])), axis=1)\n",
    "\n",
    "# Drop the individual 'Data Source 1' and 'Data Source 2' columns\n",
    "#merged_data_sources.drop(columns=['Source 1', 'Source 2'], inplace=True)\n",
    "\n",
    "# Merge merged_data_sources with invasive_summary_priority on 'SEZ_ID' and 'Year'\n",
    "#invasive_summary_priority = pd.merge(invasive_summary_priority, merged_data_sources, on=['SEZ_ID', 'Year'], how='left')\n",
    "\n",
    "#invasive_summary_priority['Data_Sources']= invasivedfinvasivedf.groupby(['SED_ID', 'Year'])[Data Source 1 ] merge with DataSource 1 separate with comma if there are both \n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to incision called stage_incision GDB location\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Data_Sources': 'Invasives_Data_Source',\n",
    "    'Number_of_Invasives': 'Invasives_Number_of_Invasives',\n",
    "    'Invasives_Rating': 'Invasives_Rating',\n",
    "    'Invasives_Score': 'Invasives_Scores',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'percent_cover': 'Invasives_Percent_Cover',\n",
    "    'plant_type': 'Invasives_Plant_Type',\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "#readydf = invasive_summary_priority.rename(columns=field_mapping).drop(columns=[col for col in headcut_summary_sml.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "#print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "#data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "#field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(stage_headcuts, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data type of 'plant_type' column in the DataFrame\n",
    "print(\"Data type of 'plant_type' column in DataFrame:\", invasivedf['plant_type'].dtype)\n",
    "\n",
    "# Check the data type of values in the lookup dictionary\n",
    "for key, value in Invasives_lookup.items():\n",
    "    print(\"Data type of value for key\", key, \"in lookup dictionary:\", type(value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headcuts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Headcuts\n",
    "#--------------------------------#\n",
    "#Get Data from GDB's with assessment unit name \n",
    "#--------------------------------#\n",
    "\n",
    "# Paths to the feature classes\n",
    "headcut19fc = os.path.join(headcut19gdb, \"Stream_Headcut_2019\")\n",
    "headcut20fc = os.path.join(headcut20gdb, \"Stream_Headcut_2020\")\n",
    "headcut22fc = os.path.join(headcut22gdb, \"Stream_Headcut_2022\")\n",
    "headcut23fc = os.path.join(headcut23gdb, \"sez_stream_headcut\")\n",
    "\n",
    "\n",
    "#Use gdb because they have assessment unit name in them... 2022 i had to manually add in PRO\n",
    "headcut23fields = ['Assessment_Unit', 'headcut_depth', 'created_date']\n",
    "headcut22fields = ['Assessment_Unit', 'Headcut_Depth', 'synced_date']\n",
    "headcut20fields = ['Assessment_Unit_Name', 'Headcut_Depth','Survey_Date']\n",
    "headcut19fields = ['SITE_NAME', 'HEADCUT_DEPTH', 'SURVEY_DATE' ]\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "\n",
    "# Function to read feature class into DataFrame\n",
    "def feature_class_to_dataframe(feature_class, fields):\n",
    "    data = [row for row in arcpy.da.SearchCursor(feature_class, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Read feature classes into DataFrames\n",
    "# Read feature classes into DataFrames\n",
    "headcut19df = feature_class_to_dataframe(headcut19fc, headcut19fields)\n",
    "headcut20df = feature_class_to_dataframe(headcut20fc, headcut20fields)\n",
    "headcut22df = feature_class_to_dataframe(headcut22fc, headcut22fields)\n",
    "headcut23df = feature_class_to_dataframe(headcut23fc, headcut23fields)\n",
    "\n",
    "# Rename fields\n",
    "headcut19df.rename(columns={'SITE_NAME': 'Assessment_Unit_Name', 'HEADCUT_DEPTH': 'headcut_depth', 'SURVEY_DATE': 'created_date'}, inplace=True)\n",
    "headcut20df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'Headcut_Depth': 'headcut_depth', 'Survey_Date': 'created_date'}, inplace=True)\n",
    "headcut22df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'Headcut_Depth': 'headcut_depth', 'synced_date': 'created_date'}, inplace=True)\n",
    "headcut23df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'headcut_depth': 'headcut_depth', 'created_date': 'created_date'}, inplace=True)\n",
    "\n",
    "# Concatenate DataFrames\n",
    "headcutdf = pd.concat([headcut19df, headcut20df, headcut22df, headcut23df], ignore_index=True)\n",
    "\n",
    "# Display merged DataFrame\n",
    "print(headcutdf)\n",
    "\n",
    "#hopefully this is just one time\n",
    "\n",
    "\n",
    "#--------------------------------#\n",
    "#Get Data from exported table where Assessment Unit is joined---parentglobalids don't match up for years before 2023. need to redo sde.collect 2019-2022 data append.... \n",
    "#--------------------------------#\n",
    "\n",
    "#--------------------------------#\n",
    "#Get Data from SDE.Collect...instead of GDB\n",
    "#--------------------------------#\n",
    "#----------------------------------------------------------------#\n",
    "#Add correct info to dataframe\n",
    "#----------------------------------------------------------------#\n",
    "# Create a new column 'SEZ ID'\n",
    "#This code is for the excel look up dictionary---ADD last? this is goo dfor QA on assessment unit names because 0's tell you the name is wrong\n",
    "headcutdf['SEZ_ID'] = headcutdf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "headcutdf['SEZ_ID'] = headcutdf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "headcutdf['SEZ_ID'] = headcutdf['SEZ_ID'].astype(int)\n",
    "headcutdf['Assessment_Unit_Name'] = headcutdf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky meadows - 1': 'Sky Meadows'})\n",
    "\n",
    "#calculate year column \n",
    "headcutdf['Year'] = headcutdf['created_date'].dt.year\n",
    "\n",
    "#----------------------------------------------#\n",
    "# Process Data\n",
    "#----------------------------------------------#\n",
    "# assign small, medium, large to headcut\n",
    "headcutdf['Headcut_Size']=headcutdf['headcut_depth'].apply(categorize_headcut)\n",
    "\n",
    "\n",
    "# Group by 'SEZ_ID', 'Year', and 'Headcut_Size', and count the number of occurrences for each group\n",
    "headcut_summary = headcutdf.groupby(['Assessment_Unit_Name', 'Year', 'Headcut_Size']).size().reset_index(name='Count')\n",
    "\n",
    "print(type(headcut_summary))\n",
    "# Pivot the table to have 'Headcut_Size' categories as columns\n",
    "headcut_summary_sml = headcut_summary.pivot_table(index=['Assessment_Unit_Name', 'Year'], columns='Headcut_Size', values='Count', fill_value=0)\n",
    "\n",
    "# Reset the index to flatten the DataFrame\n",
    "headcut_summary_sml.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the summarized DataFrame\n",
    "print(headcut_summary_sml)\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score, # of Headcuts\n",
    "#----------------------------------------------------------------#\n",
    "    \n",
    "    # Process Data\n",
    "def rate_headcut(row):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if row['large'] >= 1:\n",
    "        return 'D'  # Assign score D\n",
    "    elif row['medium'] >= 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif row['small'] >= 1:\n",
    "       return 'B'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no headcuts)\n",
    "\n",
    "# Apply the rating function to the summary DataFrame\n",
    "headcut_summary_sml['Headcuts_Rating'] = headcut_summary_sml.apply(rate_headcut, axis=1)\n",
    "\n",
    "#Calculate total number of headcuts per sez per year\n",
    "headcut_summary_sml['Number_of_Headcuts']= headcut_summary_sml[['large', 'medium', 'small']].sum(axis=1)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "headcut_summary_sml['Headcuts_Score']= headcut_summary_sml['Headcuts_Rating'].apply(score_indicator)\n",
    "\n",
    "#Add Datasource\n",
    "headcut_summary_sml['Headcuts_Data_Source'] = 'TRPA'\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to incision called stage_incision GDB location\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Headcuts_Data_Source': 'Headcuts_Data_Source',\n",
    "    'Number_of_Headcuts': 'Number_of_Headcuts',\n",
    "    'Headcuts_Rating': 'Headcuts_Rating',\n",
    "    'Headcuts_Score': 'Headcuts_Score',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'small': 'small',\n",
    "    'medium': 'medium',\n",
    "    'large': 'large'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = headcut_summary_sml.rename(columns=field_mapping).drop(columns=[col for col in headcut_summary_sml.columns if col not in field_mapping])\n",
    "\n",
    "readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "with arcpy.da.InsertCursor(stage_headcuts, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in External Data from USFS and Calflora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------#\n",
    "#Get Data from external sources and spatially join to our data\n",
    "#--------------------------------#\n",
    "\n",
    "\n",
    "#Calfora Data\n",
    "CArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines07/MapServer/1\"\n",
    "#CAfc = os.path.join(CArest, \"Non-Native and Invasive Plants - Calflora\")\n",
    "CAsdf = pd.DataFrame.spatial.from_layer(CArest)\n",
    "# Delete the existing feature layer if it exists\n",
    "\n",
    "CA_fc = arcpy.MakeFeatureLayer_management(CArest, \"Calfora_invasives\")\n",
    "\n",
    "\n",
    "usfsrest= \"https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer/0\"\n",
    "#usfsfc =os.path.join(usfsrest, \"Current Invasive Plant Locations\") \n",
    "#usfssdf = pd.DataFrame.spatial.from_layer(usfsrest)\n",
    "if arcpy.Exists(\"usfs_layer\"):\n",
    "    arcpy.Delete_management(\"usfs_layer\")\n",
    "usfs_fc = arcpy.MakeFeatureLayer_management(usfsrest, \"usfs_layer\")\n",
    "#Spatially join usfs layers to see if any of the points or polygons intersect with our sez's123\n",
    "\n",
    "# Set the workspace environment\n",
    "arcpy.env.workspace = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Data Management 2023.gdb\"\n",
    "\n",
    "# Define the target and join layers\n",
    "target_feature = SEZ_Master #\"AssessmentUnit_Master\"\n",
    "CAjoin_feature = CA_fc\n",
    "USFSjoin_feature = usfs_fc\n",
    "\n",
    "# Define the output feature class\n",
    "out_feature_class = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Data Management 2023.gdb\\externalinvasivesez\"\n",
    "\n",
    "# Define the fields you want to keep from both layers\n",
    "#field_mappings = arcpy.FieldMappings()\n",
    "#field_mappings.addTable(target_features)\n",
    "#field_mappings.addTable(join_features)\n",
    "\n",
    "# Keep only specific fields from the target layer\n",
    "#keep_fields_target = [\"Assessment_Unit_Name\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_target:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "#Keep only specific fields from the calfora join layer\n",
    "#keep_fields_CA = [\"CName\", \"SName\", \"Source\", \"Obs_Date\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_CA:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "# Keep only specific fields from the usfs join_features\n",
    "#keep_fields_USFS = [\"SCIENTIFIC_NAME\", \"COMMON_NAME\", \"DATE_COLLECTED\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_USFS:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "\n",
    "# Perform spatial join with specified field mappings\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=SEZ_Master,\n",
    "    join_features=[CAjoin_feature],\n",
    "    out_feature_class=out_feature_class,\n",
    "    join_operation=\"JOIN_ONE_TO_MANY\",\n",
    "    join_type=\"KEEP_ALL\"\n",
    "    #field_mapping=field_mappings\n",
    ")\n",
    "\n",
    "#Create Dataframe for spatially joined data\n",
    "# Define the fields you want to retrieve from the output feature class\n",
    "fields = [\"Assessment_Unit_Name\", \"CName\", \"SName\", \"Source\", \"Obs_Date\", \"SCIENTIFIC_NAME\", \"COMMON_NAME\", \"DATE_COLLECTED\"]\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "data = []\n",
    "\n",
    "# Use SearchCursor to read data from the output feature class\n",
    "with arcpy.da.SearchCursor(out_feature_class, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the list of tuples into a pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bioassessment/ Biotic Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bioassessment scores- get all stream data into sde.Stream first... then look at Biotic Integrit Data Source to find which stream was used to evaluate each meadow.. this will help with percent of stream miles I believe\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Get Data-- all the data we need is in sde.collect.sde.survey.sde.sez_survey\n",
    "#----------------------------------------------------------------##Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "\n",
    "#Create SEDF setup\n",
    "#streamdata is the path to the feature class in sde\n",
    "# Set the workspace to your SDE connection file\n",
    "arcpy.env.workspace = streamdata\n",
    "feature_class= \"Stream\"\n",
    "\n",
    "# Convert feature class to a pandas DataFrame\n",
    "fields = [field.name for field in arcpy.ListFields(feature_class)]\n",
    "\n",
    "# Create DataFrame\n",
    "streamsdf = pd.DataFrame.spatial.from_featureclass(feature_class, columns=fields)\n",
    "\n",
    "# Create the spatially enabled DataFrame (sdf) for target feature SEZ assessment units\n",
    "SEZsdf = pd.DataFrame.spatial.from_featureclass(SEZ_Master)\n",
    "#spatial reference stuff\n",
    "streamsdf.spatial.sr = SEZsdf.spatial.sr\n",
    "\n",
    "#perform spatial join of sde.stream and sez units\n",
    "thesdf = SEZsdf.spatial.join(streamsdf, how='inner')\n",
    "\n",
    "#Notes to self, Stream Miles?\n",
    "#Keep only Riverine?, this may be what the smaller polygons are for \n",
    "# Filter for SEZ type 'Riverine'\n",
    "#riverine_df = bioticsdf[bioticsdf['Feature_Type'] == 'Riverine']\n",
    "\n",
    "#if the layer contains Riverine? or just for any spatial join.. see what it does\n",
    "#spatial join this layer to asessment unit master layer\n",
    "#ASsessment unit master layer is called SEZ_Master\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------#\n",
    "#DATA PREP\n",
    "#----------------------#\n",
    "# List of columns to keep\n",
    "columns_to_keep = ['Assessment_Unit_Name', 'SEZ_Type', 'Feature_Type', 'SEZ_ID','SITE_NAME', 'COUNT_VALUE', 'YEAR_OF_COUNT', 'STATION_TYPE', 'LONGITUDE', 'LATITUDE', ]\n",
    "##Try this instead\n",
    "# Select only the desired columns\n",
    "bioticdf = thesdf.loc[:, columns_to_keep].copy()  \n",
    "\n",
    "#DATA PREP\n",
    "# Filter for years 2020 to 2023\n",
    "filtered_df = bioticdf.loc[(bioticdf['YEAR_OF_COUNT'] >= 2020) & (bioticdf['YEAR_OF_COUNT'] <= 2023)].copy()\n",
    "\n",
    "# Define SEZ ID based on Assessment_Unit_Name for QA on SEZ Name THIS METHOD USES LARGER POLYGONES \n",
    "filtered_df.loc[:, 'SEZ_ID'] = filtered_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "filtered_df.loc[:, 'SEZ_ID'] = filtered_df['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "filtered_df.loc[:, 'SEZ_ID'] = filtered_df['SEZ_ID'].astype(int)\n",
    "\n",
    "# Replace values in the 'Assessment_Unit_Name' column\n",
    "filtered_df.loc[:, 'Assessment_Unit_Name'] = filtered_df['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky Meadows meadow - 1': 'Sky meadows'})\n",
    "\n",
    "# Add data source information\n",
    "filtered_df['Source'] = 'TRPA, ' + filtered_df['SITE_NAME'].astype(str) + ', ' + filtered_df['YEAR_OF_COUNT'].astype(str)\n",
    "\n",
    "#Group by Year and Assessment Unit and Site NAME and remove duplicates\n",
    "\n",
    "filtered_df['SITE_NAME'] = filtered_df['SITE_NAME'].str.strip()\n",
    "filtered_df['YEAR_OF_COUNT'] = filtered_df['YEAR_OF_COUNT'].astype(str).str.strip().astype(int)\n",
    "filtered_df['YEAR_OF_COUNT'] = pd.to_numeric(filtered_df['YEAR_OF_COUNT'], errors='coerce')\n",
    "\n",
    "\n",
    "# Group by Assessment_Unit_Name, SITE_NAME, and YEAR_OF_COUNT and drop duplicates\n",
    "BIdf = filtered_df.groupby(['SEZ_ID', 'SITE_NAME', 'YEAR_OF_COUNT', 'COUNT_VALUE']).apply(lambda x: x.drop_duplicates()).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_df['YEAR_OF_COUNT'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------#\n",
    "#Grade and Score biotic integrity\n",
    "#----------------------#\n",
    "\n",
    "#Rate the score\n",
    "#ef categorize_csci(biotic_integrity):\n",
    "# Apply the rating function to the summary DataFrame\n",
    "BIdf['Biotic_Rating'] = BIdf['COUNT_VALUE'].apply(categorize_csci)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "BIdf['Biotic_Score']= BIdf['Biotic_Rating'].apply(score_indicator) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Prep and post ending dataframe to invasives table in SEZ_Data.GDB\n",
    "#----------------------------------------------------------------#\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'YEAR_OF_COUNT': 'Year',\n",
    "    'Source': 'Biotic_Integrity_Data_Source',\n",
    "    'COUNT_VALUE': 'Biotic_Integrity_CSCI',\n",
    "    'Biotic_Rating': 'Biotic_Integrity_Rating',\n",
    "    'Biotic_Score': 'Biotic_Integrity_Score',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = BIdf.rename(columns=field_mapping).drop(columns=[col for col in BIdf.columns if col not in field_mapping])\n",
    "\n",
    "readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_biotic_integrity, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")\n",
    "\n",
    "#Delete duplicates yourself.. not that much data to go through, can't figure out why it won't remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conifer Encroachment Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import arcpy\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Conifer_Encroachment_Data_Sourc',\n",
    "                        'Conifer_Encroachment_Rating',                    \n",
    "                        'Conifer_Encroachment_Percent_En',\n",
    "                        'Conifer_Encroachment_Score',\n",
    "                        'SEZ_ID',\n",
    "                        'Assessment_Unit_Name',\n",
    "                        'ConiferEncroachment_Comments']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    conifer_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "conifer_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Conifer_Encroachment_Data_Sourc': 'Conifer_Encroachment_Data_Source',\n",
    "                'Conifer_Encroachment_Rating': 'Conifer_Encroachment_Rating',                    \n",
    "                'Conifer_Encroachment_Percent_En': 'Conifer_Percent_Encroached',\n",
    "                'Conifer_Encroachment_Score': 'Conifer_Encroachment_Score',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'ConiferEncroachment_Comments': 'ConiferEncroachment_Comments'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = conifer_df.rename(columns=field_mapping).drop(columns=[col for col in conifer_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_conifer, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aquatic Organism Passage STagin table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SEZ_Master.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aquatic Organism /Fish Passage- only old data for now\n",
    "import os\n",
    "import pandas as pd\n",
    "import arcpy\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['AquaticOrganismPassage_Barriers',\n",
    "                        'AquaticOrganismPassage_DataSour',                    \n",
    "                        'AquaticOrganismPassage_NumberOf',\n",
    "                        'AquaticOrganismPassage_Rating',\n",
    "                        'SEZ_ID',\n",
    "                        'Assessment_Unit_Name',\n",
    "                        'AquaticOrganismPassage_Score',\n",
    "                        'AquaticOrganismPassage_StreamMi']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    AOP_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "AOP_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'AquaticOrganismPassage_Barriers': 'AOP_BarriersPerMile',\n",
    "                'AquaticOrganismPassage_DataSour': 'AOP_DataSource',                    \n",
    "                'AquaticOrganismPassage_NumberOf': 'AOP_NumberofBarriers',\n",
    "                'AquaticOrganismPassage_Score': 'AOP_Score',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'AquaticOrganismPassage_StreamMi': 'AOP_StreamMiles',\n",
    "                'AquaticOrganismPassage_Rating': 'AOP_Rating'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = AOP_df.rename(columns=field_mapping).drop(columns=[col for col in AOP_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_aquatic, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Habitat Fragmentation Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create STaging Tables for External Data Put in STagin Table\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Habitat_Fragmentation_Data_Sour',\n",
    "                        'Habitat_Fragmentation_Imperviou',\n",
    "                        'Habitat_Fragmentation_Percent_I',\n",
    "                        'Habitat_Fragmentation_Rating',\n",
    "                        'Habitat_Fragmentation_Score',\n",
    "                        'SEZ_ID', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    HabFrag_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "HabFrag_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Habitat_Fragmentation_Data_Sour': 'Habitat_Frag_Data_Source',\n",
    "                'Habitat_Fragmentation_Percent_I': 'HAbitat_Frag_Percent_Impervious',                    \n",
    "                'Habitat_Fragmentation_Score': 'Habitat_Frag_Score',\n",
    "                'Habitat_Fragmentation_Imperviou': 'Habitat_Frag_Impervious_Acres',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'Habitat_Fragmentation_Rating': 'Habitat_Frag_Rating'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = HabFrag_df.rename(columns=field_mapping).drop(columns=[col for col in HabFrag_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_habitat, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ditches Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create STaging Tables for External Data Put in STagin Table\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Ditches_Data_Source',\n",
    "                        'Ditches_Length',\n",
    "                        'Ditches_Meadow_Length',\n",
    "                        'Ditches_Percent',\n",
    "                        'Ditches_Rating',\n",
    "                        'Ditches_Score',\n",
    "                        'SEZ_ID', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    Ditch_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "Ditch_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Ditches_Data_Source': 'Ditches_Data_Source',\n",
    "                'Ditches_Length': 'Ditches_Length',                    \n",
    "                'Ditches_Meadow_Length': 'Ditches_Meadow_Length',\n",
    "                'Ditches_Percent': 'Ditches_Percent',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'Ditches_Rating': 'Ditches_Rating',\n",
    "                'Ditches_Score': 'Ditches_Score'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = Ditch_df.rename(columns=field_mapping).drop(columns=[col for col in Ditch_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_ditches, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vegetation Vigor- old data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create STaging Tables for External Data Put in STagin Table\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['VegetationVigor_DataSource',\n",
    "                        'NDVI_ID',\n",
    "                        'VegetationVigor_Raw',\n",
    "                        'VegetationVigor_Rating',\n",
    "                        'VegetationVigor_Score',\n",
    "                        'SEZ_ID', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    vegetation_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "vegetation_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'VegetationVigor_DataSource': 'VegetationVigor_DataSource',\n",
    "                'NDVI_ID': 'NDVI_ID',                    \n",
    "                'VegetationVigor_Raw': 'VegetationVigor_Raw',\n",
    "                'VegetationVigor_Rating': 'VegetationVigor_Rating',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'VegetationVigor_Score': 'VegetationVigor_Score'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = vegetation_df.rename(columns=field_mapping).drop(columns=[col for col in vegetation_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_vegetation, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEZ data from 2020 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All SEZ Scores from current Data\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Acres',\n",
    "                        'Final_Percent',\n",
    "                        'Final_Points_Possible',\n",
    "                        'Final_Rating',\n",
    "                        'Final_Total_Points',\n",
    "                        'SEZ_ID',\n",
    "                        'Comments',\n",
    "                        'SEZ_Type', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    SEZ20_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "SEZ20_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Acres': 'Acres',\n",
    "                'SEZ_Type': 'SEZ_Type',                    \n",
    "                'Final_Percent': 'Final_Percent',\n",
    "                'Final_Total_Points': 'Final_Total_Points',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'Final_Points_Possible': 'Final_Points_Possible',\n",
    "                'Final_Rating': 'Final_Rating',\n",
    "                'Comments': 'Comments'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = SEZ20_df.rename(columns=field_mapping).drop(columns=[col for col in SEZ20_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_All_SEZ_Scores, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final SEZ Scores Calculations for SEZ Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Data with REST SERVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Paths to Staging tables in SDE... via REST service\n",
    "# Use rest service to get data \n",
    "#Setup\n",
    "import pandas as pd\n",
    "import os\n",
    "from arcgis.features import FeatureLayer\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def get_fs_data(fs_url):\n",
    "    feature_layer = FeatureLayer(fs_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "def get_fs_data_spatial(fs_url):\n",
    "    feature_layer = FeatureLayer(fs_url)\n",
    "    query_result = feature_layer.query().sdf\n",
    "    query_result.spatial.sr = 26910\n",
    "    return query_result\n",
    "\n",
    "\n",
    "#def get_fs_data(SEZ_url):\n",
    " #   feature_layer = FeatureLayer(SEZ_url)\n",
    "  #  query_result = feature_layer.query()\n",
    "   # feature_list = query_result.features\n",
    "    #all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    #return all_data\n",
    "\n",
    "bank_stability_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/4\"\n",
    "biotic_integrity_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/5\"\n",
    "conifer_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/6\"\n",
    "ditches_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/7\"\n",
    "invasives_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/11\"\n",
    "Hab_Frag_url = 'https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/8'\n",
    "vegetation_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/12\"\n",
    "incision_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/10\"\n",
    "headcuts_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/9\"\n",
    "AOP_url= \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/3\"\n",
    "#all_sez_scores_url = \"\"\n",
    "\n",
    "SEZ_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/0\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Where data will be appended\n",
    "#All_SEZ_Scores_df = table_to_dataframe(os.path.join(master_path, \"All_SEZ_Scores\"))\n",
    "\n",
    "#Where do the comments come from??? Maybe SDE Collect in SEZ Site information? sde.sez_survey\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframes from Rest Services\n",
    "\n",
    "dfbanks = get_fs_data(bank_stability_url)\n",
    "dfbiotic = get_fs_data(biotic_integrity_url)\n",
    "dfconifer = get_fs_data(conifer_url)\n",
    "dfditch = get_fs_data(ditches_url)\n",
    "dfinvasive = get_fs_data(invasives_url)\n",
    "dfhabitat = get_fs_data(Hab_Frag_url)\n",
    "dfvegetation = get_fs_data(vegetation_url)\n",
    "dfincision = get_fs_data(incision_url)\n",
    "dfheadcuts = get_fs_data(headcuts_url)\n",
    "dfAOP = get_fs_data(AOP_url)\n",
    "dfSEZ = get_fs_data_spatial(SEZ_url)\n",
    "#dfscores = get_fs_data(all_sez_scores_url)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data in staging tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "#Biotic Integrity\n",
    "#------------------#\n",
    "#Prep data- Add any scores and find average oif there are two stream sites for one sez.\n",
    "\n",
    "# Assuming dfbiotic is already loaded and has columns 'Assessment_Unit_Name', 'Year', and 'biotic_integrity_score'\n",
    "\n",
    "# Function to average scores for each Year and Assessment_Unit_Name\n",
    "def average_biotic_scores(dfbiotic, unit_col='Assessment_Unit_Name', year_col='Year', score='Biotic_Integrity_Score'):\n",
    "    # Convert year to numeric, if not already\n",
    "    #df[year_col] = pd.to_numeric(df[year_col], errors='coerce')\n",
    "    # Group by Assessment Unit and Year and calculate the mean of the scores\n",
    "    averaged_df = dfbiotic.groupby([unit_col, year_col], as_index=False)[score].mean()\n",
    "    #return averaged_df\n",
    "\n",
    "# Merge the averaged scores back with the original DataFrame to include other columns\n",
    "    merged_df = pd.merge(averaged_df, dfbiotic, on=[unit_col, year_col], suffixes=('_avg', ''))\n",
    "    \n",
    "    # Drop duplicates to keep only one entry per combination of unit and year\n",
    "    merged_df = merged_df.drop_duplicates(subset=[unit_col, year_col])\n",
    "    \n",
    "    return merged_df\n",
    "# Apply the function to dfbiotic\n",
    "averaged_biotic_df = average_biotic_scores(dfbiotic)\n",
    "\n",
    "# Output the resulting DataFrame\n",
    "print(averaged_biotic_df)\n",
    "\n",
    "\n",
    "\n",
    "#Keep only score column and SEZ ID and Year for each df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------\n",
    "#Headcuts \n",
    "#------------------\n",
    "#Reorganize dfHeadcuts to drop small medium large headcut columns\n",
    "# Drop the columns 'small', 'medium', and 'large'\n",
    "dfheadcuts = dfheadcuts.drop(columns=['small', 'medium', 'large'])\n",
    "\n",
    "# Print the DataFrame to see the changes\n",
    "print(dfheadcuts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add year to data source so we can drop the year column later\n",
    "\n",
    "#Create Dictionary of Dataframes to adjust year to be in datashource column and not its own column\n",
    "yeartodatasource = {\n",
    "    'dfbanks': dfbanks,\n",
    "    'dfheadcuts': dfheadcuts,\n",
    "    'dfincision': dfincision,\n",
    "    'dfinvasive': dfinvasive\n",
    "}\n",
    "\n",
    "# Iterate over each DataFrame in meadowdata\n",
    "for name, df in yeartodatasource.items():\n",
    "    # Iterate over columns in the DataFrame\n",
    "    for col in df.columns:\n",
    "        # Check if the column name contains 'Data'\n",
    "        if 'Data_' in col:\n",
    "            # Add Year to the column if it contains 'Data'\n",
    "            df[col] = df[col] + ', ' + df['Year'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't USE This is for All SEZ Scores Only no othe rdata.. DONT USE unless need all _sez_Score table with just scores of Indicator and SEZ no floof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DONT USEReduce all dataframes so they include assessment unit, year and score only\n",
    "\n",
    "# Define a function to keep only the required columns\n",
    "def keep_required_columns(df, score_col, unit_col='Assessment_Unit_Name', year_col='Year'):\n",
    "    reduced_df = df[[unit_col, year_col, score_col]].copy()  # Ensure a copy is made to avoid SettingWithCopyWarning\n",
    "    print(f\"\\nColumns after reduction for score column '{score_col}': {reduced_df.columns.tolist()}\")\n",
    "    print(reduced_df.head())  # Display the first few rows of the reduced DataFrame for inspection\n",
    "    return reduced_df\n",
    "\n",
    "# List of DataFrames along with their corresponding score columns\n",
    "dfs_info = [\n",
    "    (dfbanks, 'Bank_Stability_Score'),\n",
    "    (averaged_biotic_df, 'Biotic_Integrity_Score'),\n",
    "    (dfconifer, 'Conifer_Encroachment_Score'),\n",
    "    (dfditch, 'Ditches_Score'),\n",
    "    (dfinvasive, 'Invasives_Scores'),\n",
    "    (dfhabitat, 'Habitat_Frag_Score'),\n",
    "    (dfvegetation, 'VegetationVigor_Score'),\n",
    "    (dfincision, 'Incision_Score'),\n",
    "    (dfheadcuts, 'Headcuts_Score'),\n",
    "    (dfAOP, 'AOP_Score')\n",
    "]\n",
    "\n",
    "# List to store reduced DataFrames\n",
    "reduced_dfs = []\n",
    "\n",
    "# Keep only required columns for each DataFrame and inspect\n",
    "for df, score_col in dfs_info:\n",
    "    reduced_df = keep_required_columns(df, score_col)\n",
    "    reduced_dfs.append(reduced_df)\n",
    "\n",
    "# Verify the content of reduced_dfs\n",
    "for i, df in enumerate(reduced_dfs):\n",
    "    print(f\"\\nReduced DataFrame {i} for score column '{dfs_info[i][1]}':\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT USEDefine a function to get the most recent score for each assessment unit\n",
    "def get_most_recent_scores(reduced_dfs):\n",
    "    most_recent_data = []\n",
    "    for df in reduced_dfs:\n",
    "        # Group by 'Assessment_Unit_Name' and find the row with the maximum 'Year'\n",
    "        most_recent_data_df = df.loc[df.groupby('Assessment_Unit_Name')['Year'].idxmax()]\n",
    "\n",
    "        # Append the result to the list\n",
    "        most_recent_data.append(most_recent_data_df)\n",
    "\n",
    "    return most_recent_data\n",
    "\n",
    "# Example usage:\n",
    "most_recent_data = get_most_recent_scores(reduced_dfs)\n",
    "\n",
    "\n",
    "print(most_recent_data)\n",
    "\n",
    "\n",
    "#Define Year with which indicator and Data Source?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT USE Do I need to do this??Define a function to drop 'Year' column from each DataFrame\n",
    "def drop_year_column(dataframes):\n",
    "    for df in dataframes:\n",
    "        if 'Year' in df.columns:\n",
    "            df.drop(columns=['Year'], inplace=True)\n",
    "\n",
    "# Drop 'Year' column from each DataFrame in most_recent_data\n",
    "drop_year_column(most_recent_data)\n",
    "\n",
    "# Example: Print the first few rows of each modified DataFrame to verify\n",
    "for i, df in enumerate(most_recent_data):\n",
    "    print(f\"DataFrame {i+1}:\")\n",
    "    print(df.head())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedmeadow_df.SEZ_ID.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT USEMEAdow DAtaframe including all indicators\n",
    "#Meadow FEature Type Merge\n",
    "\n",
    "\n",
    "# Merge DataFrames in most_recent_data based on 'Assessment_Unit_Name'\n",
    "mergedmeadow_df = most_recent_data[0]  # Initialize merged_df with the first DataFrame\n",
    "\n",
    "for df in most_recent_data[1:]:\n",
    "    mergedmeadow_df = pd.merge(mergedmeadow_df, df, on='Assessment_Unit_Name', how='outer')\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(mergedmeadow_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT USECreate RiverineIndicators list containing specific dataframes\n",
    "RiverineIndicators = ['Assessment_Unit_Name', 'AOP_Score', 'Bank_Stability_Score', 'Biotic_Integrity_Score', 'Habitat_Frag_Score', 'Incision_Score', 'Headcuts_Score']\n",
    "\n",
    "mergedriverine_df= mergedmeadow_df[RiverineIndicators]\n",
    "\n",
    "print(mergedriverine_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT USECalculate Scores for Each SEZ This will go into table All_SEZ_Scores\n",
    "\n",
    "#FOR MEADOWS\n",
    "\n",
    "\n",
    "#mergedmeadow_df['SEZ_ID']=mergedmeadow_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "# Identify columns that contain 'score' in their names\n",
    "score_columns = [col for col in mergedmeadow_df.columns if 'Score' in col]\n",
    "\n",
    "# Sum the values across identified score columns to get the final points for the SEZ\n",
    "mergedmeadow_df['Final_Total_Points'] = mergedmeadow_df[score_columns].sum(axis=1, skipna=True)\n",
    "\n",
    "#POints Possible per SEZ\n",
    "\n",
    "mergedmeadow_df['Final_Points_Possible'] = mergedmeadow_df[score_columns].notna().sum(axis=1)*12\n",
    "\n",
    "\n",
    "#ASsign Meadow SEZ_ID to each Assessment_Unit\n",
    "\n",
    "mergedmeadow_df['SEZ_ID']=mergedmeadow_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "#FOR QA purposes add SEZ_type so we can make sure lal data is available\n",
    "mergedmeadow_df['SEZ_Type'] = dfSEZ['SEZ_Type']\n",
    "#Would it be better to just add up how many indicators have a score? 12 x5?\n",
    "#mergedmeadow_df['Final_Points_Possible']= dfSEZ['Final_Points_Possible']\n",
    "\n",
    "# Merge based on 'SEZID'\n",
    "#mergedmeadow_df = pd.merge(mergedmeadow_df, dfSEZ[['SEZ_ID', 'Final_Points_Possible']], on='SEZ_ID', how='left')\n",
    "\n",
    "# Assign 'Final_Points_Possible' from dfSEZ to mergedmeadow_df\n",
    "#mergedmeadow_df['Final_Points_Possible'] = mergedmeadow_df['Final_Points_Possible']\n",
    "#or? just base of of how many indicators are not null for each row?\n",
    "#mergedmeadow_df['Final_Points_Possible2']= (12 x number of columns per row that say score have data?) \n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------\n",
    "# Calculate the final rating\n",
    "#------------------------------------\n",
    "mergedmeadow_df['Final_Percent'] = mergedmeadow_df['Final_Total_Points'] / mergedmeadow_df['Final_Points_Possible']\n",
    "\n",
    "mergedmeadow_df['Final_Rating']= mergedmeadow_df['Final_Percent'].apply(rate_SEZ)\n",
    "\n",
    "mergedmeadow_df['Final_Score']= mergedmeadow_df['Final_Rating'].apply(score_indicator)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT USECalculate Scores for Each SEZ This will go into table All_SEZ_Scores\n",
    "\n",
    "#FOR RIVERINE SEZ\n",
    "# Identify columns that contain 'score' in their names\n",
    "score_columns = [col for col in mergedriverine_df.columns if 'Score' in col]\n",
    "\n",
    "# Sum the values across identified score columns to get the final points for the SEZ\n",
    "mergedriverine_df['Final_Total_Points'] = mergedriverine_df[score_columns].sum(axis=1, skipna=True)\n",
    "\n",
    "#POints Possible per SEZ\n",
    "\n",
    "mergedriverine_df['Final_Points_Possible'] = mergedriverine_df[score_columns].notna().sum(axis=1)*12\n",
    "\n",
    "\n",
    "#ASsigndriverine SEZ_ID to each Assessment_Unit\n",
    "\n",
    "mergedriverine_df['SEZ_ID']=mergedriverine_df['Assessment_Unit_Name'].map(lookup_riverine)\n",
    "\n",
    "#FOR QA purposes add SEZ_type so we can make sure lal data is available\n",
    "mergedriverine_df['SEZ_Type'] = dfSEZ['SEZ_Type']\n",
    "\n",
    "#--------------------------------------\n",
    "# Calculate the final rating\n",
    "#------------------------------------\n",
    "mergedriverine_df['Final_Percent'] = mergedriverine_df['Final_Total_Points'] / mergedriverine_df['Final_Points_Possible']\n",
    "\n",
    "mergedriverine_df['Final_Rating']= mergedriverine_df['Final_Percent'].apply(rate_SEZ)\n",
    "\n",
    "mergedriverine_df['Final_Score']= mergedriverine_df['Final_Rating'].apply(score_indicator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT USEMerge Riverine and Meadow dataframes for a final dataframe\n",
    "\n",
    "# Concatenate DataFrames doesn't work-try merging--Do we want to make an All_sez_scores\n",
    "both_df = pd.concat([mergedriverine_df, mergedmeadow_df], axis=0, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTS WORKINNG... WILL NEED THIS for FINAL TABLEsezsurveytable grab comments to add to dataframe--MAYBE JUST DO THIS MANUALLY\n",
    "#Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "sezsurveyfields = ['Assessment_Unit_Name', 'Comments', 'Survey_Date']\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "# Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(sezsurveytable, sezsurveyfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "sezsurvey_df= pd.DataFrame(data, columns=sezsurveyfields)\n",
    "\n",
    "sezsurvey_df['Year'] = sezsurvey_df['Survey_Date'].dt.year\n",
    "print(sezsurvey_df)\n",
    "#add comments to dataframe\n",
    "# Create a dictionary to map Assessment_Unit_Name to Comments\n",
    "comments_map = dict(zip(sezsurvey_df['Assessment_Unit_Name'], sezsurvey_df['Comments']))\n",
    "\n",
    "# Add the Comments column to both_df using the map function\n",
    "both_df['Comments'] = both_df['Assessment_Unit_Name'].map(comments_map)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#DONT USE\n",
    "# both_df['Year']= '2024'\n",
    "\n",
    "both_df = both_df.dropna(subset='SEZ_ID')\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Prep and post ending dataframe to All Scores table in SEZ_Data.GDB\n",
    "#----------------------------------------------------------------#\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Comments': 'Comments',\n",
    "    'SEZ_Type': 'SEZ_Type',\n",
    "    'Final_Rating': 'Final_Rating',\n",
    "    'Final_Percent': 'Final_Percent',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'Final_Points_Possible': 'Final_Points_Possible',\n",
    "    'Final_Total_Points': 'Final_Total_Points'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = both_df.rename(columns=field_mapping).drop(columns=[col for col in both_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Assuming ready_df is the DataFrame you want to write to CSV\n",
    "readydf.to_csv('ready_df.csv', index=False)\n",
    "\n",
    "allscoresdf=readydf\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "#data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "#field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(allsezscores, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDE.SEZ Assessment_Unit final table with all info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def get_most_recent_scores(df,groupfield):\n",
    "    df = df.loc[df.groupby(groupfield)['Year'].idxmax()]\n",
    "    return df\n",
    "\n",
    "def get_most_recent_and_clean(data_frames, lookup_dict, columns_to_drop):\n",
    "    most_recent_data = {}\n",
    "\n",
    "    # Iterate over the items in the original 'meadowdata' dictionary\n",
    "    for key, df in data_frames.items():\n",
    "        # Apply the 'get_most_recent_scores' function to each DataFrame\n",
    "        processed_df = get_most_recent_scores(df, 'Assessment_Unit_Name')\n",
    "        \n",
    "        # Store the processed DataFrame in the new dictionary using the same key\n",
    "        most_recent_data[key] = processed_df\n",
    "    \n",
    "# Columns to drop\n",
    "\n",
    "    # Drop specified columns and remove duplicate rows\n",
    "    cleaned_data = {}\n",
    "    for key, df in most_recent_data.items():\n",
    "        # Drop specified columns if they exist\n",
    "        df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "        \n",
    "        # Remove duplicate rows\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        # Append the cleaned DataFrame to the list\n",
    "        cleaned_data[key] = df\n",
    "    # Merge all DataFrames on 'Assessment_Unit_Name' using reduce\n",
    "    merged_df = reduce(lambda left, right: pd.merge(left, right, on='Assessment_Unit_Name', how='outer'), cleaned_data.values())\n",
    "\n",
    "    #Add large polygon/meadow feature type sez id\n",
    "    merged_df['SEZ_ID']=merged_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "    merged_df = merged_df.dropna(subset=['SEZ_ID'])\n",
    "    return {'merged_df':merged_df,\n",
    "            'cleaned_data':cleaned_data,\n",
    "            'most_recent_data':most_recent_data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same for meadow and riverine data\n",
    "columns_to_drop = {'Year', 'SEZ_ID', 'GlobalID', 'last_edited_user', 'created_date', 'OBJECTID', 'created_user', 'last_edited_date'}\n",
    "\n",
    "#first run set up for all scores so you have feature service data from staging tables \n",
    "meadowdata= {'dfbanks': dfbanks, \n",
    "             'dfaveraged_biotic':averaged_biotic_df,\n",
    "                'dfconifer': dfconifer,\n",
    "                'dfditch': dfditch,\n",
    "                'dfinvasive': dfinvasive,\n",
    "                'dfhabitat': dfhabitat,\n",
    "                'dfvegetation': dfvegetation,\n",
    "                'dfincision': dfincision,\n",
    "                'dfheadcuts': dfheadcuts,\n",
    "                'dfAOP': dfAOP\n",
    "}\n",
    "\n",
    "\n",
    "#Staging Tables Riverine--add in allscoresdf when that is ready\n",
    "riverinedata = {'dfheadcuts': dfbanks,\n",
    "                'dfbiotic': averaged_biotic_df,\n",
    "                'dfhabitat': dfhabitat,\n",
    "                'dfincision': dfincision,\n",
    "                'dfheadcuts': dfheadcuts,\n",
    "                'dfAOP': dfAOP\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meadowdata_processed = get_most_recent_and_clean(meadowdata, lookup_dict, columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the same for Riverine only indicators\n",
    "\n",
    "riverinedata_processed = get_most_recent_and_clean(riverinedata, lookup_riverine, columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of how to access the processed DataFrames\n",
    "riverinedata_processed['merged_df'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Scores for Each SEZ This will go into table All_SEZ_Scores\n",
    "\n",
    "#FOR MEADOWS\n",
    "\n",
    "\n",
    "#mergedmeadow_df['SEZ_ID']=mergedmeadow_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "# Identify columns that contain 'score' in their names\n",
    "mergedmeadow_df = meadowdata_processed['merged_df']\n",
    "score_columns = [col for col in mergedmeadow_df.columns if 'Score' in col]\n",
    "\n",
    "# Sum the values across identified score columns to get the final points for the SEZ\n",
    "mergedmeadow_df['Final_Total_Points'] = mergedmeadow_df[score_columns].sum(axis=1, skipna=True)\n",
    "\n",
    "#POints Possible per SEZ\n",
    "\n",
    "mergedmeadow_df['Final_Points_Possible'] = mergedmeadow_df[score_columns].notna().sum(axis=1)*12\n",
    "\n",
    "\n",
    "#ASsign Meadow SEZ_ID to each Assessment_Unit\n",
    "\n",
    "mergedmeadow_df['SEZ_ID']=mergedmeadow_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "mergedmeadow_df['SEZ_ID'] = mergedmeadow_df['SEZ_ID'].astype(str)\n",
    "\n",
    "#--------------------------------------\n",
    "# Calculate the final rating\n",
    "#------------------------------------\n",
    "mergedmeadow_df['Final_Percent'] = mergedmeadow_df['Final_Total_Points'] / mergedmeadow_df['Final_Points_Possible']\n",
    "\n",
    "mergedmeadow_df['Final_Rating']= mergedmeadow_df['Final_Percent'].apply(rate_SEZ)\n",
    "\n",
    "mergedmeadow_df['Final_Score']= mergedmeadow_df['Final_Rating'].apply(score_indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Scores for Each SEZ This will go into final table\n",
    "\n",
    "#FOR RIVERINE SEZ\n",
    "# Identify columns that contain 'score' in their names\n",
    "mergedriverine_df = riverinedata_processed['merged_df']\n",
    "score_columns = [col for col in mergedriverine_df.columns if 'Score' in col]\n",
    "\n",
    "# Sum the values across identified score columns to get the final points for the SEZ\n",
    "mergedriverine_df['Final_Total_Points'] = mergedriverine_df[score_columns].sum(axis=1, skipna=True)\n",
    "\n",
    "#POints Possible per SEZ\n",
    "\n",
    "mergedriverine_df['Final_Points_Possible'] = mergedriverine_df[score_columns].notna().sum(axis=1)*12\n",
    "\n",
    "\n",
    "#ASsigndriverine SEZ_ID to each Assessment_Unit\n",
    "\n",
    "mergedriverine_df['SEZ_ID']=mergedriverine_df['Assessment_Unit_Name'].map(lookup_riverine)\n",
    "\n",
    "mergedriverine_df['SEZ_ID'] = mergedriverine_df['SEZ_ID'].astype(int)\n",
    "\n",
    "#FOR QA purposes add SEZ_type so we can make sure lal data is available\n",
    "#mergedriverine_df['SEZ_Type'] = dfSEZ['SEZ_Type']\n",
    "\n",
    "#--------------------------------------\n",
    "# Calculate the final rating\n",
    "#------------------------------------\n",
    "mergedriverine_df['Final_Percent'] = mergedriverine_df['Final_Total_Points'] / mergedriverine_df['Final_Points_Possible']\n",
    "\n",
    "mergedriverine_df['Final_Rating']= mergedriverine_df['Final_Percent'].apply(rate_SEZ)\n",
    "\n",
    "mergedriverine_df['Final_Score']= mergedriverine_df['Final_Rating'].apply(score_indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfSEZ.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Prep SEZ Baseline Data for assessment unit.. does this make the polygons roll over? how do i do that ==make spatially enabled dataframe and add 'SHAPE'\n",
    "keep_columns = ['Assessment_Unit_Name', 'SHAPE', 'SEZ_ID', 'Feature_Type', 'SEZ_Type', 'Ownership_Primary', 'Ownership_Secondary', 'Ownership_Secondary_2', 'Ownership_Secondary_3', 'Acres']\n",
    "\n",
    "dfSEZinfo=dfSEZ[keep_columns]\n",
    "\n",
    "dfSEZinfo['SEZ_ID']= dfSEZinfo['SEZ_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate mergedmeadow_df and mergedriverine_df\n",
    "riverinemeadowdf = pd.concat([mergedmeadow_df, mergedriverine_df], axis=0, ignore_index=True)\n",
    "\n",
    "#riverinemeadowdf['SEZ_ID']= riverinemeadowdf['SEZ_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riverinemeadowdf_duplicate_sez = riverinemeadowdf[riverinemeadowdf.duplicated(subset='SEZ_ID', keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTS WORKINNG... WILL NEED THIS for FINAL TABLEsezsurveytable grab comments to add to dataframe--MAYBE JUST DO THIS MANUALLY\n",
    "#Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "sezsurveyfields = ['Assessment_Unit_Name', 'Comments', 'Survey_Date']\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "# Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(sezsurveytable, sezsurveyfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "sezsurvey_df= pd.DataFrame(data, columns=sezsurveyfields)\n",
    "\n",
    "sezsurvey_df['Year'] = sezsurvey_df['Survey_Date'].dt.year\n",
    "print(sezsurvey_df)\n",
    "#add comments to dataframe\n",
    "# Create a dictionary to map Assessment_Unit_Name to Comments\n",
    "comments_map = dict(zip(sezsurvey_df['Assessment_Unit_Name'], sezsurvey_df['Comments']))\n",
    "\n",
    "# Add the Comments column to both_df using the map function\n",
    "riverinemeadowdf['Comments'] = riverinemeadowdf['Assessment_Unit_Name'].map(comments_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riverin.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join Base data to riverine and meadow data\n",
    "\n",
    "Final_mergeddf =pd.merge(dfSEZinfo,riverinemeadowdf, on='SEZ_ID', how='outer', indicator= True)\n",
    "\n",
    "Final_mergeddf['Year'] = '2024'\n",
    "Final_mergeddf.to_csv('ready_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_mergeddf['_merge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final set up\n",
    "\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Threshold Year',\n",
    "    'Comments': 'Comments',\n",
    "    'SEZ_Type': 'SEZ_Type',\n",
    "    'Final_Rating': 'Final_Rating',\n",
    "    'Final_Percent': 'Final_Percent',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'Final_Points_Possible': 'Final_Points_Possible',\n",
    "    'Final_Total_Points': 'Final_Total_Points',\n",
    "    'Acres': 'Acres',\n",
    "    '':'AquaticOrganismPassage_Barriers',\n",
    "    '': 'AquaticOrganismPassage_NumberOf',\n",
    "    '': 'AquaticOrganismPassage_Score',\n",
    "    '': 'AquaticOrganismPassage_StreamMiles',\n",
    "    '': 'Bank_Stability_Data_Source',\n",
    "    '': 'Bank_Stability_Percent_Unstable',\n",
    "    '': 'Bank_Stability_Rating',\n",
    "    '': 'Bank_Stability_Score',\n",
    "    '': 'Biotic_Integrity_Rating',\n",
    "    '': 'Biotic_Integrity_CSCI',\n",
    "    '': 'Biotic_Integrity_Data_Source',\n",
    "    '': 'Biotic_Integrity_Score',\n",
    "    '': 'Comments',\n",
    "    '': 'Conifer_Encroachment_Percent_En',\n",
    "    '': 'Conifer_Encroachment_Data_Sourc',\n",
    "    '': 'Conifer_Encroachment_Rating',\n",
    "    '': 'Conifer_Encroachment_Score',\n",
    "    '': 'Conifer_Encroachment_Comments',\n",
    "    '': 'CountAttachments',\n",
    "    '': 'Ditches_Data_Source',\n",
    "    '': 'Ditches_Length',\n",
    "    '': 'Ditches_Meadow_Length',\n",
    "    '': 'Ditches_Percent',\n",
    "    '': 'Ditches_Rating',\n",
    "    '': 'Ditches_Score',\n",
    "    '': 'Feature_Type',\n",
    "    '': 'Habitat_Fragmentation_Data_Sour',\n",
    "    '': 'Habitat_Fragmentation_Imperviou',\n",
    "    '': 'Habitat_Fragmentation_Percent_I',\n",
    "    '': 'Habitat_Fragmentation_Rating',\n",
    "    '': 'Habitat_Fragmentation_Score',\n",
    "    '': 'Headcuts_Data_Source',\n",
    "    '': 'Headcuts_Number_of_Headcuts',\n",
    "    '': 'Headcuts_Rating',\n",
    "    '': 'Headcuts_Score',\n",
    "    '': 'Incision_Data_Source',\n",
    "    '': 'Incision_Rating',\n",
    "    '': 'Incision_Score',\n",
    "    '': 'Incision_Ratio',\n",
    "    '': 'Invasive_Percent_Cover',\n",
    "    '': 'Invasive_Rating',\n",
    "    '': 'Invasives_Data_Source',\n",
    "    '': 'Invasives_Invasives_Number_of_Invasives',\n",
    "    '': 'Invasives_Plant_Types',\n",
    "    '': 'Invasives_Scores',\n",
    "    '': 'NDVI_ID',\n",
    "    '': 'Ownership_Primary',\n",
    "    '': 'Ownership_Secondary',\n",
    "    '': 'Ownership_Secondary_2',\n",
    "    '': 'Ownership_Secondary_3',\n",
    "    '': 'VegetationVigor_DataSource',\n",
    "    '': 'VegetationVigor_Rating',\n",
    "    '': 'VegetationVigor_Raw',\n",
    "    '': 'VegetationVigor_Score',\n",
    "}\n",
    "# Rename fields based on field mappings\n",
    "readydf = both_df.rename(columns=field_mapping).drop(columns=[col for col in both_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Assuming ready_df is the DataFrame you want to write to CSV\n",
    "readydf.to_csv('ready_df.csv', index=False)\n",
    "\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "#data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "#field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(allsezscores, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the current working directory\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Habitat Condition use mergedriverine_df but add IPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import IPI Data and add it to mergedriverine_df\n",
    "IPIfolder = \"F:\\Research and Analysis\\Fisheries\\Streams\\Bioassessment\"\n",
    "IPI22 = os.path.join(IPIfolder, \"2022\", \"IPI_22.csv\")\n",
    "IPI20 = os.path.join(IPIfolder, \"2020\", \"IPI_20.csv\")\n",
    "\n",
    "#Create IPI Dataframes\n",
    "IPI22df = pd.read_csv(IPI22)\n",
    "IPI20df = pd.read_csv(IPI20)\n",
    "\n",
    "IPI22df['IPIYear']= '2022'\n",
    "IPI20df['IPIYear']= '2020'\n",
    "#Merge dataframes into one \n",
    "# Concatenate IPIdf1 and IPIdf2 along rows (axis=0)\n",
    "concatIPI_df = pd.concat([IPI22df, IPI20df], axis=0, ignore_index=True)\n",
    "\n",
    "#Calculate Scores in IPI\n",
    "#Code for Grading IPI\n",
    "    #Define Grade for IPI Score - Used only for Stream HAbitat Condition\n",
    "def categorize_phab(IPI):\n",
    "     if   IPI >= 0.94:\n",
    "        return 'A'\n",
    "     elif 0.83 < IPI < 0.94:\n",
    "        return 'B'\n",
    "     elif 0.7 < IPI <= 0.83:\n",
    "        return 'C'\n",
    "     else:\n",
    "        return 'D'\n",
    "\n",
    "concatIPI_df['IPI_Rating']=concatIPI_df['IPI'].apply(categorize_phab)\n",
    "concatIPI_df['IPI_Score']= concatIPI_df['IPI_Rating'].apply(score_indicator)\n",
    "\n",
    "\n",
    "\n",
    "concatIPI_df.head()\n",
    "\n",
    "columns_to_keep = ['StationCode', 'IPI', 'IPI_Rating', 'IPI_Score', 'IPIYear']\n",
    "\n",
    "concatIPI_df = concatIPI_df[columns_to_keep]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS MATCHES THE BIOTIC INTEGRITY SCORES, next code block will do a spatial join with the data\n",
    "\n",
    "#prep biotic data so that we have a station code to match to so we have an sez assessment unit\n",
    "dfbiotic['StationCode'] = dfbiotic['Biotic_Integrity_Data_Source'].str.split(',').str[1].str.strip()\n",
    "\n",
    "sezipi_df = pd.merge(dfbiotic, concatIPI_df, on='StationCode', how='outer')\n",
    "\n",
    "\n",
    "sezipi_df = sezipi_df.dropna(subset=['IPI'])\n",
    "\n",
    "# Define a function to get the most recent score for each assessment unit\n",
    "def get_most_recent_scores(reduced_dfs):\n",
    "    \n",
    "# Group by 'Assessment_Unit_Name' and find the row with the maximum 'Year' or do i want an average?\n",
    "most_recent_IPI = df.loc[df.groupby('Assessment_Unit_Name')['Year'].idxmax()]\n",
    "\n",
    "#AVerage IPI for sez assessment unit\n",
    "\n",
    "print(most_recent_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep mergedriverine_df calculate stream  maybe match scores to biotic integrity since those will all get phab?\n",
    "#mergedriverine_df = StationCode so I can match data from IPI df to merged riverine\n",
    "#mergedmeadow_df = pd.merge(mergedmeadow_df, df, on='Assessment_Unit_Name', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code to make it a spatially enabled dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to conver to spatially enable geodataframe\n",
    "# Create a new DataFrame Final_df with columns based on the field mapping\n",
    "#df = df.rename(columns=field_mapping)\n",
    "\n",
    "# Convert DataFrame to Spatially Enabled DataFrame\n",
    "#sedf = GeoAccessor.from_xy(df, x_column='LONGITUDE', y_column='LATITUDE')\n",
    "\n",
    "# Convert the SEDF to a feature class without sanitizing columns\n",
    "#sedf.spatial.to_featureclass(location=os.path.join(workspace, 'PlanAreaNoise_23_Staging'), sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
