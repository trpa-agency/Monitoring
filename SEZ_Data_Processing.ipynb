{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install arcgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureSet, GeoAccessor, GeoSeriesAccessor, FeatureLayer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.geometry import SpatialReference\n",
    "#import geopandas as gpd to use spatial.reference stuff\n",
    "gis = GIS()\n",
    "# # Set Pandas display options to show all rows and columns\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# in memory output file path\n",
    "wk_memory = \"memory\" + \"\\\\\"\n",
    "\n",
    "\n",
    "# set workspace and sde connections \n",
    "#working_folder = \"C:\\GIS\"\n",
    "\n",
    "#workspace      = \"C:\\GIS\\Scratch.gdb\"\n",
    "#arcpy.env.workspace = \"C:\\GIS\\Scratch.gdb\"\n",
    "#workspace ='F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "#arcpy.env.workspace = 'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "\n",
    "# network path to connection files\n",
    "filePath = \"F:\\\\GIS\\\\DB_CONNECT\"\n",
    "\n",
    "#path to GDB's to update and master data\n",
    "master_path = \"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "SEZ_Master = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "#set workspace for connection to GDB\n",
    "#workspace=master_path\n",
    "# database file paths \n",
    "### SDE Collection New data collected is put into SDE.Survey under the indicator name\n",
    "### SDE Vector is where the data will go \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "#sdeBase    = os.path.join(filePath, \"SarahVector.sde\")\n",
    "#sdeCollect = os.path.join(filepath, \"SarahCollect.sde\")\n",
    "\n",
    "# setup connection string???\n",
    "#connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=sql12;DATABASE=sde;UID=sde;PWD=staff\"\n",
    "#connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "#engine = create_engine(connection_url)\n",
    "\n",
    "#with engine.begin() as sdeConnect:\n",
    " #   erosiondf      = pd.read_sql(\"SELECT * FROM sde.SDE.Stream_Erosion\", sdeConnect)\n",
    "\n",
    "# local variables sdata is starting data and f data is finishing datatables\n",
    "ffdata = os.path.join(sdeBase, \"sde.SDE.Monitoring\")\n",
    "sdata = os.path.join(sdeCollect, \"sde.SDE.Survey\")\n",
    "fdata = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "\n",
    "##Tables we get the data from in Collect 2010-2022 globalids don'tmatch\n",
    "sezsurveytable = os.path.join(sdata, \"sde.SDE.sez_survey\")\n",
    "erosiondata = os.path.join(sdata, \"sde.SDE.Stream_Erosion\")\n",
    "incisiondata = os.path.join(sdata, \"sde.SDE.sez_channel_incision\")\n",
    "invasivedata = os.path.join(sdata, \"sde.SDE.sez_invasive_plant\")\n",
    "headcutdata = os.path.join(sdata, \"sde.SDE.sez_stream_headcut\")\n",
    "\n",
    "#make this a spatial df\n",
    "streamdata = os.path.join(ffdata, \"sde.SDE.Stream\")\n",
    "\n",
    "\n",
    "#Staging Tables currently living in SEZ_Data.GDB\n",
    "stage_bank_stability = os.path.join(master_path, \"bank_stability\") \n",
    "stage_All_SEZ_Scores = os.path.join(master_path, \"All_SEZ_Scores\")\n",
    "stage_biotic_integrity = os.path.join(master_path, \"biotic_integrity\")\n",
    "stage_headcuts = os.path.join(master_path, \"headcuts_table\")\n",
    "stage_incision = os.path.join(master_path, \"incision\")\n",
    "stage_invasives = os.path.join(master_path, \"invasives\")\n",
    "stage_vegetation = os.path.join(master_path, \"vegetation_vigor\")\n",
    "stage_conifer = os.path.join(master_path, \"conifer_encroachment\")\n",
    "stage_aquatic = os.path.join(master_path, \"aquatic_organism_passage_table\")\n",
    "stage_ditches = os.path.join(master_path, \"ditches\")\n",
    "stage_habitat = os.path.join(master_path, \"habitat_fragmentation\")\n",
    "#Final table to append to\n",
    "#finalSEZtable = os.path.join(ffdata, \"sde.SDE.SEZ_Assessment_Unit\")\n",
    "#finalSEZtable = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "\n",
    "\n",
    "# network path to connection files??????\n",
    "#filePath = \"C:\\\\GIS\\\\DB_CONNECT\"\n",
    "# network path to connection files\n",
    "#filePath = r'F:\\Research and Analysis\\Workspace\\Sarah'\n",
    "\n",
    "#--------------------------------------------#\n",
    "#Notes to self\n",
    "#--------------------------------------------#\n",
    "#F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb. \n",
    "###'Assessment_Unit_Master' has all data for SEZ\n",
    "###“Bioassessment Sample Locations” has stream sites in SEZs\n",
    "\n",
    "gdbworking_folder = \"F:\\GIS\\GIS_DATA\\Monitoring\"\n",
    "headcutgdbfolder = os.path.join(gdbworking_folder, \"Stream_Headcut\", \"StreamHeadcut_Survey\")\n",
    "invasivegdbfolder = os.path.join(gdbworking_folder, \"Invasive_Species\", \"Invasive_Species_Survey\")\n",
    "## GDB with Raw Data straight from S123 not in the original folder (that one is not edited)\n",
    "headcut23gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2023.gdb\")\n",
    "headcut22gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2022.gdb\")\n",
    "headcut20gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2020.gdb\")\n",
    "headcut19gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2019.gdb\")\n",
    "#erosion23gdb = os.path.join(gdbworking_folder, \"Stream_Erosion\",\"Stream_Erosion_Survey\", \"Stream_Erosion_Survey_2023.gdb\")\n",
    "#erosion22gdb = os.path.join(working_folder, \"Stream_Erosion\",\"Stream_Erosion_Survey\", \"Stream_Erosion_Survey_2022.gdb\")\n",
    "#erosion20gdb = os.path.join(working_folder, \"Stream_Erosion\",\"Stream_Erosion_Survey\", \"Stream_Erosion_Survey_2020.gdb\")\n",
    "#channelincision23gdb = os.path.join(working_folder,\"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2023.gdb\")\n",
    "#channelincision22gdb = os.path.join(working_folder, \"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2022.gdb\")\n",
    "#channelincision20gdb = os.path.join(working_folder, \"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2020.gdb\")\n",
    "invasiveplant23gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2023.gdb\")\n",
    "invasiveplant22gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2022.gdb\")\n",
    "invasiveplant20gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2020.gdb\")\n",
    "invasiveplant19gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2019.gdb\")\n",
    "\n",
    "\n",
    "#This is thelocatoin for the final SEZ GDB to be updated in the gdb on f drive in the AssessmentUnits Master (polygon) i believe\n",
    "#FinalGDBtoupdate:F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data \\SEZ_Data.gdb\n",
    "\n",
    "#Location of USFS Invasive Species Data\n",
    "#https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer\n",
    "\n",
    "#Monitoring Dashboard location\n",
    "#Finalsdelocation:f'Vector.SDE' Sde.Monitoring Sde. SEZ_Assessment_Unit\n",
    "\n",
    "#Threshold Location? sde.tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fs_data_spatial(fs_url):\n",
    "    feature_layer = FeatureLayer(fs_url)\n",
    "    query_result = feature_layer.query().sdf\n",
    "    query_result.spatial.sr = 26910\n",
    "    return query_result\n",
    "\n",
    "SEZ_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/0\"\n",
    "\n",
    "dfSEZ = get_fs_data_spatial(SEZ_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use only if need to make SEZ ID dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE LOOKUP FOR SMALL ACRES SEZ ID AND FEATURE TYPE\n",
    "# Filter Assessment_Unit_Name entries with more than one unique Acres value\n",
    "grouped = dfSEZ.groupby('Assessment_Unit_Name')\n",
    "filtered_df = grouped.filter(lambda x: x['Acres'].nunique() > 1)\n",
    "\n",
    "# Determine the minimum Acres for each Assessment_Unit_Name\n",
    "min_acres_df = filtered_df.groupby('Assessment_Unit_Name', as_index=False)['Acres'].min()\n",
    "\n",
    "# Merge to get the corresponding SEZ_ID and SEZ_Type\n",
    "result_df = pd.merge(min_acres_df, filtered_df, on=['Assessment_Unit_Name', 'Acres'])\n",
    "\n",
    "# Select columns of interest for the CSV\n",
    "columns_of_interest = ['Assessment_Unit_Name', 'SEZ_ID', 'SEZ_Type']\n",
    "\n",
    "# Create a new DataFrame with selected columns\n",
    "df_selected = result_df[columns_of_interest].copy()\n",
    "\n",
    "# Specify the file path where you want to save the CSV file\n",
    "csv_file_path = 'Small_Polygon_Lookup.csv'\n",
    "\n",
    "# Save the selected data to CSV file\n",
    "df_selected.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"CSV file saved successfully to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SEZID and SEZType Lookup Dictionaries Small Polygone, Large Polygon, All Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Angora Creek - tributary': 519, 'Angora Creek - upper': 520, 'Angora meadows - 1': 87, 'Angora meadows - 2': 90, 'Angora meadows - 3': 91, 'Angora meadows - 4': 143, 'Angora meadows - 5': 144, 'Angora meadows - 6': 142, 'Angora meadows - 7': 89, 'Angora meadows - 8': 88, 'Angora meadows - 9': 92, 'Angora meadows tributary - 1': 217, 'Angora meadows tributary - 2': 99, 'Angora meadows tributary - 3': 97, 'Angora meadows tributary - 4': 94, 'Angora meadows tributary - 5': 214, 'Angora meadows tributary - 6': 146, 'Angora meadows tributary - 7': 93, 'Angora meadows tributary - 8': 95, 'Angora meadows tributary - 9': 96, 'Angora tributary': 446, 'Antone meadows': 187, 'Baldwin marsh - 1': 160, 'Benwood meadows - 1': 129, 'Benwood meadows - 2': 131, 'Big Meadow - 1': 47, 'Big Meadow - 2': 48, 'Big Meadow - 3': 38, 'Big Meadow - 4': 39, 'Big Meadow - 5': 37, 'Big Meadow - 6': 40, 'Big Meadow Creek - lower': 521, 'Big Meadow Creek - upper': 491, 'Big Meadow Creek - upper 2': 522, 'Big meadow - 7': 126, 'Bijou Park Creek meadows - 1': 219, 'Bijou Park Creek meadows - 2': 166, 'Bijou Park Creek meadows - 3': 167, 'Bijou Park Creek meadows - 4': 163, 'Bijou Park Creek meadows - 5': 489, 'Bijou Park Creek meadows - 6': 488, 'Bijou meadows - 1': 115, 'Bijou meadows - 2': 116, 'Bijou meadows - 3': 164, 'Bijou meadows - private': 114, 'Bijou meadows - tributary 1': 226, 'Blackwood Creek - Upper 1': 599, 'Blackwood Creek - Upper 2': 526, 'Blackwood Creek - lower 1': 523, 'Blackwood Creek - lower 2': 596, 'Blackwood Creek - middle 1': 597, 'Blackwood Creek - middle 2': 524, 'Blackwood Creek - middle 3': 598, 'Blackwood Creek - middle 4': 525, 'Blackwood Creek - upper 3': 527, 'Blackwood meadows - 1': 66, 'Blackwood meadows - 3': 65, 'Buck Lake meadows': 176, 'Buddhas meadow': 168, 'Burke Creek - middle': 528, 'Burke Creek - upper': 503, 'Burke Creek meadows - 1': 171, 'Burke Creek meadows - 2': 3, 'Burke Creek tributary': 461, 'Burton Creek - lower': 633, 'Burton Creek - upper': 529, 'Carnelian Canyon Creek - lower': 620, 'Carnelian Canyon Creek - upper': 621, 'Cascade Creek - lower': 501, 'Cascade Creek - upper': 498, 'Casino meadows': 170, 'Christmas Valley meadows - 1': 136, 'Christmas Valley meadows - 2': 134, 'Christmas Valley meadows - 3': 130, 'Christmas Valley meadows - 4': 225, 'Cold Creek - Highland Woods': 110, 'Cold Creek - middle': 530, 'Cold Creek - tributary 1': 532, 'Cold Creek - tributary 2': 533, 'Cold Creek - tributary 3': 531, 'Cold Creek - upper': 534, 'Cold Creek tributary - 4': 465, 'Cold Creek tributary - 5': 466, 'Colony Inn meadows - lower': 169, 'Colony Inn meadows - upper': 485, 'Cookhouse meadow': 128, 'Deer Creek - headwaters': 535, 'Deer Creek - lower': 537, 'Deer Creek - middle': 538, 'Deer Creek - middle 2': 539, 'Deer Creek - upper': 536, 'Deer Creek meadows': 30, 'Dollar Creek - lower': 540, 'Dollar Creek - upper': 512, 'Eagle Creek': 497, 'Echo Creek - below lake': 541, 'Echo Creek - upper': 494, 'Edgewood Creek - middle': 542, 'Edgewood Creek tributary - 2 - headwaters': 462, 'Edgewood Creek tributary - 2 - lower': 476, 'Edgewood Creek tributary - 2 - upper': 635, 'Edgewood Creek tributary - 3 - lower': 628, 'Edgewood Creek tributary - 3 - upper': 629, 'Edgewood meadows': 221, 'Elks Club meadows - 1': 141, 'Elks Club meadows - 2': 85, 'Fallen Leaf meadows - 1': 154, 'Fallen Leaf meadows - 2': 155, 'Fallen Leaf meadows - 3': 147, 'Fallen Leaf meadows - 4': 76, 'First Creek - lower': 543, 'First Creek - upper': 516, 'Freel Meadows - 1': 25, 'Freel Meadows - 2': 24, 'Gardner meadow': 150, 'General Creek - lower': 544, 'General Creek - middle': 506, 'General Creek - upper': 545, 'General Creek meadows': 61, 'Ginny Lake Meadows': 193, 'Glen Alpine Creek - lower': 546, 'Glen Alpine Creek - upper': 606, 'Glenbrook Creek - middle': 510, 'Glenbrook Creek - upper': 547, 'Glenbrook meadows - 1': 177, 'Glenbrook meadows - 2': 178, 'Golden Bear meadows - 1': 212, 'Golden Bear meadows - 2': 196, 'Grass Lake Creek': 609, 'Grass Lake meadow': 127, 'Griff Creek - lower': 514, 'Griff Creek - tributary': 548, 'Griff Creek - upper': 549, 'Griff Creek meadows': 35, 'Haypress Meadows': 50, 'Heavenly Valley Creek - middle': 550, 'Heavenly Valley Creek - upper': 499, 'Heavenly Valley Creek meadows - 1': 111, 'Heavenly Valley Creek meadows - 2': 112, 'Heavenly Valley Creek meadows - 3': 152, 'Heavenly Valley Creek meadows - 4': 113, 'Hell Hole Meadows - 2': 8, 'Hell Hole meadows - 1': 230, 'Hidden Valley Creek - lower': 551, 'Hidden Valley Creek - upper': 552, 'High Meadows - 3': 200, 'High meadows - 1': 203, 'High meadows - 2': 198, 'High meadows - 4': 202, 'High meadows - 5': 201, 'High meadows - 6': 199, 'Homewood Canyon Creek - lower': 600, 'Homewood Canyon Creek - upper': 508, 'Incline Creek - lower': 553, 'Incline Creek - middle 1': 554, 'Incline Creek - middle 2': 555, 'Incline Creek - middle 3': 636, 'Incline Creek - ski run': 556, 'Incline Creek - upper': 557, 'Incline Lake meadows - 1': 192, 'Incline Lake meadows - 2': 190, 'Incline Village tributary - 1': 458, 'Incline Village tributary - 2': 459, 'Incline Village tributary - 3': 460, 'Incline Village tributary - 4': 457, 'Kahle meadows - 2': 204, 'Kahle meadows - 3': 172, 'Kahle meadows - 4': 468, 'Kahle meadows - 5': 118, 'Kahle meadows - 6': 469, 'Kahle meadows - 7': 119, 'Kahle meadows - 8': 486, 'Kingsbury meadows': 222, 'Lake Forest meadows - 1': 185, 'Lake Forest meadows - 2': 186, 'Lake Forest meadows - 3': 184, 'Lake Forest meadows - 4': 487, 'Lake Forest meadows - 5': 223, 'Lake Forest meadows - 6': 183, 'Lake Forest tributary': 624, 'Lakeshore meadows': 72, 'Logan House Creek - lower': 558, 'Logan House Creek - upper': 507, 'Logan House meadow': 13, 'Lonely Gulch Creek - lower': 559, 'Lonely Gulch Creek - middle': 560, 'Lonely Gulch Creek - upper': 504, 'Madden Creek': 561, 'Marlette Creek - lower': 562, 'Marlette Creek - old dam site': 564, 'Marlette Creek - south fork (lower)': 642, 'Marlette Creek - south fork (upper)': 563, 'Marlette Creek - upper': 631, 'Marlette Lake meadows': 32, 'McFaul Creek - lower': 565, 'McFual meadow': 71, 'McKinney Creek - lower': 566, 'McKinney Creek - middle': 567, 'McKinney Creek - upper': 505, 'McKinney tributary - 1': 626, 'McKinney tributary - 2': 453, 'Meeks Bay Lagoon': 235, 'Meeks Bay meadows - 1': 205, 'Meeks Bay meadows - 2': 207, 'Meeks Bay meadows - 3': 36, 'Meeks Bay meadows - 4': 206, 'Meeks Creek - upper': 502, 'Meiss meadows - 1': 123, 'Meiss meadows - 2': 122, 'Meiss meadows - 3': 124, 'Meiss meadows - 4': 210, 'Meiss meadows - 5': 209, 'Meyers meadow': 218, 'Meyers tributary - 1': 447, 'Meyers tributary - 2': 467, 'Mill Creek - lower': 568, 'Mill Creek - upper': 515, 'Mill Creek meadows': 234, 'Mount Rainier Drive meadows - 1': 215, 'Mount Rainier Drive meadows - 2': 216, 'Muskawi Drive meadows': 83, 'North Logan House Creek': 509, 'North Logan House meadows': 12, 'North Zephyr Creek - lower': 570, 'North Zephyr Creek - middle': 615, 'North Zephyr Creek - tributary': 569, 'North Zephyr Creek - upper': 571, 'Nottaway Drive meadows': 213, 'Osgood Creek - above road': 572, 'Osgood Creek - below road': 573, 'Osgood Swamp': 482, 'Paige meadows': 182, 'Pope marsh meadow': 483, 'Quail Creek - lower': 574, 'Quail Creek - upper': 575, 'Quail Creek meadow': 26, 'Rosewood Creek - lower': 576, 'Rosewood Creek - middle 1': 586, 'Rosewood Creek - middle 2': 587, 'Rosewood Creek - middle 3': 588, 'Rubicon Creek': 601, 'Rubicon Creek - tributary': 602, 'Rubicon Meadows': 60, 'Saxon Creek - headwaters': 495, 'Saxon Creek - upper': 641, 'Saxon Creek meadows - above Fountain Place 1': 2, 'Saxon Creek meadows - above Fountain Place 2': 102, 'Saxon Creek meadows - below Fountain Place': 1, 'Saxon Creek tributary meadows - 1': 101, 'Saxon Creek tributary meadows - 3': 100, 'Saxon Creek tributary meadows - 4': 140, 'Saxon Creek tributary meadows - 5': 197, 'Saxon Creek tributary meadows - 6': 139, 'Saxon Creek tributary meadows - 7': 231, 'Second Creek - lower': 591, 'Second Creek - lower 2': 592, 'Second Creek - middle': 593, 'Second Creek - upper': 517, 'Secret Harbor Creek - lower': 618, 'Secret Harbor Creek - upper': 619, 'Sierra Tract wetlands': 211, 'Ski Run meadows': 220, 'Sky meadows': 79, 'Skylandia SEZ': 622, 'Slaughterhouse Creek - lower': 614, 'Slaughterhouse Creek - middle': 616, 'Slaughterhouse Creek - upper': 617, 'Slaughterhouse Meadows - 1': 6, 'Slaughterhouse meadows - 2': 180, 'Snow Creek tributary - 1': 451, 'Snow Creek tributary - 2': 452, 'Snow Creek wetlands - 1': 188, 'Snow Creek wetlands - 2': 189, 'South Lake Tahoe - wetland 1': 229, 'South Lake Tahoe airport': 484, 'South Lake Tahoe tributary - 1': 463, 'South Lake Tahoe tributary - 2': 464, 'South Lake Tahoe tributary - 3': 627, 'Spooner Meadows - 4': 5, 'Spooner meadows - 1': 233, 'Spooner meadows - 2': 179, 'Spooner meadows - 3': 120, 'Spooner meadows - 5': 121, 'Star Lake meadows': 45, 'Susquehana meadows - 1': 108, 'Susquehana meadows - 2': 107, 'Tahoe City meadow': 224, 'Tahoe City tributary - 1': 449, 'Tahoe City tributary - 2': 450, 'Tahoe Island meadows - 1': 158, 'Tahoe Island meadows - 2': 156, 'Tahoe Keys': 162, 'Tahoe Paradise golf course': 632, 'Tahoe Valley meadows - 1': 153, 'Tahoe Valley meadows - 2': 228, 'Tahoe Vista meadows': 227, 'Tallac Creek - abv highway - 1': 334, 'Tallac Creek - abv highway - 2': 604, 'Tallac Creek - tributary': 500, 'Tallac marsh': 159, 'Tallac meadows': 157, 'Taylor Creek': 605, 'Taylor Creek marsh': 208, 'Third Creek - headwaters': 585, 'Third Creek - lower': 577, 'Third Creek - lower 2': 578, 'Third Creek - middle': 581, 'Third Creek - middle 1': 579, 'Third Creek - middle 2': 580, 'Third Creek - upper 1': 582, 'Third Creek - upper 2': 584, 'Third Creek - upper 3': 583, 'Third Creek meadows - 1': 191, 'Third Creek meadows - 3': 34, 'Third Creek meadows - 4': 195, 'Third Creek meadows - 6': 194, 'Third Creek meadows - 7': 16, 'Third Creek meadows - 8': 15, 'Trout Creek - Highland Woods': 109, 'Trout Creek - tributary 2': 613, 'Trout Creek - tributary 3': 612, 'Trout Creek - upper': 496, 'Trout Creek above Black Bart': 149, 'Trout Creek below Black Bart': 161, 'Trout Creek headwaters meadows - 1': 137, 'Trout Creek headwaters meadows - 2': 9, 'Trout Creek meadows - above Fountain Place': 103, 'Trout Creek meadows - above Pioneer 1': 148, 'Trout Creek meadows - above Pioneer 2': 106, 'Trout Creek meadows - above Pioneer 3': 105, 'Trout Creek meadows - above Pioneer 4': 104, 'UTR - Airport reach': 82, 'UTR - Christmas Valley 1': 608, 'UTR - Christmas Valley 3': 493, 'UTR - Johnson meadows - 2': 151, 'UTR - Johnson meadows - 3': 81, 'UTR - Reach 5': 80, 'UTR - Reach 6': 84, 'UTR - Washoe Meadows': 607, 'UTR - golf course meadows': 86, 'UTR - middle': 492, 'UTR - tributary 1': 610, 'UTR - tributary 3': 611, 'UTR - upper': 490, 'UTR Marsh - UTR side': 78, 'UTR marsh - Trout Creek side': 165, 'Upper Truckee River - Meyers': 138, 'Upper Truckee River - Tahoe Paradise': 7, 'Van Sickle meadows': 117, 'Ward Creek - lower': 594, 'Ward Creek - middle': 595, 'Ward Creek - upper': 511, 'Ward Creek meadow': 625, 'Washoan Blvd meadows': 145, 'Washoe State Parks meadow - 1': 4, 'Washoe State Parks meadow - 2': 98, 'Watson Creek': 513, 'West Shore tributary - 1': 455, 'West Shore tributary - 2 - lower': 639, 'West Shore tributary - 2 - upper': 448, 'West Shore tributary - 3': 456, 'West Shore tributary - 4': 454, 'Woods Creek - lower': 589, 'Woods Creek - middle': 590, 'Woods Creek - upper': 518, 'small meadow 1': 11, 'small meadow 10': 64, 'small meadow 100': 77, 'small meadow 105': 125, 'small meadow 111': 132, 'small meadow 112': 133, 'small meadow 113': 135, 'small meadow 116': 181, 'small meadow 13': 20, 'small meadow 14': 19, 'small meadow 15': 18, 'small meadow 16': 21, 'small meadow 17': 28, 'small meadow 19': 63, 'small meadow 2': 10, 'small meadow 20': 623, 'small meadow 21': 67, 'small meadow 22': 53, 'small meadow 23': 54, 'small meadow 24': 56, 'small meadow 25': 55, 'small meadow 26': 57, 'small meadow 27': 58, 'small meadow 28': 59, 'small meadow 29': 174, 'small meadow 3': 70, 'small meadow 30': 175, 'small meadow 32': 51, 'small meadow 35': 49, 'small meadow 36': 52, 'small meadow 40': 74, 'small meadow 5': 29, 'small meadow 50': 23, 'small meadow 51': 42, 'small meadow 52': 41, 'small meadow 54': 22, 'small meadow 56': 46, 'small meadow 57': 73, 'small meadow 58': 173, 'small meadow 59': 14, 'small meadow 6': 17, 'small meadow 7': 27, 'small meadow 8': 69, 'small meadow 82': 232, 'small meadow 9': 68, 'small meadow 92': 31, 'small meadow 93': 33, 'small meadow 95': 43, 'small meadow 96': 44, 'small meadow 98': 62, 'small meadow 99': 75}\n",
      "{'Angora Creek - tributary': 265, 'Angora Creek - upper': 266, 'Angora meadows - 1': 403, 'Angora meadows - 2': 405, 'Angora meadows - 3': 406, 'Angora meadows - 6': 424, 'Angora meadows - 8': 404, 'Antone meadows': 366, 'Baldwin marsh - 1': 426, 'Benwood meadows - 1': 420, 'Benwood meadows - 2': 422, 'Big Meadow - 1': 393, 'Big Meadow - 4': 389, 'Big Meadow Creek - lower': 267, 'Big Meadow Creek - upper': 237, 'Big Meadow Creek - upper 2': 268, 'Blackwood Creek - Upper 1': 481, 'Blackwood Creek - Upper 2': 272, 'Blackwood Creek - lower 1': 269, 'Blackwood Creek - lower 2': 470, 'Blackwood Creek - middle 1': 479, 'Blackwood Creek - middle 2': 270, 'Blackwood Creek - middle 3': 471, 'Blackwood Creek - middle 4': 271, 'Blackwood Creek - upper 3': 273, 'Buck Lake meadows': 429, 'Burke Creek - middle': 274, 'Burke Creek - upper': 249, 'Burke Creek meadows - 1': 428, 'Burton Creek - lower': 634, 'Burton Creek - upper': 275, 'Cascade Creek - lower': 247, 'Cascade Creek - upper': 244, 'Christmas Valley meadows - 1': 359, 'Christmas Valley meadows - 2': 358, 'Christmas Valley meadows - 3': 421, 'Cold Creek - Highland Woods': 413, 'Cold Creek - middle': 276, 'Cold Creek - tributary 2': 278, 'Cold Creek - tributary 3': 277, 'Cold Creek - upper': 279, 'Deer Creek - headwaters': 280, 'Deer Creek - lower': 282, 'Deer Creek - middle': 283, 'Deer Creek - middle 2': 284, 'Deer Creek - upper': 281, 'Dollar Creek - lower': 285, 'Dollar Creek - upper': 258, 'Eagle Creek': 243, 'Echo Creek - below lake': 286, 'Echo Creek - upper': 240, 'Edgewood Creek - middle': 287, 'First Creek - lower': 288, 'First Creek - upper': 262, 'General Creek - lower': 289, 'General Creek - middle': 252, 'General Creek - upper': 290, 'Ginny Lake Meadows': 374, 'Glen Alpine Creek - lower': 291, 'Glen Alpine Creek - upper': 355, 'Glenbrook Creek - middle': 256, 'Glenbrook Creek - upper': 292, 'Glenbrook meadows - 1': 430, 'Grass Lake Creek': 356, 'Griff Creek - lower': 260, 'Griff Creek - tributary': 293, 'Griff Creek - upper': 294, 'Griff Creek meadows': 387, 'Heavenly Valley Creek - middle': 295, 'Heavenly Valley Creek - upper': 245, 'Heavenly Valley Creek meadows - 1': 414, 'Heavenly Valley Creek meadows - 2': 415, 'Heavenly Valley Creek meadows - 3': 416, 'Heavenly Valley Creek meadows - 4': 417, 'Hell Hole Meadows - 2': 371, 'Hell Hole meadows - 1': 444, 'Hidden Valley Creek - lower': 296, 'Hidden Valley Creek - upper': 297, 'High meadows - 1': 439, 'High meadows - 2': 436, 'High meadows - 4': 438, 'High meadows - 6': 437, 'Homewood Canyon Creek - lower': 480, 'Homewood Canyon Creek - upper': 254, 'Incline Creek - lower': 298, 'Incline Creek - middle 1': 299, 'Incline Creek - middle 2': 300, 'Incline Creek - middle 3': 637, 'Incline Creek - ski run': 301, 'Incline Creek - upper': 302, 'Incline Lake meadows - 2': 434, 'Kahle meadows - 3': 472, 'Kahle meadows - 5': 418, 'Kahle meadows - 6': 474, 'Lake Forest meadows - 3': 432, 'Lake Forest meadows - 4': 638, 'Logan House Creek - lower': 303, 'Logan House Creek - upper': 253, 'Lonely Gulch Creek - lower': 304, 'Lonely Gulch Creek - middle': 305, 'Lonely Gulch Creek - upper': 250, 'Madden Creek': 306, 'Marlette Creek - lower': 307, 'Marlette Creek - old dam site': 477, 'Marlette Creek - south fork (lower)': 643, 'Marlette Creek - south fork (upper)': 308, 'Marlette Creek - upper': 630, 'Marlette Lake meadows': 385, 'McFaul Creek - lower': 309, 'McFual meadow': 395, 'McKinney Creek - lower': 310, 'McKinney Creek - middle': 311, 'McKinney Creek - upper': 251, 'Meeks Bay Lagoon': 473, 'Meeks Bay meadows - 1': 441, 'Meeks Bay meadows - 2': 440, 'Meeks Bay meadows - 3': 388, 'Meeks Creek - upper': 248, 'Meiss meadows - 1': 357, 'Meiss meadows - 3': 419, 'Meiss meadows - 4': 442, 'Meiss meadows - 5': 443, 'Mill Creek - lower': 312, 'Mill Creek - upper': 261, 'North Logan House Creek': 255, 'North Zephyr Creek - lower': 314, 'North Zephyr Creek - middle': 475, 'North Zephyr Creek - tributary': 313, 'North Zephyr Creek - upper': 315, 'Osgood Creek - above road': 316, 'Osgood Creek - below road': 317, 'Quail Creek - lower': 318, 'Quail Creek - upper': 319, 'Rosewood Creek - lower': 320, 'Rosewood Creek - middle 1': 321, 'Rosewood Creek - middle 2': 322, 'Rosewood Creek - middle 3': 323, 'Rubicon Creek': 324, 'Rubicon Creek - tributary': 325, 'Rubicon Meadows': 394, 'Saxon Creek - headwaters': 241, 'Saxon Creek - upper': 640, 'Saxon Creek meadows - above Fountain Place 1': 369, 'Saxon Creek meadows - above Fountain Place 2': 408, 'Saxon Creek meadows - below Fountain Place': 368, 'Saxon Creek tributary meadows - 1': 407, 'Second Creek - lower': 326, 'Second Creek - lower 2': 327, 'Second Creek - middle': 328, 'Second Creek - upper': 263, 'Secret Harbor Creek - lower': 329, 'Secret Harbor Creek - upper': 330, 'Slaughterhouse Creek - lower': 331, 'Slaughterhouse Creek - middle': 332, 'Slaughterhouse Creek - upper': 333, 'Slaughterhouse meadows - 2': 365, 'Snow Creek wetlands - 1': 367, 'Snow Creek wetlands - 2': 433, 'Spooner meadows - 1': 445, 'Spooner meadows - 2': 431, 'Star Lake meadows': 392, 'Tallac Creek - abv highway - 1': 603, 'Tallac Creek - abv highway - 2': 478, 'Tallac Creek - tributary': 246, 'Taylor Creek': 335, 'Taylor Creek marsh': 362, 'Third Creek - headwaters': 336, 'Third Creek - lower': 337, 'Third Creek - lower 2': 338, 'Third Creek - middle': 340, 'Third Creek - middle 1': 339, 'Third Creek - middle 2': 341, 'Third Creek - upper 1': 342, 'Third Creek - upper 2': 343, 'Third Creek - upper 3': 344, 'Third Creek meadows - 1': 435, 'Third Creek meadows - 3': 386, 'Third Creek meadows - 7': 376, 'Third Creek meadows - 8': 375, 'Trout Creek - Highland Woods': 412, 'Trout Creek - tributary 2': 345, 'Trout Creek - tributary 3': 346, 'Trout Creek - upper': 242, 'Trout Creek above Black Bart': 425, 'Trout Creek below Black Bart': 427, 'Trout Creek headwaters meadows - 2': 372, 'Trout Creek meadows - above Fountain Place': 360, 'Trout Creek meadows - above Pioneer 1': 411, 'Trout Creek meadows - above Pioneer 2': 410, 'Trout Creek meadows - above Pioneer 3': 361, 'Trout Creek meadows - above Pioneer 4': 409, 'UTR - Airport reach': 400, 'UTR - Christmas Valley 1': 347, 'UTR - Christmas Valley 3': 239, 'UTR - Johnson meadows - 2': 398, 'UTR - Johnson meadows - 3': 399, 'UTR - Reach 5': 397, 'UTR - Reach 6': 401, 'UTR - Washoe Meadows': 350, 'UTR - golf course meadows': 402, 'UTR - middle': 238, 'UTR - tributary 1': 349, 'UTR - tributary 3': 348, 'UTR - upper': 236, 'UTR Marsh - UTR side': 396, 'UTR marsh - Trout Creek side': 363, 'Upper Truckee River - Tahoe Paradise': 645, 'Ward Creek - lower': 351, 'Ward Creek - middle': 352, 'Ward Creek - upper': 257, 'Watson Creek': 259, 'Woods Creek - lower': 353, 'Woods Creek - middle': 354, 'Woods Creek - upper': 264, 'small meadow 1': 373, 'small meadow 111': 381, 'small meadow 113': 423, 'small meadow 13': 378, 'small meadow 14': 364, 'small meadow 15': 377, 'small meadow 16': 379, 'small meadow 17': 383, 'small meadow 50': 380, 'small meadow 52': 390, 'small meadow 7': 382, 'small meadow 92': 384, 'small meadow 95': 391}\n",
      "{'Saxon Creek meadows - below Fountain Place': 368, 'Saxon Creek meadows - above Fountain Place 1': 369, 'Burke Creek meadows - 2': 3, 'Washoe State Parks meadow - 1': 4, 'Spooner Meadows - 4': 5, 'Slaughterhouse Meadows - 1': 6, 'Upper Truckee River - Tahoe Paradise': 645, 'Hell Hole Meadows - 2': 371, 'Trout Creek headwaters meadows - 2': 372, 'small meadow 2': 10, 'small meadow 1': 373, 'North Logan House meadows': 12, 'Logan House meadow': 13, 'small meadow 59': 14, 'Third Creek meadows - 8': 375, 'Third Creek meadows - 7': 376, 'small meadow 6': 17, 'small meadow 15': 377, 'small meadow 14': 364, 'small meadow 13': 378, 'small meadow 16': 379, 'small meadow 54': 22, 'small meadow 50': 380, 'Freel Meadows - 2': 24, 'Freel Meadows - 1': 25, 'Quail Creek meadow': 26, 'small meadow 7': 382, 'small meadow 17': 383, 'small meadow 5': 29, 'Deer Creek meadows': 30, 'small meadow 92': 384, 'Marlette Lake meadows': 385, 'small meadow 93': 33, 'Third Creek meadows - 3': 386, 'Griff Creek meadows': 387, 'Meeks Bay meadows - 3': 388, 'Big Meadow - 5': 37, 'Big Meadow - 3': 38, 'Big Meadow - 4': 389, 'Big Meadow - 6': 40, 'small meadow 52': 390, 'small meadow 51': 42, 'small meadow 95': 391, 'small meadow 96': 44, 'Star Lake meadows': 392, 'small meadow 56': 46, 'Big Meadow - 1': 393, 'Big Meadow - 2': 48, 'small meadow 35': 49, 'Haypress Meadows': 50, 'small meadow 32': 51, 'small meadow 36': 52, 'small meadow 22': 53, 'small meadow 23': 54, 'small meadow 25': 55, 'small meadow 24': 56, 'small meadow 26': 57, 'small meadow 27': 58, 'small meadow 28': 59, 'Rubicon Meadows': 394, 'General Creek meadows': 61, 'small meadow 98': 62, 'small meadow 19': 63, 'small meadow 10': 64, 'Blackwood meadows - 3': 65, 'Blackwood meadows - 1': 66, 'small meadow 21': 67, 'small meadow 9': 68, 'small meadow 8': 69, 'small meadow 3': 70, 'McFual meadow': 395, 'Lakeshore meadows': 72, 'small meadow 57': 73, 'small meadow 40': 74, 'small meadow 99': 75, 'Fallen Leaf meadows - 4': 76, 'small meadow 100': 77, 'UTR Marsh - UTR side': 396, 'Sky meadows': 79, 'UTR - Reach 5': 397, 'UTR - Johnson meadows - 3': 399, 'UTR - Airport reach': 400, 'Muskawi Drive meadows': 83, 'UTR - Reach 6': 401, 'Elks Club meadows - 2': 85, 'UTR - golf course meadows': 402, 'Angora meadows - 1': 403, 'Angora meadows - 8': 404, 'Angora meadows - 7': 89, 'Angora meadows - 2': 405, 'Angora meadows - 3': 406, 'Angora meadows - 9': 92, 'Angora meadows tributary - 7': 93, 'Angora meadows tributary - 4': 94, 'Angora meadows tributary - 8': 95, 'Angora meadows tributary - 9': 96, 'Angora meadows tributary - 3': 97, 'Washoe State Parks meadow - 2': 98, 'Angora meadows tributary - 2': 99, 'Saxon Creek tributary meadows - 3': 100, 'Saxon Creek tributary meadows - 1': 407, 'Saxon Creek meadows - above Fountain Place 2': 408, 'Trout Creek meadows - above Fountain Place': 360, 'Trout Creek meadows - above Pioneer 4': 409, 'Trout Creek meadows - above Pioneer 3': 361, 'Trout Creek meadows - above Pioneer 2': 410, 'Susquehana meadows - 2': 107, 'Susquehana meadows - 1': 108, 'Trout Creek - Highland Woods': 412, 'Cold Creek - Highland Woods': 413, 'Heavenly Valley Creek meadows - 1': 414, 'Heavenly Valley Creek meadows - 2': 415, 'Heavenly Valley Creek meadows - 4': 417, 'Bijou meadows - private': 114, 'Bijou meadows - 1': 115, 'Bijou meadows - 2': 116, 'Van Sickle meadows': 117, 'Kahle meadows - 5': 418, 'Kahle meadows - 7': 119, 'Spooner meadows - 3': 120, 'Spooner meadows - 5': 121, 'Meiss meadows - 2': 122, 'Meiss meadows - 1': 357, 'Meiss meadows - 3': 419, 'small meadow 105': 125, 'Big meadow - 7': 126, 'Grass Lake meadow': 127, 'Cookhouse meadow': 128, 'Benwood meadows - 1': 420, 'Christmas Valley meadows - 3': 421, 'Benwood meadows - 2': 422, 'small meadow 111': 381, 'small meadow 112': 133, 'Christmas Valley meadows - 2': 358, 'small meadow 113': 423, 'Christmas Valley meadows - 1': 359, 'Trout Creek headwaters meadows - 1': 137, 'Upper Truckee River - Meyers': 138, 'Saxon Creek tributary meadows - 6': 139, 'Saxon Creek tributary meadows - 4': 140, 'Elks Club meadows - 1': 141, 'Angora meadows - 6': 424, 'Angora meadows - 4': 143, 'Angora meadows - 5': 144, 'Washoan Blvd meadows': 145, 'Angora meadows tributary - 6': 146, 'Fallen Leaf meadows - 3': 147, 'Trout Creek meadows - above Pioneer 1': 411, 'Trout Creek above Black Bart': 425, 'Gardner meadow': 150, 'UTR - Johnson meadows - 2': 398, 'Heavenly Valley Creek meadows - 3': 416, 'Tahoe Valley meadows - 1': 153, 'Fallen Leaf meadows - 1': 154, 'Fallen Leaf meadows - 2': 155, 'Tahoe Island meadows - 2': 156, 'Tallac meadows': 157, 'Tahoe Island meadows - 1': 158, 'Tallac marsh': 159, 'Baldwin marsh - 1': 426, 'Trout Creek below Black Bart': 427, 'Tahoe Keys': 162, 'Bijou Park Creek meadows - 4': 163, 'Bijou meadows - 3': 164, 'UTR marsh - Trout Creek side': 363, 'Bijou Park Creek meadows - 2': 166, 'Bijou Park Creek meadows - 3': 167, 'Buddhas meadow': 168, 'Colony Inn meadows - lower': 169, 'Casino meadows': 170, 'Burke Creek meadows - 1': 428, 'Kahle meadows - 3': 472, 'small meadow 58': 173, 'small meadow 29': 174, 'small meadow 30': 175, 'Buck Lake meadows': 429, 'Glenbrook meadows - 1': 430, 'Glenbrook meadows - 2': 178, 'Spooner meadows - 2': 431, 'Slaughterhouse meadows - 2': 365, 'small meadow 116': 181, 'Paige meadows': 182, 'Lake Forest meadows - 6': 183, 'Lake Forest meadows - 3': 432, 'Lake Forest meadows - 1': 185, 'Lake Forest meadows - 2': 186, 'Antone meadows': 366, 'Snow Creek wetlands - 1': 367, 'Snow Creek wetlands - 2': 433, 'Incline Lake meadows - 2': 434, 'Third Creek meadows - 1': 435, 'Incline Lake meadows - 1': 192, 'Ginny Lake Meadows': 374, 'Third Creek meadows - 6': 194, 'Third Creek meadows - 4': 195, 'Golden Bear meadows - 2': 196, 'Saxon Creek tributary meadows - 5': 197, 'High meadows - 2': 436, 'High meadows - 6': 437, 'High Meadows - 3': 200, 'High meadows - 5': 201, 'High meadows - 4': 438, 'High meadows - 1': 439, 'Kahle meadows - 2': 204, 'Meeks Bay meadows - 1': 441, 'Meeks Bay meadows - 4': 206, 'Meeks Bay meadows - 2': 440, 'Taylor Creek marsh': 362, 'Meiss meadows - 5': 443, 'Meiss meadows - 4': 442, 'Sierra Tract wetlands': 211, 'Golden Bear meadows - 1': 212, 'Nottaway Drive meadows': 213, 'Angora meadows tributary - 5': 214, 'Mount Rainier Drive meadows - 1': 215, 'Mount Rainier Drive meadows - 2': 216, 'Angora meadows tributary - 1': 217, 'Meyers meadow': 218, 'Bijou Park Creek meadows - 1': 219, 'Ski Run meadows': 220, 'Edgewood meadows': 221, 'Kingsbury meadows': 222, 'Lake Forest meadows - 5': 223, 'Tahoe City meadow': 224, 'Christmas Valley meadows - 4': 225, 'Bijou meadows - tributary 1': 226, 'Tahoe Vista meadows': 227, 'Tahoe Valley meadows - 2': 228, 'South Lake Tahoe - wetland 1': 229, 'Hell Hole meadows - 1': 444, 'Saxon Creek tributary meadows - 7': 231, 'small meadow 82': 232, 'Spooner meadows - 1': 445, 'Mill Creek meadows': 234, 'Meeks Bay Lagoon': 473, 'UTR - upper': 490, 'Big Meadow Creek - upper': 491, 'UTR - middle': 492, 'UTR - Christmas Valley 3': 493, 'Echo Creek - upper': 494, 'Saxon Creek - headwaters': 495, 'Trout Creek - upper': 496, 'Eagle Creek': 497, 'Cascade Creek - upper': 498, 'Heavenly Valley Creek - upper': 499, 'Tallac Creek - tributary': 500, 'Cascade Creek - lower': 501, 'Meeks Creek - upper': 502, 'Burke Creek - upper': 503, 'Lonely Gulch Creek - upper': 504, 'McKinney Creek - upper': 505, 'General Creek - middle': 506, 'Logan House Creek - upper': 507, 'Homewood Canyon Creek - upper': 508, 'North Logan House Creek': 509, 'Glenbrook Creek - middle': 510, 'Ward Creek - upper': 511, 'Dollar Creek - upper': 512, 'Watson Creek': 513, 'Griff Creek - lower': 514, 'Mill Creek - upper': 515, 'First Creek - upper': 516, 'Second Creek - upper': 517, 'Woods Creek - upper': 518, 'Angora Creek - tributary': 519, 'Angora Creek - upper': 520, 'Big Meadow Creek - lower': 521, 'Big Meadow Creek - upper 2': 522, 'Blackwood Creek - lower 1': 523, 'Blackwood Creek - middle 2': 524, 'Blackwood Creek - middle 4': 525, 'Blackwood Creek - Upper 2': 526, 'Blackwood Creek - upper 3': 527, 'Burke Creek - middle': 528, 'Burton Creek - upper': 529, 'Cold Creek - middle': 530, 'Cold Creek - tributary 3': 531, 'Cold Creek - tributary 2': 533, 'Cold Creek - upper': 534, 'Deer Creek - headwaters': 535, 'Deer Creek - upper': 536, 'Deer Creek - lower': 537, 'Deer Creek - middle': 538, 'Deer Creek - middle 2': 539, 'Dollar Creek - lower': 540, 'Echo Creek - below lake': 541, 'Edgewood Creek - middle': 542, 'First Creek - lower': 543, 'General Creek - lower': 544, 'General Creek - upper': 545, 'Glen Alpine Creek - lower': 546, 'Glenbrook Creek - upper': 547, 'Griff Creek - tributary': 548, 'Griff Creek - upper': 549, 'Heavenly Valley Creek - middle': 550, 'Hidden Valley Creek - lower': 551, 'Hidden Valley Creek - upper': 552, 'Incline Creek - lower': 553, 'Incline Creek - middle 1': 554, 'Incline Creek - middle 2': 555, 'Incline Creek - ski run': 556, 'Incline Creek - upper': 557, 'Logan House Creek - lower': 558, 'Lonely Gulch Creek - lower': 559, 'Lonely Gulch Creek - middle': 560, 'Madden Creek': 561, 'Marlette Creek - lower': 562, 'Marlette Creek - south fork (upper)': 563, 'McFaul Creek - lower': 565, 'McKinney Creek - lower': 566, 'McKinney Creek - middle': 567, 'Mill Creek - lower': 568, 'North Zephyr Creek - tributary': 569, 'North Zephyr Creek - lower': 570, 'North Zephyr Creek - upper': 571, 'Osgood Creek - above road': 572, 'Osgood Creek - below road': 573, 'Quail Creek - lower': 574, 'Quail Creek - upper': 575, 'Rosewood Creek - lower': 576, 'Rosewood Creek - middle 1': 586, 'Rosewood Creek - middle 2': 587, 'Rosewood Creek - middle 3': 588, 'Rubicon Creek': 601, 'Rubicon Creek - tributary': 602, 'Second Creek - lower': 591, 'Second Creek - lower 2': 592, 'Second Creek - middle': 593, 'Secret Harbor Creek - lower': 618, 'Secret Harbor Creek - upper': 619, 'Slaughterhouse Creek - lower': 614, 'Slaughterhouse Creek - middle': 616, 'Slaughterhouse Creek - upper': 617, 'Tallac Creek - abv highway - 1': 603, 'Taylor Creek': 605, 'Third Creek - headwaters': 585, 'Third Creek - lower': 577, 'Third Creek - lower 2': 578, 'Third Creek - middle 1': 579, 'Third Creek - middle': 581, 'Third Creek - middle 2': 580, 'Third Creek - upper 1': 582, 'Third Creek - upper 2': 584, 'Third Creek - upper 3': 583, 'Trout Creek - tributary 2': 613, 'Trout Creek - tributary 3': 612, 'UTR - Christmas Valley 1': 608, 'UTR - tributary 3': 611, 'UTR - tributary 1': 610, 'UTR - Washoe Meadows': 607, 'Ward Creek - lower': 594, 'Ward Creek - middle': 595, 'Woods Creek - lower': 589, 'Woods Creek - middle': 590, 'Glen Alpine Creek - upper': 606, 'Grass Lake Creek': 609, 'Angora tributary': 446, 'Meyers tributary - 1': 447, 'West Shore tributary - 2 - upper': 448, 'Tahoe City tributary - 1': 449, 'Tahoe City tributary - 2': 450, 'Snow Creek tributary - 1': 451, 'Snow Creek tributary - 2': 452, 'McKinney tributary - 2': 453, 'West Shore tributary - 4': 454, 'West Shore tributary - 1': 455, 'West Shore tributary - 3': 456, 'Incline Village tributary - 4': 457, 'Incline Village tributary - 1': 458, 'Incline Village tributary - 2': 459, 'Incline Village tributary - 3': 460, 'Burke Creek tributary': 461, 'Edgewood Creek tributary - 2 - headwaters': 462, 'South Lake Tahoe tributary - 1': 463, 'South Lake Tahoe tributary - 2': 464, 'Cold Creek tributary - 4': 465, 'Cold Creek tributary - 5': 466, 'Meyers tributary - 2': 467, 'Kahle meadows - 4': 468, 'Kahle meadows - 6': 474, 'Blackwood Creek - lower 2': 596, 'Blackwood Creek - middle 3': 598, 'North Zephyr Creek - middle': 615, 'Edgewood Creek tributary - 2 - lower': 476, 'Marlette Creek - old dam site': 564, 'Tallac Creek - abv highway - 2': 604, 'Blackwood Creek - middle 1': 597, 'Homewood Canyon Creek - lower': 600, 'Blackwood Creek - Upper 1': 599, 'Osgood Swamp': 482, 'Pope marsh meadow': 483, 'South Lake Tahoe airport': 484, 'Colony Inn meadows - upper': 485, 'Kahle meadows - 8': 486, 'Lake Forest meadows - 4': 638, 'Bijou Park Creek meadows - 6': 488, 'Bijou Park Creek meadows - 5': 489, 'Cold Creek - tributary 1': 532, 'Carnelian Canyon Creek - lower': 620, 'Carnelian Canyon Creek - upper': 621, 'Skylandia SEZ': 622, 'small meadow 20': 623, 'Lake Forest tributary': 624, 'Ward Creek meadow': 625, 'McKinney tributary - 1': 626, 'South Lake Tahoe tributary - 3': 627, 'Edgewood Creek tributary - 3 - lower': 628, 'Edgewood Creek tributary - 3 - upper': 629, 'Marlette Creek - upper': 631, 'Tahoe Paradise golf course': 632, 'Burton Creek - lower': 634, 'Edgewood Creek tributary - 2 - upper': 635, 'Incline Creek - middle 3': 637, 'West Shore tributary - 2 - lower': 639, 'Saxon Creek - upper': 641, 'Marlette Creek - south fork (lower)': 643}\n"
     ]
    }
   ],
   "source": [
    "#Large Polygons or only polygon shapes lookup dictionary for Assessment Units with lerger values of acreage\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "excel_data = pd.read_csv(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Large_Polygon_Lookup.csv\")  \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "lookup_dict = {}\n",
    "\n",
    "for index, row in excel_data.iterrows():\n",
    "    lookup_dict[row['Assessment_Unit_Name']] = row['SEZ_ID']\n",
    "\n",
    "# See dictionary where keys are Assessment Unit Names and values are SEZ IDs\n",
    "print(lookup_dict)\n",
    "\n",
    "#Small Polygon if there are two acres for an SEZ\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "excel_data = pd.read_csv(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Small_Polygon_Lookup.csv\")  \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "lookup_riverine = {}\n",
    "\n",
    "for index, row in excel_data.iterrows():\n",
    "    lookup_riverine[row['Assessment_Unit_Name']] = row['SEZ_ID']\n",
    "\n",
    "# See dictionary where keys are Assessment Unit Names and values are SEZ IDs\n",
    "print(lookup_riverine)\n",
    "\n",
    "#All Polygons\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "excel_data = pd.read_csv(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\All_SEZID_Lookup.csv\")  \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "lookup_all = {}\n",
    "\n",
    "for index, row in excel_data.iterrows():\n",
    "    lookup_all[row['Assessment_Unit_Name']] = row['SEZ_ID']\n",
    "\n",
    "# See dictionary where keys are Assessment Unit Names and values are SEZ IDs\n",
    "print(lookup_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Grading each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grading for each parameter \n",
    "#Defining Grade for Bank Stability based on Erosiondf[percent_unstable]\n",
    "def categorize_erosion(Percent_Unstable):\n",
    "    if pd.isna(Percent_Unstable):\n",
    "        return np.nan\n",
    "    elif 0 <= Percent_Unstable < 5:\n",
    "        return 'A'\n",
    "    elif 5 <= Percent_Unstable < 20:\n",
    "        return 'B'\n",
    "    elif 20 <= Percent_Unstable < 50:\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'D'\n",
    "    \n",
    "#Scoring based off of grading - check this\n",
    "def score_indicator(Rating):\n",
    "    if pd.isna(Rating):\n",
    "        return np.nan\n",
    "    elif  Rating == 'A':\n",
    "        return '12'\n",
    "    elif Rating == 'B':\n",
    "        return '9'\n",
    "    elif Rating == 'C':\n",
    "        return '6'\n",
    "    else:\n",
    "        return '3'\n",
    "\n",
    "#Define Grade for Incision based off of incisino ratio\n",
    "\n",
    "def categorize_incision(bankfull_ratio):\n",
    "    if pd.isna(bankfull_ratio):\n",
    "        return np.nan\n",
    "    elif 0 <= bankfull_ratio < 1.2:\n",
    "        return 'A'\n",
    "    elif 1.2 <= bankfull_ratio < 1.6:\n",
    "        return 'B'\n",
    "    elif 1.6 <= bankfull_ratio < 2.1:\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'D'\n",
    "\n",
    "#Define Grade for Bioassessment Score\n",
    "def categorize_csci(biotic_integrity):\n",
    "     if pd.isna(biotic_integrity):\n",
    "        return np.nan\n",
    "     elif   biotic_integrity > 0.92:\n",
    "        return 'A'\n",
    "     elif 0.79 < biotic_integrity <= 0.92:\n",
    "        return 'B'\n",
    "     elif 0.62 < biotic_integrity <= 0.79:\n",
    "        return 'C'\n",
    "     else:\n",
    "        return 'D'\n",
    "\n",
    "#Define Priority List Level of Invasive Plant Species\n",
    "    \n",
    "\n",
    "# Process Data\n",
    "def rate_invasive(priority):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if (priority[3] + priority[4] == 1):\n",
    "        return 'B'  # Assign score D\n",
    "    elif (priority[3] + priority[4] == 2) or priority[1] == 1 or priority[2] == 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif (priority[3] + priority[4] >= 3)or priority[1] >= 2 or priority[2] >= 2 or (priority[1] + priority[2] >= 2):  \n",
    "       return 'D'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no invasives of any priority are present)\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Define Size for Headcut based off of headcut size\n",
    "##A = 0 headcut, B 1+small headcut\n",
    "def categorize_headcut(headcutdepth):\n",
    "    if pd.isnull(headcutdepth) or headcutdepth == 0:\n",
    "        return 'None'\n",
    "    elif 0.1 <= headcutdepth < 0.5:\n",
    "        return 'small'\n",
    "    elif 0.5 <= headcutdepth < 1:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'large'\n",
    "\n",
    "#define rating for headcut health per sez\n",
    "def rate_headcut(row):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if row['large'] >= 1:\n",
    "        return 'D'  # Assign score D\n",
    "    elif row['medium'] >= 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif row['small'] >= 1:\n",
    "       return 'B'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no headcuts)\n",
    "\n",
    "\n",
    "#define rating SEZ Rating\n",
    "def rate_SEZ(percent):\n",
    "    if 0 <= percent < .69:\n",
    "        return 'D'\n",
    "    elif .7 <= percent < .79:\n",
    "        return 'C'\n",
    "    elif .80 <= percent <= .89:\n",
    "        return 'B'\n",
    "    else:\n",
    "        return 'A'\n",
    "    \n",
    "    #Define Grade for IPI Score - Used only for Stream HAbitat Condition\n",
    "def categorize_phab(IPI):\n",
    "     if   IPI >= 0.94:\n",
    "        return 'A'\n",
    "     elif 0.83 < IPI < 0.94:\n",
    "        return 'B'\n",
    "     elif 0.7 < IPI <= 0.83:\n",
    "        return 'C'\n",
    "     else:\n",
    "        return 'D'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Erosion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Get Data\n",
    "#----------------------------------------------------------------#\n",
    "#Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "erosionfields = ['Assessment_Unit_Name', 'Shape.STLength()', 'Bank_Type', 'Survey_Date']\n",
    "#erosiondf = pd.DataFrame.from_records(data=arcpy.da.SearchCursor(erosiondata, erosionfields), columns=erosionfields)\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    " # Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(erosiondata, erosionfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "erosiondf = pd.DataFrame(data, columns=erosionfields)\n",
    "\n",
    "# Replace NaN values in 'Assessment_Unit_Name' column with 'Skylandia SEZ'\n",
    "#erosiondf['Assessment_Unit_Name'] = erosiondf['Assessment_Unit_Name'].fillna('Skylandia SEZ')\n",
    "# Replace specific values in 'Assessment_Unit_Name' column\n",
    "erosiondf['Assessment_Unit_Name'] = erosiondf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh'})\n",
    "\n",
    "\n",
    "#Add SEZ_ID column using lookup dictionary\n",
    "#erosiondf['SEZ_ID'] = erosiondf['Assessment_Unit_Name'].map(lambda x: lookup_dict.get(x, {}).get('SEZ_ID', None))\n",
    "\n",
    "#This code is for the excel look up dictionary\n",
    "erosiondf['SEZ_ID'] = erosiondf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "erosiondf['SEZ_ID'] = erosiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "erosiondf['SEZ_ID'] = erosiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#erosiondf = erosiondf.merge(df, on='Assessment_Unit_Name', how='left')\n",
    "\n",
    "#calculate year column \n",
    "erosiondf['Year'] = erosiondf['Survey_Date'].dt.year\n",
    "\n",
    "# Replace 'both_banks' with 'Both Banks' in Bank_Type column\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['both_banks', 'Both banks'], 'Both Banks' )\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['one_bank', 'One bank'], 'One Bank')\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['no_bank', 'No bank'], 'No Bank')\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Process Data\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "# Initialize variables\n",
    "erosiondf['bank_multiplier'] = erosiondf['Bank_Type'].apply(lambda x: 2 if x == 'Both Banks' else (1 if x == 'One Bank' else 0))\n",
    "\n",
    "\n",
    "# Calculate the product of 'Shape.STLength()' and 'bank_multiplier' to get the eroded banks per row\n",
    "erosiondf['eroded_banks_per_row'] = erosiondf['Shape.STLength()'] * erosiondf['bank_multiplier']\n",
    "\n",
    "# Group by Assessment_Unit_Name and year and sum the lengths of banks for each unit to get total banks assessed\n",
    "erosiondf['banks_assessed_per_unit'] = erosiondf.groupby(['Assessment_Unit_Name', 'Year'])['Shape.STLength()'].transform('sum') * 2\n",
    "\n",
    "# Group by Assessment_Unit_Name and sum the eroded banks per row for each unit\n",
    "erosiondf['SEZ_total_eroded'] = erosiondf.groupby(['Assessment_Unit_Name', 'Year'])['eroded_banks_per_row'].transform('sum')\n",
    "\n",
    "# Calculate percent unstable Bank Stability per Assessment Unit\n",
    "erosiondf['Bank_Stability_Percent_Unstable'] = (erosiondf['SEZ_total_eroded'] / erosiondf['banks_assessed_per_unit']) * 100\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score\n",
    "#----------------------------------------------------------------#\n",
    "erosiondf['Bank_Stability_Rating']=erosiondf['Bank_Stability_Percent_Unstable'].apply(categorize_erosion)\n",
    "erosiondf['Bank_Stability_Score']= erosiondf['Bank_Stability_Rating'].apply(score_indicator)\n",
    "\n",
    "erosiondf['Bank_Stability_Data_Source'] = 'TRPA'\n",
    "\n",
    "erosiondf.head()\n",
    "\n",
    "print(erosiondf)\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to bank_stability called stage_bank_stability GDB location\n",
    "#----------------------------------------------------------------#\n",
    "# Define the name of the feature class\n",
    "feature_class_name = 'bank_stability'\n",
    "\n",
    "# Define the full path to the feature class\n",
    "feature_class_path = stage_bank_stability \n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Bank_Stability_Data_Source': 'Bank_Stability_Data_Source',\n",
    "    'Bank_Stability_Percent_Unstable': 'Bank_Stability_Percent_Unstable',\n",
    "    'Bank_Stability_Rating': 'Bank_Stability_Rating',\n",
    "    'Bank_Stability_Score': 'Bank_Stability_Score',\n",
    "    'SEZ_ID': 'SEZ_ID'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "bank_stabilitydf = erosiondf.rename(columns=field_mapping).drop(columns=[col for col in erosiondf.columns if col not in field_mapping])\n",
    "\n",
    "readybankdf = bank_stabilitydf.groupby(['SEZ_ID', 'Year']).first().reset_index()\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "# Fix data type of Year so it writes to table\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y', errors='coerce')\n",
    "\n",
    "# Setting the frequency to 'Y' for year\n",
    "#readydf['Year'] = readydf['Year'].dt.to_period('Y')\n",
    "\n",
    "print(readybankdf)\n",
    "\n",
    "#Write dataframe to fc in SEZ_Data.GDB\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readybankdf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readybankdf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(stage_bank_stability, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")\n",
    "\n",
    "\n",
    "#Write dataframe to sde.collect.bank.stability eventually, current code write it to GDB in SEZ_Data.GDB\n",
    "# Set environment workspace to your SDE connection file\n",
    "#arcpy.env.workspace = master_path\n",
    "\n",
    "# Convert DataFrame to Feature Class\n",
    "#output_feature_class = \"ErosionUpdate\"  # Name for the output feature class\n",
    "#output_fc_path = os.path.join(arcpy.env.workspace, output_feature_class)\n",
    "\n",
    "# Assuming your DataFrame is already converted to a feature class\n",
    "# Replace \"path_to_your_feature_class\" with the actual path to your feature class\n",
    "#arcpy.conversion.TableToTable(\"path_to_your_feature_class\", arcpy.env.workspace, output_feature_class)\n",
    "\n",
    "# Overwrite Feature Class in SDE\n",
    "# Replace \"path_to_your_dataframe\" with the actual path to your DataFrame\n",
    "#arcpy.management.CopyFeatures(\"path_to_your_dataframe\", output_fc_path)\n",
    "\n",
    "\n",
    "#print(ready_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to CSV\n",
    "#csv_path = os.path.join(working_folder, \"Erosiondatamaster.csv\")\n",
    "#ready_df.to_csv(csv_path, index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Incision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Assessment_Unit_Name  incision_ratio         survey_date  \\\n",
      "0             Angora Creek - tributary             NaN 2023-05-24 12:00:00   \n",
      "1                   Angora meadows - 8             NaN 2023-05-24 12:00:00   \n",
      "2             Angora Creek - tributary             NaN 2023-05-24 12:00:00   \n",
      "3             Glenbrook Creek - middle             NaN 2023-05-30 12:00:00   \n",
      "4                Glenbrook meadows - 1             1.0 2023-05-30 12:00:00   \n",
      "..                                 ...             ...                 ...   \n",
      "230              Meeks Bay meadows - 2             1.0 2023-10-04 12:00:00   \n",
      "231           Angora Creek - tributary             NaN 2024-06-18 12:00:00   \n",
      "232              Meeks Bay meadows - 4             NaN 2023-10-04 12:00:00   \n",
      "233         Blackwood Creek - middle 4             NaN 2023-09-18 12:00:00   \n",
      "234  Heavenly Valley Creek meadows - 2             NaN 2024-06-17 12:00:00   \n",
      "\n",
      "     SEZ_ID  Year  \n",
      "0       519  2023  \n",
      "1        88  2023  \n",
      "2       519  2023  \n",
      "3       510  2023  \n",
      "4       177  2023  \n",
      "..      ...   ...  \n",
      "230     207  2023  \n",
      "231     519  2024  \n",
      "232     206  2023  \n",
      "233     525  2023  \n",
      "234     112  2024  \n",
      "\n",
      "[235 rows x 5 columns]\n",
      "     SEZ_ID  Year                          Assessment_Unit_Name  \\\n",
      "0         2  2019  Saxon Creek meadows - above Fountain Place 1   \n",
      "1         2  2022  Saxon Creek meadows - above Fountain Place 1   \n",
      "2         5  2023                           Spooner Meadows - 4   \n",
      "3         6  2023                    Slaughterhouse Meadows - 1   \n",
      "4         7  2023          Upper Truckee River - Tahoe Paradise   \n",
      "..      ...   ...                                           ...   \n",
      "204     633  2019                          Burton Creek - lower   \n",
      "205     641  2020                           Saxon Creek - upper   \n",
      "206     641  2022                           Saxon Creek - upper   \n",
      "207     641  2023                           Saxon Creek - upper   \n",
      "208     642  2020           Marlette Creek - south fork (lower)   \n",
      "\n",
      "     Incision_Ratio Incision_Rating Incision_Score Incision_Data_Source  \n",
      "0          2.542367               D              3                 TRPA  \n",
      "1          2.107667               D              3                 TRPA  \n",
      "2               NaN            None           None                 TRPA  \n",
      "3               NaN            None           None                 TRPA  \n",
      "4          1.651775               C              6                 TRPA  \n",
      "..              ...             ...            ...                  ...  \n",
      "204        1.000000               A             12                 TRPA  \n",
      "205        2.467957               D              3                 TRPA  \n",
      "206        1.529471               B              9                 TRPA  \n",
      "207        1.779729               C              6                 TRPA  \n",
      "208        2.363650               D              3                 TRPA  \n",
      "\n",
      "[209 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Get Data-- all the data we need is in sde.collect.sde.survey.sde.sez_survey\n",
    "#----------------------------------------------------------------##Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "incisionfields = ['Assessment_Unit_Name', 'incision_ratio', 'survey_date']\n",
    "#incisiondf = pd.DataFrame.from_records(data=arcpy.da.SearchCursor(erosiondata, erosionfields), columns=erosionfields)\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    " # Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(sezsurveytable, incisionfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "incisiondf = pd.DataFrame(data, columns=incisionfields)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Add correct info to dataframe\n",
    "#----------------------------------------------------------------#\n",
    "#use this until we fix the domain\n",
    "incisiondf['Assessment_Unit_Name'] = incisiondf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh'})\n",
    "# Create a new column 'SEZ ID'\n",
    "#This code is for the excel look up dictionary\n",
    "incisiondf['SEZ_ID'] = incisiondf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#calculate year column \n",
    "incisiondf['Year'] = incisiondf['survey_date'].dt.year\n",
    "\n",
    "incisiondf\n",
    "\n",
    "# Display the updated incisiondf\n",
    "print(incisiondf)\n",
    "\n",
    "#Add SEZ_ID column using lookup dictionary\n",
    "#incisiondf['SEZ_ID'] = SEZIDdf['GlobalID'].map(lambda x: lookup_dict.get(x, {}).get('SEZ_ID', None))\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "#incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "#incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#incisiondf = incisiondf.merge(df, on='Assessment_Unit_Name', how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score\n",
    "#----------------------------------------------------------------#\n",
    "incisiondf['Incision_Rating']=incisiondf['incision_ratio'].apply(categorize_incision)\n",
    "incisiondf['Incision_Score']= incisiondf['Incision_Rating'].apply(score_indicator)\n",
    "\n",
    "incisiondf['Incision_Data_Source'] = 'TRPA'\n",
    "\n",
    "incisiondf.head()\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to incision called stage_incision GDB location\n",
    "#----------------------------------------------------------------#\n",
    "# Define the name of the feature class\n",
    "feature_class_name = 'incision'\n",
    "\n",
    "# Define the full path to the feature class\n",
    "feature_class_path = stage_incision \n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Incision_Data_Source': 'Incision_Data_Source',\n",
    "    'incision_ratio': 'Incision_Ratio',\n",
    "    'Incision_Rating': 'Incision_Rating',\n",
    "    'Incision_Score': 'Incision_Score',\n",
    "    'SEZ_ID': 'SEZ_ID'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "incisionfinaldf = incisiondf.rename(columns=field_mapping).drop(columns=[col for col in incisiondf.columns if col not in field_mapping])\n",
    "\n",
    "readyincisiondf = incisionfinaldf.groupby(['SEZ_ID', 'Year']).first().reset_index()\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readyincisiondf)\n",
    "\n",
    "#Write dataframe to fc in SEZ_Data.GDB\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readyincisiondf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readyincisiondf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(stage_incision, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge readyincisiondf with SEZinfo_df to add the SEZ_Type\n",
    "readyincisiondf = readyincisiondf.merge(dfSEZ[['SEZ_ID', 'SEZ_Type']], on='SEZ_ID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invasive Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calfora Data points--dates only go to 2012?\n",
    "# dates only go to 2012 hereCArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines07/MapServer/1\"\n",
    "#CArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines27/MapServer/0\"\n",
    "# Create a feature layer object\n",
    "#cafeature_layer = FeatureLayer(CArest)\n",
    "#Query caflora layer to that it only shows 2020-2023 or only county?\n",
    "#start_year = 'January 1, 2020'\n",
    "#end_year= 'December 31, 2023'\n",
    "# Convert to datetime objects\n",
    "#start_date = pd.to_datetime(start_year)\n",
    "#end_date = pd.to_datetime(end_year)\n",
    "\n",
    "#Query before or after??\n",
    "#query = (CAsdf['Date_']>= start_date) & (CAsdf['Date_']<= end_date)\n",
    "\n",
    "#CA_filtereddf = CAsdf.join[query].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code using REST Service- most likely will reuse this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from arcgis.features import FeatureLayer\n",
    "from utils import get_fs_data_spatial_query\n",
    "\n",
    "# Define the USFS REST endpoint\n",
    "usfsrest = \"https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer/0\"\n",
    "where    = \"FS_UNIT_ID = '0519'\"\n",
    "\n",
    "# Query the feature layer\n",
    "sdfUSFS = get_fs_data_spatial_query(usfsrest, where)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spatial join of sdf and sez master\n",
    "\n",
    "\n",
    "# Create the spatially enabled DataFrame (sdf) for target feature SEZ assessment units\n",
    "SEZsdf = pd.DataFrame.spatial.from_featureclass(SEZ_Master)\n",
    "#spatial reference stuff\n",
    "sdfUSFS.spatial.sr = SEZsdf.spatial.sr\n",
    "\n",
    "#CAsdf.spatial.set_spatial_reference(SEZsdf.spatial.sr)\n",
    "#perform spatial join\n",
    "usfsdata = SEZsdf.spatial.join(sdfUSFS, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(invasivedf['plant_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "#SET UP DATA WRANGLE\n",
    "#-----------------------------------------\n",
    "\n",
    "#Path to external data usfs with rest service--This assumes rest service is up to date to 2023\n",
    "\n",
    "# Paths to the feature classes\n",
    "invasive19fc = os.path.join(invasiveplant19gdb, \"Invasive_Species_2019\")\n",
    "invasive20fc = os.path.join(invasiveplant20gdb, \"Invasive_Species_2020\")\n",
    "invasive22fc = os.path.join(invasiveplant22gdb, \"Invasive_Species_2022\")\n",
    "invasive23fc = os.path.join(invasiveplant23gdb, \"sez_invasive_plant\")\n",
    "\n",
    "\n",
    "#Use gdb because they have assessment unit name in them... 2022 i had to manually add in PRO\n",
    "invasive23fields = ['Assessment_Unit', 'invasives_percent_cover','invasives_plant_type', 'Species_Level', 'invasive_type_other', 'created_date']\n",
    "invasive22fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover', 'InvasiveType_Other', 'Survey_Date']\n",
    "invasive20fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover',  'Other', 'Survey_Date']\n",
    "invasive19fields = ['SITE_ID', 'SURVEY_DATE', 'INVASIVE_PLANT', 'PERCENT_COVER' ]\n",
    "usfsfields = ['Assessment_Unit_Name', 'COMMON_NAME', 'SCIENTIFIC_NAME']\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "\n",
    "# Function to read feature class into DataFrame\n",
    "def feature_class_to_dataframe(feature_class, fields):\n",
    "    data = [row for row in arcpy.da.SearchCursor(feature_class, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Read feature classes into DataFrames\n",
    "# Read feature classes into DataFrames\n",
    "invasive19df = feature_class_to_dataframe(invasive19fc, invasive19fields)\n",
    "invasive20df = feature_class_to_dataframe(invasive20fc, invasive20fields)\n",
    "invasive22df = feature_class_to_dataframe(invasive22fc, invasive22fields)\n",
    "invasive23df = feature_class_to_dataframe(invasive23fc, invasive23fields)\n",
    "# Create a new DataFrame with only the specified columns\n",
    "# Convert SpatialDataFrame to DataFrame\n",
    "\n",
    "usfsdf = usfsdata[usfsfields]\n",
    "#usfsdf = usfsdata.drop(columns='SHAPE')\n",
    "\n",
    "print(usfsdf)\n",
    "#usfs23df = feature_class_to_dataframe(usfsdata, usfsfields)\n",
    "\n",
    "\n",
    "invasive19df['Source'] = 'TRPA'\n",
    "invasive20df['Source'] = 'TRPA'\n",
    "invasive22df['Source'] = 'TRPA'\n",
    "invasive23df['Source'] = 'TRPA'\n",
    "usfsdf['Source'] = 'USFS'\n",
    "\n",
    "#usfsdf['Year'] = '2023'\n",
    "\n",
    "\n",
    "# Rename fields\n",
    "invasive19df.rename(columns={'SITE_ID': 'Assessment_Unit_Name', 'INVASIVE_PLANT': 'plant_type', 'PERCENT_COVER': 'percent_cover', 'SURVEY_DATE': 'created_date','Source':'Source'}, inplace=True)\n",
    "invasive20df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date', 'Source':'Source'}, inplace=True)\n",
    "invasive22df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'InvasiveType_Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date','Source':'Source'}, inplace=True)\n",
    "invasive23df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'invasives_percent_cover': 'percent_cover', 'invasives_plant_type': 'plant_type', 'invasive_type_other': 'other', 'created_date': 'created_date','Source':'Source'}, inplace=True)\n",
    "usfsdf.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'COMMON_NAME': 'plant_type', 'Source':'Source'}, inplace=True)\n",
    "#Remove null plant types for usfs data\n",
    "usfsdf = usfsdf[~usfsdf['plant_type'].isna()]\n",
    "\n",
    "\n",
    "# Concatenate DataFrames\n",
    "invasivedf = pd.concat([usfsdf, invasive19df, invasive20df, invasive22df, invasive23df], ignore_index=True)\n",
    "# Display merged DataFrame\n",
    "print(invasivedf)\n",
    "\n",
    "\n",
    "#Define SEZ ID based on Assessment_Unit_Name for QA on SEZ Name \n",
    "invasivedf['SEZ_ID'] = invasivedf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "#invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].astype(int)\n",
    "invasivedf['Assessment_Unit_Name'] = invasivedf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky Meadows meadow - 1': 'Sky meadows'})\n",
    "# Remove rows where data source is USFS and plant type is NaN\n",
    "#invasivedf = invasivedf[~((invasivedf['Source'] == 'USFS') & invasivedf['plant_type'].isna())]\n",
    "\n",
    "# Set 'Year' column based on data source\n",
    "\n",
    "invasivedf.loc[invasivedf['Source'] == 'USFS', 'Year'] = '2023'\n",
    "invasivedf.loc[invasivedf['Source'] == 'TRPA', 'Year'] = invasivedf['created_date'].dt.year\n",
    "\n",
    "#invasivedf.reset_index(drop=True, inplace=True)\n",
    "invasivedf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#invasivedf = invasivedf.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip this\n",
    "invasivedf_values_unique = invasivedf.values.flatten()\n",
    "is_unique = len(invasivedf_values_unique) == len(set(invasivedf_values_unique))\n",
    "print(is_unique)\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = invasivedf[invasivedf.duplicated(keep=False)]\n",
    "\n",
    "# Display duplicate rows\n",
    "print(duplicate_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------#\n",
    "    #Prep Plant_type Data\n",
    "#---------------------------#\n",
    "#Make a dataframe to capture 'other' plants in trpa data and then add it to invasive df\n",
    "other_plants_df = invasivedf[['Source', 'Year', 'SEZ_ID', 'Assessment_Unit_Name', 'Year', 'other']].copy()\n",
    "\n",
    "#Get rid of Null values\n",
    "other_plants_df = other_plants_df[~other_plants_df['other'].isna()]\n",
    "\n",
    "#invasivedf.reset_index(drop=True, inplace=True)\n",
    "other_plants_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "#Rename 'other to plant_type\n",
    "other_plants_df.rename(columns={'other': 'plant_type'}, inplace=True)\n",
    "\n",
    "#invasivedf = invasivedf.drop_duplicates()\n",
    "\n",
    "\n",
    "# Concatenate other_plants_df with invasivedf JUST DO THIS MANUALLY \n",
    "#invasivedf = pd.concat([invasivedf, other_plants_df], ignore_index=True)\n",
    "\n",
    "#Append\n",
    "#invasivesdf=invasivedf.append(other_plants_df)\n",
    "#Concatenate the new DataFrame with the existing invasivedf\n",
    "#invasivedf = pd.concat([invasivedf, other_plants_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Replace various representations of null values with 'none'\n",
    "null_representations = ['<null>', '<Null>', '', 'NA', 'N/A', 'nan', 'NaN', 'None', 'NULL', None]\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace(null_representations, 'none')\n",
    "\n",
    "# Split plant types by comma and create new rows\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.split(pat=',')\n",
    "invasivedf = invasivedf.explode('plant_type')\n",
    "\n",
    "# Capitalize the first word and replace underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.split('_').str[0].str.capitalize() + ' ' + invasivedf['plant_type'].str.split('_').str[1:].str.join(' ')\n",
    "\n",
    "# Remove spaces after the words and capitalize the first word while replacing underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].astype(str)\n",
    "\n",
    "\n",
    "# Remove any spaces at the beginning or end of a word\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.strip()\n",
    "\n",
    "\n",
    "#---------------------#\n",
    "#These replacements are so the same weed with different common name doesn't get counted twice toward score MAY NEED TO ADD MORE DESCREPENCIES in coming years if more pop up or find more elopquent solution\n",
    "#---------------------#\n",
    "# Replace 'Common mullein' with 'Wooly mullein' in the 'plant_type' column \n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common mullein', 'Wooly mullein')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Nodding plumeless thistle', 'Musk thistle')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Field bindweed', 'Common bindweed')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common st. johnswort', 'Klamathweed')\n",
    "# Replace 'Broadleaf and leaved' with 'Perennial' in the 'plant_type' column\n",
    "# Define the replacements dictionary\n",
    "replacements = {'Broadleaf Pepperweed': 'Perennial pepperweed', 'Broadleaved pepperweed': 'Perennial pepperweed'}\n",
    "\n",
    "# Replace values in the 'plant_type' column using the dictionary\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace(replacements)\n",
    "\n",
    "#Replace Sulphur Cinquefoil with sulfur\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sulphur cinquefoil', 'Sulfur cinquefoil')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sweetclover', 'White sweetclover')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Reed canary grass', 'Reed canarygrass')\n",
    "#Replace\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Salt cedar', 'Tamarisk')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Butter and eggs', 'Yellow toadflax')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Canada cottonthistle', 'Canada thistle')\n",
    "# Replace empty strings or other placeholders with NaN\n",
    "#invasivedf['plant_type'] = invasivedf['plant_type'].replace('', np.nan)\n",
    "\n",
    "# Drop duplicates based on 'Assessment_Unit_Name' and 'Year' in the remaining DataFrame\n",
    "invasivedf = invasivedf.drop_duplicates(subset=['Assessment_Unit_Name', 'Year', 'plant_type'], keep='first')\n",
    "\n",
    "\n",
    "grouped_df = invasivedf.groupby(['Assessment_Unit_Name', 'Year'])['plant_type']\n",
    "\n",
    "# Aggregate the plant types into one column separated by commas\n",
    "combined_plant_types = grouped_df.apply(lambda x: ', '.join(x)).reset_index(name='all_plant_types')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group, data in invasivedf.groupby(['Assessment_Unit_Name', 'Year']):\n",
    "    print(group)\n",
    "    print(data)\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------#\n",
    "#Create Look up dictionary to assign priority group for each plant and then use it\n",
    "#----------------------------------------------------#\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "csv_data = pd.read_csv(r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Invasives Priority lookup.csv\") \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "Invasives_lookup = {}\n",
    "\n",
    "key = 'Common'\n",
    "values = ['Scientific', 'Priority'] \n",
    "\n",
    "Invasives_lookup= csv_data.set_index(key)[values].to_dict(orient='index')\n",
    "\n",
    "# Define a custom function to map plant types to priorities\n",
    "def map_priority(plant_type):\n",
    "    if pd.isnull(plant_type):\n",
    "        return 'None' # Return NaN for NaN values\n",
    "    else:\n",
    "        # Extract the priority from the dictionary, or return 'Unknown' if not found\n",
    "        plant_info = Invasives_lookup.get(plant_type)\n",
    "        if plant_info:\n",
    "            return plant_info['Priority']\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "\n",
    "\n",
    "# Create a new column 'Priority' based on the mapping from the dictionary\n",
    "invasivedf['Priority'] = invasivedf['plant_type'].map(map_priority)\n",
    "\n",
    "print(invasivedf.columns)\n",
    "#invasivedf['Priority'] = pd.to_numeric(invasivedf['Priority'], errors='coerce')\n",
    "\n",
    "# group by assessment unit and year and summarize the priority level of plants in each unit\n",
    "invasive_summary = invasivedf.groupby(['Assessment_Unit_Name', 'Year', 'Priority','Source'], dropna=False).size().reset_index(name='Count')\n",
    "\n",
    "invasive_summary_priority = invasive_summary.pivot_table(index=['Assessment_Unit_Name', 'Year','Source'], columns='Priority', values='Count', fill_value=0)\n",
    "\n",
    "\n",
    "# Reset the index to flatten the DataFrame\n",
    "invasive_summary_priority.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Grade, Score, # of Invasives\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "# Process Data\n",
    "def rate_invasive(priority):\n",
    "    if (priority[3] + priority[4] == 1):\n",
    "        return 'B'  # Assign score D\n",
    "    elif (priority[3] + priority[4] == 2) or priority[1] == 1 or priority[2] == 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif (priority[3] + priority[4] >= 3)or priority[1] >= 2 or priority[2] >= 2 or (priority[1] + priority[2] >= 2):  \n",
    "       return 'D'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no invasives of any priority are present)\n",
    "\n",
    "# Apply the rating function to the summary DataFrame\n",
    "invasive_summary_priority['Invasives_Rating'] = invasive_summary_priority[[1, 2, 3, 4]].apply(rate_invasive, axis=1)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "invasive_summary_priority['Invasives_Score']= invasive_summary_priority['Invasives_Rating'].apply(score_indicator) \n",
    "\n",
    "#Calculate total number of invasives per sez per year\n",
    "invasive_summary_priority['Number_of_Invasives']= invasive_summary_priority[[1, 2, 3, 4]].sum(axis=1)\n",
    "\n",
    "#Define SEZ ID based on Assessment_Unit_Name for QA on SEZ Name \n",
    "invasive_summary_priority['SEZ_ID'] = invasive_summary_priority['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "invasive_summary_priority['all_plants']= combined_plant_types['all_plant_types']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Prep and post ending dataframe to invasives table in SEZ_Data.GDB\n",
    "#----------------------------------------------------------------#\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Source': 'Invasives_Data_Source',\n",
    "    'Number_of_Invasives': 'Invasives_Number_of_Invasives',\n",
    "    'Invasives_Rating': 'Invasives_Rating',\n",
    "    'Invasives_Score': 'Invasives_Scores',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'percent_cover': 'Invasives_Percent_Cover',\n",
    "    'all_plants': 'Invasives_Plant_Types',\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readyinvasivedf = invasive_summary_priority.rename(columns=field_mapping).drop(columns=[col for col in invasive_summary_priority.columns if col not in field_mapping])\n",
    "\n",
    "readyinvasivedf['SEZ_ID'] = readyinvasivedf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readyinvasivedf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readyinvasivedf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readyinvasivedf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "#with arcpy.da.InsertCursor(stage_invasives, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invasive with gdb and usfs pre joined layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invasive Species Use if need to use GDB to import data--shouldn't have to\n",
    "\n",
    "\n",
    "#Path to external data usfs \n",
    "usfsdata = os.path.join(master_path, \"usfsinvasivesez24\")\n",
    "#Path to external data Calflora aka State Park Data\n",
    "#calfloradata = os.path.join(master_path, \"\")\n",
    "\n",
    "# Paths to the feature classes\n",
    "invasive19fc = os.path.join(invasiveplant19gdb, \"Invasive_Species_2019\")\n",
    "invasive20fc = os.path.join(invasiveplant20gdb, \"Invasive_Species_2020\")\n",
    "invasive22fc = os.path.join(invasiveplant22gdb, \"Invasive_Species_2022\")\n",
    "invasive23fc = os.path.join(invasiveplant23gdb, \"sez_invasive_plant\")\n",
    "\n",
    "\n",
    "#Use gdb because they have assessment unit name in them... 2022 i had to manually add in PRO\n",
    "invasive23fields = ['Assessment_Unit', 'invasives_percent_cover','invasives_plant_type', 'Species_Level', 'invasive_type_other', 'created_date']\n",
    "invasive22fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover', 'InvasiveType_Other', 'Survey_Date']\n",
    "invasive20fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover',  'Other', 'Survey_Date']\n",
    "invasive19fields = ['SITE_ID', 'SURVEY_DATE', 'INVASIVE_PLANT', 'PERCENT_COVER' ]\n",
    "usfsfields = ['Assessment_Unit_Name', 'PLANT_COMM', 'SCIENTIFIC', 'DATE_COLLE', 'Eradicated']\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "\n",
    "# Function to read feature class into DataFrame\n",
    "def feature_class_to_dataframe(feature_class, fields):\n",
    "    data = [row for row in arcpy.da.SearchCursor(feature_class, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Read feature classes into DataFrames\n",
    "# Read feature classes into DataFrames\n",
    "invasive19df = feature_class_to_dataframe(invasive19fc, invasive19fields)\n",
    "invasive20df = feature_class_to_dataframe(invasive20fc, invasive20fields)\n",
    "invasive22df = feature_class_to_dataframe(invasive22fc, invasive22fields)\n",
    "invasive23df = feature_class_to_dataframe(invasive23fc, invasive23fields)\n",
    "usfs23df = feature_class_to_dataframe(usfsdata, usfsfields)\n",
    "\n",
    "\n",
    "invasive19df['Source'] = 'TRPA'\n",
    "invasive20df['Source'] = 'TRPA'\n",
    "invasive22df['Source'] = 'TRPA'\n",
    "invasive23df['Source'] = 'TRPA'\n",
    "usfs23df['Source'] = 'USFS'\n",
    "\n",
    "# Rename fields\n",
    "invasive19df.rename(columns={'SITE_ID': 'Assessment_Unit_Name', 'INVASIVE_PLANT': 'plant_type', 'PERCENT_COVER': 'percent_cover', 'SURVEY_DATE': 'created_date','Source':'Source'}, inplace=True)\n",
    "invasive20df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date', 'Source':'Source1'}, inplace=True)\n",
    "invasive22df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'InvasiveType_Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date','Source':'Source1'}, inplace=True)\n",
    "invasive23df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'invasives_percent_cover': 'percent_cover', 'invasives_plant_type': 'plant_type', 'invasive_type_other': 'other', 'created_date': 'created_date','Source':'Source1'}, inplace=True)\n",
    "usfs23df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'PLANT_COMM':'plant_type', 'DATE_COLLE':'created_date', 'Source':'Source2'}, inplace=True)\n",
    "\n",
    "\n",
    "# Concatenate DataFrames\n",
    "invasivedf = pd.concat([usfs23df, invasive19df, invasive20df, invasive22df, invasive23df], ignore_index=True)\n",
    "\n",
    "# Display merged DataFrame\n",
    "print(invasivedf)\n",
    "\n",
    "#Define SEZ ID based on Assessment_Unit_Name\n",
    "invasivedf['SEZ_ID'] = invasivedf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].astype(int)\n",
    "invasivedf['Assessment_Unit_Name'] = invasivedf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky Meadows meadow - 1': 'Sky meadows'})\n",
    "\n",
    "#calculate year column \n",
    "invasivedf['Year'] = invasivedf['created_date'].dt.year\n",
    "\n",
    "\n",
    "#---------------------------#\n",
    "    #Prep Data\n",
    "#---------------------------#\n",
    "# Replace 'other' or 'Other' in 'plant_type' column with values from 'other' column\n",
    "invasivedf['plant_type'] = invasivedf.apply(lambda row: row['other'] if pd.notna(row['plant_type']) and row['plant_type'].lower() in ['other', 'Other'] else row['plant_type'], axis=1)\n",
    "\n",
    "# Drop the 'other' column\n",
    "invasivedf.drop(columns=['other'], inplace=True)\n",
    "\n",
    "# Function to separate plant types and create new rows\n",
    "def separate_species(df):\n",
    "    # Split plant types by comma and create new rows\n",
    "    df['plant_type'] = df['plant_type'].str.split(',')\n",
    "    df = df.explode('plant_type')\n",
    "    return df\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "invasivedf = separate_species(invasivedf)\n",
    "\n",
    "# Remove rows where data source is USFS and plant type is NaN\n",
    "invasivedf = invasivedf[~((invasivedf['Source'] == 'USFS') & invasivedf['plant_type'].isna())]\n",
    "\n",
    "# Capitalize the first word and replace underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.split('_').str[0].str.capitalize() + ' ' + invasivedf['plant_type'].str.split('_').str[1:].str.join(' ')\n",
    "\n",
    "# Remove spaces after the words and capitalize the first word while replacing underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].astype(str)\n",
    "\n",
    "\n",
    "# Remove any spaces at the beginning or end of a word\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.strip()\n",
    "\n",
    "\n",
    "# Remove rows where data source is USFS and plant type is NaN\n",
    "invasivedf = invasivedf[~((invasivedf['Source'] == 'USFS') & invasivedf['plant_type'].isna())]\n",
    "#Remove Eradicated \n",
    "\n",
    "# Filter out rows where 'eradicated' column is 'Yes'\n",
    "invasivedf = invasivedf[invasivedf['Eradicated'] != 'Yes']\n",
    "\n",
    "#These replacements are so the same weed with different common name doesn't get counted twice toward score MAY NEED TO ADD MORE DESCREPENCIES in coming years if more pop up or find more elopquent solution\n",
    "# Replace 'Common mullein' with 'Wooly mullein' in the 'plant_type' column \n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common mullein', 'Wooly mullein')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Nodding plumeless thistle', 'Musk thistle')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Field bindweed', 'Common bindweed')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common st. johnswort', 'Klamathweed')\n",
    "# Replace 'Broadleaf and leaved' with 'Perennial' in the 'plant_type' column\n",
    "# Define the replacements dictionary\n",
    "replacements = {'Broadleaf Pepperweed': 'Perennial pepperweed', 'Broadleaved pepperweed': 'Perennial pepperweed'}\n",
    "\n",
    "# Replace values in the 'plant_type' column using the dictionary\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace(replacements)\n",
    "\n",
    "#Replace Sulphur Cinquefoil with sulfur\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sulphur cinquefoil', 'Sulfur cinquefoil')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sweetclover', 'White sweetclover')\n",
    "\n",
    "#Replace\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Salt cedar', 'Tamarisk')\n",
    "\n",
    "# Now, drop duplicates based on the specified subset of columns\n",
    "invasivedf = invasivedf.drop_duplicates(subset=['SEZ_ID', 'Year', 'plant_type'])\n",
    "\n",
    "# Reset index if needed\n",
    "invasivedf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Remove duplicates based on SEZ, Year, and plant_type\n",
    "invasivedf = invasivedf.drop_duplicates(subset=['SEZ_ID', 'Year', 'plant_type'])\n",
    "#----------------------------------------------------#\n",
    "#Create Look up dictionary to assign priority group for each plant and then use it\n",
    "#----------------------------------------------------#\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "csv_data = pd.read_csv(r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Invasives Priority lookup.csv\") \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "Invasives_lookup = {}\n",
    "\n",
    "key = 'Common'\n",
    "values = ['Scientific', 'Priority'] \n",
    "\n",
    "Invasives_lookup= csv_data.set_index(key)[values].to_dict(orient='index')\n",
    "\n",
    "# Define a custom function to map plant types to priorities\n",
    "def map_priority(plant_type):\n",
    "    if pd.isnull(plant_type):\n",
    "        return np.nan  # Return NaN for NaN values\n",
    "    else:\n",
    "        # Extract the priority from the dictionary, or return 'Unknown' if not found\n",
    "        plant_info = Invasives_lookup.get(plant_type)\n",
    "        if plant_info:\n",
    "            return plant_info['Priority']\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "\n",
    "#Create a new column [Scientific based on look up dictionary\n",
    "#invasivedf['Scientific']=invasivedf['plant_type'].map(map_priority)\n",
    "\n",
    "# Create a new column 'Priority' based on the mapping from the dictionary\n",
    "invasivedf['Priority'] = invasivedf['plant_type'].map(map_priority)\n",
    "\n",
    "#invasivedf['Priority'] = pd.to_numeric(invasivedf['Priority'], errors='coerce')\n",
    "# group by assessment unit and year and summarize the priority level of plants in each unit\n",
    "invasive_summary = invasivedf.groupby(['Assessment_Unit_Name', 'Year', 'Priority']).size().reset_index(name='Count')\n",
    "\n",
    "invasive_summary_priority2 = invasive_summary.pivot_table(index=['Assessment_Unit_Name', 'Year'], columns='Priority', values='Count', fill_value=0)\n",
    "\n",
    "# Reset the index to flatten the DataFrame\n",
    "invasive_summary_priority2.reset_index(inplace=True)\n",
    "\n",
    "#invasive_summary_priority['Source'] = invasivedf['Source']\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score, # of Headcuts\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "# Process Data\n",
    "def rate_invasive(priority):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if (priority[3] + priority[4] == 1):\n",
    "        return 'B'  # Assign score D\n",
    "    elif (priority[3] + priority[4] == 2) or priority[1] == 1 or priority[2] == 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif (priority[3] + priority[4] >= 3)or priority[1] >= 2 or priority[2] >= 2 or (priority[1] + priority[2] >= 2):  \n",
    "       return 'B'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no invasives of any priority are present)\n",
    "\n",
    "# Apply the rating function to the summary DataFrame\n",
    "invasive_summary_priority2['Invasives_Rating'] = invasive_summary_priority2.apply(rate_invasive, axis=1)\n",
    "\n",
    "#Calculate total number of invasives per sez per year\n",
    "invasive_summary_priority2['Number_of_Invasives']= invasive_summary_priority2[[1, 2, 3, 4,'Unknown']].sum(axis=1)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "invasive_summary_priority2['Invasives_Score']= invasive_summary_priority2['Invasives_Rating'].apply(score_indicator)    \n",
    "\n",
    "# make a columns in invasive summary that totals up percent cover per sez/year\n",
    "invasive_summary_priority2['Invasives_Percent_Cover'] = invasivedf.groupby(['SEZ_ID', 'Year'])['percent_cover'].sum().reset_index(drop=True)\n",
    "\n",
    "# combine the source column so that it shows all data sources that contributed to the data\n",
    "# Group invasivedf by 'SEZ_ID' and 'Year' and concatenate 'Data Source 1' values\n",
    "#data_source_1_combined = invasivedf.groupby(['SEZ_ID', 'Year'])['Source 1'].apply(lambda x: ', '.join(x.dropna())).reset_index()\n",
    "\n",
    "# Group invasivedf by 'SEZ_ID' and 'Year' and concatenate 'Data Source 2' values\n",
    "#data_source_2_combined = invasivedf.groupby(['SEZ_ID', 'Year'])['Source 2'].apply(lambda x: ', '.join(x.dropna())).reset_index()\n",
    "\n",
    "# Merge data_source_1_combined and data_source_2_combined on 'SEZ_ID' and 'Year'\n",
    "#merged_data_sources = pd.merge(data_source_1_combined, data_source_2_combined, on=['SEZ_ID', 'Year'], how='outer')\n",
    "\n",
    "# Combine 'Data Source 1' and 'Data Source 2' values with a comma separator\n",
    "#merged_data_sources['Data_Sources'] = merged_data_sources.apply(lambda row: ', '.join(filter(None, [row['Source 1'], row['Source 2']])), axis=1)\n",
    "\n",
    "# Drop the individual 'Data Source 1' and 'Data Source 2' columns\n",
    "#merged_data_sources.drop(columns=['Source 1', 'Source 2'], inplace=True)\n",
    "\n",
    "# Merge merged_data_sources with invasive_summary_priority on 'SEZ_ID' and 'Year'\n",
    "#invasive_summary_priority = pd.merge(invasive_summary_priority, merged_data_sources, on=['SEZ_ID', 'Year'], how='left')\n",
    "\n",
    "#invasive_summary_priority['Data_Sources']= invasivedfinvasivedf.groupby(['SED_ID', 'Year'])[Data Source 1 ] merge with DataSource 1 separate with comma if there are both \n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to incision called stage_incision GDB location\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Data_Sources': 'Invasives_Data_Source',\n",
    "    'Number_of_Invasives': 'Invasives_Number_of_Invasives',\n",
    "    'Invasives_Rating': 'Invasives_Rating',\n",
    "    'Invasives_Score': 'Invasives_Scores',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'percent_cover': 'Invasives_Percent_Cover',\n",
    "    'plant_type': 'Invasives_Plant_Type',\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "#readydf = invasive_summary_priority.rename(columns=field_mapping).drop(columns=[col for col in headcut_summary_sml.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "#print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "#data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "#field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(stage_headcuts, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data type of 'plant_type' column in the DataFrame\n",
    "print(\"Data type of 'plant_type' column in DataFrame:\", invasivedf['plant_type'].dtype)\n",
    "\n",
    "# Check the data type of values in the lookup dictionary\n",
    "for key, value in Invasives_lookup.items():\n",
    "    print(\"Data type of value for key\", key, \"in lookup dictionary:\", type(value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headcuts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Assessment_Unit_Name  headcut_depth        created_date\n",
      "0                   Susquehana meadows - 1           0.50 2019-05-13 00:00:00\n",
      "1                   Susquehana meadows - 1           0.50 2019-05-13 00:00:00\n",
      "2                  Golden Bear meadows - 1           0.60 2019-05-13 00:00:00\n",
      "3                  Golden Bear meadows - 1           0.40 2019-05-13 00:00:00\n",
      "4                  Golden Bear meadows - 1           0.70 2019-05-13 00:00:00\n",
      "..                                     ...            ...                 ...\n",
      "601     Trout Creek headwaters meadows - 1           0.40 2023-08-18 18:10:40\n",
      "602     Trout Creek headwaters meadows - 1           0.44 2023-08-18 18:10:40\n",
      "603  Trout Creek meadows - above Pioneer 4           1.30 2023-08-30 23:05:09\n",
      "604               Incline Lake meadows - 2           0.50 2023-09-18 15:26:54\n",
      "605               Incline Lake meadows - 2           1.30 2023-09-18 15:26:54\n",
      "\n",
      "[606 rows x 3 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Headcut_Size      Assessment_Unit_Name  Year  None  large  medium  small\n",
      "0             Angora Creek - tributary  2020     1      0       0      0\n",
      "1             Angora Creek - tributary  2022     1      0       0      0\n",
      "2             Angora Creek - tributary  2023     1      0       0      4\n",
      "3               Angora creek-tributary  2023     1      0       0      0\n",
      "4                   Angora meadows - 1  2019     1      0       0      0\n",
      "..                                 ...   ...   ...    ...     ...    ...\n",
      "343                    small meadow 96  2019     1      0       0      0\n",
      "344                    small meadow 98  2019     1      0       0      0\n",
      "345                    small meadow 99  2020     1      0       0      0\n",
      "346                    small meadow 99  2022     1      0       0      0\n",
      "347                     unnamed meadow  2019     1      0       0      0\n",
      "\n",
      "[348 rows x 6 columns]\n",
      "Headcut_Size      Assessment_Unit_Name  Year  large  medium  small  \\\n",
      "0             Angora Creek - tributary  2020      0       0      0   \n",
      "1             Angora Creek - tributary  2022      0       0      0   \n",
      "2             Angora Creek - tributary  2023      0       0      4   \n",
      "3               Angora creek-tributary  2023      0       0      0   \n",
      "4                   Angora meadows - 1  2019      0       0      0   \n",
      "..                                 ...   ...    ...     ...    ...   \n",
      "343                    small meadow 96  2019      0       0      0   \n",
      "344                    small meadow 98  2019      0       0      0   \n",
      "345                    small meadow 99  2020      0       0      0   \n",
      "346                    small meadow 99  2022      0       0      0   \n",
      "347                     unnamed meadow  2019      0       0      0   \n",
      "\n",
      "Headcut_Size Headcuts_Rating  Number_of_Headcuts Headcuts_Score  \\\n",
      "0                          A                   0             12   \n",
      "1                          A                   0             12   \n",
      "2                          B                   4              9   \n",
      "3                          A                   0             12   \n",
      "4                          A                   0             12   \n",
      "..                       ...                 ...            ...   \n",
      "343                        A                   0             12   \n",
      "344                        A                   0             12   \n",
      "345                        A                   0             12   \n",
      "346                        A                   0             12   \n",
      "347                        A                   0             12   \n",
      "\n",
      "Headcut_Size Headcuts_Data_Source  SEZ_ID  \n",
      "0                            TRPA   519.0  \n",
      "1                            TRPA   519.0  \n",
      "2                            TRPA   519.0  \n",
      "3                            TRPA     NaN  \n",
      "4                            TRPA    87.0  \n",
      "..                            ...     ...  \n",
      "343                          TRPA    44.0  \n",
      "344                          TRPA    62.0  \n",
      "345                          TRPA    75.0  \n",
      "346                          TRPA    75.0  \n",
      "347                          TRPA     NaN  \n",
      "\n",
      "[348 rows x 10 columns]\n",
      "Data appended to table successfully.\n"
     ]
    }
   ],
   "source": [
    "#Headcuts\n",
    "#--------------------------------#\n",
    "#Get Data from GDB's with assessment unit name \n",
    "#--------------------------------#\n",
    "\n",
    "# Paths to the feature classes\n",
    "headcut19fc = os.path.join(headcut19gdb, \"Stream_Headcut_2019\")\n",
    "headcut20fc = os.path.join(headcut20gdb, \"Stream_Headcut_2020\")\n",
    "headcut22fc = os.path.join(headcut22gdb, \"Stream_Headcut_2022\")\n",
    "headcut23fc = os.path.join(headcut23gdb, \"sez_stream_headcut\")\n",
    "\n",
    "\n",
    "#Use gdb because they have assessment unit name in them... 2022 i had to manually add in PRO\n",
    "headcut23fields = ['Assessment_Unit', 'headcut_depth', 'created_date']\n",
    "headcut22fields = ['Assessment_Unit', 'Headcut_Depth', 'synced_date']\n",
    "headcut20fields = ['Assessment_Unit_Name', 'Headcut_Depth','Survey_Date']\n",
    "headcut19fields = ['SITE_NAME', 'HEADCUT_DEPTH', 'SURVEY_DATE' ]\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "\n",
    "# Function to read feature class into DataFrame\n",
    "def feature_class_to_dataframe(feature_class, fields):\n",
    "    data = [row for row in arcpy.da.SearchCursor(feature_class, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Read feature classes into DataFrames\n",
    "# Read feature classes into DataFrames\n",
    "headcut19df = feature_class_to_dataframe(headcut19fc, headcut19fields)\n",
    "headcut20df = feature_class_to_dataframe(headcut20fc, headcut20fields)\n",
    "headcut22df = feature_class_to_dataframe(headcut22fc, headcut22fields)\n",
    "headcut23df = feature_class_to_dataframe(headcut23fc, headcut23fields)\n",
    "\n",
    "# Rename fields\n",
    "headcut19df.rename(columns={'SITE_NAME': 'Assessment_Unit_Name', 'HEADCUT_DEPTH': 'headcut_depth', 'SURVEY_DATE': 'created_date'}, inplace=True)\n",
    "headcut20df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'Headcut_Depth': 'headcut_depth', 'Survey_Date': 'created_date'}, inplace=True)\n",
    "headcut22df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'Headcut_Depth': 'headcut_depth', 'synced_date': 'created_date'}, inplace=True)\n",
    "headcut23df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'headcut_depth': 'headcut_depth', 'created_date': 'created_date'}, inplace=True)\n",
    "\n",
    "# Concatenate DataFrames\n",
    "headcutdf = pd.concat([headcut19df, headcut20df, headcut22df, headcut23df], ignore_index=True)\n",
    "\n",
    "# Display merged DataFrame\n",
    "print(headcutdf)\n",
    "\n",
    "#hopefully this is just one time\n",
    "\n",
    "\n",
    "#--------------------------------#\n",
    "#Get Data from exported table where Assessment Unit is joined---parentglobalids don't match up for years before 2023. need to redo sde.collect 2019-2022 data append.... \n",
    "#--------------------------------#\n",
    "\n",
    "#--------------------------------#\n",
    "#Get Data from SDE.Collect...instead of GDB\n",
    "#--------------------------------#\n",
    "#----------------------------------------------------------------#\n",
    "#Add correct info to dataframe\n",
    "#----------------------------------------------------------------#\n",
    "# Create a new column 'SEZ ID'\n",
    "#This code is for the excel look up dictionary---ADD last? this is goo dfor QA on assessment unit names because 0's tell you the name is wrong\n",
    "headcutdf['SEZ_ID'] = headcutdf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "headcutdf['SEZ_ID'] = headcutdf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "headcutdf['SEZ_ID'] = headcutdf['SEZ_ID'].astype(int)\n",
    "headcutdf['Assessment_Unit_Name'] = headcutdf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky meadows - 1': 'Sky Meadows'})\n",
    "\n",
    "#calculate year column \n",
    "headcutdf['Year'] = headcutdf['created_date'].dt.year\n",
    "\n",
    "#----------------------------------------------#\n",
    "# Process Data\n",
    "#----------------------------------------------#\n",
    "# assign small, medium, large to headcut\n",
    "headcutdf['Headcut_Size']=headcutdf['headcut_depth'].apply(categorize_headcut)\n",
    "\n",
    "\n",
    "# Group by 'SEZ_ID', 'Year', and 'Headcut_Size', and count the number of occurrences for each group\n",
    "headcut_summary = headcutdf.groupby(['Assessment_Unit_Name', 'Year', 'Headcut_Size']).size().reset_index(name='Count')\n",
    "\n",
    "print(type(headcut_summary))\n",
    "# Pivot the table to have 'Headcut_Size' categories as columns\n",
    "headcut_summary_sml = headcut_summary.pivot_table(index=['Assessment_Unit_Name', 'Year'], columns='Headcut_Size', values='Count', fill_value=0)\n",
    "\n",
    "# Reset the index to flatten the DataFrame\n",
    "headcut_summary_sml.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the summarized DataFrame\n",
    "print(headcut_summary_sml)\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score, # of Headcuts\n",
    "#----------------------------------------------------------------#\n",
    "    \n",
    "    # Process Data\n",
    "def rate_headcut(row):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if row['large'] >= 1:\n",
    "        return 'D'  # Assign score D\n",
    "    elif row['medium'] >= 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif row['small'] >= 1:\n",
    "       return 'B'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no headcuts)\n",
    "\n",
    "# Apply the rating function to the summary DataFrame\n",
    "headcut_summary_sml['Headcuts_Rating'] = headcut_summary_sml.apply(rate_headcut, axis=1)\n",
    "\n",
    "#Calculate total number of headcuts per sez per year\n",
    "headcut_summary_sml['Number_of_Headcuts']= headcut_summary_sml[['large', 'medium', 'small']].sum(axis=1)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "headcut_summary_sml['Headcuts_Score']= headcut_summary_sml['Headcuts_Rating'].apply(score_indicator)\n",
    "\n",
    "#Add Datasource\n",
    "headcut_summary_sml['Headcuts_Data_Source'] = 'TRPA'\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to incision called stage_incision GDB location\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Headcuts_Data_Source': 'Headcuts_Data_Source',\n",
    "    'Number_of_Headcuts': 'Number_of_Headcuts',\n",
    "    'Headcuts_Rating': 'Headcuts_Rating',\n",
    "    'Headcuts_Score': 'Headcuts_Score',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'small': 'small',\n",
    "    'medium': 'medium',\n",
    "    'large': 'large'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = headcut_summary_sml.rename(columns=field_mapping).drop(columns=[col for col in headcut_summary_sml.columns if col not in field_mapping])\n",
    "\n",
    "readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "with arcpy.da.InsertCursor(stage_headcuts, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in External Data from USFS and Calflora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------#\n",
    "#Get Data from external sources and spatially join to our data\n",
    "#--------------------------------#\n",
    "\n",
    "\n",
    "#Calfora Data\n",
    "CArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines07/MapServer/1\"\n",
    "#CAfc = os.path.join(CArest, \"Non-Native and Invasive Plants - Calflora\")\n",
    "CAsdf = pd.DataFrame.spatial.from_layer(CArest)\n",
    "# Delete the existing feature layer if it exists\n",
    "\n",
    "CA_fc = arcpy.MakeFeatureLayer_management(CArest, \"Calfora_invasives\")\n",
    "\n",
    "\n",
    "usfsrest= \"https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer/0\"\n",
    "#usfsfc =os.path.join(usfsrest, \"Current Invasive Plant Locations\") \n",
    "#usfssdf = pd.DataFrame.spatial.from_layer(usfsrest)\n",
    "if arcpy.Exists(\"usfs_layer\"):\n",
    "    arcpy.Delete_management(\"usfs_layer\")\n",
    "usfs_fc = arcpy.MakeFeatureLayer_management(usfsrest, \"usfs_layer\")\n",
    "#Spatially join usfs layers to see if any of the points or polygons intersect with our sez's123\n",
    "\n",
    "# Set the workspace environment\n",
    "arcpy.env.workspace = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Data Management 2023.gdb\"\n",
    "\n",
    "# Define the target and join layers\n",
    "target_feature = SEZ_Master #\"AssessmentUnit_Master\"\n",
    "CAjoin_feature = CA_fc\n",
    "USFSjoin_feature = usfs_fc\n",
    "\n",
    "# Define the output feature class\n",
    "out_feature_class = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Data Management 2023.gdb\\externalinvasivesez\"\n",
    "\n",
    "# Define the fields you want to keep from both layers\n",
    "#field_mappings = arcpy.FieldMappings()\n",
    "#field_mappings.addTable(target_features)\n",
    "#field_mappings.addTable(join_features)\n",
    "\n",
    "# Keep only specific fields from the target layer\n",
    "#keep_fields_target = [\"Assessment_Unit_Name\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_target:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "#Keep only specific fields from the calfora join layer\n",
    "#keep_fields_CA = [\"CName\", \"SName\", \"Source\", \"Obs_Date\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_CA:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "# Keep only specific fields from the usfs join_features\n",
    "#keep_fields_USFS = [\"SCIENTIFIC_NAME\", \"COMMON_NAME\", \"DATE_COLLECTED\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_USFS:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "\n",
    "# Perform spatial join with specified field mappings\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=SEZ_Master,\n",
    "    join_features=[CAjoin_feature],\n",
    "    out_feature_class=out_feature_class,\n",
    "    join_operation=\"JOIN_ONE_TO_MANY\",\n",
    "    join_type=\"KEEP_ALL\"\n",
    "    #field_mapping=field_mappings\n",
    ")\n",
    "\n",
    "#Create Dataframe for spatially joined data\n",
    "# Define the fields you want to retrieve from the output feature class\n",
    "fields = [\"Assessment_Unit_Name\", \"CName\", \"SName\", \"Source\", \"Obs_Date\", \"SCIENTIFIC_NAME\", \"COMMON_NAME\", \"DATE_COLLECTED\"]\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "data = []\n",
    "\n",
    "# Use SearchCursor to read data from the output feature class\n",
    "with arcpy.da.SearchCursor(out_feature_class, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the list of tuples into a pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bioassessment/ Biotic Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bioassessment scores- get all stream data into sde.Stream first... then look at Biotic Integrit Data Source to find which stream was used to evaluate each meadow.. this will help with percent of stream miles I believe\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Get Data-- all the data we need is in sde.collect.sde.survey.sde.sez_survey\n",
    "#----------------------------------------------------------------##Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "\n",
    "#Create SEDF setup\n",
    "#streamdata is the path to the feature class in sde\n",
    "# Set the workspace to your SDE connection file\n",
    "arcpy.env.workspace = streamdata\n",
    "feature_class= \"Stream\"\n",
    "\n",
    "# Convert feature class to a pandas DataFrame\n",
    "fields = [field.name for field in arcpy.ListFields(feature_class)]\n",
    "\n",
    "# Create DataFrame\n",
    "streamsdf = pd.DataFrame.spatial.from_featureclass(feature_class, columns=fields)\n",
    "\n",
    "# Create the spatially enabled DataFrame (sdf) for target feature SEZ assessment units\n",
    "SEZsdf = pd.DataFrame.spatial.from_featureclass(SEZ_Master)\n",
    "#spatial reference stuff\n",
    "streamsdf.spatial.sr = SEZsdf.spatial.sr\n",
    "\n",
    "#perform spatial join of sde.stream and sez units\n",
    "thesdf = SEZsdf.spatial.join(streamsdf, how='inner')\n",
    "\n",
    "#Notes to self, Stream Miles?\n",
    "#Keep only Riverine?, this may be what the smaller polygons are for \n",
    "# Filter for SEZ type 'Riverine'\n",
    "#riverine_df = bioticsdf[bioticsdf['Feature_Type'] == 'Riverine']\n",
    "\n",
    "#if the layer contains Riverine? or just for any spatial join.. see what it does\n",
    "#spatial join this layer to asessment unit master layer\n",
    "#ASsessment unit master layer is called SEZ_Master\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------#\n",
    "#DATA PREP\n",
    "#----------------------#\n",
    "# List of columns to keep\n",
    "columns_to_keep = ['Assessment_Unit_Name', 'SEZ_Type', 'Feature_Type', 'SEZ_ID','SITE_NAME', 'COUNT_VALUE', 'YEAR_OF_COUNT', 'STATION_TYPE', 'LONGITUDE', 'LATITUDE', ]\n",
    "##Try this instead\n",
    "# Select only the desired columns\n",
    "bioticdf = thesdf.loc[:, columns_to_keep].copy()  \n",
    "\n",
    "#DATA PREP\n",
    "# Filter for years 2020 to 2023\n",
    "filtered_df = bioticdf.loc[(bioticdf['YEAR_OF_COUNT'] >= 2020) & (bioticdf['YEAR_OF_COUNT'] <= 2023)].copy()\n",
    "\n",
    "# Define SEZ ID based on Assessment_Unit_Name for QA on SEZ Name THIS METHOD USES LARGER POLYGONES \n",
    "filtered_df.loc[:, 'SEZ_ID'] = filtered_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "filtered_df.loc[:, 'SEZ_ID'] = filtered_df['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "filtered_df.loc[:, 'SEZ_ID'] = filtered_df['SEZ_ID'].astype(int)\n",
    "\n",
    "# Replace values in the 'Assessment_Unit_Name' column\n",
    "filtered_df.loc[:, 'Assessment_Unit_Name'] = filtered_df['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky Meadows meadow - 1': 'Sky meadows'})\n",
    "\n",
    "# Add data source information\n",
    "filtered_df['Source'] = 'TRPA, ' + filtered_df['SITE_NAME'].astype(str) + ', ' + filtered_df['YEAR_OF_COUNT'].astype(str)\n",
    "\n",
    "#Group by Year and Assessment Unit and Site NAME and remove duplicates\n",
    "\n",
    "filtered_df['SITE_NAME'] = filtered_df['SITE_NAME'].str.strip()\n",
    "filtered_df['YEAR_OF_COUNT'] = filtered_df['YEAR_OF_COUNT'].astype(str).str.strip().astype(int)\n",
    "filtered_df['YEAR_OF_COUNT'] = pd.to_numeric(filtered_df['YEAR_OF_COUNT'], errors='coerce')\n",
    "\n",
    "\n",
    "# Group by Assessment_Unit_Name, SITE_NAME, and YEAR_OF_COUNT and drop duplicates\n",
    "BIdf = filtered_df.groupby(['SEZ_ID', 'SITE_NAME', 'YEAR_OF_COUNT', 'COUNT_VALUE']).apply(lambda x: x.drop_duplicates()).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_df['YEAR_OF_COUNT'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------#\n",
    "#Grade and Score biotic integrity\n",
    "#----------------------#\n",
    "\n",
    "#Rate the score\n",
    "#ef categorize_csci(biotic_integrity):\n",
    "# Apply the rating function to the summary DataFrame\n",
    "BIdf['Biotic_Rating'] = BIdf['COUNT_VALUE'].apply(categorize_csci)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "BIdf['Biotic_Score']= BIdf['Biotic_Rating'].apply(score_indicator) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Prep and post ending dataframe to invasives table in SEZ_Data.GDB\n",
    "#----------------------------------------------------------------#\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'YEAR_OF_COUNT': 'Year',\n",
    "    'Source': 'Biotic_Integrity_Data_Source',\n",
    "    'COUNT_VALUE': 'Biotic_Integrity_CSCI',\n",
    "    'Biotic_Rating': 'Biotic_Integrity_Rating',\n",
    "    'Biotic_Score': 'Biotic_Integrity_Score',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = BIdf.rename(columns=field_mapping).drop(columns=[col for col in BIdf.columns if col not in field_mapping])\n",
    "\n",
    "readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_biotic_integrity, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")\n",
    "\n",
    "#Delete duplicates yourself.. not that much data to go through, can't figure out why it won't remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conifer Encroachment Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import arcpy\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Conifer_Encroachment_Data_Sourc',\n",
    "                        'Conifer_Encroachment_Rating',                    \n",
    "                        'Conifer_Encroachment_Percent_En',\n",
    "                        'Conifer_Encroachment_Score',\n",
    "                        'SEZ_ID',\n",
    "                        'Assessment_Unit_Name',\n",
    "                        'ConiferEncroachment_Comments']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    conifer_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "conifer_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Conifer_Encroachment_Data_Sourc': 'Conifer_Encroachment_Data_Source',\n",
    "                'Conifer_Encroachment_Rating': 'Conifer_Encroachment_Rating',                    \n",
    "                'Conifer_Encroachment_Percent_En': 'Conifer_Percent_Encroached',\n",
    "                'Conifer_Encroachment_Score': 'Conifer_Encroachment_Score',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'ConiferEncroachment_Comments': 'ConiferEncroachment_Comments'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = conifer_df.rename(columns=field_mapping).drop(columns=[col for col in conifer_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_conifer, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aquatic Organism Passage STagin table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SEZ_Master.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aquatic Organism /Fish Passage- only old data for now\n",
    "import os\n",
    "import pandas as pd\n",
    "import arcpy\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['AquaticOrganismPassage_Barriers',\n",
    "                        'AquaticOrganismPassage_DataSour',                    \n",
    "                        'AquaticOrganismPassage_NumberOf',\n",
    "                        'AquaticOrganismPassage_Rating',\n",
    "                        'SEZ_ID',\n",
    "                        'Assessment_Unit_Name',\n",
    "                        'AquaticOrganismPassage_Score',\n",
    "                        'AquaticOrganismPassage_StreamMi']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    AOP_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "AOP_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'AquaticOrganismPassage_Barriers': 'AOP_BarriersPerMile',\n",
    "                'AquaticOrganismPassage_DataSour': 'AOP_DataSource',                    \n",
    "                'AquaticOrganismPassage_NumberOf': 'AOP_NumberofBarriers',\n",
    "                'AquaticOrganismPassage_Score': 'AOP_Score',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'AquaticOrganismPassage_StreamMi': 'AOP_StreamMiles',\n",
    "                'AquaticOrganismPassage_Rating': 'AOP_Rating'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = AOP_df.rename(columns=field_mapping).drop(columns=[col for col in AOP_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_aquatic, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Habitat Fragmentation Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create STaging Tables for External Data Put in STagin Table\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Habitat_Fragmentation_Data_Sour',\n",
    "                        'Habitat_Fragmentation_Imperviou',\n",
    "                        'Habitat_Fragmentation_Percent_I',\n",
    "                        'Habitat_Fragmentation_Rating',\n",
    "                        'Habitat_Fragmentation_Score',\n",
    "                        'SEZ_ID', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    HabFrag_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "HabFrag_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Habitat_Fragmentation_Data_Sour': 'Habitat_Frag_Data_Source',\n",
    "                'Habitat_Fragmentation_Percent_I': 'HAbitat_Frag_Percent_Impervious',                    \n",
    "                'Habitat_Fragmentation_Score': 'Habitat_Frag_Score',\n",
    "                'Habitat_Fragmentation_Imperviou': 'Habitat_Frag_Impervious_Acres',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'Habitat_Fragmentation_Rating': 'Habitat_Frag_Rating'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = HabFrag_df.rename(columns=field_mapping).drop(columns=[col for col in HabFrag_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_habitat, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ditches Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create STaging Tables for External Data Put in STagin Table\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Ditches_Data_Source',\n",
    "                        'Ditches_Length',\n",
    "                        'Ditches_Meadow_Length',\n",
    "                        'Ditches_Percent',\n",
    "                        'Ditches_Rating',\n",
    "                        'Ditches_Score',\n",
    "                        'SEZ_ID', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    Ditch_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "Ditch_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Ditches_Data_Source': 'Ditches_Data_Source',\n",
    "                'Ditches_Length': 'Ditches_Length',                    \n",
    "                'Ditches_Meadow_Length': 'Ditches_Meadow_Length',\n",
    "                'Ditches_Percent': 'Ditches_Percent',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'Ditches_Rating': 'Ditches_Rating',\n",
    "                'Ditches_Score': 'Ditches_Score'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = Ditch_df.rename(columns=field_mapping).drop(columns=[col for col in Ditch_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_ditches, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vegetation Vigor- old data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create STaging Tables for External Data Put in STagin Table\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['VegetationVigor_DataSource',\n",
    "                        'NDVI_ID',\n",
    "                        'VegetationVigor_Raw',\n",
    "                        'VegetationVigor_Rating',\n",
    "                        'VegetationVigor_Score',\n",
    "                        'SEZ_ID', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    vegetation_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "vegetation_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'VegetationVigor_DataSource': 'VegetationVigor_DataSource',\n",
    "                'NDVI_ID': 'NDVI_ID',                    \n",
    "                'VegetationVigor_Raw': 'VegetationVigor_Raw',\n",
    "                'VegetationVigor_Rating': 'VegetationVigor_Rating',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'VegetationVigor_Score': 'VegetationVigor_Score'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = vegetation_df.rename(columns=field_mapping).drop(columns=[col for col in vegetation_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_vegetation, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEZ data from 2020 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All SEZ Scores from current Data\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Acres',\n",
    "                        'Final_Percent',\n",
    "                        'Final_Points_Possible',\n",
    "                        'Final_Rating',\n",
    "                        'Final_Total_Points',\n",
    "                        'SEZ_ID',\n",
    "                        'Comments',\n",
    "                        'SEZ_Type', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    SEZ20_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "SEZ20_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Acres': 'Acres',\n",
    "                'SEZ_Type': 'SEZ_Type',                    \n",
    "                'Final_Percent': 'Final_Percent',\n",
    "                'Final_Total_Points': 'Final_Total_Points',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'Final_Points_Possible': 'Final_Points_Possible',\n",
    "                'Final_Rating': 'Final_Rating',\n",
    "                'Comments': 'Comments'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = SEZ20_df.rename(columns=field_mapping).drop(columns=[col for col in SEZ20_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_All_SEZ_Scores, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final SEZ Scores Calculations for SEZ Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Data with REST SERVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Paths to Staging tables in SDE... via REST service\n",
    "# Use rest service to get data \n",
    "#Setup\n",
    "import pandas as pd\n",
    "import os\n",
    "from arcgis.features import FeatureLayer\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def get_fs_data(fs_url):\n",
    "    feature_layer = FeatureLayer(fs_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "def get_fs_data_spatial(fs_url):\n",
    "    feature_layer = FeatureLayer(fs_url)\n",
    "    query_result = feature_layer.query().sdf\n",
    "    query_result.spatial.sr = 26910\n",
    "    return query_result\n",
    "\n",
    "\n",
    "#def get_fs_data(SEZ_url):\n",
    " #   feature_layer = FeatureLayer(SEZ_url)\n",
    "  #  query_result = feature_layer.query()\n",
    "   # feature_list = query_result.features\n",
    "    #all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    #return all_data\n",
    "\n",
    "bank_stability_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/4\"\n",
    "biotic_integrity_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/5\"\n",
    "conifer_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/6\"\n",
    "ditches_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/7\"\n",
    "invasives_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/11\"\n",
    "Hab_Frag_url = 'https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/8'\n",
    "vegetation_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/12\"\n",
    "incision_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/10\"\n",
    "headcuts_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/9\"\n",
    "AOP_url= \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/3\"\n",
    "#all_sez_scores_url = \"\"\n",
    "\n",
    "SEZ_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/0\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Where data will be appended\n",
    "#All_SEZ_Scores_df = table_to_dataframe(os.path.join(master_path, \"All_SEZ_Scores\"))\n",
    "\n",
    "#Where do the comments come from??? Maybe SDE Collect in SEZ Site information? sde.sez_survey\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframes from Rest Services\n",
    "\n",
    "dfbanks = get_fs_data(bank_stability_url)\n",
    "dfbiotic = get_fs_data(biotic_integrity_url)\n",
    "dfconifer = get_fs_data(conifer_url)\n",
    "dfditch = get_fs_data(ditches_url)\n",
    "dfinvasive = get_fs_data(invasives_url)\n",
    "dfhabitat = get_fs_data(Hab_Frag_url)\n",
    "dfvegetation = get_fs_data(vegetation_url)\n",
    "dfincision = get_fs_data(incision_url)\n",
    "dfheadcuts = get_fs_data(headcuts_url)\n",
    "dfAOP = get_fs_data(AOP_url)\n",
    "dfSEZ = get_fs_data_spatial(SEZ_url)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data in staging tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Assessment_Unit_Name  Year  Biotic_Integrity_CSCI  \\\n",
      "0    Angora Creek - tributary  2013               0.996000   \n",
      "1        Angora Creek - upper  2013               0.996000   \n",
      "2          Angora meadows - 1  2019               0.690000   \n",
      "3          Angora meadows - 2  2019               0.820000   \n",
      "4          Angora meadows - 3  2017               0.940000   \n",
      "..                        ...   ...                    ...   \n",
      "226       Woods Creek - lower  2018               0.874000   \n",
      "227      Woods Creek - middle  2015               1.014000   \n",
      "228       Woods Creek - upper  2010               1.101000   \n",
      "229            small meadow 1  2018               1.010000   \n",
      "230           small meadow 57  2022               0.564053   \n",
      "\n",
      "    Biotic_Integrity_Data_Source Biotic_Integrity_Rating  \\\n",
      "0          TRPA, 634S13217, 2013                       A   \n",
      "1          TRPA, 634S13217, 2013                       A   \n",
      "2          TRPA, 634S19606, 2019                       C   \n",
      "3          TRPA, 634S19498, 2019                       B   \n",
      "4          TRPA, 634S17345, 2017                       A   \n",
      "..                           ...                     ...   \n",
      "226        TRPA, 634TPB112, 2018                       B   \n",
      "227        TRPA, 634S15348, 2015                       A   \n",
      "228        TRPA, 634S10092, 2010                       A   \n",
      "229        TRPA, 634REFNLH, 2018                       A   \n",
      "230         TRPA, 634HVC-1, 2022                       D   \n",
      "\n",
      "     Biotic_Integrity_Score  \n",
      "0                        12  \n",
      "1                        12  \n",
      "2                         6  \n",
      "3                         9  \n",
      "4                        12  \n",
      "..                      ...  \n",
      "226                       9  \n",
      "227                      12  \n",
      "228                      12  \n",
      "229                      12  \n",
      "230                       3  \n",
      "\n",
      "[231 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# #------------------#\n",
    "#Biotic Integrity\n",
    "#------------------#\n",
    "#Prep data- Add any scores and find average oif there are two stream sites for one sez. Also rename data source so it includes are streams that were averaged\n",
    "# Function to average scores and concatenate data sources for each Year and Assessment_Unit_Name\n",
    "def average_biotic_scores(dfbiotic, unit_col='Assessment_Unit_Name', year_col='Year', score='Biotic_Integrity_CSCI', source_col='Biotic_Integrity_Data_Source'):\n",
    "    # Group by Assessment Unit and Year\n",
    "    group = dfbiotic.groupby([unit_col, year_col])\n",
    "    \n",
    "    # Calculate the mean of the scores\n",
    "    averaged_scores = group[score].mean().reset_index()\n",
    "    \n",
    "    # Concatenate the data sources with specific formatting\n",
    "    def concatenate_sources(x, year):\n",
    "        formatted_sources = []\n",
    "        for entry in x:\n",
    "            parts = entry.split(\",\")\n",
    "            if len(parts) >= 3:\n",
    "                formatted_sources.append(f'TRPA, {parts[1].strip()}, {parts[-1].strip()}')  # Extract station code and year\n",
    "        if formatted_sources:\n",
    "            return '/ '.join(formatted_sources)\n",
    "        else:\n",
    "            return None  # Return None if all entries are invalid\n",
    "    \n",
    "    # Apply concatenate_sources to each group\n",
    "    concatenated_sources = group.apply(lambda grp: concatenate_sources(grp[source_col], grp[year_col])).reset_index(name=source_col)\n",
    "    \n",
    "    # Merge the averaged scores with concatenated sources\n",
    "    averaged_df = pd.merge(averaged_scores, concatenated_sources, on=[unit_col, year_col], how='left')\n",
    "    \n",
    "    return averaged_df\n",
    "\n",
    "\n",
    "# Drop duplicates based on 'Assessment_Unit_Name' and 'Year'\n",
    "dfbiotic = dfbiotic.drop_duplicates(subset=['Assessment_Unit_Name', 'Year', 'Biotic_Integrity_CSCI'])\n",
    "\n",
    "# Apply the function to dfbiotic\n",
    "averaged_biotic_df = average_biotic_scores(dfbiotic)\n",
    "\n",
    "# Apply the rating function to the averaged biotic integrity scores\n",
    "averaged_biotic_df['Biotic_Integrity_Rating'] = averaged_biotic_df['Biotic_Integrity_CSCI'].apply(categorize_csci)\n",
    "\n",
    "# Calculate the biotic score for each SEZ\n",
    "averaged_biotic_df['Biotic_Integrity_Score'] = averaged_biotic_df['Biotic_Integrity_Rating'].apply(score_indicator)\n",
    "\n",
    "averaged_biotic_df['Biotic_Integrity_Score']=averaged_biotic_df['Biotic_Integrity_Score'].astype(int)\n",
    "\n",
    "# Output the resulting DataFrame\n",
    "print(averaged_biotic_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     OBJECTID  SEZ_ID          Assessment_Unit_Name  Year  Number_of_Headcuts  \\\n",
      "0           1   519.0      Angora Creek - tributary  2023                   4   \n",
      "1           2   144.0            Angora meadows - 5  2019                   4   \n",
      "2           5   217.0  Angora meadows tributary - 1  2019                   1   \n",
      "3           6    99.0  Angora meadows tributary - 2  2019                   7   \n",
      "4           7    99.0  Angora meadows tributary - 2  2023                   4   \n",
      "..        ...     ...                           ...   ...                 ...   \n",
      "341       444    44.0               small meadow 96  2019                   0   \n",
      "342       445    62.0               small meadow 98  2019                   0   \n",
      "343       446    75.0               small meadow 99  2020                   0   \n",
      "344       447    75.0               small meadow 99  2022                   0   \n",
      "345       448     NaN                unnamed meadow  2019                   0   \n",
      "\n",
      "    Headcuts_Data_Source  Headcuts_Score Headcuts_Rating  \\\n",
      "0                   TRPA             9.0               B   \n",
      "1                   TRPA             3.0               D   \n",
      "2                   TRPA             6.0               C   \n",
      "3                   TRPA             3.0               D   \n",
      "4                   TRPA             3.0               D   \n",
      "..                   ...             ...             ...   \n",
      "341                 TRPA            12.0               A   \n",
      "342                 TRPA            12.0               A   \n",
      "343                 TRPA            12.0               A   \n",
      "344                 TRPA            12.0               A   \n",
      "345                 TRPA            12.0               A   \n",
      "\n",
      "                                   GlobalID created_user  created_date  \\\n",
      "0    {FAB7FBA6-B001-49C5-B818-61D4ADB66BC6}         None           NaN   \n",
      "1    {2A998016-8601-4961-A99A-3B17BE72F9B6}         None           NaN   \n",
      "2    {A702C130-249D-47F3-9055-7B556EDE47A6}         None           NaN   \n",
      "3    {3B8A31B5-725A-45C8-9EFD-6B2F8D6D958B}         None           NaN   \n",
      "4    {47619859-4303-4DB6-94C7-9B9DCEAB9D92}         None           NaN   \n",
      "..                                      ...          ...           ...   \n",
      "341  {6A9F6EDA-C871-4AD2-ADD4-B5BD00A1CE7E}     SNEWSOME  1.719944e+12   \n",
      "342  {360D9C43-647F-472E-827F-5434F518C156}     SNEWSOME  1.719944e+12   \n",
      "343  {7CDFAE39-FBA7-492C-977C-137965B7401E}     SNEWSOME  1.719944e+12   \n",
      "344  {B4A2C109-5C02-4719-8966-994E34197A73}     SNEWSOME  1.719944e+12   \n",
      "345  {EF32E54B-BEAB-48FA-94A2-192FDF70720A}     SNEWSOME  1.719944e+12   \n",
      "\n",
      "    last_edited_user  last_edited_date  \n",
      "0               None               NaN  \n",
      "1               None               NaN  \n",
      "2               None               NaN  \n",
      "3               None               NaN  \n",
      "4               None               NaN  \n",
      "..               ...               ...  \n",
      "341         SNEWSOME      1.719944e+12  \n",
      "342         SNEWSOME      1.719944e+12  \n",
      "343         SNEWSOME      1.719944e+12  \n",
      "344         SNEWSOME      1.719944e+12  \n",
      "345         SNEWSOME      1.719944e+12  \n",
      "\n",
      "[346 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "#-------------------\n",
    "#Headcuts \n",
    "#------------------\n",
    "#Reorganize dfHeadcuts to drop small medium large headcut columns\n",
    "# Drop the columns 'small', 'medium', and 'large'\n",
    "dfheadcuts = dfheadcuts.drop(columns=['small', 'medium', 'large'])\n",
    "\n",
    "# Print the DataFrame to see the changes\n",
    "print(dfheadcuts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Prep SEZ Baseline Data for assessment unit.. does this make the polygons roll over? how do i do that ==make spatially enabled dataframe and add 'SHAPE'\n",
    "keep_columns = ['SHAPE', 'SEZ_ID', 'Feature_Type', 'SEZ_Type', 'Ownership_Primary', 'Ownership_Secondary', 'Ownership_Secondary_2', 'Ownership_Secondary_3', 'Acres', 'Comments']\n",
    "\n",
    "dfSEZinfo=dfSEZ.loc[:,keep_columns].copy()\n",
    "\n",
    "dfSEZinfo['SEZ_ID']= dfSEZinfo['SEZ_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add year to data source so we can drop the year column later\n",
    "\n",
    "#Create Dictionary of Dataframes to adjust year to be in datashource column and not its own column\n",
    "yeartodatasource = {\n",
    "    'dfbanks': dfbanks,\n",
    "    'dfheadcuts': dfheadcuts,\n",
    "    'dfincision': dfincision,\n",
    "    'dfinvasive': dfinvasive\n",
    "}\n",
    "\n",
    "# Iterate over each DataFrame in meadowdata\n",
    "for name, df in yeartodatasource.items():\n",
    "    # Iterate over columns in the DataFrame\n",
    "    for col in df.columns:\n",
    "        # Check if the column name contains 'Data'\n",
    "        if 'Data_' in col:\n",
    "            # Add Year to the column if it contains 'Data'\n",
    "            df[col] = df[col] + ', ' + df['Year'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't USE This is for All SEZ Scores Only no othe rdata.. DONT USE unless need all _sez_Score table with just scores of Indicator and SEZ no floof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT USECreate RiverineIndicators list containing specific dataframes\n",
    "RiverineIndicators = ['Assessment_Unit_Name', 'AOP_Score', 'Bank_Stability_Score', 'Biotic_Integrity_Score', 'Habitat_Frag_Score', 'Incision_Score', 'Headcuts_Score']\n",
    "\n",
    "mergedriverine_df= mergedmeadow_df[RiverineIndicators]\n",
    "\n",
    "print(mergedriverine_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT USECalculate Scores for Each SEZ This will go into table All_SEZ_Scores\n",
    "\n",
    "#FOR MEADOWS\n",
    "\n",
    "\n",
    "#mergedmeadow_df['SEZ_ID']=mergedmeadow_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "# Identify columns that contain 'score' in their names\n",
    "score_columns = [col for col in mergedmeadow_df.columns if 'Score' in col]\n",
    "\n",
    "# Sum the values across identified score columns to get the final points for the SEZ\n",
    "mergedmeadow_df['Final_Total_Points'] = mergedmeadow_df[score_columns].sum(axis=1, skipna=True)\n",
    "\n",
    "#POints Possible per SEZ\n",
    "\n",
    "mergedmeadow_df['Final_Points_Possible'] = mergedmeadow_df[score_columns].notna().sum(axis=1)*12\n",
    "\n",
    "\n",
    "#ASsign Meadow SEZ_ID to each Assessment_Unit\n",
    "\n",
    "mergedmeadow_df['SEZ_ID']=mergedmeadow_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "#FOR QA purposes add SEZ_type so we can make sure lal data is available\n",
    "mergedmeadow_df['SEZ_Type'] = dfSEZ['SEZ_Type']\n",
    "#Would it be better to just add up how many indicators have a score? 12 x5?\n",
    "#mergedmeadow_df['Final_Points_Possible']= dfSEZ['Final_Points_Possible']\n",
    "\n",
    "# Merge based on 'SEZID'\n",
    "#mergedmeadow_df = pd.merge(mergedmeadow_df, dfSEZ[['SEZ_ID', 'Final_Points_Possible']], on='SEZ_ID', how='left')\n",
    "\n",
    "# Assign 'Final_Points_Possible' from dfSEZ to mergedmeadow_df\n",
    "#mergedmeadow_df['Final_Points_Possible'] = mergedmeadow_df['Final_Points_Possible']\n",
    "#or? just base of of how many indicators are not null for each row?\n",
    "#mergedmeadow_df['Final_Points_Possible2']= (12 x number of columns per row that say score have data?) \n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------\n",
    "# Calculate the final rating\n",
    "#------------------------------------\n",
    "mergedmeadow_df['Final_Percent'] = mergedmeadow_df['Final_Total_Points'] / mergedmeadow_df['Final_Points_Possible']\n",
    "\n",
    "mergedmeadow_df['Final_Rating']= mergedmeadow_df['Final_Percent'].apply(rate_SEZ)\n",
    "\n",
    "mergedmeadow_df['Final_Score']= mergedmeadow_df['Final_Rating'].apply(score_indicator)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT USECalculate Scores for Each SEZ This will go into table All_SEZ_Scores\n",
    "\n",
    "#FOR RIVERINE SEZ\n",
    "# Identify columns that contain 'score' in their names\n",
    "score_columns = [col for col in mergedriverine_df.columns if 'Score' in col]\n",
    "\n",
    "# Sum the values across identified score columns to get the final points for the SEZ\n",
    "mergedriverine_df['Final_Total_Points'] = mergedriverine_df[score_columns].sum(axis=1, skipna=True)\n",
    "\n",
    "#POints Possible per SEZ\n",
    "\n",
    "mergedriverine_df['Final_Points_Possible'] = mergedriverine_df[score_columns].notna().sum(axis=1)*12\n",
    "\n",
    "\n",
    "#ASsigndriverine SEZ_ID to each Assessment_Unit\n",
    "\n",
    "mergedriverine_df['SEZ_ID']=mergedriverine_df['Assessment_Unit_Name'].map(lookup_riverine)\n",
    "\n",
    "#FOR QA purposes add SEZ_type so we can make sure lal data is available\n",
    "mergedriverine_df['SEZ_Type'] = dfSEZ['SEZ_Type']\n",
    "\n",
    "#--------------------------------------\n",
    "# Calculate the final rating\n",
    "#------------------------------------\n",
    "mergedriverine_df['Final_Percent'] = mergedriverine_df['Final_Total_Points'] / mergedriverine_df['Final_Points_Possible']\n",
    "\n",
    "mergedriverine_df['Final_Rating']= mergedriverine_df['Final_Percent'].apply(rate_SEZ)\n",
    "\n",
    "mergedriverine_df['Final_Score']= mergedriverine_df['Final_Rating'].apply(score_indicator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT USEMerge Riverine and Meadow dataframes for a final dataframe\n",
    "\n",
    "# Concatenate DataFrames doesn't work-try merging--Do we want to make an All_sez_scores\n",
    "both_df = pd.concat([mergedriverine_df, mergedmeadow_df], axis=0, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTS WORKINNG... WILL NEED THIS for FINAL TABLEsezsurveytable grab comments to add to dataframe--MAYBE JUST DO THIS MANUALLY\n",
    "#Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "sezsurveyfields = ['Assessment_Unit_Name', 'Comments', 'Survey_Date']\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "# Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(sezsurveytable, sezsurveyfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "sezsurvey_df= pd.DataFrame(data, columns=sezsurveyfields)\n",
    "\n",
    "sezsurvey_df['Year'] = sezsurvey_df['Survey_Date'].dt.year\n",
    "print(sezsurvey_df)\n",
    "#add comments to dataframe\n",
    "# Create a dictionary to map Assessment_Unit_Name to Comments\n",
    "comments_map = dict(zip(sezsurvey_df['Assessment_Unit_Name'], sezsurvey_df['Comments']))\n",
    "\n",
    "# Add the Comments column to both_df using the map function\n",
    "both_df['Comments'] = both_df['Assessment_Unit_Name'].map(comments_map)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#DONT USE\n",
    "# both_df['Year']= '2024'\n",
    "\n",
    "both_df = both_df.dropna(subset='SEZ_ID')\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Prep and post ending dataframe to All Scores table in SEZ_Data.GDB\n",
    "#----------------------------------------------------------------#\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Comments': 'Comments',\n",
    "    'SEZ_Type': 'SEZ_Type',\n",
    "    'Final_Rating': 'Final_Rating',\n",
    "    'Final_Percent': 'Final_Percent',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'Final_Points_Possible': 'Final_Points_Possible',\n",
    "    'Final_Total_Points': 'Final_Total_Points'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = both_df.rename(columns=field_mapping).drop(columns=[col for col in both_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Assuming ready_df is the DataFrame you want to write to CSV\n",
    "readydf.to_csv('ready_df.csv', index=False)\n",
    "\n",
    "allscoresdf=readydf\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "#data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "#field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(allsezscores, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDE.SEZ Assessment_Unit final table with all info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Data, Assign SEZ's based on polygon size, get most recent\n",
    "#Take df's and create a riverine and meadow df using small polygon size and large polygon size\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "\n",
    "# Function to Get most recent year of data from each DataFrame\n",
    "def get_most_recent_scores(df, groupfield):\n",
    "    df = df.loc[df.groupby(groupfield)['Year'].idxmax()]\n",
    "    return df\n",
    "\n",
    "# Function to merge DataFrames and assign SEZ IDs\n",
    "def merge_and_assign_sez_ids(cleaned_data, lookup_dict):\n",
    "    # Merge all DataFrames on 'Assessment_Unit_Name' using reduce\n",
    "    merged_df = reduce(lambda left, right: pd.merge(left, right, on='Assessment_Unit_Name', how='outer'), cleaned_data.values())\n",
    "    \n",
    "    # Assign SEZ_ID based on 'Assessment_Unit_Name' using lookup_dict\n",
    "    merged_df['SEZ_ID'] = merged_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "    \n",
    "    # Drop rows with missing SEZ_ID\n",
    "    merged_df = merged_df.dropna(subset=['SEZ_ID'])\n",
    "    \n",
    "    merged_df['SEZ_ID'] = merged_df['SEZ_ID'].astype(int)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Main function to process data frames, get most recent data, clean, and merge\n",
    "def get_most_recent_and_clean(meadowdata, riverinedata, lookup_dict, lookup_riverine, columns_to_drop):\n",
    "    most_recent_data_meadow = {}\n",
    "    cleaned_data_meadow = {}\n",
    "    \n",
    "    most_recent_data_riverine = {}\n",
    "    cleaned_data_riverine = {}\n",
    "    \n",
    "    # Process meadowdata\n",
    "    for key, df in meadowdata.items():\n",
    "        processed_df = get_most_recent_scores(df, 'Assessment_Unit_Name')\n",
    "        most_recent_data_meadow[key] = processed_df\n",
    "        \n",
    "        # Drop specified columns and remove duplicate rows\n",
    "        df_cleaned = processed_df.drop(columns=[col for col in columns_to_drop if col in processed_df.columns])\n",
    "        df_cleaned = df_cleaned.drop_duplicates()\n",
    "        cleaned_data_meadow[key] = df_cleaned\n",
    "    \n",
    "    # Process riverinedata\n",
    "    for key, df in riverinedata.items():\n",
    "        processed_df = get_most_recent_scores(df, 'Assessment_Unit_Name')\n",
    "        most_recent_data_riverine[key] = processed_df\n",
    "        \n",
    "        # Drop specified columns and remove duplicate rows\n",
    "        df_cleaned = processed_df.drop(columns=[col for col in columns_to_drop if col in processed_df.columns])\n",
    "        df_cleaned = df_cleaned.drop_duplicates()\n",
    "        cleaned_data_riverine[key] = df_cleaned\n",
    "    \n",
    "    # Merge and assign SEZ IDs for meadowdata\n",
    "    merged_meadow_df = merge_and_assign_sez_ids(cleaned_data_meadow, lookup_dict)\n",
    "    \n",
    "    # Merge and assign SEZ IDs for riverinedata\n",
    "    merged_riverine_df = merge_and_assign_sez_ids(cleaned_data_riverine, lookup_riverine)\n",
    "    \n",
    "    # Combine merged_meadow_df and merged_riverine_df\n",
    "    combined_df = pd.concat([merged_meadow_df, merged_riverine_df], axis=0, join='outer')\n",
    "\n",
    "    return {\n",
    "        'merged_meadow_df': merged_meadow_df,\n",
    "        'merged_riverine_df': merged_riverine_df,\n",
    "        'combined_df': combined_df,\n",
    "        'cleaned_data_meadow': cleaned_data_meadow,\n",
    "        'cleaned_data_riverine': cleaned_data_riverine,\n",
    "        'most_recent_data_meadow': most_recent_data_meadow,\n",
    "        'most_recent_data_riverine': most_recent_data_riverine\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same for meadow and riverine data drop these columns because not needed in final merge, will assign SEZ ID later\n",
    "columns_to_drop = {'Year', 'SEZ_ID', 'GlobalID', 'last_edited_user', 'created_date', 'OBJECTID', 'created_user', 'last_edited_date'}\n",
    "\n",
    "#Name dataframes so we can reference later\n",
    "meadowdata= {'dfbanks': dfbanks, \n",
    "             'dfaveraged_biotic':averaged_biotic_df,\n",
    "                'dfconifer': dfconifer,\n",
    "                'dfditch': dfditch,\n",
    "                'dfinvasive': dfinvasive,\n",
    "                'dfhabitat': dfhabitat,\n",
    "                'dfvegetation': dfvegetation,\n",
    "                'dfincision': dfincision,\n",
    "                'dfheadcuts': dfheadcuts,\n",
    "                'dfAOP': dfAOP\n",
    "}\n",
    "\n",
    "\n",
    "#Staging Tables Riverine/ small polygons\n",
    "riverinedata = {'dfheadcuts': dfbanks,\n",
    "                'dfbiotic': averaged_biotic_df,\n",
    "                'dfhabitat': dfhabitat,\n",
    "                'dfincision': dfincision,\n",
    "                'dfheadcuts': dfheadcuts,\n",
    "                'dfAOP': dfAOP\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with all the required arguments\n",
    "result = get_most_recent_and_clean(meadowdata, riverinedata, lookup_dict, lookup_riverine, columns_to_drop)\n",
    "\n",
    "# Access the results\n",
    "merged_meadow_df = result['merged_meadow_df']\n",
    "merged_riverine_df = result['merged_riverine_df']\n",
    "combined_df=result['combined_df']\n",
    "cleaned_data_meadow = result['cleaned_data_meadow']\n",
    "cleaned_data_riverine = result['cleaned_data_riverine']\n",
    "most_recent_data_meadow = result['most_recent_data_meadow']\n",
    "most_recent_data_riverine = result['most_recent_data_riverine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join SEZinfo to combined df to get a dataframe with more info about each Assessment Unit\n",
    "\n",
    "# Join SEZinfo to the combined_df using SEZ_ID\n",
    "final_df = pd.merge(combined_df, dfSEZinfo, on='SEZ_ID')\n",
    "\n",
    "#Assign Threshold Calculations which is the Threshold Year--> is just the most recent data within the past 4 years\n",
    "final_df['Threshold_Year'] = '2023'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of SEZ_ID in combined_df: int32\n",
      "Data type of SEZ_ID in dfSEZinfo: int32\n"
     ]
    }
   ],
   "source": [
    "# Check the data type of the SEZ_ID column in combined_df\n",
    "print(\"Data type of SEZ_ID in combined_df:\", combined_df['SEZ_ID'].dtype)\n",
    "\n",
    "# Check the data type of the SEZ_ID column in dfSEZinfo\n",
    "print(\"Data type of SEZ_ID in dfSEZinfo:\", dfSEZinfo['SEZ_ID'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score columns in final_df: Index(['Bank_Stability_Score', 'Biotic_Integrity_Score',\n",
      "       'Conifer_Encroachment_Score', 'Ditches_Score', 'Invasives_Scores',\n",
      "       'Habitat_Frag_Score', 'VegetationVigor_Score', 'Incision_Score',\n",
      "       'Headcuts_Score', 'AOP_Score', 'Final_Score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "score_columns = [col for col in final_df.columns if 'Score' in col]\n",
    "print(\"score columns in final_df:\", final_df[score_columns].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'ready_df.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23756\\160738473.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# List of columns to export (excluding 'geometry' or 'Shape')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcolumns_to_export\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfinal_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'SHAPE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfinal_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumns_to_export\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ready_df.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3770\u001b[0m         )\n\u001b[0;32m   3771\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3772\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3773\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3774\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1184\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         )\n\u001b[1;32m-> 1186\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    238\u001b[0m         \"\"\"\n\u001b[0;32m    239\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    241\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'ready_df.csv'"
     ]
    }
   ],
   "source": [
    "##RUN FOR QA so you can see the resulting dataframe/scores/formatting etc.\n",
    "#Will need to rerun to fix dropping the shape column\n",
    "# List of columns to export (excluding 'geometry' or 'Shape')\n",
    "columns_to_export = [col for col in final_df.columns if col != 'SHAPE']\n",
    "final_df[columns_to_export].to_csv('ready_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Score based on SEZ_Type\n",
    "#Use SEZ_Type to select only needed indicators for SEZ Type\n",
    "\n",
    "# Define the score columns needed for each SEZ Type\n",
    "RiverineIndicators = ['AOP_Score', 'Bank_Stability_Score', 'Biotic_Integrity_Score', 'Habitat_Frag_Score', 'Incision_Score', 'Headcuts_Score']\n",
    "ForestRiverineIndicators =['AOP_Score', 'Bank_Stability_Score', 'Biotic_Integrity_Score', 'Invasives_Scores', 'Ditches_Score', 'Habitat_Frag_Score', 'Incision_Score', 'Headcuts_Score']\n",
    "NonChanneledIndicators=['Invasives_Scores', 'Conifer_Encroachment_Score', 'Ditches_Score', 'Habitat_Frag_Score', 'Headcuts_Score', 'VegetationVigor_Score']\n",
    "ChanneledIndicators= ['AOP_Score', 'Bank_Stability_Score', 'Biotic_Integrity_Score', 'Invasives_Scores', 'Conifer_Encroachment_Score', 'Ditches_Score', 'Habitat_Frag_Score', 'Incision_Score', 'Headcuts_Score', 'VegetationVigor_Score']\n",
    "ForestIndicators= ['Bank_Stability_Score', 'Ditches_Score', 'Habitat_Frag_Score', 'Headcuts_Score']\n",
    "\n",
    "# Function to get the score columns based on SEZ_Type\n",
    "def get_score_columns(sez_type):\n",
    "    if sez_type == 'Riverine (Perennial)':\n",
    "        return RiverineIndicators\n",
    "    elif sez_type == 'Riverine (Perennial) + Forested':\n",
    "        return ForestRiverineIndicators\n",
    "    elif sez_type == 'Non-Channeled Meadow':\n",
    "        return NonChanneledIndicators\n",
    "    elif sez_type == 'Channeled Meadow':\n",
    "        return ChanneledIndicators\n",
    "    elif sez_type == 'Forested':\n",
    "        return ForestIndicators\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Apply the appropriate score columns based on SEZ_Type\n",
    "final_df['Score_Columns'] = final_df['SEZ_Type'].apply(get_score_columns)\n",
    "\n",
    "# Function to calculate the final points and points possible\n",
    "def calculate_scores(row):\n",
    "    score_columns = row['Score_Columns']\n",
    "    if not score_columns:\n",
    "        return pd.Series([None, None])\n",
    "    total_points = row[score_columns].sum(skipna=True)\n",
    "    points_possible = row[score_columns].notna().sum() * 12\n",
    "    return pd.Series([total_points, points_possible])\n",
    "\n",
    "# Apply the score calculation to each row\n",
    "final_df[['Final_Total_Points', 'Final_Points_Possible']] = final_df.apply(calculate_scores, axis=1)\n",
    "\n",
    "# Calculate the final percent\n",
    "final_df['Final_Percent'] = final_df['Final_Total_Points'] / final_df['Final_Points_Possible']\n",
    "\n",
    "# Calculate the final rating and score\n",
    "final_df['Final_Rating'] = final_df['Final_Percent'].apply(rate_SEZ)\n",
    "final_df['Final_Score'] = final_df['Final_Rating'].apply(score_indicator)\n",
    "\n",
    "# Drop the temporary 'Score_Columns' column\n",
    "final_df = final_df.drop(columns=['Score_Columns'])\n",
    "\n",
    "# Convert SEZ_ID to string\n",
    "#final_df['SEZ_ID'] = final_df['SEZ_ID'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "##RUN FOR QA so you can see the resulting dataframe/scores/formatting etc.\n",
    "#Will need to rerun to fix dropping the shape column\n",
    "# List of columns to export (excluding 'geometry' or 'Shape')\n",
    "columns_to_export = [col for col in final_df.columns if col != 'SHAPE']\n",
    "final_df[columns_to_export].to_csv('ready_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Assessment_Unit_Name', 'Bank_Stability_Data_Source',\n",
      "       'Bank_Stability_Percent_Unstable', 'Bank_Stability_Rating',\n",
      "       'Bank_Stability_Score', 'Biotic_Integrity_CSCI',\n",
      "       'Biotic_Integrity_Data_Source', 'Biotic_Integrity_Rating',\n",
      "       'Biotic_Integrity_Score', 'Conifer_Encroachment_Data_Sourc',\n",
      "       'Conifer_Percent_Encroached', 'Conifer_Encroachment_Rating',\n",
      "       'Conifer_Encroachment_Score', 'ConiferEncroachment_Comments',\n",
      "       'Ditches_Data_Source', 'Ditches_Length', 'Ditches_Meadow_Length',\n",
      "       'Ditches_Percent', 'Ditches_Rating', 'Ditches_Score',\n",
      "       'Invasives_Data_Source', 'Invasives_Number_of_Invasives',\n",
      "       'Invasive_Percent_Cover', 'Invasives_Plant_Types', 'Invasives_Rating',\n",
      "       'Invasives_Scores', 'Habitat_Frag_Data_Source',\n",
      "       'Habitat_Frag_Impervious_Acres', 'Habitat_Frag_Percent_Impervious',\n",
      "       'Habitat_Frag_Rating', 'Habitat_Frag_Score',\n",
      "       'VegetationVigor_DataSource', 'VegetationVigor_Rating',\n",
      "       'VegetationVigor_Score', 'VegetationVigor_Raw', 'NDVI_ID',\n",
      "       'Incision_Data_Source', 'Incision_Ratio', 'Incision_Rating',\n",
      "       'Incision_Score', 'Number_of_Headcuts', 'Headcuts_Data_Source',\n",
      "       'Headcuts_Score', 'Headcuts_Rating', 'AOP_BarriersPerMile',\n",
      "       'AOP_DataSource', 'AOP_NumberofBarriers', 'AOP_Rating', 'AOP_Score',\n",
      "       'AOP_StreamMiles', 'SEZ_ID', 'SHAPE', 'Feature_Type', 'SEZ_Type',\n",
      "       'Ownership_Primary', 'Ownership_Secondary', 'Ownership_Secondary_2',\n",
      "       'Ownership_Secondary_3', 'Acres', 'Comments', 'Threshold_Year',\n",
      "       'Final_Total_Points', 'Final_Points_Possible', 'Final_Percent',\n",
      "       'Final_Rating', 'Final_Score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(final_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Assessment_Unit_Name Bank_Stability_Data_Source  \\\n",
      "0        Angora Creek - tributary                 TRPA, 2023   \n",
      "1              Angora meadows - 1                 TRPA, 2019   \n",
      "2              Angora meadows - 2                 TRPA, 2019   \n",
      "3              Angora meadows - 3                 TRPA, 2022   \n",
      "4              Angora meadows - 6                 TRPA, 2019   \n",
      "..                            ...                        ...   \n",
      "637   Secret Harbor Creek - lower                        NaN   \n",
      "638  Slaughterhouse Creek - upper                        NaN   \n",
      "639       Snow Creek wetlands - 1                        NaN   \n",
      "640         Third Creek - upper 3                        NaN   \n",
      "641               small meadow 50                        NaN   \n",
      "\n",
      "     Bank_Stability_Percent_Unstable Bank_Stability_Rating  \\\n",
      "0                           0.859340                     A   \n",
      "1                           0.000000                     A   \n",
      "2                           0.000000                     A   \n",
      "3                           5.327053                     B   \n",
      "4                           1.368514                     A   \n",
      "..                               ...                   ...   \n",
      "637                              NaN                   NaN   \n",
      "638                              NaN                   NaN   \n",
      "639                              NaN                   NaN   \n",
      "640                              NaN                   NaN   \n",
      "641                              NaN                   NaN   \n",
      "\n",
      "     Bank_Stability_Score  Biotic_Integrity_CSCI Biotic_Integrity_Data_Source  \\\n",
      "0                    12.0               0.996000        TRPA, 634S13217, 2013   \n",
      "1                    12.0               0.690000        TRPA, 634S19606, 2019   \n",
      "2                    12.0               0.820000        TRPA, 634S19498, 2019   \n",
      "3                     9.0               0.940000        TRPA, 634S17345, 2017   \n",
      "4                    12.0               0.812284        TRPA, 634TPB153, 2020   \n",
      "..                    ...                    ...                          ...   \n",
      "637                   NaN                    NaN                          NaN   \n",
      "638                   NaN                    NaN                          NaN   \n",
      "639                   NaN                    NaN                          NaN   \n",
      "640                   NaN                    NaN                          NaN   \n",
      "641                   NaN                    NaN                          NaN   \n",
      "\n",
      "    Biotic_Integrity_Rating  Biotic_Integrity_Score  \\\n",
      "0                         A                    12.0   \n",
      "1                         C                     6.0   \n",
      "2                         B                     9.0   \n",
      "3                         A                    12.0   \n",
      "4                         B                     9.0   \n",
      "..                      ...                     ...   \n",
      "637                     NaN                     NaN   \n",
      "638                     NaN                     NaN   \n",
      "639                     NaN                     NaN   \n",
      "640                     NaN                     NaN   \n",
      "641                     NaN                     NaN   \n",
      "\n",
      "    Conifer_Encroachment_Data_Sourc  ...  Ownership_Secondary  \\\n",
      "0                              None  ...                 <NA>   \n",
      "1                  TRPA LIDAR, 2009  ...                 USFS   \n",
      "2                  TRPA LIDAR, 2009  ...              private   \n",
      "3                  TRPA LIDAR, 2009  ...              private   \n",
      "4                  TRPA LIDAR, 2009  ...                local   \n",
      "..                              ...  ...                  ...   \n",
      "637                             NaN  ...                 <NA>   \n",
      "638                             NaN  ...                 <NA>   \n",
      "639                             NaN  ...              private   \n",
      "640                             NaN  ...                 <NA>   \n",
      "641                             NaN  ...                 <NA>   \n",
      "\n",
      "    Ownership_Secondary_2  Ownership_Secondary_3      Acres  \\\n",
      "0                    <NA>                   <NA>  71.030026   \n",
      "1                 private                   <NA>  23.009646   \n",
      "2                    <NA>                   <NA>  51.464984   \n",
      "3                    USFS                   <NA>   36.74867   \n",
      "4                 private                   <NA>  16.357205   \n",
      "..                    ...                    ...        ...   \n",
      "637                  <NA>                   <NA>   0.916305   \n",
      "638                  <NA>                   <NA>   1.567064   \n",
      "639                  <NA>                   <NA>   1.362636   \n",
      "640                  <NA>                   <NA>   0.676143   \n",
      "641                  <NA>                   <NA>   0.219156   \n",
      "\n",
      "                                              Comments  Threshold Year  \\\n",
      "0    Mostly stable and healthy after restoration pr...            2023   \n",
      "1    No indications of degradation. Restoration pro...            2023   \n",
      "2    No indications of degradation. Restoration pro...            2023   \n",
      "3    Lots of former meadow developed with ditching ...            2023   \n",
      "4    No indications of degradation. Restoration pro...            2023   \n",
      "..                                                 ...             ...   \n",
      "637                     No indications of degradation.            2023   \n",
      "638                                      Not assessed.            2023   \n",
      "639  Area was graded for development in 1960's but ...            2023   \n",
      "640                     No indications of degradation.            2023   \n",
      "641                     No indications of degradation.            2023   \n",
      "\n",
      "     Final_Total_Points  Final_Points_Possible Final_Percent  Final_Rating  \n",
      "0                  60.0                   72.0      0.833333             B  \n",
      "1                  90.0                  108.0      0.833333             B  \n",
      "2                 105.0                  120.0      0.875000             B  \n",
      "3                  90.0                  120.0      0.750000             C  \n",
      "4                  81.0                   96.0      0.843750             B  \n",
      "..                  ...                    ...           ...           ...  \n",
      "637                18.0                   24.0      0.750000             C  \n",
      "638                24.0                   24.0      1.000000             A  \n",
      "639                33.0                   36.0      0.916667             A  \n",
      "640                24.0                   24.0      1.000000             A  \n",
      "641                36.0                   36.0      1.000000             A  \n",
      "\n",
      "[642 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "#Final set up\n",
    "\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'SHAPE': 'SHAPE',\n",
    "    'Threshold_Year': 'Threshold Year',\n",
    "    'Comments': 'Comments',\n",
    "    'SEZ_Type': 'SEZ_Type',\n",
    "    'Final_Rating': 'Final_Rating',\n",
    "    'Final_Percent': 'Final_Percent',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'Final_Points_Possible': 'Final_Points_Possible',\n",
    "    'Final_Total_Points': 'Final_Total_Points',\n",
    "    'Acres': 'Acres',\n",
    "    'AOP_BarriersPerMile':'AquaticOrganismPassage_Barriers',\n",
    "    'AOP_NumberofBarriers': 'AquaticOrganismPassage_NumberOf',\n",
    "    'AOP_Score': 'AquaticOrganismPassage_Score',\n",
    "    'AOP_Rating': 'AquaticOrganismPassage_Rating',\n",
    "    'AOP_StreamMiles': 'AquaticOrganismPassage_StreamMiles',\n",
    "    'AOP_DataSource': 'AquaticOrganismPassage_DataSour',\n",
    "    'Bank_Stability_Data_Source': 'Bank_Stability_Data_Source',\n",
    "    'Bank_Stability_Percent_Unstable': 'Bank_Stability_Percent_Unstable',\n",
    "    'Bank_Stability_Rating': 'Bank_Stability_Rating',\n",
    "    'Bank_Stability_Score': 'Bank_Stability_Score',\n",
    "    'Biotic_Integrity_Rating': 'Biotic_Integrity_Rating',\n",
    "    'Biotic_Integrity_CSCI': 'Biotic_Integrity_CSCI',\n",
    "    'Biotic_Integrity_Data_Source': 'Biotic_Integrity_Data_Source',\n",
    "    'Biotic_Integrity_Score': 'Biotic_Integrity_Score',\n",
    "    'Comments': 'Comments',\n",
    "    'Conifer_Percent_Encroached': 'Conifer_Encroachment_Percent_En',\n",
    "    'Conifer_Encroachment_Data_Sourc': 'Conifer_Encroachment_Data_Sourc',\n",
    "    'Conifer_Encroachment_Rating': 'Conifer_Encroachment_Rating',\n",
    "    'Conifer_Encroachment_Score': 'Conifer_Encroachment_Score',\n",
    "    'ConiferEncroachment_Comments': 'Conifer_Encroachment_Comments',\n",
    "    'Ditches_Data_Source': 'Ditches_Data_Source',\n",
    "    'Ditches_Length': 'Ditches_Length',\n",
    "    'Ditches_Meadow_Length': 'Ditches_Meadow_Length',\n",
    "    'Ditches_Percent': 'Ditches_Percent',\n",
    "    'Ditches_Rating': 'Ditches_Rating',\n",
    "    'Ditches_Score': 'Ditches_Score',\n",
    "    'Feature_Type': 'Feature_Type',\n",
    "    'Habitat_Frag_Data_Source': 'Habitat_Fragmentation_Data_Sour',\n",
    "    'Habitat_Frag_Impervious_Acres': 'Habitat_Fragmentation_Imperviou',\n",
    "    'Habitat_Frag_Percent_Impervious': 'Habitat_Fragmentation_Percent_I',\n",
    "    'Habitat_Frag_Rating': 'Habitat_Fragmentation_Rating',\n",
    "    'Habitat_Frag_Score': 'Habitat_Fragmentation_Score',\n",
    "    'Headcuts_Data_Source': 'Headcuts_Data_Source',\n",
    "    'Number_of_Headcuts': 'Headcuts_Number_of_Headcuts',\n",
    "    'Headcuts_Rating': 'Headcuts_Rating',\n",
    "    'Headcuts_Score': 'Headcuts_Score',\n",
    "    'Incision_Data_Source': 'Incision_Data_Source',\n",
    "    'Incision_Rating': 'Incision_Rating',\n",
    "    'Incision_Score': 'Incision_Score',\n",
    "    'Incision_Ratio': 'Incision_Ratio',\n",
    "    'Invasive_Percent_Cover': 'Invasive_Percent_Cover',\n",
    "    'Invasives_Rating': 'Invasive_Rating',\n",
    "    'Invasives_Data_Source': 'Invasives_Data_Source',\n",
    "    'Invasives_Number_of_Invasives': 'Invasives_Number_of_Invasives',\n",
    "    'Invasives_Plant_Types': 'Invasives_Plant_Types',\n",
    "    'Invasives_Scores': 'Invasives_Scores',\n",
    "    'NDVI_ID': 'NDVI_ID',\n",
    "    'Ownership_Primary': 'Ownership_Primary',\n",
    "    'Ownership_Secondary': 'Ownership_Secondary',\n",
    "    'Ownership_Secondary_2': 'Ownership_Secondary_2',\n",
    "    'Ownership_Secondary_3': 'Ownership_Secondary_3',\n",
    "    'VegetationVigor_DataSource': 'VegetationVigor_DataSource',\n",
    "    'VegetationVigor_Rating': 'VegetationVigor_Rating',\n",
    "    'VegetationVigor_Raw': 'VegetationVigor_Raw',\n",
    "    'VegetationVigor_Score': 'VegetationVigor_Score',\n",
    "}\n",
    "# Rename fields based on field mappings\n",
    "SEZscores_readydf = final_df.rename(columns=field_mapping).drop(columns=[col for col in final_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(SEZscores_readydf)\n",
    "\n",
    "#Post results to CSV in gis/projects/Researchanalysis/SEZ for further QA\n",
    "columns_to_export = [col for col in SEZscores_readydf.columns if col != 'SHAPE']\n",
    "#Store csv on F drive for QA/add comments manually on F drive and to change up comments later based on SEZ's that scores changed\n",
    "final_results = \"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Threshold24_SEZScores.csv\"\n",
    "# Assuming ready_df is the DataFrame you want to write to CSV\n",
    "SEZscores_readydf[columns_to_export].to_csv(final_results, index=False)\n",
    "\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "#data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "#field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(allsezscores, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before Final comparison do more QA\n",
    "    # Check to make sure there are 643 ASsessment Units\n",
    "    #Check Final Points and make sure there are no really low numbers could be missing data\n",
    "    #Check all staging tables for completeness- There are ratings A-D, check on null data for that SEZ\n",
    "    #other QA methods can be added to this list\n",
    "\n",
    "#Check to see which SEZ Scores Changed\n",
    "Threshold24sezdata= SEZscores_readydf\n",
    "Threshold24sezdata =\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QA only\n",
    "# DONT USE unless want to use for QA Make a column for QA for SEZ_Type/final points possible\n",
    "def categorize_finalpoints(SEZ_Type):\n",
    "     if pd.isna(SEZ_Type):\n",
    "        return np.nan\n",
    "     elif   SEZ_Type == 'Non-Channeled Meadow':\n",
    "        return '72'\n",
    "     elif SEZ_Type == 'Channeled Meadow':\n",
    "        return '120'\n",
    "     elif SEZ_Type == 'Riverine (Perennial)':\n",
    "        return 'C'\n",
    "     elif SEZ_Type == 'Forested':\n",
    "        return '48'\n",
    "     elif SEZ_Type == 'Riverine (Perennial) + Forested':\n",
    "        return '96'\n",
    "     \n",
    "final_df['TESTFinalPointsPossible']= final_df['SEZ_Type'].apply(categorize_finalpoints)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orginal code in case we need it--will probably delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't USeoriginal code.. merge needs work it seems like it is using lookup dict for both\n",
    "\n",
    "from functools import reduce\n",
    "# Functino to Get most recent year of data from each Dataframe\n",
    "def get_most_recent_scores(df,groupfield):\n",
    "    df = df.loc[df.groupby(groupfield)['Year'].idxmax()]\n",
    "    return df\n",
    "\n",
    "def get_most_recent_and_clean(data_frames, lookup_dict, columns_to_drop):\n",
    "    most_recent_data = {}\n",
    "\n",
    "    # Iterate over the items in the original 'meadowdata' dictionary\n",
    "    for key, df in data_frames.items():\n",
    "        # Apply the 'get_most_recent_scores' function to each DataFrame\n",
    "        processed_df = get_most_recent_scores(df, 'Assessment_Unit_Name')\n",
    "        \n",
    "        # Store the processed DataFrame in the new dictionary using the same key\n",
    "        most_recent_data[key] = processed_df\n",
    "    \n",
    "# Columns to drop\n",
    "\n",
    "    # Drop specified columns and remove duplicate rows\n",
    "    cleaned_data = {}\n",
    "    for key, df in most_recent_data.items():\n",
    "        # Drop specified columns if they exist\n",
    "        df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "        \n",
    "        # Remove duplicate rows\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        # Append the cleaned DataFrame to the list\n",
    "        cleaned_data[key] = df\n",
    "    \n",
    "    # Merge all DataFrames on 'Assessment_Unit_Name' using reduce\n",
    "    merged_df = {}\n",
    "    for key, df in cleaned_data.items():\n",
    "        merged_df = reduce(lambda left, right: pd.merge(left, right, on='Assessment_Unit_Name', how='outer'), cleaned_data.values())\n",
    "\n",
    "\n",
    "    # Meadowdata gets the lookup_dict\n",
    "    # Riverinedatagets lookup_riverine dictionary how the hell do i do this \n",
    "\n",
    "    #Add large polygon/meadow feature type sez id\n",
    "    merged_df['SEZ_ID']=merged_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "    merged_df = merged_df.dropna(subset=['SEZ_ID'])\n",
    "\n",
    "    merged_df[key] = df\n",
    "    \n",
    "    return {'merged_df':merged_df,\n",
    "            'cleaned_data':cleaned_data,\n",
    "            'most_recent_data':most_recent_data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT USE FOR NOW --maybe for final points.. Try SEZ_Type to say what indicators are.. so it would be Foresteddata, Forested+ Riverine(perennial), Riverine(Perennial), Channeled MEadow, Non_Channeled meadow\n",
    "#MIGHT NOT USE\n",
    "#Forest and Riverine Indicators\n",
    "#Name dataframes so we can reference later\n",
    "forestriverine= {'dfbanks': dfbanks, \n",
    "                    'dfaveraged_biotic':averaged_biotic_df,\n",
    "                    'dfditch': dfditch,\n",
    "                    'dfinvasive': dfinvasive,\n",
    "                    'dfhabitat': dfhabitat,\n",
    "                    'dfincision': dfincision,\n",
    "                    'dfheadcuts': dfheadcuts,\n",
    "                    'dfAOP': dfAOP\n",
    "}\n",
    "                \n",
    "#Forest Indicators\n",
    "forest= {'dfbanks': dfbanks, \n",
    "            'dfditch': dfditch,\n",
    "            'dfhabitat': dfhabitat,\n",
    "            'dfheadcuts': dfheadcuts,\n",
    "}\n",
    "#Channeled Meadow Indicators\n",
    "channeled= {'dfbanks': dfbanks, \n",
    "             'dfaveraged_biotic':averaged_biotic_df,\n",
    "                'dfconifer': dfconifer,\n",
    "                'dfditch': dfditch,\n",
    "                'dfinvasive': dfinvasive,\n",
    "                'dfhabitat': dfhabitat,\n",
    "                'dfvegetation': dfvegetation,\n",
    "                'dfincision': dfincision,\n",
    "                'dfheadcuts': dfheadcuts,\n",
    "                'dfAOP': dfAOP\n",
    "}\n",
    "\n",
    "\n",
    "#Non-Channeled Meadow Indicators\n",
    "NonChanneled= {'dfconifer': dfconifer,\n",
    "                'dfditch': dfditch,\n",
    "                'dfinvasive': dfinvasive,\n",
    "                'dfhabitat': dfhabitat,\n",
    "                'dfvegetation': dfvegetation,\n",
    "                'dfheadcuts': dfheadcuts\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Scores for Each SEZ This will go into table All_SEZ_Scores\n",
    "\n",
    "#FOR MEADOWS\n",
    "\n",
    "\n",
    "#mergedmeadow_df['SEZ_ID']=mergedmeadow_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "# Identify columns that contain 'score' in their names\n",
    "mergedmeadow_df = meadowdata_processed['merged_df']\n",
    "score_columns = [col for col in mergedmeadow_df.columns if 'Score' in col]\n",
    "\n",
    "# Sum the values across identified score columns to get the final points for the SEZ\n",
    "mergedmeadow_df['Final_Total_Points'] = mergedmeadow_df[score_columns].sum(axis=1, skipna=True)\n",
    "\n",
    "#POints Possible per SEZ\n",
    "\n",
    "mergedmeadow_df['Final_Points_Possible'] = mergedmeadow_df[score_columns].notna().sum(axis=1)*12\n",
    "\n",
    "\n",
    "#ASsign Meadow SEZ_ID to each Assessment_Unit\n",
    "\n",
    "mergedmeadow_df['SEZ_ID']=mergedmeadow_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "mergedmeadow_df['SEZ_ID'] = mergedmeadow_df['SEZ_ID'].astype(str)\n",
    "\n",
    "#--------------------------------------\n",
    "# Calculate the final rating\n",
    "#------------------------------------\n",
    "mergedmeadow_df['Final_Percent'] = mergedmeadow_df['Final_Total_Points'] / mergedmeadow_df['Final_Points_Possible']\n",
    "\n",
    "mergedmeadow_df['Final_Rating']= mergedmeadow_df['Final_Percent'].apply(rate_SEZ)\n",
    "\n",
    "mergedmeadow_df['Final_Score']= mergedmeadow_df['Final_Rating'].apply(score_indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Scores for Each SEZ This will go into final table\n",
    "\n",
    "#FOR RIVERINE SEZ\n",
    "# Identify columns that contain 'score' in their names\n",
    "mergedriverine_df = riverinedata_processed['merged_df']\n",
    "score_columns = [col for col in mergedriverine_df.columns if 'Score' in col]\n",
    "\n",
    "# Sum the values across identified score columns to get the final points for the SEZ\n",
    "mergedriverine_df['Final_Total_Points'] = mergedriverine_df[score_columns].sum(axis=1, skipna=True)\n",
    "\n",
    "#POints Possible per SEZ\n",
    "\n",
    "mergedriverine_df['Final_Points_Possible'] = mergedriverine_df[score_columns].notna().sum(axis=1)*12\n",
    "\n",
    "\n",
    "#ASsigndriverine SEZ_ID to each Assessment_Unit\n",
    "\n",
    "mergedriverine_df['SEZ_ID']=mergedriverine_df['Assessment_Unit_Name'].map(lookup_riverine)\n",
    "\n",
    "mergedriverine_df['SEZ_ID'] = mergedriverine_df['SEZ_ID'].astype(int)\n",
    "\n",
    "#FOR QA purposes add SEZ_type so we can make sure lal data is available\n",
    "#mergedriverine_df['SEZ_Type'] = dfSEZ['SEZ_Type']\n",
    "\n",
    "#--------------------------------------\n",
    "# Calculate the final rating\n",
    "#------------------------------------\n",
    "mergedriverine_df['Final_Percent'] = mergedriverine_df['Final_Total_Points'] / mergedriverine_df['Final_Points_Possible']\n",
    "\n",
    "mergedriverine_df['Final_Rating']= mergedriverine_df['Final_Percent'].apply(rate_SEZ)\n",
    "\n",
    "mergedriverine_df['Final_Score']= mergedriverine_df['Final_Rating'].apply(score_indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfSEZ.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate mergedmeadow_df and mergedriverine_df\n",
    "riverinemeadowdf = pd.concat([mergedmeadow_df, mergedriverine_df], axis=0, ignore_index=True)\n",
    "\n",
    "#riverinemeadowdf['SEZ_ID']= riverinemeadowdf['SEZ_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riverinemeadowdf_duplicate_sez = riverinemeadowdf[riverinemeadowdf.duplicated(subset='SEZ_ID', keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTS WORKINNG... WILL NEED THIS for FINAL TABLEsezsurveytable grab comments to add to dataframe--MAYBE JUST DO THIS MANUALLY\n",
    "#Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "sezsurveyfields = ['Assessment_Unit_Name', 'Comments', 'Survey_Date']\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "# Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(sezsurveytable, sezsurveyfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "sezsurvey_df= pd.DataFrame(data, columns=sezsurveyfields)\n",
    "\n",
    "sezsurvey_df['Year'] = sezsurvey_df['Survey_Date'].dt.year\n",
    "print(sezsurvey_df)\n",
    "#add comments to dataframe\n",
    "# Create a dictionary to map Assessment_Unit_Name to Comments\n",
    "comments_map = dict(zip(sezsurvey_df['Assessment_Unit_Name'], sezsurvey_df['Comments']))\n",
    "\n",
    "# Add the Comments column to both_df using the map function\n",
    "riverinemeadowdf['Comments'] = riverinemeadowdf['Assessment_Unit_Name'].map(comments_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join Base data to riverine and meadow data\n",
    "\n",
    "Final_mergeddf =pd.merge(dfSEZinfo,riverinemeadowdf, on='SEZ_ID', how='outer', indicator= True)\n",
    "\n",
    "Final_mergeddf['Year'] = '2024'\n",
    "Final_mergeddf.to_csv('ready_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_mergeddf['_merge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final set up\n",
    "\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Threshold Year',\n",
    "    'Comments': 'Comments',\n",
    "    'SEZ_Type': 'SEZ_Type',\n",
    "    'Final_Rating': 'Final_Rating',\n",
    "    'Final_Percent': 'Final_Percent',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'Final_Points_Possible': 'Final_Points_Possible',\n",
    "    'Final_Total_Points': 'Final_Total_Points',\n",
    "    'Acres': 'Acres',\n",
    "    '':'AquaticOrganismPassage_Barriers',\n",
    "    '': 'AquaticOrganismPassage_NumberOf',\n",
    "    '': 'AquaticOrganismPassage_Score',\n",
    "    '': 'AquaticOrganismPassage_StreamMiles',\n",
    "    '': 'Bank_Stability_Data_Source',\n",
    "    '': 'Bank_Stability_Percent_Unstable',\n",
    "    '': 'Bank_Stability_Rating',\n",
    "    '': 'Bank_Stability_Score',\n",
    "    '': 'Biotic_Integrity_Rating',\n",
    "    '': 'Biotic_Integrity_CSCI',\n",
    "    '': 'Biotic_Integrity_Data_Source',\n",
    "    '': 'Biotic_Integrity_Score',\n",
    "    '': 'Comments',\n",
    "    '': 'Conifer_Encroachment_Percent_En',\n",
    "    '': 'Conifer_Encroachment_Data_Sourc',\n",
    "    '': 'Conifer_Encroachment_Rating',\n",
    "    '': 'Conifer_Encroachment_Score',\n",
    "    '': 'Conifer_Encroachment_Comments',\n",
    "    '': 'CountAttachments',\n",
    "    '': 'Ditches_Data_Source',\n",
    "    '': 'Ditches_Length',\n",
    "    '': 'Ditches_Meadow_Length',\n",
    "    '': 'Ditches_Percent',\n",
    "    '': 'Ditches_Rating',\n",
    "    '': 'Ditches_Score',\n",
    "    '': 'Feature_Type',\n",
    "    '': 'Habitat_Fragmentation_Data_Sour',\n",
    "    '': 'Habitat_Fragmentation_Imperviou',\n",
    "    '': 'Habitat_Fragmentation_Percent_I',\n",
    "    '': 'Habitat_Fragmentation_Rating',\n",
    "    '': 'Habitat_Fragmentation_Score',\n",
    "    '': 'Headcuts_Data_Source',\n",
    "    '': 'Headcuts_Number_of_Headcuts',\n",
    "    '': 'Headcuts_Rating',\n",
    "    '': 'Headcuts_Score',\n",
    "    '': 'Incision_Data_Source',\n",
    "    '': 'Incision_Rating',\n",
    "    '': 'Incision_Score',\n",
    "    '': 'Incision_Ratio',\n",
    "    '': 'Invasive_Percent_Cover',\n",
    "    '': 'Invasive_Rating',\n",
    "    '': 'Invasives_Data_Source',\n",
    "    '': 'Invasives_Invasives_Number_of_Invasives',\n",
    "    '': 'Invasives_Plant_Types',\n",
    "    '': 'Invasives_Scores',\n",
    "    '': 'NDVI_ID',\n",
    "    '': 'Ownership_Primary',\n",
    "    '': 'Ownership_Secondary',\n",
    "    '': 'Ownership_Secondary_2',\n",
    "    '': 'Ownership_Secondary_3',\n",
    "    '': 'VegetationVigor_DataSource',\n",
    "    '': 'VegetationVigor_Rating',\n",
    "    '': 'VegetationVigor_Raw',\n",
    "    '': 'VegetationVigor_Score',\n",
    "}\n",
    "# Rename fields based on field mappings\n",
    "readydf = both_df.rename(columns=field_mapping).drop(columns=[col for col in both_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Assuming ready_df is the DataFrame you want to write to CSV\n",
    "readydf.to_csv('ready_df.csv', index=False)\n",
    "\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "#data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "#field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(allsezscores, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the current working directory\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Habitat Condition use mergedriverine_df but add IPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import IPI Data and add it to mergedriverine_df\n",
    "IPIfolder = \"F:\\Research and Analysis\\Fisheries\\Streams\\Bioassessment\"\n",
    "IPI22 = os.path.join(IPIfolder, \"2022\", \"IPI_22.csv\")\n",
    "IPI20 = os.path.join(IPIfolder, \"2020\", \"IPI_20.csv\")\n",
    "\n",
    "#Create IPI Dataframes\n",
    "IPI22df = pd.read_csv(IPI22)\n",
    "IPI20df = pd.read_csv(IPI20)\n",
    "\n",
    "IPI22df['IPIYear']= '2022'\n",
    "IPI20df['IPIYear']= '2020'\n",
    "#Merge dataframes into one \n",
    "# Concatenate IPIdf1 and IPIdf2 along rows (axis=0)\n",
    "concatIPI_df = pd.concat([IPI22df, IPI20df], axis=0, ignore_index=True)\n",
    "\n",
    "#Calculate Scores in IPI\n",
    "#Code for Grading IPI\n",
    "#Define Grade for IPI Score - Used only for Stream HAbitat Condition\n",
    "def categorize_phab(IPI):\n",
    "     if   IPI >= 0.94:\n",
    "        return 'A'\n",
    "     elif 0.83 < IPI < 0.94:\n",
    "        return 'B'\n",
    "     elif 0.7 < IPI <= 0.83:\n",
    "        return 'C'\n",
    "     else:\n",
    "        return 'D'\n",
    "\n",
    "concatIPI_df['IPI_Rating']=concatIPI_df['IPI'].apply(categorize_phab)\n",
    "concatIPI_df['IPI_Score']= concatIPI_df['IPI_Rating'].apply(score_indicator)\n",
    "\n",
    "\n",
    "\n",
    "concatIPI_df.head()\n",
    "\n",
    "columns_to_keep = ['StationCode', 'IPI', 'IPI_Rating', 'IPI_Score', 'IPIYear']\n",
    "\n",
    "concatIPI_df = concatIPI_df[columns_to_keep]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS MATCHES THE BIOTIC INTEGRITY SCORES, next code block will do a spatial join with the data\n",
    "\n",
    "#prep biotic data so that we have a station code to match to so we have an sez assessment unit\n",
    "dfbiotic['StationCode'] = dfbiotic['Biotic_Integrity_Data_Source'].str.split(',').str[1].str.strip()\n",
    "\n",
    "sezipi_df = pd.merge(dfbiotic, concatIPI_df, on='StationCode', how='outer')\n",
    "\n",
    "\n",
    "sezipi_df = sezipi_df.dropna(subset=['IPI'])\n",
    "\n",
    "# Define a function to get the most recent score for each assessment unit\n",
    "def get_most_recent_scores(reduced_dfs):\n",
    "    \n",
    "# Group by 'Assessment_Unit_Name' and find the row with the maximum 'Year' or do i want an average?\n",
    "most_recent_IPI = df.loc[df.groupby('Assessment_Unit_Name')['Year'].idxmax()]\n",
    "\n",
    "#AVerage IPI for sez assessment unit\n",
    "\n",
    "print(most_recent_IPI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep mergedriverine_df calculate stream  maybe match scores to biotic integrity since those will all get phab?\n",
    "#mergedriverine_df = StationCode so I can match data from IPI df to merged riverine\n",
    "#mergedmeadow_df = pd.merge(mergedmeadow_df, df, on='Assessment_Unit_Name', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code to make it a spatially enabled dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to conver to spatially enable geodataframe\n",
    "# Create a new DataFrame Final_df with columns based on the field mapping\n",
    "#df = df.rename(columns=field_mapping)\n",
    "\n",
    "# Convert DataFrame to Spatially Enabled DataFrame\n",
    "#sedf = GeoAccessor.from_xy(df, x_column='LONGITUDE', y_column='LATITUDE')\n",
    "\n",
    "# Convert the SEDF to a feature class without sanitizing columns\n",
    "#sedf.spatial.to_featureclass(location=os.path.join(workspace, 'PlanAreaNoise_23_Staging'), sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
