{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: arcgis in c:\\program files\\arcgis\\pro\\bin\\python\\envs\\arcgispro-py3\\lib\\site-packages (2.2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install arcgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "import os\n",
    "import sys\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureSet, GeoAccessor, GeoSeriesAccessor, FeatureLayer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from arcgis.geometry import SpatialReference\n",
    "#import geopandas as gpd to use spatial.reference stuff\n",
    "gis = GIS()\n",
    "# # Set Pandas display options to show all rows and columns\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# in memory output file path\n",
    "wk_memory = \"memory\" + \"\\\\\"\n",
    "\n",
    "\n",
    "# set workspace and sde connections \n",
    "#working_folder = \"C:\\GIS\"\n",
    "\n",
    "#workspace      = \"C:\\GIS\\Scratch.gdb\"\n",
    "#arcpy.env.workspace = \"C:\\GIS\\Scratch.gdb\"\n",
    "#workspace ='F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "#arcpy.env.workspace = 'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "\n",
    "# network path to connection files\n",
    "filePath = \"F:\\\\GIS\\\\DB_CONNECT\"\n",
    "\n",
    "#path to GDB's to update and master data\n",
    "master_path = \"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "#SEZ_Master = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "#set workspace for connection to GDB\n",
    "#workspace=master_path\n",
    "# database file paths \n",
    "### SDE Collection New data collected is put into SDE.Survey under the indicator name\n",
    "### SDE Vector is where the data will go \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "#sdeBase    = os.path.join(filePath, \"SarahVector.sde\")\n",
    "#sdeCollect = os.path.join(filepath, \"SarahCollect.sde\")\n",
    "\n",
    "# setup connection string???\n",
    "#connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=sql12;DATABASE=sde;UID=sde;PWD=staff\"\n",
    "#connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "#engine = create_engine(connection_url)\n",
    "\n",
    "#with engine.begin() as sdeConnect:\n",
    " #   erosiondf      = pd.read_sql(\"SELECT * FROM sde.SDE.Stream_Erosion\", sdeConnect)\n",
    "\n",
    "# local variables sdata is starting data and f data is finishing datatables\n",
    "sdemonitoring= os.path.join(sdeBase, \"sde.SDE.Monitoring\")\n",
    "sdata = os.path.join(sdeCollect, \"sde.SDE.Survey\")\n",
    "#SEZdf = os.path.join(sdemonitoring, \"SEZ_Assessment_Units\")\n",
    "\n",
    "##Tables we get the data from in Collect 2010-2022 globalids don'tmatch\n",
    "sezsurveytable = os.path.join(sdata, \"sde.SDE.sez_survey\")\n",
    "erosiondata = os.path.join(sdata, \"sde.SDE.Stream_Erosion\")\n",
    "incisiondata = os.path.join(sdata, \"sde.SDE.sez_channel_incision\")\n",
    "invasivedata = os.path.join(sdata, \"sde.SDE.sez_invasive_plant\")\n",
    "headcutdata = os.path.join(sdata, \"sde.SDE.sez_stream_headcut\")\n",
    "\n",
    "#make this a spatial df\n",
    "streamdata = os.path.join(sdemonitoring, \"sde.SDE.Stream\")\n",
    "\n",
    "\n",
    "#Staging Tables currently living in SEZ_Data.GDB\n",
    "stage_bank_stability = os.path.join(master_path, \"bank_stability\") \n",
    "stage_All_SEZ_Scores = os.path.join(master_path, \"All_SEZ_Scores\")\n",
    "stage_biotic_integrity = os.path.join(master_path, \"biotic_integrity\")\n",
    "stage_headcuts = os.path.join(master_path, \"headcuts_table\")\n",
    "stage_incision = os.path.join(master_path, \"incision\")\n",
    "stage_invasives = os.path.join(master_path, \"invasives\")\n",
    "stage_vegetation = os.path.join(master_path, \"vegetation_vigor\")\n",
    "stage_conifer = os.path.join(master_path, \"conifer_encroachment\")\n",
    "stage_aquatic = os.path.join(master_path, \"aquatic_organism_passage_table\")\n",
    "stage_ditches = os.path.join(master_path, \"ditches\")\n",
    "stage_habitat = os.path.join(master_path, \"habitat_fragmentation\")\n",
    "#Final table to append to\n",
    "#finalSEZtable = os.path.join(ffdata, \"sde.SDE.SEZ_Assessment_Unit\")\n",
    "#finalSEZtable = os.path.join(master_path, \"AssessmentUnits_Master\")\n",
    "\n",
    "\n",
    "# network path to connection files??????\n",
    "#filePath = \"C:\\\\GIS\\\\DB_CONNECT\"\n",
    "# network path to connection files\n",
    "#filePath = r'F:\\Research and Analysis\\Workspace\\Sarah'\n",
    "\n",
    "#--------------------------------------------#\n",
    "#Notes to self\n",
    "#--------------------------------------------#\n",
    "#F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb. \n",
    "###'Assessment_Unit_Master' has all data for SEZ\n",
    "###“Bioassessment Sample Locations” has stream sites in SEZs\n",
    "\n",
    "gdbworking_folder = \"F:\\GIS\\GIS_DATA\\Monitoring\"\n",
    "headcutgdbfolder = os.path.join(gdbworking_folder, \"Stream_Headcut\", \"StreamHeadcut_Survey\")\n",
    "invasivegdbfolder = os.path.join(gdbworking_folder, \"Invasive_Species\", \"Invasive_Species_Survey\")\n",
    "sezgdbfolder= os.path.join(gdbworking_folder, \"SEZ\", \"SEZ_Survey\")\n",
    "## GDB with Raw Data straight from S123 not in the original folder (that one is not edited)\n",
    "headcut23gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2023.gdb\")\n",
    "headcut22gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2022.gdb\")\n",
    "headcut20gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2020.gdb\")\n",
    "headcut19gdb = os.path.join(headcutgdbfolder, \"Stream_Headcut_Survey_2019.gdb\")\n",
    "sez_surveygdb = os.path.join(sezgdbfolder, \"SEZ_Survey_2023.gdb\")\n",
    "\n",
    "#channelincision23gdb = os.path.join(working_folder,\"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2023.gdb\")\n",
    "#channelincision22gdb = os.path.join(working_folder, \"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2022.gdb\")\n",
    "#channelincision20gdb = os.path.join(working_folder, \"Channel_Incision\",\"Channel_Incision_Survey\",\"Channel_Incision_Survey_2020.gdb\")\n",
    "invasiveplant23gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2023.gdb\")\n",
    "invasiveplant22gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2022.gdb\")\n",
    "invasiveplant20gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2020.gdb\")\n",
    "invasiveplant19gdb= os.path.join(invasivegdbfolder,\"Invasive_Species_Survey_2019.gdb\")\n",
    "\n",
    "\n",
    "#This is thelocatoin for the final SEZ GDB to be updated in the gdb on f drive in the AssessmentUnits Master (polygon) i believe\n",
    "#FinalGDBtoupdate:F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data \\SEZ_Data.gdb\n",
    "\n",
    "#Location of USFS Invasive Species Data\n",
    "#https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer\n",
    "\n",
    "#Monitoring Dashboard location\n",
    "#Finalsdelocation:f'Vector.SDE' Sde.Monitoring Sde. SEZ_Assessment_Unit\n",
    "\n",
    "#Threshold Location? sde.tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fs_data_spatial(fs_url):\n",
    "    feature_layer = FeatureLayer(fs_url)\n",
    "    query_result = feature_layer.query().sdf\n",
    "    query_result.spatial.sr = 26910\n",
    "    return query_result\n",
    "\n",
    "SEZ_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/0\"\n",
    "\n",
    "dfSEZ = get_fs_data_spatial(SEZ_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use only if need to make SEZ ID dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE LOOKUP FOR SMALL ACRES SEZ ID AND FEATURE TYPE\n",
    "# Filter Assessment_Unit_Name entries with more than one unique Acres value\n",
    "grouped = dfSEZ.groupby('Assessment_Unit_Name')\n",
    "filtered_df = grouped.filter(lambda x: x['Acres'].nunique() > 1)\n",
    "\n",
    "# Determine the minimum Acres for each Assessment_Unit_Name\n",
    "min_acres_df = filtered_df.groupby('Assessment_Unit_Name', as_index=False)['Acres'].min()\n",
    "\n",
    "# Merge to get the corresponding SEZ_ID and SEZ_Type\n",
    "result_df = pd.merge(min_acres_df, filtered_df, on=['Assessment_Unit_Name', 'Acres'])\n",
    "\n",
    "# Select columns of interest for the CSV\n",
    "columns_of_interest = ['Assessment_Unit_Name', 'SEZ_ID', 'SEZ_Type']\n",
    "\n",
    "# Create a new DataFrame with selected columns\n",
    "df_selected = result_df[columns_of_interest].copy()\n",
    "\n",
    "# Specify the file path where you want to save the CSV file\n",
    "csv_file_path = 'Small_Polygon_Lookup.csv'\n",
    "\n",
    "# Save the selected data to CSV file\n",
    "df_selected.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"CSV file saved successfully to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SEZID and SEZType Lookup Dictionaries Small Polygone, Large Polygon, All Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Large Polygons or only polygon shapes lookup dictionary for Assessment Units with lerger values of acreage\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "excel_data = pd.read_csv(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Large_Polygon_Lookup.csv\")  \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "lookup_dict = {}\n",
    "\n",
    "for index, row in excel_data.iterrows():\n",
    "    lookup_dict[row['Assessment_Unit_Name']] = row['SEZ_ID']\n",
    "\n",
    "# See dictionary where keys are Assessment Unit Names and values are SEZ IDs\n",
    "print(lookup_dict)\n",
    "\n",
    "#Small Polygon if there are two acres for an SEZ\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "excel_data = pd.read_csv(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Small_Polygon_Lookup.csv\")  \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "lookup_riverine = {}\n",
    "\n",
    "for index, row in excel_data.iterrows():\n",
    "    lookup_riverine[row['Assessment_Unit_Name']] = row['SEZ_ID']\n",
    "\n",
    "# See dictionary where keys are Assessment Unit Names and values are SEZ IDs\n",
    "print(lookup_riverine)\n",
    "\n",
    "#All Polygons\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "excel_data = pd.read_csv(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\All_SEZID_Lookup.csv\")  \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "lookup_all = {}\n",
    "\n",
    "for index, row in excel_data.iterrows():\n",
    "    lookup_all[row['SEZ_ID']] = {'SEZ_Type': row['SEZ_Type']}\n",
    "\n",
    "# See dictionary where keys are Assessment Unit Names and values are SEZ IDs\n",
    "print(lookup_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Grading each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grading for each parameter \n",
    "#Defining Grade for Bank Stability based on Erosiondf[percent_unstable]\n",
    "def categorize_erosion(Percent_Unstable):\n",
    "    if pd.isna(Percent_Unstable):\n",
    "        return np.nan\n",
    "    elif 0 <= Percent_Unstable < 5:\n",
    "        return 'A'\n",
    "    elif 5 <= Percent_Unstable < 20:\n",
    "        return 'B'\n",
    "    elif 20 <= Percent_Unstable < 50:\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'D'\n",
    "    \n",
    "#Scoring based off of grades\n",
    "def score_indicator(Rating):\n",
    "    if pd.isna(Rating):\n",
    "        return np.nan\n",
    "    elif  Rating == 'A':\n",
    "        return '12'\n",
    "    elif Rating == 'B':\n",
    "        return '9'\n",
    "    elif Rating == 'C':\n",
    "        return '6'\n",
    "    else:\n",
    "        return '3'\n",
    "\n",
    "#Define Grade for Incision based off of incisino ratio\n",
    "\n",
    "def categorize_incision(bankfull_ratio):\n",
    "    if pd.isna(bankfull_ratio):\n",
    "        return np.nan\n",
    "    elif 0 <= bankfull_ratio < 1.2:\n",
    "        return 'A'\n",
    "    elif 1.2 <= bankfull_ratio < 1.6:\n",
    "        return 'B'\n",
    "    elif 1.6 <= bankfull_ratio < 2.1:\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'D'\n",
    "\n",
    "#Define Grade for Bioassessment Score\n",
    "def categorize_csci(biotic_integrity):\n",
    "     if pd.isna(biotic_integrity):\n",
    "        return np.nan\n",
    "     elif   biotic_integrity > 0.92:\n",
    "        return 'A'\n",
    "     elif 0.79 < biotic_integrity <= 0.92:\n",
    "        return 'B'\n",
    "     elif 0.62 < biotic_integrity <= 0.79:\n",
    "        return 'C'\n",
    "     else:\n",
    "        return 'D'\n",
    "\n",
    "#Define Priority List Level of Invasive Plant Species\n",
    "    \n",
    "\n",
    "# Process Data\n",
    "def rate_invasive(priority):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if (priority[3] + priority[4] == 1):\n",
    "        return 'B'  # Assign score D\n",
    "    elif (priority[3] + priority[4] == 2) or priority[1] == 1 or priority[2] == 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif (priority[3] + priority[4] >= 3)or priority[1] >= 2 or priority[2] >= 2 or (priority[1] + priority[2] >= 2):  \n",
    "       return 'D'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no invasives of any priority are present)\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Define Size for Headcut based off of headcut size\n",
    "##A = 0 headcut, B 1+small headcut\n",
    "def categorize_headcut(headcutdepth):\n",
    "    if pd.isnull(headcutdepth) or headcutdepth == 0:\n",
    "        return 'None'\n",
    "    elif 0.1 <= headcutdepth < 0.5:\n",
    "        return 'small'\n",
    "    elif 0.5 <= headcutdepth < 1:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'large'\n",
    "\n",
    "#define rating for headcut health per sez\n",
    "def rate_headcut(row):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if row['large'] >= 1:\n",
    "        return 'D'  # Assign score D\n",
    "    elif row['medium'] >= 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif row['small'] >= 1:\n",
    "       return 'B'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no headcuts)\n",
    "\n",
    "\n",
    "#define rating SEZ Rating\n",
    "def rate_SEZ(percent):\n",
    "    if 0 <= percent < .70:\n",
    "        return 'D'\n",
    "    elif .7 <= percent < .80:\n",
    "        return 'C'\n",
    "    elif .80 <= percent < .90:\n",
    "        return 'B'\n",
    "    else:\n",
    "        return 'A'\n",
    "    \n",
    "    #Define Grade for IPI Score - Used only for Stream HAbitat Condition\n",
    "def categorize_phab(IPI):\n",
    "     if   IPI >= 0.94:\n",
    "        return 'A'\n",
    "     elif 0.83 < IPI < 0.94:\n",
    "        return 'B'\n",
    "     elif 0.7 < IPI <= 0.83:\n",
    "        return 'C'\n",
    "     else:\n",
    "        return 'D'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Erosion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Get Data\n",
    "#----------------------------------------------------------------#\n",
    "#Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "erosionfields = ['Assessment_Unit_Name', 'Shape.STLength()', 'Bank_Type', 'Survey_Date']\n",
    "#erosiondf = pd.DataFrame.from_records(data=arcpy.da.SearchCursor(erosiondata, erosionfields), columns=erosionfields)\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    " # Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(erosiondata, erosionfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "erosiondf = pd.DataFrame(data, columns=erosionfields)\n",
    "\n",
    "# Replace NaN values in 'Assessment_Unit_Name' column with 'Skylandia SEZ'\n",
    "#erosiondf['Assessment_Unit_Name'] = erosiondf['Assessment_Unit_Name'].fillna('Skylandia SEZ')\n",
    "# Replace specific values in 'Assessment_Unit_Name' column\n",
    "erosiondf['Assessment_Unit_Name'] = erosiondf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh'})\n",
    "\n",
    "\n",
    "#Add SEZ_ID column using lookup dictionary\n",
    "#erosiondf['SEZ_ID'] = erosiondf['Assessment_Unit_Name'].map(lambda x: lookup_dict.get(x, {}).get('SEZ_ID', None))\n",
    "\n",
    "#This code is for the excel look up dictionary\n",
    "erosiondf['SEZ_ID'] = erosiondf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "erosiondf['SEZ_ID'] = erosiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "erosiondf['SEZ_ID'] = erosiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#erosiondf = erosiondf.merge(df, on='Assessment_Unit_Name', how='left')\n",
    "\n",
    "#calculate year column \n",
    "erosiondf['Year'] = erosiondf['Survey_Date'].dt.year\n",
    "\n",
    "# Replace 'both_banks' with 'Both Banks' in Bank_Type column\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['both_banks', 'Both banks'], 'Both Banks' )\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['one_bank', 'One bank'], 'One Bank')\n",
    "erosiondf['Bank_Type'] = erosiondf['Bank_Type'].replace(['no_bank', 'No bank'], 'No Bank')\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Process Data\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "# Initialize variables\n",
    "erosiondf['bank_multiplier'] = erosiondf['Bank_Type'].apply(lambda x: 2 if x == 'Both Banks' else (1 if x == 'One Bank' else 0))\n",
    "\n",
    "\n",
    "# Calculate the product of 'Shape.STLength()' and 'bank_multiplier' to get the eroded banks per row\n",
    "erosiondf['eroded_banks_per_row'] = erosiondf['Shape.STLength()'] * erosiondf['bank_multiplier']\n",
    "\n",
    "# Group by Assessment_Unit_Name and year and sum the lengths of banks for each unit to get total banks assessed\n",
    "erosiondf['banks_assessed_per_unit'] = erosiondf.groupby(['Assessment_Unit_Name', 'Year'])['Shape.STLength()'].transform('sum') * 2\n",
    "\n",
    "# Group by Assessment_Unit_Name and sum the eroded banks per row for each unit\n",
    "erosiondf['SEZ_total_eroded'] = erosiondf.groupby(['Assessment_Unit_Name', 'Year'])['eroded_banks_per_row'].transform('sum')\n",
    "\n",
    "# Calculate percent unstable Bank Stability per Assessment Unit\n",
    "erosiondf['Bank_Stability_Percent_Unstable'] = (erosiondf['SEZ_total_eroded'] / erosiondf['banks_assessed_per_unit']) * 100\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score\n",
    "#----------------------------------------------------------------#\n",
    "erosiondf['Bank_Stability_Rating']=erosiondf['Bank_Stability_Percent_Unstable'].apply(categorize_erosion)\n",
    "erosiondf['Bank_Stability_Score']= erosiondf['Bank_Stability_Rating'].apply(score_indicator)\n",
    "\n",
    "erosiondf['Bank_Stability_Data_Source'] = 'TRPA' #baseline condition assessment?'\n",
    "\n",
    "erosiondf.head()\n",
    "\n",
    "print(erosiondf)\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to bank_stability called stage_bank_stability GDB location\n",
    "#----------------------------------------------------------------#\n",
    "# Define the name of the feature class\n",
    "feature_class_name = 'bank_stability'\n",
    "\n",
    "# Define the full path to the feature class\n",
    "feature_class_path = stage_bank_stability \n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Bank_Stability_Data_Source': 'Bank_Stability_Data_Source',\n",
    "    'Bank_Stability_Percent_Unstable': 'Bank_Stability_Percent_Unstable',\n",
    "    'Bank_Stability_Rating': 'Bank_Stability_Rating',\n",
    "    'Bank_Stability_Score': 'Bank_Stability_Score',\n",
    "    'SEZ_ID': 'SEZ_ID'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "bank_stabilitydf = erosiondf.rename(columns=field_mapping).drop(columns=[col for col in erosiondf.columns if col not in field_mapping])\n",
    "\n",
    "readybankdf = bank_stabilitydf.groupby(['SEZ_ID', 'Year']).first().reset_index()\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "# Fix data type of Year so it writes to table\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y', errors='coerce')\n",
    "\n",
    "# Setting the frequency to 'Y' for year\n",
    "#readydf['Year'] = readydf['Year'].dt.to_period('Y')\n",
    "\n",
    "print(readybankdf)\n",
    "\n",
    "#Write dataframe to fc in SEZ_Data.GDB\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readybankdf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readybankdf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(stage_bank_stability, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")\n",
    "\n",
    "\n",
    "#Write dataframe to sde.collect.bank.stability eventually, current code write it to GDB in SEZ_Data.GDB\n",
    "# Set environment workspace to your SDE connection file\n",
    "#arcpy.env.workspace = master_path\n",
    "\n",
    "# Convert DataFrame to Feature Class\n",
    "#output_feature_class = \"ErosionUpdate\"  # Name for the output feature class\n",
    "#output_fc_path = os.path.join(arcpy.env.workspace, output_feature_class)\n",
    "\n",
    "# Assuming your DataFrame is already converted to a feature class\n",
    "# Replace \"path_to_your_feature_class\" with the actual path to your feature class\n",
    "#arcpy.conversion.TableToTable(\"path_to_your_feature_class\", arcpy.env.workspace, output_feature_class)\n",
    "\n",
    "# Overwrite Feature Class in SDE\n",
    "# Replace \"path_to_your_dataframe\" with the actual path to your DataFrame\n",
    "#arcpy.management.CopyFeatures(\"path_to_your_dataframe\", output_fc_path)\n",
    "\n",
    "\n",
    "#print(ready_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to CSV\n",
    "#csv_path = os.path.join(working_folder, \"Erosiondatamaster.csv\")\n",
    "#ready_df.to_csv(csv_path, index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Incision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Get Data-- all the data we need is in sde.collect.sde.survey.sde.sez_survey\n",
    "#----------------------------------------------------------------##Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "incisionfields = ['Assessment_Unit_Name', 'incision_ratio', 'survey_date']\n",
    "#incisiondf = pd.DataFrame.from_records(data=arcpy.da.SearchCursor(erosiondata, erosionfields), columns=erosionfields)\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    " # Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(sezsurveytable, incisionfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "incisiondf = pd.DataFrame(data, columns=incisionfields)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Add correct info to dataframe\n",
    "#----------------------------------------------------------------#\n",
    "#use this until we fix the domain\n",
    "incisiondf['Assessment_Unit_Name'] = incisiondf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh'})\n",
    "# Create a new column 'SEZ ID'\n",
    "#This code is for the excel look up dictionary\n",
    "incisiondf['SEZ_ID'] = incisiondf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#calculate year column \n",
    "incisiondf['Year'] = incisiondf['survey_date'].dt.year\n",
    "\n",
    "incisiondf\n",
    "\n",
    "# Display the updated incisiondf\n",
    "print(incisiondf)\n",
    "\n",
    "#Add SEZ_ID column using lookup dictionary\n",
    "#incisiondf['SEZ_ID'] = SEZIDdf['GlobalID'].map(lambda x: lookup_dict.get(x, {}).get('SEZ_ID', None))\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "#incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "#incisiondf['SEZ_ID'] = incisiondf['SEZ_ID'].astype(int)\n",
    "\n",
    "#incisiondf = incisiondf.merge(df, on='Assessment_Unit_Name', how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score\n",
    "#----------------------------------------------------------------#\n",
    "incisiondf['Incision_Rating']=incisiondf['incision_ratio'].apply(categorize_incision)\n",
    "incisiondf['Incision_Score']= incisiondf['Incision_Rating'].apply(score_indicator)\n",
    "\n",
    "incisiondf['Incision_Data_Source'] = 'TRPA' #baseline condition assessment?'\n",
    "\n",
    "incisiondf.head()\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to incision called stage_incision GDB location\n",
    "#----------------------------------------------------------------#\n",
    "# Define the name of the feature class\n",
    "feature_class_name = 'incision'\n",
    "\n",
    "# Define the full path to the feature class\n",
    "feature_class_path = stage_incision \n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Incision_Data_Source': 'Incision_Data_Source',\n",
    "    'incision_ratio': 'Incision_Ratio',\n",
    "    'Incision_Rating': 'Incision_Rating',\n",
    "    'Incision_Score': 'Incision_Score',\n",
    "    'SEZ_ID': 'SEZ_ID'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "incisionfinaldf = incisiondf.rename(columns=field_mapping).drop(columns=[col for col in incisiondf.columns if col not in field_mapping])\n",
    "\n",
    "readyincisiondf = incisionfinaldf.groupby(['SEZ_ID', 'Year']).first().reset_index()\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readyincisiondf)\n",
    "\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readyincisiondf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readyincisiondf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(stage_incision, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge readyincisiondf with SEZinfo_df to add the SEZ_Type\n",
    "readyincisiondf = readyincisiondf.merge(dfSEZ[['SEZ_ID', 'SEZ_Type']], on='SEZ_ID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invasive Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calfora Data points--dates only go to 2012?\n",
    "# dates only go to 2012 hereCArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines07/MapServer/1\"\n",
    "#CArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines27/MapServer/0\"\n",
    "# Create a feature layer object\n",
    "#cafeature_layer = FeatureLayer(CArest)\n",
    "#Query caflora layer to that it only shows 2020-2023 or only county?\n",
    "#start_year = 'January 1, 2020'\n",
    "#end_year= 'December 31, 2023'\n",
    "# Convert to datetime objects\n",
    "#start_date = pd.to_datetime(start_year)\n",
    "#end_date = pd.to_datetime(end_year)\n",
    "\n",
    "#Query before or after??\n",
    "#query = (CAsdf['Date_']>= start_date) & (CAsdf['Date_']<= end_date)\n",
    "\n",
    "#CA_filtereddf = CAsdf.join[query].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code using REST Service- most likely will reuse this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from arcgis.features import FeatureLayer\n",
    "from SEZ_scripts.utils import get_fs_data_spatial_query\n",
    "\n",
    "# Define the USFS REST endpoint\n",
    "usfsrest = \"https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer/0\"\n",
    "where    = \"FS_UNIT_ID = '0519'\"\n",
    "\n",
    "# Query the feature layer\n",
    "sdfUSFS = get_fs_data_spatial_query(usfsrest, where)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spatial join of sdf and sez master\n",
    "\n",
    "\n",
    "# Create the spatially enabled DataFrame (sdf) for target feature SEZ assessment units\n",
    "#SEZsdf = pd.DataFrame.spatial.from_featureclass(SEZ_Master)\n",
    "#spatial reference stuff\n",
    "sdfUSFS.spatial.sr = dfSEZ.spatial.sr\n",
    "\n",
    "#CAsdf.spatial.set_spatial_reference(SEZsdf.spatial.sr)\n",
    "#perform spatial join\n",
    "usfsdata = dfSEZ.spatial.join(sdfUSFS, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "#SET UP DATA WRANGLE\n",
    "#-----------------------------------------\n",
    "\n",
    "#Path to external data usfs with rest service--This assumes rest service is up to date to 2023\n",
    "\n",
    "# Paths to the feature classes\n",
    "invasive19fc = os.path.join(invasiveplant19gdb, \"Invasive_Species_2019\")\n",
    "invasive20fc = os.path.join(invasiveplant20gdb, \"Invasive_Species_2020\")\n",
    "invasive22fc = os.path.join(invasiveplant22gdb, \"Invasive_Species_2022\")\n",
    "invasive23fc = os.path.join(invasiveplant23gdb, \"sez_invasive_plant\")\n",
    "sez_surveyfc = os.path.join(sez_surveygdb, \"sez_survey\")\n",
    "\n",
    "#Use gdb because they have assessment unit name in them... 2022 i had to manually add in PRO\n",
    "invasive23fields = ['ParentGlobalID', 'invasives_percent_cover','invasives_plant_type', 'invasive_type_other']\n",
    "invasive22fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover', 'InvasiveType_Other', 'Survey_Date']\n",
    "invasive20fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover',  'Other', 'Survey_Date']\n",
    "invasive19fields = ['SITE_ID', 'SURVEY_DATE', 'INVASIVE_PLANT', 'PERCENT_COVER' ]\n",
    "usfsfields = ['Assessment_Unit_Name', 'COMMON_NAME', 'SCIENTIFIC_NAME']\n",
    "sez_surveyfields = ['GlobalID', 'invasives_percent_cover', 'Assessment_Unit_Name', 'invasives_number_of_species', 'survey_date']\n",
    "\n",
    "\n",
    "# Function to read feature class into DataFrame\n",
    "def feature_class_to_dataframe(feature_class, fields):\n",
    "    data = [row for row in arcpy.da.SearchCursor(feature_class, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "\n",
    "# Read feature classes into DataFrames\n",
    "invasive19df = feature_class_to_dataframe(invasive19fc, invasive19fields)\n",
    "invasive20df = feature_class_to_dataframe(invasive20fc, invasive20fields)\n",
    "invasive22df = feature_class_to_dataframe(invasive22fc, invasive22fields)\n",
    "invasivemeasurements23df = feature_class_to_dataframe(invasive23fc, invasive23fields)\n",
    "sez_surveyinvasivedf = feature_class_to_dataframe(sez_surveyfc, sez_surveyfields)\n",
    "\n",
    "\n",
    "#Set invasives_plant_type to NaN if invasives_number_of_species is 0 or NaN\n",
    "#sez_survey- groupby\n",
    "\n",
    "#sez_surveyinvasivedf.loc[(sez_surveyinvasivedf['invasives_number_of_species'] == 0) | (sez_surveyinvasivedf['invasives_number_of_species'].isna()), 'invasives_plant_type'] = np.nan\n",
    "sez_surveyinvasivedf['created_date']= sez_surveyinvasivedf['survey_date']\n",
    "#Join sez_survey and headcut23\n",
    "# Perform the join\n",
    "invasive23df = invasivemeasurements23df.merge(sez_surveyinvasivedf, left_on='ParentGlobalID', right_on='GlobalID', how='right')\n",
    "\n",
    "invasive23df.drop('invasives_percent_cover_y', axis=1, inplace=True)\n",
    "\n",
    "invasive23df.rename(columns={'invasives_percent_cover_x': 'invasives_percent_cover'}, inplace=True)\n",
    "\n",
    "invasive23df['invasives_percent_cover'].fillna(0, inplace=True)\n",
    "invasive23df['invasives_plant_type'].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(invasivedf['plant_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Capture data if there were no invasives observed\n",
    "\n",
    "#invasive23df.loc[invasive23df['invasives_number_of_species'] == 0, 'invasives_plant_type'] = None\n",
    "# Convert SpatialDataFrame to DataFrame\n",
    "\n",
    "usfsdf = usfsdata[usfsfields]\n",
    "#usfsdf = usfsdata.drop(columns='SHAPE')\n",
    "\n",
    "#print(usfsdf)\n",
    "#usfs23df = feature_class_to_dataframe(usfsdata, usfsfields)\n",
    "# Rename fields for consistency\n",
    "invasive19df.rename(columns={'SITE_ID': 'Assessment_Unit_Name', 'INVASIVE_PLANT': 'plant_type', 'PERCENT_COVER': 'percent_cover', 'SURVEY_DATE': 'created_date'}, inplace=True)\n",
    "invasive20df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date'}, inplace=True)\n",
    "invasive22df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'InvasiveType_Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date'}, inplace=True)\n",
    "invasive23df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'invasives_percent_cover': 'percent_cover', 'invasives_plant_type': 'plant_type', 'invasive_type_other': 'other', 'created_date': 'created_date'}, inplace=True)\n",
    "usfsdf.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'COMMON_NAME': 'plant_type'}, inplace=True)\n",
    "\n",
    "required_columns = ['Assessment_Unit_Name', 'plant_type', 'percent_cover', 'other', 'created_date', 'Source']\n",
    "\n",
    "# Define a function to add missing columns and keep only required columns\n",
    "def add_and_keep_columns(df, required_columns):\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "    return df[required_columns]\n",
    "\n",
    "# Add missing columns to each dataframe\n",
    "invasive19df = add_and_keep_columns(invasive19df, required_columns)\n",
    "invasive20df = add_and_keep_columns(invasive20df, required_columns)\n",
    "invasive22df = add_and_keep_columns(invasive22df, required_columns)\n",
    "invasive23df = add_and_keep_columns(invasive23df, required_columns)\n",
    "usfsdf = add_and_keep_columns(usfsdf, required_columns)\n",
    "\n",
    "\n",
    "#Remove null plant types for usfs data\n",
    "usfsdf = usfsdf[~usfsdf['plant_type'].isna()]\n",
    "# Remove records where plant_type is 'Eurasian watermilfoil'\n",
    "usfsdf = usfsdf[usfsdf['plant_type'] != 'Eurasian watermilfoil']\n",
    "\n",
    "\n",
    "#Add Source\n",
    "invasive19df['Source'] = 'TRPA'\n",
    "invasive20df['Source'] = 'TRPA'\n",
    "invasive22df['Source'] = 'TRPA'\n",
    "invasive23df['Source'] = 'TRPA'\n",
    "usfsdf['Source'] = 'USFS'\n",
    "\n",
    "# Concatenate DataFrames\n",
    "invasivedf = pd.concat([usfsdf, invasive19df, invasive20df, invasive22df, invasive23df], ignore_index=True)\n",
    "\n",
    "#Define SEZ ID based on Assessment_Unit_Name for QA on SEZ Name \n",
    "invasivedf['SEZ_ID'] = invasivedf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].fillna(0)\n",
    "\n",
    "#remove meadow from 2019 test period that are not actually meadows- \n",
    "# first run code without this  in case new data has wrongly spelled assessmne tunit name\n",
    "invasivedf = invasivedf[invasivedf['SEZ_ID'] != 0]\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "#invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].astype(int)\n",
    "#invasivedf['Assessment_Unit_Name'] = invasivedf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky Meadows meadow - 1': 'Sky meadows'})\n",
    "\n",
    "\n",
    "# Set 'Year' column based on data source\n",
    "invasivedf['Year'] = invasivedf['created_date'].dt.year\n",
    "\n",
    "\n",
    "invasivedf.loc[invasivedf['Source'] == 'USFS', 'Year'] = '2023'\n",
    "invasivedf.loc[invasivedf['Source'] == 'TRPA', 'Year'] = invasivedf['created_date'].dt.year\n",
    "\n",
    "#invasivedf.reset_index(drop=True, inplace=True)\n",
    "invasivedf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#invasivedf = invasivedf.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip this\n",
    "invasivedf_values_unique = invasivedf.values.flatten()\n",
    "is_unique = len(invasivedf_values_unique) == len(set(invasivedf_values_unique))\n",
    "print(is_unique)\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = invasivedf[invasivedf.duplicated(keep=False)]\n",
    "\n",
    "# Display duplicate rows\n",
    "print(duplicate_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------#\n",
    "    #Prep Plant_type Data\n",
    "#---------------------------#\n",
    "#Make a dataframe to capture 'other' plants in trpa data and then add it to invasive df\n",
    "other_plants_df = invasivedf[['Source', 'Year', 'SEZ_ID', 'Assessment_Unit_Name', 'other']].copy()\n",
    "\n",
    "#Get rid of Null values\n",
    "other_plants_df = other_plants_df[~other_plants_df['other'].isna()]\n",
    "\n",
    "#invasivedf.reset_index(drop=True, inplace=True)\n",
    "other_plants_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "#Rename 'other to plant_type\n",
    "other_plants_df.rename(columns={'other': 'plant_type'}, inplace=True)\n",
    "\n",
    "#invasivedf = invasivedf.drop_duplicates()\n",
    "\n",
    "\n",
    "# Concatenate other_plants_df with invasivedf JUST DO THIS MANUALLY \n",
    "#invasivedf = pd.concat([invasivedf, other_plants_df], ignore_index=True)\n",
    "\n",
    "#Append\n",
    "#invasivesdf=invasivedf.append(other_plants_df)\n",
    "#Concatenate the new DataFrame with the existing invasivedf\n",
    "#invasivedf = pd.concat([invasivedf, other_plants_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Replace various representations of null values with 'none'\n",
    "null_representations = ['<null>', '<Null>', '', 'NA', 'N/A', 'nan', 'NaN', 'None', 'NULL', None]\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace(null_representations, 'none')\n",
    "\n",
    "# Split plant types by comma and create new rows\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.split(pat=',')\n",
    "invasivedf = invasivedf.explode('plant_type')\n",
    "\n",
    "# Capitalize the first word and replace underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.split('_').str[0].str.capitalize() + ' ' + invasivedf['plant_type'].str.split('_').str[1:].str.join(' ')\n",
    "\n",
    "# Remove spaces after the words and capitalize the first word while replacing underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].astype(str)\n",
    "\n",
    "\n",
    "# Remove any spaces at the beginning or end of a word\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.strip()\n",
    "\n",
    "\n",
    "#---------------------#\n",
    "#These replacements are so the same weed with different common name doesn't get counted twice toward score MAY NEED TO ADD MORE DESCREPENCIES in coming years if more pop up or find more elopquent solution\n",
    "#---------------------#\n",
    "# Replace 'Common mullein' with 'Wooly mullein' in the 'plant_type' column \n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common mullein', 'Wooly mullein')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Nodding plumeless thistle', 'Musk thistle')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Field bindweed', 'Common bindweed')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common st. johnswort', 'Klamathweed')\n",
    "# Replace 'Broadleaf and leaved' with 'Perennial' in the 'plant_type' column\n",
    "# Define the replacements dictionary\n",
    "replacements = {'Broadleaf Pepperweed': 'Perennial pepperweed', 'Broadleaved pepperweed': 'Perennial pepperweed'}\n",
    "\n",
    "# Replace values in the 'plant_type' column using the dictionary\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace(replacements)\n",
    "\n",
    "#Replace Sulphur Cinquefoil with sulfur\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sulphur cinquefoil', 'Sulfur cinquefoil')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sweetclover', 'White sweetclover')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Reed canary grass', 'Reed canarygrass')\n",
    "#Replace\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Salt cedar', 'Tamarisk')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Butter and eggs', 'Yellow toadflax')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Canada cottonthistle', 'Canada thistle')\n",
    "# Replace empty strings or other placeholders with NaN\n",
    "#invasivedf['plant_type'] = invasivedf['plant_type'].replace('', np.nan)\n",
    "\n",
    "# Drop duplicates based on 'Assessment_Unit_Name' and 'Year' in the remaining DataFrame\n",
    "invasivedf = invasivedf.drop_duplicates(subset=['Assessment_Unit_Name', 'Year', 'plant_type'], keep='first')\n",
    "\n",
    "\n",
    "grouped_df = invasivedf.groupby(['Assessment_Unit_Name', 'Year'])['plant_type']\n",
    "\n",
    "# Aggregate the plant types into one column separated by commas\n",
    "combined_plant_types = grouped_df.apply(lambda x: ', '.join(x)).reset_index(name='all_plant_types')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group, data in invasivedf.groupby(['Assessment_Unit_Name', 'Year']):\n",
    "    print(group)\n",
    "    print(data)\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------#\n",
    "#Create Look up dictionary to assign priority group for each plant and then use it\n",
    "#----------------------------------------------------#\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "csv_data = pd.read_csv(r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Invasives Priority lookup.csv\") \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "Invasives_lookup = {}\n",
    "\n",
    "key = 'Common'\n",
    "values = ['Scientific', 'Priority'] \n",
    "\n",
    "Invasives_lookup= csv_data.set_index(key)[values].to_dict(orient='index')\n",
    "\n",
    "# Define a custom function to map plant types to priorities\n",
    "def map_priority(plant_type):\n",
    "    if pd.isnull(plant_type):\n",
    "        return 'None' # Return NaN for NaN values\n",
    "    else:\n",
    "        # Extract the priority from the dictionary, or return 'Unknown' if not found\n",
    "        plant_info = Invasives_lookup.get(plant_type)\n",
    "        if plant_info:\n",
    "            return plant_info['Priority']\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "\n",
    "\n",
    "# Create a new column 'Priority' based on the mapping from the dictionary\n",
    "invasivedf['Priority'] = invasivedf['plant_type'].map(map_priority)\n",
    "\n",
    "print(invasivedf.columns)\n",
    "#invasivedf['Priority'] = pd.to_numeric(invasivedf['Priority'], errors='coerce')\n",
    "\n",
    "# group by assessment unit and year and summarize the priority level of plants in each unit\n",
    "invasive_summary = invasivedf.groupby(['Assessment_Unit_Name', 'Year', 'Priority','Source'], dropna=False).size().reset_index(name='Count')\n",
    "\n",
    "invasive_summary_priority = invasive_summary.pivot_table(index=['Assessment_Unit_Name', 'Year','Source'], columns='Priority', values='Count', fill_value=0)\n",
    "\n",
    "\n",
    "# Reset the index to flatten the DataFrame\n",
    "invasive_summary_priority.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Grade, Score, # of Invasives\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "# Process Data\n",
    "def rate_invasive(priority):\n",
    "    if (priority[3] + priority[4] == 1):\n",
    "        return 'B'  # Assign score D\n",
    "    elif (priority[3] + priority[4] == 2) or priority[1] == 1 or priority[2] == 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif (priority[3] + priority[4] >= 3)or priority[1] >= 2 or priority[2] >= 2 or (priority[1] + priority[2] >= 2):  \n",
    "       return 'D'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no invasives of any priority are present)\n",
    "\n",
    "# Apply the rating function to the summary DataFrame\n",
    "invasive_summary_priority['Invasives_Rating'] = invasive_summary_priority[[1, 2, 3, 4]].apply(rate_invasive, axis=1)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "invasive_summary_priority['Invasives_Score']= invasive_summary_priority['Invasives_Rating'].apply(score_indicator) \n",
    "\n",
    "#Calculate total number of invasives per sez per year\n",
    "invasive_summary_priority['Number_of_Invasives']= invasive_summary_priority[[1, 2, 3, 4]].sum(axis=1)\n",
    "\n",
    "#Define SEZ ID based on Assessment_Unit_Name for QA on SEZ Name \n",
    "invasive_summary_priority['SEZ_ID'] = invasive_summary_priority['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "invasive_summary_priority['all_plants']= combined_plant_types['all_plant_types']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Prep and post ending dataframe to invasives table in SEZ_Data.GDB\n",
    "#----------------------------------------------------------------#\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Source': 'Invasives_Data_Source',\n",
    "    'Number_of_Invasives': 'Invasives_Number_of_Invasives',\n",
    "    'Invasives_Rating': 'Invasives_Rating',\n",
    "    'Invasives_Score': 'Invasives_Scores',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'percent_cover': 'Invasives_Percent_Cover',\n",
    "    'all_plants': 'Invasives_Plant_Types',\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readyinvasivedf = invasive_summary_priority.rename(columns=field_mapping).drop(columns=[col for col in invasive_summary_priority.columns if col not in field_mapping])\n",
    "\n",
    "readyinvasivedf['SEZ_ID'] = readyinvasivedf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readyinvasivedf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readyinvasivedf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readyinvasivedf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_invasives, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invasive with gdb and usfs pre joined layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invasive Species Use if need to use GDB to import data--shouldn't have to\n",
    "\n",
    "\n",
    "#Path to external data usfs \n",
    "usfsdata = os.path.join(master_path, \"usfsinvasivesez24\")\n",
    "#Path to external data Calflora aka State Park Data\n",
    "#calfloradata = os.path.join(master_path, \"\")\n",
    "\n",
    "# Paths to the feature classes\n",
    "invasive19fc = os.path.join(invasiveplant19gdb, \"Invasive_Species_2019\")\n",
    "invasive20fc = os.path.join(invasiveplant20gdb, \"Invasive_Species_2020\")\n",
    "invasive22fc = os.path.join(invasiveplant22gdb, \"Invasive_Species_2022\")\n",
    "invasive23fc = os.path.join(invasiveplant23gdb, \"sez_invasive_plant\")\n",
    "\n",
    "\n",
    "#Use gdb because they have assessment unit name in them... 2022 i had to manually add in PRO\n",
    "invasive23fields = ['Assessment_Unit', 'invasives_percent_cover','invasives_plant_type', 'Species_Level', 'invasive_type_other', 'created_date']\n",
    "invasive22fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover', 'InvasiveType_Other', 'Survey_Date']\n",
    "invasive20fields = ['Assessment_Unit_Name', 'Invasives_Plant_Type', 'Invasives_Percent_Cover',  'Other', 'Survey_Date']\n",
    "invasive19fields = ['SITE_ID', 'SURVEY_DATE', 'INVASIVE_PLANT', 'PERCENT_COVER' ]\n",
    "usfsfields = ['Assessment_Unit_Name', 'PLANT_COMM', 'SCIENTIFIC', 'DATE_COLLE', 'Eradicated']\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "\n",
    "# Function to read feature class into DataFrame\n",
    "def feature_class_to_dataframe(feature_class, fields):\n",
    "    data = [row for row in arcpy.da.SearchCursor(feature_class, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Read feature classes into DataFrames\n",
    "# Read feature classes into DataFrames\n",
    "invasive19df = feature_class_to_dataframe(invasive19fc, invasive19fields)\n",
    "invasive20df = feature_class_to_dataframe(invasive20fc, invasive20fields)\n",
    "invasive22df = feature_class_to_dataframe(invasive22fc, invasive22fields)\n",
    "invasive23df = feature_class_to_dataframe(invasive23fc, invasive23fields)\n",
    "usfs23df = feature_class_to_dataframe(usfsdata, usfsfields)\n",
    "\n",
    "\n",
    "invasive19df['Source'] = 'TRPA'\n",
    "invasive20df['Source'] = 'TRPA'\n",
    "invasive22df['Source'] = 'TRPA'\n",
    "invasive23df['Source'] = 'TRPA'\n",
    "usfs23df['Source'] = 'USFS'\n",
    "\n",
    "# Rename fields\n",
    "invasive19df.rename(columns={'SITE_ID': 'Assessment_Unit_Name', 'INVASIVE_PLANT': 'plant_type', 'PERCENT_COVER': 'percent_cover', 'SURVEY_DATE': 'created_date','Source':'Source'}, inplace=True)\n",
    "invasive20df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date', 'Source':'Source1'}, inplace=True)\n",
    "invasive22df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'Invasives_Plant_Type': 'plant_type', 'InvasiveType_Other': 'other', 'Invasives_Percent_Cover': 'percent_cover', 'Survey_Date': 'created_date','Source':'Source1'}, inplace=True)\n",
    "invasive23df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'invasives_percent_cover': 'percent_cover', 'invasives_plant_type': 'plant_type', 'invasive_type_other': 'other', 'created_date': 'created_date','Source':'Source1'}, inplace=True)\n",
    "usfs23df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'PLANT_COMM':'plant_type', 'DATE_COLLE':'created_date', 'Source':'Source2'}, inplace=True)\n",
    "\n",
    "\n",
    "# Concatenate DataFrames\n",
    "invasivedf = pd.concat([usfs23df, invasive19df, invasive20df, invasive22df, invasive23df], ignore_index=True)\n",
    "\n",
    "# Display merged DataFrame\n",
    "print(invasivedf)\n",
    "\n",
    "#Define SEZ ID based on Assessment_Unit_Name\n",
    "invasivedf['SEZ_ID'] = invasivedf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "invasivedf['SEZ_ID'] = invasivedf['SEZ_ID'].astype(int)\n",
    "invasivedf['Assessment_Unit_Name'] = invasivedf['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky Meadows meadow - 1': 'Sky meadows'})\n",
    "\n",
    "#calculate year column \n",
    "invasivedf['Year'] = invasivedf['created_date'].dt.year\n",
    "\n",
    "\n",
    "#---------------------------#\n",
    "    #Prep Data\n",
    "#---------------------------#\n",
    "# Replace 'other' or 'Other' in 'plant_type' column with values from 'other' column\n",
    "invasivedf['plant_type'] = invasivedf.apply(lambda row: row['other'] if pd.notna(row['plant_type']) and row['plant_type'].lower() in ['other', 'Other'] else row['plant_type'], axis=1)\n",
    "\n",
    "# Drop the 'other' column\n",
    "invasivedf.drop(columns=['other'], inplace=True)\n",
    "\n",
    "# Function to separate plant types and create new rows\n",
    "def separate_species(df):\n",
    "    # Split plant types by comma and create new rows\n",
    "    df['plant_type'] = df['plant_type'].str.split(',')\n",
    "    df = df.explode('plant_type')\n",
    "    return df\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "invasivedf = separate_species(invasivedf)\n",
    "\n",
    "# Remove rows where data source is USFS and plant type is NaN\n",
    "invasivedf = invasivedf[~((invasivedf['Source'] == 'USFS') & invasivedf['plant_type'].isna())]\n",
    "\n",
    "# Capitalize the first word and replace underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.split('_').str[0].str.capitalize() + ' ' + invasivedf['plant_type'].str.split('_').str[1:].str.join(' ')\n",
    "\n",
    "# Remove spaces after the words and capitalize the first word while replacing underscores with spaces\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].astype(str)\n",
    "\n",
    "\n",
    "# Remove any spaces at the beginning or end of a word\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].str.strip()\n",
    "\n",
    "\n",
    "# Remove rows where data source is USFS and plant type is NaN\n",
    "invasivedf = invasivedf[~((invasivedf['Source'] == 'USFS') & invasivedf['plant_type'].isna())]\n",
    "#Remove Eradicated \n",
    "\n",
    "# Filter out rows where 'eradicated' column is 'Yes'\n",
    "invasivedf = invasivedf[invasivedf['Eradicated'] != 'Yes']\n",
    "\n",
    "#These replacements are so the same weed with different common name doesn't get counted twice toward score MAY NEED TO ADD MORE DESCREPENCIES in coming years if more pop up or find more elopquent solution\n",
    "# Replace 'Common mullein' with 'Wooly mullein' in the 'plant_type' column \n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common mullein', 'Wooly mullein')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Nodding plumeless thistle', 'Musk thistle')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Field bindweed', 'Common bindweed')\n",
    "# Replace 'Nodding plumeless thistle' with 'Musk thistle' in the 'plant_type' column\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Common st. johnswort', 'Klamathweed')\n",
    "# Replace 'Broadleaf and leaved' with 'Perennial' in the 'plant_type' column\n",
    "# Define the replacements dictionary\n",
    "replacements = {'Broadleaf Pepperweed': 'Perennial pepperweed', 'Broadleaved pepperweed': 'Perennial pepperweed'}\n",
    "\n",
    "# Replace values in the 'plant_type' column using the dictionary\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace(replacements)\n",
    "\n",
    "#Replace Sulphur Cinquefoil with sulfur\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sulphur cinquefoil', 'Sulfur cinquefoil')\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Sweetclover', 'White sweetclover')\n",
    "\n",
    "#Replace\n",
    "invasivedf['plant_type'] = invasivedf['plant_type'].replace('Salt cedar', 'Tamarisk')\n",
    "\n",
    "# Now, drop duplicates based on the specified subset of columns\n",
    "invasivedf = invasivedf.drop_duplicates(subset=['SEZ_ID', 'Year', 'plant_type'])\n",
    "\n",
    "# Reset index if needed\n",
    "invasivedf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Remove duplicates based on SEZ, Year, and plant_type\n",
    "invasivedf = invasivedf.drop_duplicates(subset=['SEZ_ID', 'Year', 'plant_type'])\n",
    "#----------------------------------------------------#\n",
    "#Create Look up dictionary to assign priority group for each plant and then use it\n",
    "#----------------------------------------------------#\n",
    "\n",
    "# Step 1: Read the Excel file into a DataFrame\n",
    "csv_data = pd.read_csv(r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Invasives Priority lookup.csv\") \n",
    "\n",
    "#Define Empty look up dataframe\n",
    "Invasives_lookup = {}\n",
    "\n",
    "key = 'Common'\n",
    "values = ['Scientific', 'Priority'] \n",
    "\n",
    "Invasives_lookup= csv_data.set_index(key)[values].to_dict(orient='index')\n",
    "\n",
    "# Define a custom function to map plant types to priorities\n",
    "def map_priority(plant_type):\n",
    "    if pd.isnull(plant_type):\n",
    "        return np.nan  # Return NaN for NaN values\n",
    "    else:\n",
    "        # Extract the priority from the dictionary, or return 'Unknown' if not found\n",
    "        plant_info = Invasives_lookup.get(plant_type)\n",
    "        if plant_info:\n",
    "            return plant_info['Priority']\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "\n",
    "#Create a new column [Scientific based on look up dictionary\n",
    "#invasivedf['Scientific']=invasivedf['plant_type'].map(map_priority)\n",
    "\n",
    "# Create a new column 'Priority' based on the mapping from the dictionary\n",
    "invasivedf['Priority'] = invasivedf['plant_type'].map(map_priority)\n",
    "\n",
    "#invasivedf['Priority'] = pd.to_numeric(invasivedf['Priority'], errors='coerce')\n",
    "# group by assessment unit and year and summarize the priority level of plants in each unit\n",
    "invasive_summary = invasivedf.groupby(['Assessment_Unit_Name', 'Year', 'Priority']).size().reset_index(name='Count')\n",
    "\n",
    "invasive_summary_priority2 = invasive_summary.pivot_table(index=['Assessment_Unit_Name', 'Year'], columns='Priority', values='Count', fill_value=0)\n",
    "\n",
    "# Reset the index to flatten the DataFrame\n",
    "invasive_summary_priority2.reset_index(inplace=True)\n",
    "\n",
    "#invasive_summary_priority['Source'] = invasivedf['Source']\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score, # of Headcuts\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "# Process Data\n",
    "def rate_invasive(priority):\n",
    "    # Check if the SEZ has at least one large headcut\n",
    "    if (priority[3] + priority[4] == 1):\n",
    "        return 'B'  # Assign score D\n",
    "    elif (priority[3] + priority[4] == 2) or priority[1] == 1 or priority[2] == 1:\n",
    "        return 'C'  # Assign score C\n",
    "    elif (priority[3] + priority[4] >= 3)or priority[1] >= 2 or priority[2] >= 2 or (priority[1] + priority[2] >= 2):  \n",
    "       return 'B'  # Assign score B\n",
    "    else:\n",
    "        return 'A'  # Assign score A (no invasives of any priority are present)\n",
    "\n",
    "# Apply the rating function to the summary DataFrame\n",
    "invasive_summary_priority2['Invasives_Rating'] = invasive_summary_priority2.apply(rate_invasive, axis=1)\n",
    "\n",
    "#Calculate total number of invasives per sez per year\n",
    "invasive_summary_priority2['Number_of_Invasives']= invasive_summary_priority2[[1, 2, 3, 4,'Unknown']].sum(axis=1)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "invasive_summary_priority2['Invasives_Score']= invasive_summary_priority2['Invasives_Rating'].apply(score_indicator)    \n",
    "\n",
    "# make a columns in invasive summary that totals up percent cover per sez/year\n",
    "invasive_summary_priority2['Invasives_Percent_Cover'] = invasivedf.groupby(['SEZ_ID', 'Year'])['percent_cover'].sum().reset_index(drop=True)\n",
    "\n",
    "# combine the source column so that it shows all data sources that contributed to the data\n",
    "# Group invasivedf by 'SEZ_ID' and 'Year' and concatenate 'Data Source 1' values\n",
    "#data_source_1_combined = invasivedf.groupby(['SEZ_ID', 'Year'])['Source 1'].apply(lambda x: ', '.join(x.dropna())).reset_index()\n",
    "\n",
    "# Group invasivedf by 'SEZ_ID' and 'Year' and concatenate 'Data Source 2' values\n",
    "#data_source_2_combined = invasivedf.groupby(['SEZ_ID', 'Year'])['Source 2'].apply(lambda x: ', '.join(x.dropna())).reset_index()\n",
    "\n",
    "# Merge data_source_1_combined and data_source_2_combined on 'SEZ_ID' and 'Year'\n",
    "#merged_data_sources = pd.merge(data_source_1_combined, data_source_2_combined, on=['SEZ_ID', 'Year'], how='outer')\n",
    "\n",
    "# Combine 'Data Source 1' and 'Data Source 2' values with a comma separator\n",
    "#merged_data_sources['Data_Sources'] = merged_data_sources.apply(lambda row: ', '.join(filter(None, [row['Source 1'], row['Source 2']])), axis=1)\n",
    "\n",
    "# Drop the individual 'Data Source 1' and 'Data Source 2' columns\n",
    "#merged_data_sources.drop(columns=['Source 1', 'Source 2'], inplace=True)\n",
    "\n",
    "# Merge merged_data_sources with invasive_summary_priority on 'SEZ_ID' and 'Year'\n",
    "#invasive_summary_priority = pd.merge(invasive_summary_priority, merged_data_sources, on=['SEZ_ID', 'Year'], how='left')\n",
    "\n",
    "#invasive_summary_priority['Data_Sources']= invasivedfinvasivedf.groupby(['SED_ID', 'Year'])[Data Source 1 ] merge with DataSource 1 separate with comma if there are both \n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to incision called stage_incision GDB location\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Data_Sources': 'Invasives_Data_Source',\n",
    "    'Number_of_Invasives': 'Invasives_Number_of_Invasives',\n",
    "    'Invasives_Rating': 'Invasives_Rating',\n",
    "    'Invasives_Score': 'Invasives_Scores',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'percent_cover': 'Invasives_Percent_Cover',\n",
    "    'plant_type': 'Invasives_Plant_Type',\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "#readydf = invasive_summary_priority.rename(columns=field_mapping).drop(columns=[col for col in headcut_summary_sml.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "#print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "#data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "#field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(stage_headcuts, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data type of 'plant_type' column in the DataFrame\n",
    "print(\"Data type of 'plant_type' column in DataFrame:\", invasivedf['plant_type'].dtype)\n",
    "\n",
    "# Check the data type of values in the lookup dictionary\n",
    "for key, value in Invasives_lookup.items():\n",
    "    print(\"Data type of value for key\", key, \"in lookup dictionary:\", type(value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headcuts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sez_stream_headcuts doesn't capture the sez_survey data when there is 0!\n",
    "# Paths to the feature classes in GIS/GIS/DATA/Monitoring\n",
    "headcut19fc = os.path.join(headcut19gdb, \"Stream_Headcut_2019\")\n",
    "headcut20fc = os.path.join(headcut20gdb, \"Stream_Headcut_2020\")\n",
    "headcut22fc = os.path.join(headcut22gdb, \"Stream_Headcut_2022\")\n",
    "headcut23fc = os.path.join(headcut23gdb, \"sez_stream_headcut\")\n",
    "sez_surveyfc = os.path.join(sez_surveygdb, \"sez_survey\")\n",
    "\n",
    "headcut23fields = ['ParentGlobalID', 'headcut_depth']\n",
    "headcut22fields = ['Assessment_Unit', 'Headcut_Depth', 'synced_date']\n",
    "headcut20fields = ['Assessment_Unit_Name', 'Headcut_Depth','Survey_Date']\n",
    "headcut19fields = ['SITE_NAME', 'HEADCUT_DEPTH', 'SURVEY_DATE' ]\n",
    "sez_surveyfields = ['GlobalID', 'Assessment_Unit_Name', 'headcuts_number_of_headcuts', 'survey_date']\n",
    "\n",
    "# Function to read feature class into DataFrame\n",
    "def feature_class_to_dataframe(feature_class, fields):\n",
    "    data = [row for row in arcpy.da.SearchCursor(feature_class, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Read feature classes into DataFrames\n",
    "\n",
    "headcut19df = feature_class_to_dataframe(headcut19fc, headcut19fields)\n",
    "headcut20df = feature_class_to_dataframe(headcut20fc, headcut20fields)\n",
    "headcut22df = feature_class_to_dataframe(headcut22fc, headcut22fields)\n",
    "headcut23df = feature_class_to_dataframe(headcut23fc, headcut23fields)\n",
    "sez_surveyheadcutdf = feature_class_to_dataframe(sez_surveyfc, sez_surveyfields)\n",
    "\n",
    "#Join sez_survey and headcut23\n",
    "# Perform the join\n",
    "joined2023_df = headcut23df.merge(sez_surveyheadcutdf, left_on='ParentGlobalID', right_on='GlobalID', how='right')\n",
    "\n",
    "#joined2023_df.drop(columns=['Assessment_Unit'], inplace=True)\n",
    "\n",
    "# Rename fields\n",
    "headcut19df.rename(columns={'SITE_NAME': 'Assessment_Unit_Name', 'HEADCUT_DEPTH': 'headcut_depth', 'SURVEY_DATE': 'created_date'}, inplace=True)\n",
    "headcut20df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'Headcut_Depth': 'headcut_depth', 'Survey_Date': 'created_date'}, inplace=True)\n",
    "headcut22df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'Headcut_Depth': 'headcut_depth', 'synced_date': 'created_date'}, inplace=True)\n",
    "#joined2023_df.rename(columns={'Assessment_Unit': 'Assessment_Unit_Name', 'headcut_depth': 'headcut_depth', 'created_date': 'created_date'}, inplace=True)\n",
    "joined2023_df.rename(columns={'Assessment_Unit_Name': 'Assessment_Unit_Name', 'survey_date': 'created_date', 'headcuts_number_of_headcuts': 'Count', 'headcut_depth': 'headcut_depth'}, inplace=True)\n",
    "# Concatenate DataFrames\n",
    "headcutdf = pd.concat([headcut19df, headcut20df, headcut22df, joined2023_df], ignore_index=True)\n",
    "\n",
    "#----------------------------------------------#\n",
    "# Process Data\n",
    "#----------------------------------------------#\n",
    "#calculate year column \n",
    "headcutdf['Year'] = headcutdf['created_date'].dt.year\n",
    "# assign small, medium, large to headcut\n",
    "headcutdf['Headcut_Size']=headcutdf['headcut_depth'].apply(categorize_headcut)\n",
    "\n",
    "\n",
    "# Group by 'SEZ_ID', 'Year', and 'Headcut_Size', and count the number of occurrences for each group\n",
    "headcut_summary = headcutdf.groupby(['Assessment_Unit_Name', 'Year', 'Headcut_Size']).size().reset_index(name='Count')\n",
    "\n",
    "#print(type(headcut_summary))\n",
    "\n",
    "#Attempt to add data from sez_survey that was not captured in sez_stream_headcut\n",
    "#allheadcutdata=pd.concat([headcut_summary, moreheadcut23df], ignore_index=True)\n",
    "\n",
    "headcut_summary_sml = headcut_summary.pivot_table(index=['Assessment_Unit_Name', 'Year'], columns='Headcut_Size', values='Count', fill_value=0)\n",
    "\n",
    "# Reset the index to flatten the DataFrame\n",
    "headcut_summary_sml.reset_index(inplace=True)\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Grade, Score, # of Headcuts\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "# Apply the rating function to the summary DataFrame\n",
    "headcut_summary_sml['Headcuts_Rating'] = headcut_summary_sml.apply(rate_headcut, axis=1)\n",
    "\n",
    "#Calculate total number of headcuts per sez per year\n",
    "headcut_summary_sml['Number_of_Headcuts']= headcut_summary_sml[['large', 'medium', 'small']].sum(axis=1)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "headcut_summary_sml['Headcuts_Score']= headcut_summary_sml['Headcuts_Rating'].apply(score_indicator)\n",
    "\n",
    "#Add Datasource\n",
    "headcut_summary_sml['Headcuts_Data_Source'] = 'TRPA' #baseline condition assessment?'\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#post ending dataframe to incision called stage_incision GDB location\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Headcuts_Data_Source': 'Headcuts_Data_Source',\n",
    "    'Number_of_Headcuts': 'Number_of_Headcuts',\n",
    "    'Headcuts_Rating': 'Headcuts_Rating',\n",
    "    'Headcuts_Score': 'Headcuts_Score',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'small': 'small',\n",
    "    'medium': 'medium',\n",
    "    'large': 'large'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "#readydf = headcut_summary_sml.rename(columns=field_mapping).drop(columns=[col for col in headcut_summary_sml.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "#print(readydf)\n",
    "\n",
    "#readydf.to_csv(r\"C:\\Users\\snewsome\\Documents\\SEZ\\fullheadcutdata2023.csv\", index=False)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "#data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "#field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(stage_headcuts, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------#\n",
    "#Get Data from external sources and spatially join to our data\n",
    "#--------------------------------#\n",
    "\n",
    "\n",
    "#Calfora Data\n",
    "CArest = \"https://map.dfg.ca.gov/arcgis/rest/services/Project_BIOS_Public/q_BIOS_Public_pointslines07/MapServer/1\"\n",
    "#CAfc = os.path.join(CArest, \"Non-Native and Invasive Plants - Calflora\")\n",
    "CAsdf = pd.DataFrame.spatial.from_layer(CArest)\n",
    "# Delete the existing feature layer if it exists\n",
    "\n",
    "CA_fc = arcpy.MakeFeatureLayer_management(CArest, \"Calfora_invasives\")\n",
    "\n",
    "\n",
    "usfsrest= \"https://apps.fs.usda.gov/arcx/rest/services/EDW/EDW_InvasiveSpecies_01/MapServer/0\"\n",
    "#usfsfc =os.path.join(usfsrest, \"Current Invasive Plant Locations\") \n",
    "#usfssdf = pd.DataFrame.spatial.from_layer(usfsrest)\n",
    "if arcpy.Exists(\"usfs_layer\"):\n",
    "    arcpy.Delete_management(\"usfs_layer\")\n",
    "usfs_fc = arcpy.MakeFeatureLayer_management(usfsrest, \"usfs_layer\")\n",
    "#Spatially join usfs layers to see if any of the points or polygons intersect with our sez's123\n",
    "\n",
    "# Set the workspace environment\n",
    "arcpy.env.workspace = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Data Management 2023.gdb\"\n",
    "\n",
    "# Define the target and join layers\n",
    "target_feature = SEZ_Master #\"AssessmentUnit_Master\"\n",
    "CAjoin_feature = CA_fc\n",
    "USFSjoin_feature = usfs_fc\n",
    "\n",
    "# Define the output feature class\n",
    "out_feature_class = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Data Management 2023.gdb\\externalinvasivesez\"\n",
    "\n",
    "# Define the fields you want to keep from both layers\n",
    "#field_mappings = arcpy.FieldMappings()\n",
    "#field_mappings.addTable(target_features)\n",
    "#field_mappings.addTable(join_features)\n",
    "\n",
    "# Keep only specific fields from the target layer\n",
    "#keep_fields_target = [\"Assessment_Unit_Name\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_target:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "#Keep only specific fields from the calfora join layer\n",
    "#keep_fields_CA = [\"CName\", \"SName\", \"Source\", \"Obs_Date\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_CA:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "# Keep only specific fields from the usfs join_features\n",
    "#keep_fields_USFS = [\"SCIENTIFIC_NAME\", \"COMMON_NAME\", \"DATE_COLLECTED\"]\n",
    "#for field in field_mappings.fields:\n",
    " #   if field.name not in keep_fields_USFS:\n",
    "  #      field_mappings.removeFieldMap(field_mappings.findFieldMapIndex(field.name))\n",
    "\n",
    "\n",
    "# Perform spatial join with specified field mappings\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=SEZ_Master,\n",
    "    join_features=[CAjoin_feature],\n",
    "    out_feature_class=out_feature_class,\n",
    "    join_operation=\"JOIN_ONE_TO_MANY\",\n",
    "    join_type=\"KEEP_ALL\"\n",
    "    #field_mapping=field_mappings\n",
    ")\n",
    "\n",
    "#Create Dataframe for spatially joined data\n",
    "# Define the fields you want to retrieve from the output feature class\n",
    "fields = [\"Assessment_Unit_Name\", \"CName\", \"SName\", \"Source\", \"Obs_Date\", \"SCIENTIFIC_NAME\", \"COMMON_NAME\", \"DATE_COLLECTED\"]\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "data = []\n",
    "\n",
    "# Use SearchCursor to read data from the output feature class\n",
    "with arcpy.da.SearchCursor(out_feature_class, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the list of tuples into a pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bioassessment/ Biotic Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bioassessment scores- get all stream data into sde.Stream first... then look at Biotic Integrit Data Source to find which stream was used to evaluate each meadow.. this will help with percent of stream miles I believe\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Get Data-- all the data we need is in sde.collect.sde.survey.sde.sez_survey\n",
    "#----------------------------------------------------------------##Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "\n",
    "#Create SEDF setup\n",
    "#streamdata is the path to the feature class in sde\n",
    "# Set the workspace to your SDE connection file\n",
    "arcpy.env.workspace = streamdata\n",
    "feature_class= \"Stream\"\n",
    "\n",
    "# Convert feature class to a pandas DataFrame\n",
    "fields = [field.name for field in arcpy.ListFields(feature_class)]\n",
    "\n",
    "# Create DataFrame\n",
    "streamsdf = pd.DataFrame.spatial.from_featureclass(feature_class, columns=fields)\n",
    "\n",
    "# Create the spatially enabled DataFrame (sdf) for target feature SEZ assessment units\n",
    "SEZsdf = pd.DataFrame.spatial.from_featureclass(SEZ_Master)\n",
    "#spatial reference stuff\n",
    "streamsdf.spatial.sr = SEZsdf.spatial.sr\n",
    "\n",
    "#perform spatial join of sde.stream and sez units\n",
    "thesdf = SEZsdf.spatial.join(streamsdf, how='inner')\n",
    "\n",
    "#Notes to self, Stream Miles?\n",
    "#Keep only Riverine?, this may be what the smaller polygons are for \n",
    "# Filter for SEZ type 'Riverine'\n",
    "#riverine_df = bioticsdf[bioticsdf['Feature_Type'] == 'Riverine']\n",
    "\n",
    "#if the layer contains Riverine? or just for any spatial join.. see what it does\n",
    "#spatial join this layer to asessment unit master layer\n",
    "#ASsessment unit master layer is called SEZ_Master\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------#\n",
    "#DATA PREP\n",
    "#----------------------#\n",
    "# List of columns to keep\n",
    "columns_to_keep = ['Assessment_Unit_Name', 'SEZ_Type', 'Feature_Type', 'SEZ_ID','SITE_NAME', 'COUNT_VALUE', 'YEAR_OF_COUNT', 'STATION_TYPE', 'LONGITUDE', 'LATITUDE', ]\n",
    "##Try this instead\n",
    "# Select only the desired columns\n",
    "bioticdf = thesdf.loc[:, columns_to_keep].copy()  \n",
    "\n",
    "#DATA PREP\n",
    "# Filter for years 2020 to 2023\n",
    "filtered_df = bioticdf.loc[(bioticdf['YEAR_OF_COUNT'] >= 2020) & (bioticdf['YEAR_OF_COUNT'] <= 2023)].copy()\n",
    "\n",
    "# Define SEZ ID based on Assessment_Unit_Name for QA on SEZ Name THIS METHOD USES LARGER POLYGONES \n",
    "filtered_df.loc[:, 'SEZ_ID'] = filtered_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Fill NaN values with a specific value, such as 0\n",
    "filtered_df.loc[:, 'SEZ_ID'] = filtered_df['SEZ_ID'].fillna(0)\n",
    "\n",
    "# Convert SEZ ID column to integer\n",
    "filtered_df.loc[:, 'SEZ_ID'] = filtered_df['SEZ_ID'].astype(int)\n",
    "\n",
    "# Replace values in the 'Assessment_Unit_Name' column\n",
    "filtered_df.loc[:, 'Assessment_Unit_Name'] = filtered_df['Assessment_Unit_Name'].replace({'Blackwood Creek - upper 2': 'Blackwood Creek - Upper 2', 'Taylor Creek marsh - 1': 'Taylor Creek marsh', 'Sky Meadows meadow - 1': 'Sky meadows'})\n",
    "\n",
    "# Add data source information\n",
    "filtered_df['Source'] = 'TRPA, ' + filtered_df['SITE_NAME'].astype(str) + ', ' + filtered_df['YEAR_OF_COUNT'].astype(str)\n",
    "\n",
    "#Group by Year and Assessment Unit and Site NAME and remove duplicates\n",
    "\n",
    "filtered_df['SITE_NAME'] = filtered_df['SITE_NAME'].str.strip()\n",
    "filtered_df['YEAR_OF_COUNT'] = filtered_df['YEAR_OF_COUNT'].astype(str).str.strip().astype(int)\n",
    "filtered_df['YEAR_OF_COUNT'] = pd.to_numeric(filtered_df['YEAR_OF_COUNT'], errors='coerce')\n",
    "\n",
    "\n",
    "# Group by Assessment_Unit_Name, SITE_NAME, and YEAR_OF_COUNT and drop duplicates\n",
    "BIdf = filtered_df.groupby(['SEZ_ID', 'SITE_NAME', 'YEAR_OF_COUNT', 'COUNT_VALUE']).apply(lambda x: x.drop_duplicates()).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_df['YEAR_OF_COUNT'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------#\n",
    "#Grade and Score biotic integrity\n",
    "#----------------------#\n",
    "\n",
    "#Rate the score\n",
    "#ef categorize_csci(biotic_integrity):\n",
    "# Apply the rating function to the summary DataFrame\n",
    "BIdf['Biotic_Rating'] = BIdf['COUNT_VALUE'].apply(categorize_csci)\n",
    "\n",
    "#Calculate the score for the sez\n",
    "BIdf['Biotic_Score']= BIdf['Biotic_Rating'].apply(score_indicator) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Prep and post ending dataframe to invasives table in SEZ_Data.GDB\n",
    "#----------------------------------------------------------------#\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'YEAR_OF_COUNT': 'Year',\n",
    "    'Source': 'Biotic_Integrity_Data_Source',\n",
    "    'COUNT_VALUE': 'Biotic_Integrity_CSCI',\n",
    "    'Biotic_Rating': 'Biotic_Integrity_Rating',\n",
    "    'Biotic_Score': 'Biotic_Integrity_Score',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = BIdf.rename(columns=field_mapping).drop(columns=[col for col in BIdf.columns if col not in field_mapping])\n",
    "\n",
    "readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_biotic_integrity, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")\n",
    "\n",
    "#Delete duplicates yourself.. not that much data to go through, can't figure out why it won't remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conifer Encroachment Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import arcpy\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Conifer_Encroachment_Data_Sourc',\n",
    "                        'Conifer_Encroachment_Rating',                    \n",
    "                        'Conifer_Encroachment_Percent_En',\n",
    "                        'Conifer_Encroachment_Score',\n",
    "                        'SEZ_ID',\n",
    "                        'Assessment_Unit_Name',\n",
    "                        'ConiferEncroachment_Comments']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    conifer_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "conifer_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Conifer_Encroachment_Data_Sourc': 'Conifer_Encroachment_Data_Source',\n",
    "                'Conifer_Encroachment_Rating': 'Conifer_Encroachment_Rating',                    \n",
    "                'Conifer_Encroachment_Percent_En': 'Conifer_Percent_Encroached',\n",
    "                'Conifer_Encroachment_Score': 'Conifer_Encroachment_Score',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'ConiferEncroachment_Comments': 'ConiferEncroachment_Comments'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = conifer_df.rename(columns=field_mapping).drop(columns=[col for col in conifer_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_conifer, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aquatic Organism Passage STagin table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SEZ_Master.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aquatic Organism /Fish Passage- only old data for now\n",
    "import os\n",
    "import pandas as pd\n",
    "import arcpy\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['AquaticOrganismPassage_Barriers',\n",
    "                        'AquaticOrganismPassage_DataSour',                    \n",
    "                        'AquaticOrganismPassage_NumberOf',\n",
    "                        'AquaticOrganismPassage_Rating',\n",
    "                        'SEZ_ID',\n",
    "                        'Assessment_Unit_Name',\n",
    "                        'AquaticOrganismPassage_Score',\n",
    "                        'AquaticOrganismPassage_StreamMi']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    AOP_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "AOP_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'AquaticOrganismPassage_Barriers': 'AOP_BarriersPerMile',\n",
    "                'AquaticOrganismPassage_DataSour': 'AOP_DataSource',                    \n",
    "                'AquaticOrganismPassage_NumberOf': 'AOP_NumberofBarriers',\n",
    "                'AquaticOrganismPassage_Score': 'AOP_Score',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'AquaticOrganismPassage_StreamMi': 'AOP_StreamMiles',\n",
    "                'AquaticOrganismPassage_Rating': 'AOP_Rating'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = AOP_df.rename(columns=field_mapping).drop(columns=[col for col in AOP_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_aquatic, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Habitat Fragmentation Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create STaging Tables for External Data Put in STagin Table\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Habitat_Fragmentation_Data_Sour',\n",
    "                        'Habitat_Fragmentation_Imperviou',\n",
    "                        'Habitat_Fragmentation_Percent_I',\n",
    "                        'Habitat_Fragmentation_Rating',\n",
    "                        'Habitat_Fragmentation_Score',\n",
    "                        'SEZ_ID', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    HabFrag_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "HabFrag_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Habitat_Fragmentation_Data_Sour': 'Habitat_Frag_Data_Source',\n",
    "                'Habitat_Fragmentation_Percent_I': 'HAbitat_Frag_Percent_Impervious',                    \n",
    "                'Habitat_Fragmentation_Score': 'Habitat_Frag_Score',\n",
    "                'Habitat_Fragmentation_Imperviou': 'Habitat_Frag_Impervious_Acres',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'Habitat_Fragmentation_Rating': 'Habitat_Frag_Rating'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = HabFrag_df.rename(columns=field_mapping).drop(columns=[col for col in HabFrag_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_habitat, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ditches Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create STaging Tables for External Data Put in STagin Table\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Ditches_Data_Source',\n",
    "                        'Ditches_Length',\n",
    "                        'Ditches_Meadow_Length',\n",
    "                        'Ditches_Percent',\n",
    "                        'Ditches_Rating',\n",
    "                        'Ditches_Score',\n",
    "                        'SEZ_ID', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    Ditch_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "Ditch_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Ditches_Data_Source': 'Ditches_Data_Source',\n",
    "                'Ditches_Length': 'Ditches_Length',                    \n",
    "                'Ditches_Meadow_Length': 'Ditches_Meadow_Length',\n",
    "                'Ditches_Percent': 'Ditches_Percent',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'Ditches_Rating': 'Ditches_Rating',\n",
    "                'Ditches_Score': 'Ditches_Score'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = Ditch_df.rename(columns=field_mapping).drop(columns=[col for col in Ditch_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_ditches, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vegetation Vigor- old data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create STaging Tables for External Data Put in STagin Table\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['VegetationVigor_DataSource',\n",
    "                        'NDVI_ID',\n",
    "                        'VegetationVigor_Raw',\n",
    "                        'VegetationVigor_Rating',\n",
    "                        'VegetationVigor_Score',\n",
    "                        'SEZ_ID', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    vegetation_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "vegetation_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'VegetationVigor_DataSource': 'VegetationVigor_DataSource',\n",
    "                'NDVI_ID': 'NDVI_ID',                    \n",
    "                'VegetationVigor_Raw': 'VegetationVigor_Raw',\n",
    "                'VegetationVigor_Rating': 'VegetationVigor_Rating',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'VegetationVigor_Score': 'VegetationVigor_Score'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = vegetation_df.rename(columns=field_mapping).drop(columns=[col for col in vegetation_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_vegetation, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEZ data from 2020 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All SEZ Scores from current Data\n",
    "\n",
    "# Path to the geodatabase\n",
    "master_path = r\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Data\\SEZ_Data.gdb\"\n",
    "\n",
    "# Table name in the geodatabase\n",
    "table_name = \"AssessmentUnits_Master\"\n",
    "\n",
    "# Function to convert geodatabase table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    fields = [field.name for field in arcpy.ListFields(table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(table, fields)]\n",
    "    return pd.DataFrame(data, columns=fields)\n",
    "\n",
    "try:\n",
    "    # Load the data from the geodatabase table into a DataFrame\n",
    "    SEZ_Master = table_to_dataframe(os.path.join(master_path, table_name))\n",
    "\n",
    "    # Remove leading and trailing whitespace from column names\n",
    "    SEZ_Master.columns = SEZ_Master.columns.str.strip()\n",
    "\n",
    "    # Columns to select from the table\n",
    "    selected_columns = ['Acres',\n",
    "                        'Final_Percent',\n",
    "                        'Final_Points_Possible',\n",
    "                        'Final_Rating',\n",
    "                        'Final_Total_Points',\n",
    "                        'SEZ_ID',\n",
    "                        'Comments',\n",
    "                        'SEZ_Type', \n",
    "                        'Assessment_Unit_Name']\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    SEZ20_df = SEZ_Master[selected_columns].copy()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\"One or more specified columns not found in the DataFrame:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "SEZ20_df['Year'] = 2020\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "                'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "                'Year': 'Year',\n",
    "                'Acres': 'Acres',\n",
    "                'SEZ_Type': 'SEZ_Type',                    \n",
    "                'Final_Percent': 'Final_Percent',\n",
    "                'Final_Total_Points': 'Final_Total_Points',\n",
    "                'SEZ_ID': 'SEZ_ID',\n",
    "                'Final_Points_Possible': 'Final_Points_Possible',\n",
    "                'Final_Rating': 'Final_Rating',\n",
    "                'Comments': 'Comments'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = SEZ20_df.rename(columns=field_mapping).drop(columns=[col for col in SEZ20_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table uncomment when ready\n",
    "with arcpy.da.InsertCursor(stage_All_SEZ_Scores, field_names) as cursor:\n",
    "    for row in data:\n",
    "        cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment Unit- Final SEZ Scores  Calculations for SEZ Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Data with REST SERVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Paths to Staging tables in SDE... via REST service\n",
    "# Use rest service to get data \n",
    "#Setup\n",
    "import pandas as pd\n",
    "import os\n",
    "from arcgis.features import FeatureLayer\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def get_fs_data(fs_url):\n",
    "    feature_layer = FeatureLayer(fs_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "def get_fs_data_spatial(fs_url):\n",
    "    feature_layer = FeatureLayer(fs_url)\n",
    "    query_result = feature_layer.query().sdf\n",
    "    query_result.spatial.sr = 26910\n",
    "    return query_result\n",
    "\n",
    "\n",
    "\n",
    "bank_stability_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/4\"\n",
    "biotic_integrity_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/5\"\n",
    "conifer_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/6\"\n",
    "ditches_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/7\"\n",
    "invasives_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/11\"\n",
    "Hab_Frag_url = 'https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/8'\n",
    "vegetation_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/12\"\n",
    "incision_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/10\"\n",
    "headcuts_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/9\"\n",
    "AOP_url= \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/3\"\n",
    "SEZ_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/0\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframes from Rest Services \n",
    "\n",
    "dfbanks = get_fs_data(bank_stability_url)\n",
    "dfbiotic = get_fs_data(biotic_integrity_url)\n",
    "dfconifer = get_fs_data(conifer_url)\n",
    "dfditch = get_fs_data(ditches_url)\n",
    "dfinvasive = get_fs_data(invasives_url)\n",
    "dfhabitat = get_fs_data(Hab_Frag_url)\n",
    "dfvegetation = get_fs_data(vegetation_url)\n",
    "dfincision = get_fs_data(incision_url)\n",
    "dfheadcuts = get_fs_data(headcuts_url)\n",
    "dfAOP = get_fs_data(AOP_url)\n",
    "dfSEZ = get_fs_data_spatial(SEZ_url)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data in staging tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #------------------#\n",
    "#Biotic Integrity\n",
    "#------------------#\n",
    "#Prep data- Add any scores and find average oif there are two stream sites for one sez. Also rename data source so it includes are streams that were averaged\n",
    "# Function to average scores and concatenate data sources for each Year and Assessment_Unit_Name\n",
    "def average_biotic_scores(dfbiotic, unit_col='Assessment_Unit_Name', year_col='Year', score='Biotic_Integrity_CSCI', source_col='Biotic_Integrity_Data_Source'):\n",
    "    # Group by Assessment Unit and Year\n",
    "    group = dfbiotic.groupby([unit_col, year_col])\n",
    "    \n",
    "    # Calculate the mean of the scores\n",
    "    averaged_scores = group[score].mean().reset_index()\n",
    "    \n",
    "    # Concatenate the data sources with specific formatting\n",
    "    def concatenate_sources(x, year):\n",
    "        formatted_sources = []\n",
    "        for entry in x:\n",
    "            parts = entry.split(\",\")\n",
    "            if len(parts) >= 3:\n",
    "                formatted_sources.append(f'TRPA, {parts[1].strip()}, {parts[-1].strip()}')  # Extract station code and year\n",
    "        if formatted_sources:\n",
    "            return '/ '.join(formatted_sources)\n",
    "        else:\n",
    "            return None  # Return None if all entries are invalid\n",
    "    \n",
    "    # Apply concatenate_sources to each group\n",
    "    concatenated_sources = group.apply(lambda grp: concatenate_sources(grp[source_col], grp[year_col])).reset_index(name=source_col)\n",
    "    \n",
    "    # Merge the averaged scores with concatenated sources\n",
    "    averaged_df = pd.merge(averaged_scores, concatenated_sources, on=[unit_col, year_col], how='left')\n",
    "    \n",
    "    return averaged_df\n",
    "\n",
    "\n",
    "# Drop duplicates based on 'Assessment_Unit_Name' and 'Year'\n",
    "dfbiotic = dfbiotic.drop_duplicates(subset=['Assessment_Unit_Name', 'Year', 'Biotic_Integrity_CSCI'])\n",
    "\n",
    "# Apply the function to dfbiotic\n",
    "averaged_biotic_df = average_biotic_scores(dfbiotic)\n",
    "\n",
    "# Apply the rating function to the averaged biotic integrity scores\n",
    "averaged_biotic_df['Biotic_Integrity_Rating'] = averaged_biotic_df['Biotic_Integrity_CSCI'].apply(categorize_csci)\n",
    "\n",
    "# Calculate the biotic score for each SEZ\n",
    "averaged_biotic_df['Biotic_Integrity_Score'] = averaged_biotic_df['Biotic_Integrity_Rating'].apply(score_indicator)\n",
    "\n",
    "averaged_biotic_df['Biotic_Integrity_Score']=averaged_biotic_df['Biotic_Integrity_Score'].astype(int)\n",
    "\n",
    "# Output the resulting DataFrame\n",
    "print(averaged_biotic_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------\n",
    "# Headcuts \n",
    "#------------------\n",
    "#Reorganize dfHeadcuts to drop small medium large headcut columns\n",
    "# Drop the columns 'small', 'medium', and 'large'\n",
    "dfheadcuts = dfheadcuts.drop(columns=['small', 'medium', 'large'])\n",
    "\n",
    "# Print the DataFrame to see the changes\n",
    "print(dfheadcuts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Prep SEZ Baseline Data for assessment unit...will need to rethink if acreage changes.. or just manually change in sde\n",
    "keep_columns = ['SHAPE', 'SEZ_ID', 'Feature_Type', 'SEZ_Type', 'Ownership_Primary', 'Ownership_Secondary', 'Ownership_Secondary_2', 'Ownership_Secondary_3', 'Acres', 'Comments']\n",
    "#dfSEZ is assessment unit information from SDE?\n",
    "dfSEZinfo=dfSEZ.loc[:,keep_columns].copy()\n",
    "\n",
    "dfSEZinfo['SEZ_ID']= dfSEZinfo['SEZ_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add year to data source so we can drop the year column later (Dont double run this)\n",
    "\n",
    "#Create Dictionary of Dataframes to adjust year to be in datashource column and not its own column\n",
    "yeartodatasource = {\n",
    "    'dfbanks': dfbanks,\n",
    "    'dfheadcuts': dfheadcuts,\n",
    "    'dfincision': dfincision,\n",
    "    'dfinvasive': dfinvasive\n",
    "}\n",
    "\n",
    "# Iterate over each DataFrame in meadowdata\n",
    "for name, df in yeartodatasource.items():\n",
    "    # Iterate over columns in the DataFrame\n",
    "    for col in df.columns:\n",
    "        # Check if the column name contains 'Data'\n",
    "        if 'Data_' in col:\n",
    "            # Add Year to the column if it contains 'Data'\n",
    "            df[col] = df[col] + ', ' + df['Year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Large Polygon and Small Polygon Data frames called meadow and riverine for now so we can assign the correct SEZ_ID\n",
    "#Same for meadow(large polygon) and riverine(small polygon) data drop these columns because not needed in final merge, will assign SEZ ID later\n",
    "columns_to_drop = {'Year', 'SEZ_ID', 'GlobalID', 'last_edited_user', 'created_date', 'OBJECTID', 'created_user', 'last_edited_date'}\n",
    "\n",
    "#Name dataframes so we can reference later\n",
    "largepolygondata= {'dfbanks': dfbanks, \n",
    "             'dfaveraged_biotic':averaged_biotic_df,\n",
    "                'dfconifer': dfconifer,\n",
    "                'dfditch': dfditch,\n",
    "                'dfinvasive': dfinvasive,\n",
    "                'dfhabitat': dfhabitat,\n",
    "                'dfvegetation': dfvegetation,\n",
    "                'dfincision': dfincision,\n",
    "                'dfheadcuts': dfheadcuts,\n",
    "                'dfAOP': dfAOP\n",
    "}\n",
    "\n",
    "\n",
    "#Staging Tables Riverine/ small polygons\n",
    "smallpolygondata = {'dfbanks': dfbanks, \n",
    "                'dfaveraged_biotic':averaged_biotic_df,\n",
    "                'dfconifer': dfconifer,\n",
    "                'dfditch': dfditch,\n",
    "                'dfinvasive': dfinvasive,\n",
    "                'dfhabitat': dfhabitat,\n",
    "                'dfvegetation': dfvegetation,\n",
    "                'dfincision': dfincision,\n",
    "                'dfheadcuts': dfheadcuts,\n",
    "                'dfAOP': dfAOP\n",
    "}\n",
    "\n",
    "#Get most recent year of data for each Assessment Unit NAme\n",
    "# Function to get the most recent year of data\n",
    "# Function to get the most recent year of data\n",
    "def get_most_recent_scores(df, groupfield):\n",
    "    return df.loc[df.groupby(groupfield)['Year'].idxmax()]\n",
    "\n",
    "#most_recent_small = get_most_recent_scores(smallpolygondata, 'Assessment_Unit_Name')\n",
    "#mosrecent_large = get_most_recent_scores(largepolygondata, 'Assessment_Unit_Name')\n",
    "\n",
    "# Function to drop unnecessary columns from DataFrames\n",
    "def drop_columns(df, columns_to_drop):\n",
    "    return df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "\n",
    "# Function to assign SEZ_ID to each DataFrame using the provided lookup dictionary\n",
    "def assign_sez_ids(df, sezid_dict):\n",
    "    df['SEZ_ID'] = df['Assessment_Unit_Name'].map(sezid_dict)\n",
    "    df = df.dropna(subset=['SEZ_ID'])\n",
    "    \n",
    "    # Use .loc to modify SEZ_ID safely\n",
    "    df.loc[:, 'SEZ_ID'] = df['SEZ_ID'].astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process data for large and small polygons\n",
    "def process_data(data_dict, sezid_dict, columns_to_drop):\n",
    "    processed_data = {}\n",
    "    for key, df in data_dict.items():\n",
    "        # Step 1: Get most recent scores\n",
    "        df_most_recent = get_most_recent_scores(df, 'Assessment_Unit_Name')\n",
    "        \n",
    "        # Step 2: Drop unnecessary columns\n",
    "        df_cleaned = drop_columns(df_most_recent, columns_to_drop)\n",
    "        \n",
    "        # Step 3: Assign SEZ_ID\n",
    "        df_with_sez_id = assign_sez_ids(df_cleaned, sezid_dict)\n",
    "        \n",
    "        # Store the processed DataFrame\n",
    "        processed_data[key] = df_with_sez_id\n",
    "    return processed_data\n",
    "\n",
    "# Process large polygon (meadow) and small polygon (riverine) data\n",
    "processed_largepolygon_data = process_data(largepolygondata, lookup_dict, columns_to_drop)\n",
    "processed_smallpolygon_data = process_data(smallpolygondata, lookup_riverine, columns_to_drop)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Create Final Dataframe for all SEZ_ID's\n",
    "# Combine small polygon  and large polygon  DataFrames into a single DataFrame\n",
    "#combined_df = pd.concat([smallpolygon_df, largepolygon_df], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to merge all DataFrames on multiple keys\n",
    "def merge_dataframes(data_dict, keys):\n",
    "    return reduce(lambda left, right: pd.merge(left, right, on=keys, how='outer'), data_dict.values())\n",
    "\n",
    "# Merge small polygon DataFrames\n",
    "smallpolygon_df = merge_dataframes(processed_smallpolygon_data, ['SEZ_ID', 'Assessment_Unit_Name'])\n",
    "\n",
    "# Merge large polygon DataFrames\n",
    "largepolygon_df = merge_dataframes(processed_largepolygon_data, ['SEZ_ID', 'Assessment_Unit_Name'])\n",
    "\n",
    "# Append smallpolygon_df to largepolygon_df\n",
    "final_combined_df = pd.concat([largepolygon_df, smallpolygon_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Print the final combined DataFrame to check\n",
    "print(\"Final Combined DataFrame:\")\n",
    "print(final_combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join SEZinfo to combined df to get a dataframe with more info about each Assessment Unit\n",
    "\n",
    "# Join SEZinfo to the combined_df using SEZ_ID\n",
    "final_df = pd.merge(final_combined_df, dfSEZinfo, on='SEZ_ID')\n",
    "\n",
    "#Assign Threshold Calculations which is the Threshold Year--> is just the most recent data within the past 4 years\n",
    "final_df['Threshold_Year'] = '2023'\n",
    "\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't USE This is for All SEZ Scores Only no othe rdata.. DONT USE unless need all _sez_Score table with just scores of Indicator and SEZ no floof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT USECreate RiverineIndicators list containing specific dataframes\n",
    "RiverineIndicators = ['Assessment_Unit_Name', 'AOP_Score', 'Bank_Stability_Score', 'Biotic_Integrity_Score', 'Habitat_Frag_Score', 'Incision_Score', 'Headcuts_Score']\n",
    "\n",
    "mergedriverine_df= mergedmeadow_df[RiverineIndicators]\n",
    "\n",
    "print(mergedriverine_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT USECalculate Scores for Each SEZ This will go into table All_SEZ_Scores\n",
    "\n",
    "#FOR MEADOWS\n",
    "\n",
    "\n",
    "#mergedmeadow_df['SEZ_ID']=mergedmeadow_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "# Identify columns that contain 'score' in their names\n",
    "score_columns = [col for col in mergedmeadow_df.columns if 'Score' in col]\n",
    "\n",
    "# Sum the values across identified score columns to get the final points for the SEZ\n",
    "mergedmeadow_df['Final_Total_Points'] = mergedmeadow_df[score_columns].sum(axis=1, skipna=True)\n",
    "\n",
    "#POints Possible per SEZ\n",
    "\n",
    "mergedmeadow_df['Final_Points_Possible'] = mergedmeadow_df[score_columns].notna().sum(axis=1)*12\n",
    "\n",
    "\n",
    "#ASsign Meadow SEZ_ID to each Assessment_Unit\n",
    "\n",
    "mergedmeadow_df['SEZ_ID']=mergedmeadow_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "#FOR QA purposes add SEZ_type so we can make sure lal data is available\n",
    "mergedmeadow_df['SEZ_Type'] = dfSEZ['SEZ_Type']\n",
    "#Would it be better to just add up how many indicators have a score? 12 x5?\n",
    "#mergedmeadow_df['Final_Points_Possible']= dfSEZ['Final_Points_Possible']\n",
    "\n",
    "# Merge based on 'SEZID'\n",
    "#mergedmeadow_df = pd.merge(mergedmeadow_df, dfSEZ[['SEZ_ID', 'Final_Points_Possible']], on='SEZ_ID', how='left')\n",
    "\n",
    "# Assign 'Final_Points_Possible' from dfSEZ to mergedmeadow_df\n",
    "#mergedmeadow_df['Final_Points_Possible'] = mergedmeadow_df['Final_Points_Possible']\n",
    "#or? just base of of how many indicators are not null for each row?\n",
    "#mergedmeadow_df['Final_Points_Possible2']= (12 x number of columns per row that say score have data?) \n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------\n",
    "# Calculate the final rating\n",
    "#------------------------------------\n",
    "mergedmeadow_df['Final_Percent'] = mergedmeadow_df['Final_Total_Points'] / mergedmeadow_df['Final_Points_Possible']\n",
    "\n",
    "mergedmeadow_df['Final_Rating']= mergedmeadow_df['Final_Percent'].apply(rate_SEZ)\n",
    "\n",
    "mergedmeadow_df['Final_Score']= mergedmeadow_df['Final_Rating'].apply(score_indicator)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT USECalculate Scores for Each SEZ This will go into table All_SEZ_Scores\n",
    "\n",
    "#FOR RIVERINE SEZ\n",
    "# Identify columns that contain 'score' in their names\n",
    "score_columns = [col for col in mergedriverine_df.columns if 'Score' in col]\n",
    "\n",
    "# Sum the values across identified score columns to get the final points for the SEZ\n",
    "mergedriverine_df['Final_Total_Points'] = mergedriverine_df[score_columns].sum(axis=1, skipna=True)\n",
    "\n",
    "#POints Possible per SEZ\n",
    "\n",
    "mergedriverine_df['Final_Points_Possible'] = mergedriverine_df[score_columns].notna().sum(axis=1)*12\n",
    "\n",
    "\n",
    "#ASsigndriverine SEZ_ID to each Assessment_Unit\n",
    "\n",
    "mergedriverine_df['SEZ_ID']=mergedriverine_df['Assessment_Unit_Name'].map(lookup_riverine)\n",
    "\n",
    "#FOR QA purposes add SEZ_type so we can make sure lal data is available\n",
    "mergedriverine_df['SEZ_Type'] = dfSEZ['SEZ_Type']\n",
    "\n",
    "#--------------------------------------\n",
    "# Calculate the final rating\n",
    "#------------------------------------\n",
    "mergedriverine_df['Final_Percent'] = mergedriverine_df['Final_Total_Points'] / mergedriverine_df['Final_Points_Possible']\n",
    "\n",
    "mergedriverine_df['Final_Rating']= mergedriverine_df['Final_Percent'].apply(rate_SEZ)\n",
    "\n",
    "mergedriverine_df['Final_Score']= mergedriverine_df['Final_Rating'].apply(score_indicator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT USEMerge Riverine and Meadow dataframes for a final dataframe\n",
    "\n",
    "# Concatenate DataFrames doesn't work-try merging--Do we want to make an All_sez_scores\n",
    "both_df = pd.concat([mergedriverine_df, mergedmeadow_df], axis=0, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTS WORKINNG... WILL NEED THIS for FINAL TABLEsezsurveytable grab comments to add to dataframe--MAYBE JUST DO THIS MANUALLY\n",
    "#Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "sezsurveyfields = ['Assessment_Unit_Name', 'Comments', 'Survey_Date']\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "# Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(sezsurveytable, sezsurveyfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "sezsurvey_df= pd.DataFrame(data, columns=sezsurveyfields)\n",
    "\n",
    "sezsurvey_df['Year'] = sezsurvey_df['Survey_Date'].dt.year\n",
    "print(sezsurvey_df)\n",
    "#add comments to dataframe\n",
    "# Create a dictionary to map Assessment_Unit_Name to Comments\n",
    "comments_map = dict(zip(sezsurvey_df['Assessment_Unit_Name'], sezsurvey_df['Comments']))\n",
    "\n",
    "# Add the Comments column to both_df using the map function\n",
    "both_df['Comments'] = both_df['Assessment_Unit_Name'].map(comments_map)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#DONT USE\n",
    "# both_df['Year']= '2024'\n",
    "\n",
    "both_df = both_df.dropna(subset='SEZ_ID')\n",
    "\n",
    "#----------------------------------------------------------------#\n",
    "#Prep and post ending dataframe to All Scores table in SEZ_Data.GDB\n",
    "#----------------------------------------------------------------#\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Year',\n",
    "    'Comments': 'Comments',\n",
    "    'SEZ_Type': 'SEZ_Type',\n",
    "    'Final_Rating': 'Final_Rating',\n",
    "    'Final_Percent': 'Final_Percent',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'Final_Points_Possible': 'Final_Points_Possible',\n",
    "    'Final_Total_Points': 'Final_Total_Points'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "readydf = both_df.rename(columns=field_mapping).drop(columns=[col for col in both_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Assuming ready_df is the DataFrame you want to write to CSV\n",
    "readydf.to_csv('ready_df.csv', index=False)\n",
    "\n",
    "allscoresdf=readydf\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "#data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "#field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(allsezscores, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SDE.SEZ Assessment_Unit final table with all info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DONT USEProblem with Riverine Data and this code--August 1, 2024\n",
    "##Clean Data, Assign SEZ's based on polygon size, get most recent\n",
    "#Take df's and create a riverine and meadow df using small polygon size and large polygon size\n",
    "\n",
    "\n",
    "#### Function to Get most recent year of data from each DataFrame\n",
    "def get_most_recent_scores(df, groupfield):\n",
    "    df = df.loc[df.groupby(groupfield)['Year'].idxmax()]\n",
    "    return df\n",
    "\n",
    "#### Function to merge DataFrames and assign SEZ IDs\n",
    "def merge_and_assign_sez_ids(cleaned_data, lookup_dict):\n",
    "    # Merge all DataFrames on 'Assessment_Unit_Name' using reduce\n",
    "    merged_df = reduce(lambda left, right: pd.merge(left, right, on='Assessment_Unit_Name', how='outer'), cleaned_data.values())\n",
    "    \n",
    "    # Assign SEZ_ID based on 'Assessment_Unit_Name' using lookup_dict\n",
    "    merged_df['SEZ_ID'] = merged_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "    \n",
    "    # Drop rows with missing SEZ_ID\n",
    "    merged_df = merged_df.dropna(subset=['SEZ_ID'])\n",
    "    \n",
    "    merged_df['SEZ_ID'] = merged_df['SEZ_ID'].astype(int)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "#### Main function to process data frames, get most recent data, clean, and merge\n",
    "def get_most_recent_and_clean(meadowdata, riverinedata, lookup_dict, lookup_riverine, columns_to_drop):\n",
    "    most_recent_data_meadow = {}\n",
    "    cleaned_data_meadow = {}\n",
    "    \n",
    "    most_recent_data_riverine = {}\n",
    "    cleaned_data_riverine = {}\n",
    "    \n",
    "    # Process meadowdata\n",
    "    for key, df in meadowdata.items():\n",
    "        processed_df = get_most_recent_scores(df, 'Assessment_Unit_Name')\n",
    "        most_recent_data_meadow[key] = processed_df\n",
    "        \n",
    "        # Drop specified columns and remove duplicate rows\n",
    "        df_cleaned = processed_df.drop(columns=[col for col in columns_to_drop if col in processed_df.columns])\n",
    "        df_cleaned = df_cleaned.drop_duplicates()\n",
    "        cleaned_data_meadow[key] = df_cleaned\n",
    "    \n",
    "    # Process riverinedata\n",
    "    for key, df in riverinedata.items():\n",
    "        processed_df = get_most_recent_scores(df, 'Assessment_Unit_Name')\n",
    "        most_recent_data_riverine[key] = processed_df\n",
    "        \n",
    "        # Drop specified columns and remove duplicate rows\n",
    "        df_cleaned = processed_df.drop(columns=[col for col in columns_to_drop if col in processed_df.columns])\n",
    "        df_cleaned = df_cleaned.drop_duplicates()\n",
    "        cleaned_data_riverine[key] = df_cleaned\n",
    "    \n",
    "    # Merge and assign SEZ IDs for meadowdata\n",
    "    merged_meadow_df = merge_and_assign_sez_ids(cleaned_data_meadow, lookup_dict)\n",
    "    \n",
    "    # Merge and assign SEZ IDs for riverinedata\n",
    "    merged_riverine_df = merge_and_assign_sez_ids(cleaned_data_riverine, lookup_riverine)\n",
    "    \n",
    "    # Combine merged_meadow_df and merged_riverine_df\n",
    "    combined_df = pd.concat([merged_meadow_df, merged_riverine_df], axis=0, join='outer')\n",
    "\n",
    "    return {\n",
    "        'merged_meadow_df': merged_meadow_df,\n",
    "        'merged_riverine_df': merged_riverine_df,\n",
    "        'combined_df': combined_df,\n",
    "        'cleaned_data_meadow': cleaned_data_meadow,\n",
    "        'cleaned_data_riverine': cleaned_data_riverine,\n",
    "        'most_recent_data_meadow': most_recent_data_meadow,\n",
    "        'most_recent_data_riverine': most_recent_data_riverine\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DONT USESame for meadow(large polygon) and riverine(small polygon) data drop these columns because not needed in final merge, will assign SEZ ID later\n",
    "columns_to_drop = {'Year', 'SEZ_ID', 'GlobalID', 'last_edited_user', 'created_date', 'OBJECTID', 'created_user', 'last_edited_date'}\n",
    "\n",
    "#Name dataframes so we can reference later meadow data is large polygone\n",
    "meadowdata= {'dfbanks': dfbanks, \n",
    "             'dfaveraged_biotic':averaged_biotic_df,\n",
    "                'dfconifer': dfconifer,\n",
    "                'dfditch': dfditch,\n",
    "                'dfinvasive': dfinvasive,\n",
    "                'dfhabitat': dfhabitat,\n",
    "                'dfvegetation': dfvegetation,\n",
    "                'dfincision': dfincision,\n",
    "                'dfheadcuts': dfheadcuts,\n",
    "                'dfAOP': dfAOP\n",
    "}\n",
    "\n",
    "\n",
    "#Staging Tables Riverine/ small polygons\n",
    "riverinedata = {'dfheadcuts': dfbanks,\n",
    "                'dfbiotic': averaged_biotic_df,\n",
    "                'dfhabitat': dfhabitat,\n",
    "                'dfincision': dfincision,\n",
    "                'dfheadcuts': dfheadcuts,\n",
    "                'dfAOP': dfAOP\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DONT USE Call the function with all the required arguments\n",
    "result = get_most_recent_and_clean(meadowdata, riverinedata, lookup_dict, lookup_riverine, columns_to_drop)\n",
    "\n",
    "##### Access the results\n",
    "merged_meadow_df = result['merged_meadow_df']\n",
    "merged_riverine_df = result['merged_riverine_df']\n",
    "combined_df=result['combined_df']\n",
    "cleaned_data_meadow = result['cleaned_data_meadow']\n",
    "cleaned_data_riverine = result['cleaned_data_riverine']\n",
    "most_recent_data_meadow = result['most_recent_data_meadow']\n",
    "most_recent_data_riverine = result['most_recent_data_riverine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join SEZinfo to combined df to get a dataframe with more info about each Assessment Unit\n",
    "\n",
    "#### Join SEZinfo to the combined_df using SEZ_ID\n",
    "final_df = pd.merge(final_combined_df, dfSEZinfo, on='SEZ_ID')\n",
    "\n",
    "#Assign Threshold Calculations which is the Threshold Year--> is just the most recent data within the past 4 years\n",
    "final_df['Threshold_Year'] = '2023'\n",
    "\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in 2019 Threshold Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bring in Last year threshold scores for any indicators thataren't in our raw data- some bank stability came from our stream surveys but aren't in the data\n",
    "#Fill in any indicators that have missing data with data from 2019\n",
    "# Field Mapping so 2019 threshol data renamed so it can be joined to new data\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Threshold_Year': 'Threshold Year',\n",
    "    'Comments': 'Comments',\n",
    "    'SEZ_Type': 'SEZ_Type',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'Acres': 'Acres',\n",
    "    'AquaticOrganismPassage_Barriers': 'AOP_BarriersPerMile',\n",
    "    'AquaticOrganismPassage_NumberOf': 'AOP_NumberofBarriers',\n",
    "    'AquaticOrganismPassage_Score': 'AOP_Score',\n",
    "    'AquaticOrganismPassage_Rating': 'AOP_Rating',\n",
    "    'AquaticOrganismPassage_StreamMi': 'AOP_StreamMiles',\n",
    "    'AquaticOrganismPassage_DataSour': 'AOP_DataSource',\n",
    "    'Bank_Stability_Data_Source': 'Bank_Stability_Data_Source',\n",
    "    'Bank_Stability_Percent_Unstable': 'Bank_Stability_Percent_Unstable',\n",
    "    'Bank_Stability_Rating': 'Bank_Stability_Rating',\n",
    "    'Bank_Stability_Score': 'Bank_Stability_Score',\n",
    "    'Biotic_Integrity_Rating': 'Biotic_Integrity_Rating',\n",
    "    'Biotic_Integrity_CSCI': 'Biotic_Integrity_CSCI',\n",
    "    'Biotic_Integrity_Data_Source': 'Biotic_Integrity_Data_Source',\n",
    "    'Biotic_Integrity_Score': 'Biotic_Integrity_Score',\n",
    "    'Conifer_Encroachment_Percent_En': 'Conifer_Percent_Encroached',\n",
    "    'Conifer_Encroachment_Data_Sourc': 'Conifer_Encroachment_Data_Sourc',\n",
    "    'Conifer_Encroachment_Rating': 'Conifer_Encroachment_Rating',\n",
    "    'Conifer_Encroachment_Score': 'Conifer_Encroachment_Score',\n",
    "    'ConiferEncroachment_Comments': 'ConiferEncroachment_Comments',\n",
    "    'Ditches_Data_Source': 'Ditches_Data_Source',\n",
    "    'Ditches_Length': 'Ditches_Length',\n",
    "    'Ditches_Meadow_Length': 'Ditches_Meadow_Length',\n",
    "    'Ditches_Percent': 'Ditches_Percent',\n",
    "    'Ditches_Rating': 'Ditches_Rating',\n",
    "    'Ditches_Score': 'Ditches_Score',\n",
    "    'Feature_Type': 'Feature_Type',\n",
    "    'Habitat_Fragmentation_Data_Sour': 'Habitat_Frag_Data_Source',\n",
    "    'Habitat_Fragmentation_Imperviou': 'Habitat_Frag_Impervious_Acres',\n",
    "    'Habitat_Fragmentation_Percent_I': 'Habitat_Frag_Percent_Impervious',\n",
    "    'Habitat_Fragmentation_Rating': 'Habitat_Frag_Rating',\n",
    "    'Habitat_Fragmentation_Score': 'Habitat_Frag_Score',\n",
    "    'Headcuts_Data_Source': 'Headcuts_Data_Source',\n",
    "    'Headcuts_Number_of_Headcuts':'Number_of_Headcuts',\n",
    "    'Headcuts_Rating': 'Headcuts_Rating',\n",
    "    'Headcuts_Score': 'Headcuts_Score',\n",
    "    'Incision_Data_Source': 'Incision_Data_Source',\n",
    "    'Incision_Rating': 'Incision_Rating',\n",
    "    'Incision_Score': 'Incision_Score',\n",
    "    'Incision_Ratio': 'Incision_Ratio',\n",
    "    'Invasive_Percent_Cover': 'Invasive_Percent_Cover',\n",
    "    'Invasive_Rating': 'Invasives_Rating',\n",
    "    'Invasives_Data_Source': 'Invasives_Data_Source',\n",
    "    'Invasives_Number_of_Invasives': 'Invasives_Number_of_Invasives',\n",
    "    'Invasives_Plant_Types': 'Invasives_Plant_Types',\n",
    "    'Invasives_Scores': 'Invasives_Scores',\n",
    "    'NDVI_ID': 'NDVI_ID',\n",
    "    'Ownership_Primary': 'Ownership_Primary',\n",
    "    'Ownership_Secondary': 'Ownership_Secondary',\n",
    "    'Ownership_Secondary_2': 'Ownership_Secondary_2',\n",
    "    'Ownership_Secondary_3': 'Ownership_Secondary_3',\n",
    "    'VegetationVigor_DataSource': 'VegetationVigor_DataSource',\n",
    "    'VegetationVigor_Rating': 'VegetationVigor_Rating',\n",
    "    'VegetationVigor_Raw': 'VegetationVigor_Raw',\n",
    "    'VegetationVigor_Score': 'VegetationVigor_Score'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "dfSEZprep = dfSEZ.rename(columns=field_mapping)\n",
    "# Filter out columns in dfSEZprep that are not in final_df\n",
    "dfSEZprep = dfSEZprep[[col for col in final_df.columns if col in dfSEZprep.columns]]\n",
    "# # Ensure dfSEZprep has the same columns as final_df\n",
    "#dfSEZprep = dfSEZprep[final_df.columns]\n",
    "\n",
    "# Set SEZ_ID as index for both dataframes\n",
    "final_df.set_index('SEZ_ID', inplace=True)\n",
    "dfSEZprep.set_index('SEZ_ID', inplace=True)\n",
    "\n",
    "# Update missing values in final_df using values from dfSEZprep\n",
    "#final_df.update(dfSEZprep)\n",
    "#final_df = final_df.fillna(dfSEZprep)\n",
    "# Reset the index to get SEZ_ID back as a column\n",
    "#final_df.reset_index(inplace=True)\n",
    "\n",
    " # Combine DataFrames to fill missing values\n",
    "final_df = final_df.combine_first(dfSEZprep)\n",
    "\n",
    "# Reset the index to get SEZ_ID back as a column\n",
    "final_df.reset_index(inplace=True)\n",
    "\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify column names\n",
    "print(\"final_df columns:\", final_df.columns)\n",
    "print(\"dfSEZprep columns:\", dfSEZprep.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data type of the SEZ_ID column in combined_df\n",
    "print(\"Data type of SEZ_ID in combined_df:\", combined_df['SEZ_ID'].dtype)\n",
    "\n",
    "# Check the data type of the SEZ_ID column in dfSEZinfo\n",
    "print(\"Data type of SEZ_ID in dfSEZinfo:\", dfSEZinfo['SEZ_ID'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----\n",
    "# #QAOnly\n",
    "#------------\n",
    "score_columns = [col for col in final_df.columns if 'Score' in col]\n",
    "print(\"score columns in final_df:\", final_df[score_columns].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------\n",
    "#QAOnly\n",
    "#------------\n",
    "# #RUN FOR QA so you can see the resulting dataframe/scores/formatting etc.\n",
    "#Will need to rerun to fix dropping the shape column\n",
    "# List of columns to export (excluding 'geometry' or 'Shape')\n",
    "columns_to_export = [col for col in final_df.columns if col != 'SHAPE']\n",
    "final_df[columns_to_export].to_csv('ready_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Scores based on SEZ_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Score based on SEZ_Type  #Future- make it so any indicator that isn't in the lists below for SEZ Type says NA\n",
    "#Use SEZ_Type to select only needed indicators for SEZ Type\n",
    "ReadytoScore= final_df\n",
    "# Define the score columns needed for each SEZ Type\n",
    "RiverineIndicators = ['AOP_Score', 'Bank_Stability_Score', 'Biotic_Integrity_Score', 'Habitat_Frag_Score', 'Incision_Score', 'Headcuts_Score']\n",
    "ForestRiverineIndicators =['AOP_Score', 'Bank_Stability_Score', 'Biotic_Integrity_Score', 'Ditches_Score', 'Habitat_Frag_Score', 'Incision_Score', 'Headcuts_Score']\n",
    "NonChanneledIndicators=['Invasives_Scores', 'Conifer_Encroachment_Score', 'Ditches_Score', 'Habitat_Frag_Score', 'Headcuts_Score', 'VegetationVigor_Score']\n",
    "ChanneledIndicators= ['AOP_Score', 'Bank_Stability_Score', 'Biotic_Integrity_Score', 'Invasives_Scores', 'Conifer_Encroachment_Score', 'Ditches_Score', 'Habitat_Frag_Score', 'Incision_Score', 'Headcuts_Score', 'VegetationVigor_Score']\n",
    "ForestIndicators= ['Bank_Stability_Score', 'Ditches_Score', 'Habitat_Frag_Score', 'Headcuts_Score']\n",
    "\n",
    "# Function to get the score columns based on SEZ_Type\n",
    "def get_score_columns(sez_type):\n",
    "    if sez_type == 'Riverine (Perennial)':\n",
    "        return RiverineIndicators\n",
    "    elif sez_type == 'Riverine (Perennial) + Forested':\n",
    "        return ForestRiverineIndicators\n",
    "    elif sez_type == 'Non-Channeled Meadow':\n",
    "        return NonChanneledIndicators\n",
    "    elif sez_type == 'Channeled Meadow':\n",
    "        return ChanneledIndicators\n",
    "    elif sez_type == 'Forested':\n",
    "        return ForestIndicators\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Apply the appropriate score columns based on SEZ_Type\n",
    "final_df['Score_Columns'] = final_df['SEZ_Type'].apply(get_score_columns)\n",
    "# Function to calculate the final points and points possible\n",
    "\n",
    "# Function to calculate the final points and points possible\n",
    "def calculate_scores(row):\n",
    "    score_columns = row['Score_Columns']\n",
    "    if not score_columns:\n",
    "        return pd.Series([None, None])\n",
    "    total_points = row[score_columns].sum(skipna=True)\n",
    "    points_possible = row[score_columns].notna().sum() * 12\n",
    "    return pd.Series([total_points, points_possible])\n",
    "\n",
    "# Apply the score calculation to each row\n",
    "final_df[['Final_Total_Points', 'Final_Points_Possible']] = final_df.apply(calculate_scores, axis=1)\n",
    "\n",
    "# Calculate the final percent\n",
    "final_df['Final_Percent'] = final_df['Final_Total_Points'] / final_df['Final_Points_Possible']\n",
    "\n",
    "# Calculate the final rating and score\n",
    "final_df['Final_Rating'] = final_df['Final_Percent'].apply(rate_SEZ)\n",
    "final_df['Final_Score'] = final_df['Final_Rating'].apply(score_indicator)\n",
    "\n",
    "# Drop the temporary 'Score_Columns' column\n",
    "final_df = final_df.drop(columns=['Score_Columns'])\n",
    "\n",
    "# Convert SEZ_ID to string\n",
    "#final_df['SEZ_ID'] = final_df['SEZ_ID'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##RUN FOR QA so you can see the resulting dataframe/scores/formatting etc.\n",
    "#Will need to rerun to fix dropping the shape column\n",
    "# List of columns to export (excluding 'geometry' or 'Shape')\n",
    "#columns_to_export = [col for col in final_df.columns if col != 'SHAPE']\n",
    "#final_df[columns_to_export].to_csv('ready_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final set up\n",
    "\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'SHAPE': 'SHAPE',\n",
    "    'Threshold_Year': 'Threshold Year',\n",
    "    'Comments': 'Comments',\n",
    "    'SEZ_Type': 'SEZ_Type',\n",
    "    'Final_Rating': 'Final_Rating',\n",
    "    'Final_Percent': 'Final_Percent',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'Final_Points_Possible': 'Final_Points_Possible',\n",
    "    'Final_Total_Points': 'Final_Total_Points',\n",
    "    'Acres': 'Acres',\n",
    "    'AOP_BarriersPerMile':'AquaticOrganismPassage_Barriers',\n",
    "    'AOP_NumberofBarriers': 'AquaticOrganismPassage_NumberOf',\n",
    "    'AOP_Score': 'AquaticOrganismPassage_Score',\n",
    "    'AOP_Rating': 'AquaticOrganismPassage_Rating',\n",
    "    'AOP_StreamMiles': 'AquaticOrganismPassage_StreamMiles',\n",
    "    'AOP_DataSource': 'AquaticOrganismPassage_DataSour',\n",
    "    'Bank_Stability_Data_Source': 'Bank_Stability_Data_Source',\n",
    "    'Bank_Stability_Percent_Unstable': 'Bank_Stability_Percent_Unstable',\n",
    "    'Bank_Stability_Rating': 'Bank_Stability_Rating',\n",
    "    'Bank_Stability_Score': 'Bank_Stability_Score',\n",
    "    'Biotic_Integrity_Rating': 'Biotic_Integrity_Rating',\n",
    "    'Biotic_Integrity_CSCI': 'Biotic_Integrity_CSCI',\n",
    "    'Biotic_Integrity_Data_Source': 'Biotic_Integrity_Data_Source',\n",
    "    'Biotic_Integrity_Score': 'Biotic_Integrity_Score',\n",
    "    'Comments': 'Comments',\n",
    "    'Conifer_Percent_Encroached': 'Conifer_Encroachment_Percent_En',\n",
    "    'Conifer_Encroachment_Data_Sourc': 'Conifer_Encroachment_Data_Sourc',\n",
    "    'Conifer_Encroachment_Rating': 'Conifer_Encroachment_Rating',\n",
    "    'Conifer_Encroachment_Score': 'Conifer_Encroachment_Score',\n",
    "    'ConiferEncroachment_Comments': 'Conifer_Encroachment_Comments',\n",
    "    'Ditches_Data_Source': 'Ditches_Data_Source',\n",
    "    'Ditches_Length': 'Ditches_Length',\n",
    "    'Ditches_Meadow_Length': 'Ditches_Meadow_Length',\n",
    "    'Ditches_Percent': 'Ditches_Percent',\n",
    "    'Ditches_Rating': 'Ditches_Rating',\n",
    "    'Ditches_Score': 'Ditches_Score',\n",
    "    'Feature_Type': 'Feature_Type',\n",
    "    'Habitat_Frag_Data_Source': 'Habitat_Fragmentation_Data_Sour',\n",
    "    'Habitat_Frag_Impervious_Acres': 'Habitat_Fragmentation_Imperviou',\n",
    "    'Habitat_Frag_Percent_Impervious': 'Habitat_Fragmentation_Percent_I',\n",
    "    'Habitat_Frag_Rating': 'Habitat_Fragmentation_Rating',\n",
    "    'Habitat_Frag_Score': 'Habitat_Fragmentation_Score',\n",
    "    'Headcuts_Data_Source': 'Headcuts_Data_Source',\n",
    "    'Number_of_Headcuts': 'Headcuts_Number_of_Headcuts',\n",
    "    'Headcuts_Rating': 'Headcuts_Rating',\n",
    "    'Headcuts_Score': 'Headcuts_Score',\n",
    "    'Incision_Data_Source': 'Incision_Data_Source',\n",
    "    'Incision_Rating': 'Incision_Rating',\n",
    "    'Incision_Score': 'Incision_Score',\n",
    "    'Incision_Ratio': 'Incision_Ratio',\n",
    "    'Invasive_Percent_Cover': 'Invasive_Percent_Cover',\n",
    "    'Invasives_Rating': 'Invasive_Rating',\n",
    "    'Invasives_Data_Source': 'Invasives_Data_Source',\n",
    "    'Invasives_Number_of_Invasives': 'Invasives_Number_of_Invasives',\n",
    "    'Invasives_Plant_Types': 'Invasives_Plant_Types',\n",
    "    'Invasives_Scores': 'Invasives_Scores',\n",
    "    'NDVI_ID': 'NDVI_ID',\n",
    "    'Ownership_Primary': 'Ownership_Primary',\n",
    "    'Ownership_Secondary': 'Ownership_Secondary',\n",
    "    'Ownership_Secondary_2': 'Ownership_Secondary_2',\n",
    "    'Ownership_Secondary_3': 'Ownership_Secondary_3',\n",
    "    'VegetationVigor_DataSource': 'VegetationVigor_DataSource',\n",
    "    'VegetationVigor_Rating': 'VegetationVigor_Rating',\n",
    "    'VegetationVigor_Raw': 'VegetationVigor_Raw',\n",
    "    'VegetationVigor_Score': 'VegetationVigor_Score'\n",
    "}\n",
    "# Rename fields based on field mappings\n",
    "SEZscores_readydf = final_df.rename(columns=field_mapping).drop(columns=[col for col in final_df.columns if col not in field_mapping])\n",
    "\n",
    "\n",
    "print(SEZscores_readydf)\n",
    "#----------------------------------------------------\n",
    "#Post results to CSV in gis/projects/Researchanalysis/SEZ for further QA\n",
    "#----------------------------------------------------\n",
    " \n",
    "#export all columns but shape\n",
    "columns_to_export = [col for col in SEZscores_readydf.columns if col != 'SHAPE']\n",
    "#Store csv on F drive for QA/add comments manually on F drive and to change up comments later based on SEZ's that scores changed\n",
    "#final_results = \"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Threshold24_SEZScores.csv\"\n",
    "\n",
    "# Write to excel\n",
    "#SEZscores_readydf[columns_to_export].to_excel(final_results, index=False)\n",
    "\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "#data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "#field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(allsezscores, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates a csv of score changes and all information in row \n",
    "\n",
    "#Before Final comparison do more QA\n",
    "    # Check to make sure there are 641 ASsessment Units\n",
    "    #Check Final Points and make sure there are no really low numbers could be missing data\n",
    "    #Check all staging tables for completeness- There are ratings A-D, check on null data for that SEZ\n",
    "    #other QA methods can be added to this list\n",
    "\n",
    "#Check to see which SEZ Scores Changed\n",
    "Threshold23sezdata= SEZscores_readydf\n",
    "Threshold19sezdata = dfSEZ\n",
    "\n",
    "# Merge the datasets on Assessment_Unit_Name and SEZ_ID\n",
    "merged_df = pd.merge(\n",
    "    Threshold23sezdata[['Assessment_Unit_Name', 'SEZ_ID', 'Final_Rating', 'SEZ_Type']],\n",
    "    Threshold19sezdata[['Assessment_Unit_Name', 'SEZ_ID', 'Final_Rating', 'SEZ_Type']],\n",
    "    on=['Assessment_Unit_Name', 'SEZ_ID'],\n",
    "    suffixes=('_2023', '_2019')\n",
    ")\n",
    "\n",
    "# Identify Assessment Unit Names with a change in Final Rating\n",
    "changed_scores_df = merged_df[merged_df['Final_Rating_2023'] != merged_df['Final_Rating_2019']]\n",
    "\n",
    "# List of Assessment Unit Names with a change in score\n",
    "changed_assessment_units = changed_scores_df[['Assessment_Unit_Name', 'SEZ_ID', 'Final_Rating_2023', 'Final_Rating_2019']]\n",
    "\n",
    "# Output the DataFrame with changed scores\n",
    "print(changed_scores_df)\n",
    "\n",
    "# Output the list of Assessment Unit Names with a change in score\n",
    "print(changed_assessment_units)\n",
    "\n",
    "#Post results to CSV in gis/projects/Researchanalysis/SEZ for further QA\n",
    "columns_to_export = [col for col in changed_assessment_units.columns if col != 'SHAPE']\n",
    "#Store csv on F drive for QA/add comments manually on F drive and to change up comments later based on SEZ's that scores changed\n",
    "#Changed_Scores_List = \"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Threshold24_SEZScoreChanges.csv\"\n",
    "# Assuming ready_df is the DataFrame you want to write to CSV\n",
    "#changed_scores_df.to_csv(Changed_Scores_List, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Otion for different CSV Look at only changed scores so we can update comments and make sure data is good, need to run this to get long format csv\n",
    "# List of all columns with 'Rating' in their name for both datasets\n",
    "rating_columns_2023 = [col for col in Threshold23sezdata.columns if 'Rating' in col]\n",
    "rating_columns_2019 = [col for col in Threshold19sezdata.columns if 'Rating' in col]\n",
    "\n",
    "# Ensure both DataFrames have the same Assessment_Unit_Name and SEZ_ID columns\n",
    "Threshold23sezdata = Threshold23sezdata[['Assessment_Unit_Name', 'SEZ_ID'] + rating_columns_2023]\n",
    "Threshold19sezdata = Threshold19sezdata[['Assessment_Unit_Name', 'SEZ_ID'] + rating_columns_2019]\n",
    "\n",
    "# Merge the datasets on Assessment_Unit_Name and SEZ_ID, for units that have changed scores\n",
    "final_merged_df = pd.merge(\n",
    "    Threshold23sezdata,\n",
    "    Threshold19sezdata,\n",
    "    on=['Assessment_Unit_Name', 'SEZ_ID'],\n",
    "    suffixes=('_2023', '_2019')\n",
    ")\n",
    "\n",
    "# Filter the final merged DataFrame to only include units that changed scores\n",
    "final_changed_scores_df = final_merged_df[final_merged_df['Assessment_Unit_Name'].isin(changed_assessment_units['Assessment_Unit_Name'])]\n",
    "\n",
    "# Print the final DataFrame with all rating columns and changed scores\n",
    "print(final_changed_scores_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "#final_changed_scores_df.to_csv(\"F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\Threshold24_SEZScoreChanges_indicators.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEST QA csv.. shows indicators scores and final score changes by year of threshold\n",
    "#futureadd in final points and points possible and comments\n",
    "# Used changed changed_assessment_units to get Rating scores from df's \n",
    "#Create CSV with assessment units that have changed , all indicator scores and final rating by year and SEZID\n",
    "\n",
    "# List of all columns with 'Rating' in their name for both datasets\n",
    "rating_columns_2023 = [col for col in Threshold23sezdata.columns if 'Rating' in col]\n",
    "rating_columns_2019 = [col for col in Threshold19sezdata.columns if 'Rating' in col]\n",
    "\n",
    "# Ensure both DataFrames have the same Assessment_Unit_Name and SEZ_ID columns\n",
    "Threshold23sezdata = Threshold23sezdata[['Assessment_Unit_Name', 'SEZ_ID'] + rating_columns_2023]\n",
    "Threshold19sezdata = Threshold19sezdata[['Assessment_Unit_Name', 'SEZ_ID'] + rating_columns_2019]\n",
    "\n",
    "# Merge changed_assessment_units with Threshold23sezdata and Threshold19sezdata to get ratings for changed units\n",
    "merged_23 = pd.merge(changed_assessment_units, Threshold23sezdata, on=['Assessment_Unit_Name', 'SEZ_ID'])\n",
    "merged_19 = pd.merge(changed_assessment_units, Threshold19sezdata, on=['Assessment_Unit_Name', 'SEZ_ID'])\n",
    "\n",
    "# Extract columns with 'Rating' for both datasets\n",
    "rating_columns_2023 = [col for col in merged_23.columns if 'Rating' in col]\n",
    "rating_columns_2019 = [col for col in merged_19.columns if 'Rating' in col]\n",
    "\n",
    "# Filtered DataFrames with only relevant columns\n",
    "filtered_23 = merged_23[['Assessment_Unit_Name', 'SEZ_ID'] + rating_columns_2023]\n",
    "filtered_19 = merged_19[['Assessment_Unit_Name', 'SEZ_ID'] + rating_columns_2019]\n",
    "\n",
    "# Add a 'Year' column to each DataFrame\n",
    "filtered_23['Year'] = '2023'\n",
    "filtered_19['Year'] = '2019'\n",
    "\n",
    "# Concatenate filtered_23 and filtered_19 DataFrames\n",
    "allratings = pd.concat([filtered_23, filtered_19], ignore_index=True)\n",
    "\n",
    "\n",
    "#Function to map SEZ_Type based on Assessment_Unit_Name\n",
    "def get_sez_type(row):\n",
    "    return lookup_all.get(row['SEZ_ID'], {}).get('SEZ_Type', None)\n",
    "\n",
    "# Apply the function to add SEZ_Type column\n",
    "allratings['SEZ_Type'] = allratings.apply(get_sez_type, axis=1)\n",
    "\n",
    "# Apply the function to add SEZ_Type column\n",
    "allratings['SEZ_Type'] = allratings.apply(get_sez_type, axis=1)\n",
    "\n",
    "# Drop the specified columns from merged_df\n",
    "columns_to_drop = ['Final_Rating_2019', 'Final_Rating_2023']\n",
    "allratings.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Save the final long format DataFrame to a CSV file\n",
    "#allratings.to_csv(\"F:\\\\GIS\\\\PROJECTS\\\\ResearchAnalysis\\\\SEZ\\\\allchangedscores_longformat.csv\", index=False)\n",
    "\n",
    "allratings.to_csv(\"C:\\\\Users\\\\snewsome\\\\Documents\\\\SEZallchangedscores_longformat.csv\", index=False)\n",
    "\n",
    "# Note to SELF - when group editing csv and fixing comments for changed scores this must be in an excel and is easiest if put online into google office sheets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Final_Rating' column is in both DataFrames\n",
    "if 'Final_Rating' not in Threshold23sezdata.columns or 'Final_Rating' not in Threshold19sezdata.columns:\n",
    "    raise ValueError(\"Both DataFrames must contain 'Final_Rating' column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QA only\n",
    "# DONT USE unless want to use for QA Make a column for QA for SEZ_Type/final points possible\n",
    "def categorize_finalpoints(SEZ_Type):\n",
    "     if pd.isna(SEZ_Type):\n",
    "        return np.nan\n",
    "     elif   SEZ_Type == 'Non-Channeled Meadow':\n",
    "        return '72'\n",
    "     elif SEZ_Type == 'Channeled Meadow':\n",
    "        return '120'\n",
    "     elif SEZ_Type == 'Riverine (Perennial)':\n",
    "        return 'C'\n",
    "     elif SEZ_Type == 'Forested':\n",
    "        return '48'\n",
    "     elif SEZ_Type == 'Riverine (Perennial) + Forested':\n",
    "        return '96'\n",
    "     \n",
    "final_df['TESTFinalPointsPossible']= final_df['SEZ_Type'].apply(categorize_finalpoints)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Basin wide SEZ Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023 data\n",
    "#or import from rest service... going to use dataframe right no\n",
    "\n",
    "BasinwideScore= dfSEZ[['Acres', 'SEZ_ID','Assessment_Unit_Name','Threshold Year', 'Final_Percent', 'SEZ_Type']]\n",
    "\n",
    "# Calculate SEZ_Quality\n",
    "BasinwideScore['SEZ_Quality'] = BasinwideScore['Final_Percent'] * 100\n",
    "\n",
    "# Calculate SEZ_Condition Index\n",
    "BasinwideScore[:, 'SEZ_Condition_Index'] = BasinwideScore['Acres'] * BasinwideScore['SEZ_Quality']\n",
    "\n",
    "# Calculate the sums\n",
    "total_sez_ci = BasinwideScore['SEZ_Condition_Index'].sum()\n",
    "total_acres = BasinwideScore['Acres'].sum()\n",
    "\n",
    "# Calculate the final number\n",
    "final_number = total_sez_ci / total_acres\n",
    "\n",
    "\n",
    "print(f\"Total SEZ Condition Index: {total_sez_ci}\")\n",
    "print(f\"Total Acres: {total_acres}\")\n",
    "print(f\"Final Number: {final_number}\")\n",
    "\n",
    "# Create a new DataFrame with Threshold Year and final number\n",
    "final_data = BasinwideScore[['Threshold Year']].drop_duplicates().copy()\n",
    "final_data['Acre-weighted average SEZ quality'] = final_number\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "final_data.to_csv('F:\\Research and Analysis\\Threshold reporting\\ThresholdData\\SEZ\\BasinwideSEZscores.csv', index=False, mode=a, Header=False)\n",
    "\n",
    "print(\"Data has been saved to 'BasinwideSEZscores.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2019 threshold data\n",
    "\n",
    "Basinwide2019 = dfSEZ[['Acres', 'SEZ_ID', 'Assessment_Unit_Name', 'Final_Percent', 'SEZ_Type']].copy()\n",
    "Basinwide2019['SEZ_Quality'] = Basinwide2019['Final_Percent'] \n",
    "Basinwide2019['SEZ_Condition_Index'] = Basinwide2019['Acres'] * Basinwide2019['SEZ_Quality']\n",
    "\n",
    "\n",
    "# Calculate the sums\n",
    "total_sez_ci = Basinwide2019['SEZ_Condition_Index'].sum()\n",
    "total_acres = Basinwide2019['Acres'].sum()\n",
    "\n",
    "# Calculate the final number\n",
    "final_number2019 = total_sez_ci / total_acres\n",
    "\n",
    "print(f\"Total SEZ Condition Index: {total_sez_ci}\")\n",
    "print(f\"Total Acres: {total_acres}\")\n",
    "print(f\"Final Number: {final_number2019}\")\n",
    "\n",
    "# Create and save the DataFrame with the result\n",
    "result2019 = pd.DataFrame({\n",
    "    'Threshold Year': ['2019'],\n",
    "    'Acre-weighted average SEZ quality': [final_number2019]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "result2019.to_csv('F:\\Research and Analysis\\Threshold reporting\\ThresholdData\\SEZ\\BasinwideSEZscores.csv', index=False, mode=a, header=False)\n",
    "\n",
    "print(\"Data has been saved to 'BasinwideSEZscores.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get SEZ Data\n",
    "def get_SEZ_data_web():\n",
    "    SEZ_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/0\"\n",
    "    dfSEZ = get_fs_data_spatial(SEZ_url)\n",
    "    return dfSEZ\n",
    "def plot_BasinwideSEZ_scores(df, draft=True):\n",
    "    BasinwideScore= df[['Acres', 'SEZ_ID','Assessment_Unit_Name','Threshold_Year', 'Final_Percent', 'SEZ_Type']]\n",
    "\n",
    "    # Calculate SEZ_Quality\n",
    "    BasinwideScore['SEZ_Quality'] = BasinwideScore['Final_Percent'] * 100\n",
    "\n",
    "    # Calculate SEZ_Condition Index\n",
    "    BasinwideScore[:, 'SEZ_Condition_Index'] = BasinwideScore['Acres'] * BasinwideScore['SEZ_Quality']\n",
    "    # Group by 'Threshold Year' and calculate the sums for each year\n",
    "    grouped = BasinwideScore.groupby('Threshold_Year').agg(\n",
    "    total_sez_ci=('SEZ_Condition_Index', 'sum'),\n",
    "    total_acres=('Acres', 'sum')\n",
    "    )\n",
    "\n",
    "    # Calculate the final number for each year\n",
    "    grouped['final_number'] = grouped['total_sez_ci'] / grouped['total_acres']\n",
    "    # Calculate the sums\n",
    "    total_sez_ci = BasinwideScore['SEZ_Condition_Index'].sum()\n",
    "    total_acres = BasinwideScore['Acres'].sum()\n",
    "\n",
    "    # Calculate the final number\n",
    "    final_number = total_sez_ci / total_acres\n",
    "\n",
    "\n",
    "    #print(f\"Total SEZ Condition Index: {total_sez_ci}\")\n",
    "    #print(f\"Total Acres: {total_acres}\")\n",
    "    #print(f\"Final Number: {final_number}\")\n",
    "\n",
    "    # Create a new DataFrame with Threshold Year and final number\n",
    "    final_data = BasinwideScore[['Threshold_Year']].drop_duplicates().copy()\n",
    "    final_data['Acre-weighted average SEZ quality'] = final_number\n",
    "\n",
    "    \n",
    "    Threshold_Value = 88\n",
    "    # setup plot\n",
    "    fig = px.box(df, x = 'Threshold_Year', y= 'SEZ Quality')\n",
    "                   \n",
    "    fig.update_traces(hovertemplate='SEZ Quality:<br>%{y:.2f}')\n",
    "\n",
    "    # set layout\n",
    "    fig.update_layout(title='Regional SEZ Quality',\n",
    "                    font_family=font,\n",
    "                    template=template,\n",
    "                    legend_title_text='',\n",
    "                    showlegend=False,\n",
    "                    hovermode=\"x unified\",\n",
    "                    xaxis = dict(\n",
    "                        tickmode = 'linear',\n",
    "                        dtick = df[\"Threshold_Year\"].unique(),\n",
    "                        #dtick = 2,\n",
    "                        #range= [2019, 2024],\n",
    "                        title_text='Year'\n",
    "                    ),\n",
    "                    yaxis = dict(\n",
    "                        tickmode = 'linear',\n",
    "                        tick0 = 0,\n",
    "                        dtick = 25,\n",
    "                        range=[0, 100],\n",
    "                        title_text='Acre-weighted SEZ Quality'\n",
    "                    )\n",
    "                 )\n",
    "\n",
    "    # create threshold line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=[Threshold_Value] * len(df),  # Create a constant line at 88\n",
    "        #x=df['Year'],\n",
    "        x=[2019,2024],\n",
    "        name= \"Threshold\",\n",
    "        line=dict(color='#333333', width=3),\n",
    "        mode='lines',\n",
    "        hovertemplate='Threshold :<br>%{y:.2f}<extra></extra>'\n",
    "    ))\n",
    "    # show figure\n",
    "    fig.show()\n",
    "    if draft == True:\n",
    "        fig.write_html(\n",
    "            config=config,\n",
    "            file= out_chart / f\"Draft/SoilConservation_BasinwideSEZScores.html\",\n",
    "            div_id=f\"SoilConservatin_BasinwideSEZScores\",\n",
    "            full_html=False,\n",
    "        )\n",
    "    elif draft == False:\n",
    "        fig.write_html(\n",
    "            config=config,\n",
    "            file= out_chart / f\"Final/SoilConservation_BasinwideSEZScores.html\",\n",
    "            div_id=f\"SoilConservation_BasinwideSEZScores\",\n",
    "            full_html=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snewsome\\AppData\\Local\\Temp\\ipykernel_6568\\1423934411.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  BasinwideScore['SEZ_Quality'] = BasinwideScore['Final_Percent'] * 100\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6568\\2323276496.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_SEZ_data_web\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Make SEZ/Wetland Restoration Chart\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplot_BasinwideSEZ_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdraft\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6568\\1423934411.py\u001b[0m in \u001b[0;36mplot_BasinwideSEZ_scores\u001b[1;34m(df, draft)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Calculate SEZ_Condition Index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mBasinwideScore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SEZ_Condition_Index'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBasinwideScore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Acres'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mBasinwideScore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SEZ_Quality'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Group by 'Threshold Year' and calculate the sums for each year\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     grouped = BasinwideScore.groupby('Threshold_Year').agg(\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3948\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3949\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3950\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3952\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4145\u001b[0m         if (\n\u001b[1;32m-> 4146\u001b[1;33m             \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4147\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4148\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m__contains__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5141\u001b[0m         \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5142\u001b[0m         \"\"\"\n\u001b[1;32m-> 5143\u001b[1;33m         \u001b[0mhash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5144\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5145\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "# Get SEZ Data\n",
    "df = get_SEZ_data_web()\n",
    "# Make SEZ/Wetland Restoration Chart\n",
    "plot_BasinwideSEZ_scores(df, draft=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orginal code in case we need it--will probably delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't USeoriginal code.. merge needs work it seems like it is using lookup dict for both\n",
    "\n",
    "from functools import reduce\n",
    "##### Functino to Get most recent year of data from each Dataframe\n",
    "def get_most_recent_scores(df,groupfield):\n",
    "    df = df.loc[df.groupby(groupfield)['Year'].idxmax()]\n",
    "    return df\n",
    "\n",
    "def get_most_recent_and_clean(data_frames, lookup_dict, columns_to_drop):\n",
    "    most_recent_data = {}\n",
    "\n",
    "    # Iterate over the items in the original 'meadowdata' dictionary\n",
    "    for key, df in data_frames.items():\n",
    "        # Apply the 'get_most_recent_scores' function to each DataFrame\n",
    "        processed_df = get_most_recent_scores(df, 'Assessment_Unit_Name')\n",
    "        \n",
    "        # Store the processed DataFrame in the new dictionary using the same key\n",
    "        most_recent_data[key] = processed_df\n",
    "    \n",
    "#Columns to drop\n",
    "\n",
    "    # Drop specified columns and remove duplicate rows\n",
    "    cleaned_data = {}\n",
    "    for key, df in most_recent_data.items():\n",
    "        # Drop specified columns if they exist\n",
    "        df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "        \n",
    "        # Remove duplicate rows\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        # Append the cleaned DataFrame to the list\n",
    "        cleaned_data[key] = df\n",
    "    \n",
    "    # Merge all DataFrames on 'Assessment_Unit_Name' using reduce\n",
    "    merged_df = {}\n",
    "    for key, df in cleaned_data.items():\n",
    "        merged_df = reduce(lambda left, right: pd.merge(left, right, on='Assessment_Unit_Name', how='outer'), cleaned_data.values())\n",
    "\n",
    "\n",
    "    # Meadowdata gets the lookup_dict\n",
    "    # Riverinedatagets lookup_riverine dictionary how the hell do i do this \n",
    "\n",
    "    #Add large polygon/meadow feature type sez id\n",
    "    merged_df['SEZ_ID']=merged_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "    merged_df = merged_df.dropna(subset=['SEZ_ID'])\n",
    "\n",
    "    merged_df[key] = df\n",
    "    \n",
    "    return {'merged_df':merged_df,\n",
    "            'cleaned_data':cleaned_data,\n",
    "            'most_recent_data':most_recent_data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DONT USE FOR NOW --maybe for final points.. Try SEZ_Type to say what indicators are.. so it would be Foresteddata, Forested+ Riverine(perennial), Riverine(Perennial), Channeled MEadow, Non_Channeled meadow\n",
    "#MIGHT NOT USE\n",
    "#Forest and Riverine Indicators\n",
    "#Name dataframes so we can reference later\n",
    "forestriverine= {'dfbanks': dfbanks, \n",
    "                    'dfaveraged_biotic':averaged_biotic_df,\n",
    "                    'dfditch': dfditch,\n",
    "                    'dfinvasive': dfinvasive,\n",
    "                    'dfhabitat': dfhabitat,\n",
    "                    'dfincision': dfincision,\n",
    "                    'dfheadcuts': dfheadcuts,\n",
    "                    'dfAOP': dfAOP\n",
    "}\n",
    "                \n",
    "#Forest Indicators\n",
    "forest= {'dfbanks': dfbanks, \n",
    "            'dfditch': dfditch,\n",
    "            'dfhabitat': dfhabitat,\n",
    "            'dfheadcuts': dfheadcuts,\n",
    "}\n",
    "#Channeled Meadow Indicators\n",
    "channeled= {'dfbanks': dfbanks, \n",
    "             'dfaveraged_biotic':averaged_biotic_df,\n",
    "                'dfconifer': dfconifer,\n",
    "                'dfditch': dfditch,\n",
    "                'dfinvasive': dfinvasive,\n",
    "                'dfhabitat': dfhabitat,\n",
    "                'dfvegetation': dfvegetation,\n",
    "                'dfincision': dfincision,\n",
    "                'dfheadcuts': dfheadcuts,\n",
    "                'dfAOP': dfAOP\n",
    "}\n",
    "\n",
    "\n",
    "#Non-Channeled Meadow Indicators\n",
    "NonChanneled= {'dfconifer': dfconifer,\n",
    "                'dfditch': dfditch,\n",
    "                'dfinvasive': dfinvasive,\n",
    "                'dfhabitat': dfhabitat,\n",
    "                'dfvegetation': dfvegetation,\n",
    "                'dfheadcuts': dfheadcuts\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate Scores for Each SEZ This will go into table All_SEZ_Scores\n",
    "\n",
    "#FOR MEADOWS\n",
    "\n",
    "\n",
    "#mergedmeadow_df['SEZ_ID']=mergedmeadow_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "#Identify columns that contain 'score' in their names\n",
    "mergedmeadow_df = meadowdata_processed['merged_df']\n",
    "score_columns = [col for col in mergedmeadow_df.columns if 'Score' in col]\n",
    "\n",
    "#Sum the values across identified score columns to get the final points for the SEZ\n",
    "mergedmeadow_df['Final_Total_Points'] = mergedmeadow_df[score_columns].sum(axis=1, skipna=True)\n",
    "\n",
    "#POints Possible per SEZ\n",
    "\n",
    "mergedmeadow_df['Final_Points_Possible'] = mergedmeadow_df[score_columns].notna().sum(axis=1)*12\n",
    "\n",
    "\n",
    "#ASsign Meadow SEZ_ID to each Assessment_Unit\n",
    "\n",
    "mergedmeadow_df['SEZ_ID']=mergedmeadow_df['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "mergedmeadow_df['SEZ_ID'] = mergedmeadow_df['SEZ_ID'].astype(str)\n",
    "\n",
    "#--------------------------------------\n",
    "#Calculate the final rating\n",
    "#------------------------------------\n",
    "mergedmeadow_df['Final_Percent'] = mergedmeadow_df['Final_Total_Points'] / mergedmeadow_df['Final_Points_Possible']\n",
    "\n",
    "mergedmeadow_df['Final_Rating']= mergedmeadow_df['Final_Percent'].apply(rate_SEZ)\n",
    "\n",
    "mergedmeadow_df['Final_Score']= mergedmeadow_df['Final_Rating'].apply(score_indicator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(dfSEZ.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate mergedmeadow_df and mergedriverine_df\n",
    "riverinemeadowdf = pd.concat([mergedmeadow_df, mergedriverine_df], axis=0, ignore_index=True)\n",
    "\n",
    "#riverinemeadowdf['SEZ_ID']= riverinemeadowdf['SEZ_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "riverinemeadowdf_duplicate_sez = riverinemeadowdf[riverinemeadowdf.duplicated(subset='SEZ_ID', keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NOTS WORKINNG... WILL NEED THIS for FINAL TABLEsezsurveytable grab comments to add to dataframe--MAYBE JUST DO THIS MANUALLY\n",
    "#Convert feature class in gdb to Pandas DataFrame this is for REST Service\n",
    "sezsurveyfields = ['Assessment_Unit_Name', 'Comments', 'Survey_Date']\n",
    "\n",
    "#### Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "#Iterate over the rows in the feature class and extract data\n",
    "with arcpy.da.SearchCursor(sezsurveytable, sezsurveyfields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "sezsurvey_df= pd.DataFrame(data, columns=sezsurveyfields)\n",
    "\n",
    "sezsurvey_df['Year'] = sezsurvey_df['Survey_Date'].dt.year\n",
    "print(sezsurvey_df)\n",
    "#add comments to dataframe\n",
    "#Create a dictionary to map Assessment_Unit_Name to Comments\n",
    "comments_map = dict(zip(sezsurvey_df['Assessment_Unit_Name'], sezsurvey_df['Comments']))\n",
    "\n",
    "#Add the Comments column to both_df using the map function\n",
    "riverinemeadowdf['Comments'] = riverinemeadowdf['Assessment_Unit_Name'].map(comments_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Join Base data to riverine and meadow data\n",
    "\n",
    "Final_mergeddf =pd.merge(dfSEZinfo,riverinemeadowdf, on='SEZ_ID', how='outer', indicator= True)\n",
    "\n",
    "Final_mergeddf['Year'] = '2024'\n",
    "Final_mergeddf.to_csv('ready_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final set up\n",
    "\n",
    "\n",
    "#Field Mapping\n",
    "field_mapping = {\n",
    "    'Assessment_Unit_Name': 'Assessment_Unit_Name',\n",
    "    'Year': 'Threshold Year',\n",
    "    'Comments': 'Comments',\n",
    "    'SEZ_Type': 'SEZ_Type',\n",
    "    'Final_Rating': 'Final_Rating',\n",
    "    'Final_Percent': 'Final_Percent',\n",
    "    'SEZ_ID': 'SEZ_ID',\n",
    "    'Final_Points_Possible': 'Final_Points_Possible',\n",
    "    'Final_Total_Points': 'Final_Total_Points',\n",
    "    'Acres': 'Acres',\n",
    "    '':'AquaticOrganismPassage_Barriers',\n",
    "    '': 'AquaticOrganismPassage_NumberOf',\n",
    "    '': 'AquaticOrganismPassage_Score',\n",
    "    '': 'AquaticOrganismPassage_StreamMiles',\n",
    "    '': 'Bank_Stability_Data_Source',\n",
    "    '': 'Bank_Stability_Percent_Unstable',\n",
    "    '': 'Bank_Stability_Rating',\n",
    "    '': 'Bank_Stability_Score',\n",
    "    '': 'Biotic_Integrity_Rating',\n",
    "    '': 'Biotic_Integrity_CSCI',\n",
    "    '': 'Biotic_Integrity_Data_Source',\n",
    "    '': 'Biotic_Integrity_Score',\n",
    "    '': 'Comments',\n",
    "    '': 'Conifer_Encroachment_Percent_En',\n",
    "    '': 'Conifer_Encroachment_Data_Sourc',\n",
    "    '': 'Conifer_Encroachment_Rating',\n",
    "    '': 'Conifer_Encroachment_Score',\n",
    "    '': 'Conifer_Encroachment_Comments',\n",
    "    '': 'CountAttachments',\n",
    "    '': 'Ditches_Data_Source',\n",
    "    '': 'Ditches_Length',\n",
    "    '': 'Ditches_Meadow_Length',\n",
    "    '': 'Ditches_Percent',\n",
    "    '': 'Ditches_Rating',\n",
    "    '': 'Ditches_Score',\n",
    "    '': 'Feature_Type',\n",
    "    '': 'Habitat_Fragmentation_Data_Sour',\n",
    "    '': 'Habitat_Fragmentation_Imperviou',\n",
    "    '': 'Habitat_Fragmentation_Percent_I',\n",
    "    '': 'Habitat_Fragmentation_Rating',\n",
    "    '': 'Habitat_Fragmentation_Score',\n",
    "    '': 'Headcuts_Data_Source',\n",
    "    '': 'Headcuts_Number_of_Headcuts',\n",
    "    '': 'Headcuts_Rating',\n",
    "    '': 'Headcuts_Score',\n",
    "    '': 'Incision_Data_Source',\n",
    "    '': 'Incision_Rating',\n",
    "    '': 'Incision_Score',\n",
    "    '': 'Incision_Ratio',\n",
    "    '': 'Invasive_Percent_Cover',\n",
    "    '': 'Invasive_Rating',\n",
    "    '': 'Invasives_Data_Source',\n",
    "    '': 'Invasives_Invasives_Number_of_Invasives',\n",
    "    '': 'Invasives_Plant_Types',\n",
    "    '': 'Invasives_Scores',\n",
    "    '': 'NDVI_ID',\n",
    "    '': 'Ownership_Primary',\n",
    "    '': 'Ownership_Secondary',\n",
    "    '': 'Ownership_Secondary_2',\n",
    "    '': 'Ownership_Secondary_3',\n",
    "    '': 'VegetationVigor_DataSource',\n",
    "    '': 'VegetationVigor_Rating',\n",
    "    '': 'VegetationVigor_Raw',\n",
    "    '': 'VegetationVigor_Score',\n",
    "}\n",
    "# Rename fields based on field mappings\n",
    "readydf = both_df.rename(columns=field_mapping).drop(columns=[col for col in both_df.columns if col not in field_mapping])\n",
    "\n",
    "#readydf['SEZ_ID'] = readydf['Assessment_Unit_Name'].map(lookup_dict)\n",
    "\n",
    "# Convert \"Year\" column to datetime format with year frequency\n",
    "#readydf['Year'] = pd.to_datetime(readydf['Year'], format='%Y')\n",
    "\n",
    "print(readydf)\n",
    "\n",
    "# Assuming ready_df is the DataFrame you want to write to CSV\n",
    "readydf.to_csv('ready_df.csv', index=False)\n",
    "\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "#data = readydf.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "#field_names = list(readydf.columns)\n",
    "\n",
    "# Append data to existing table\n",
    "#with arcpy.da.InsertCursor(allsezscores, field_names) as cursor:\n",
    " #   for row in data:\n",
    "  #      cursor.insertRow([row[field] for field in field_names])\n",
    "\n",
    "#print(\"Data appended to table successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the current working directory\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Habitat Condition use riverine indicators but add IPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up Biotic Data\n",
    "dfbiotic = get_fs_data(biotic_integrity_url)\n",
    "\n",
    "# #------------------#\n",
    "#Biotic Integrity\n",
    "#------------------#\n",
    "#Prep data- Add any scores and find average oif there are two stream sites for one sez. Also rename data source so it includes are streams that were averaged\n",
    "# Function to average scores and concatenate data sources for each Year and Assessment_Unit_Name\n",
    "def average_biotic_scores(dfbiotic, unit_col='Assessment_Unit_Name', year_col='Year', score='Biotic_Integrity_CSCI', source_col='Biotic_Integrity_Data_Source'):\n",
    "    # Group by Assessment Unit and Year\n",
    "    group = dfbiotic.groupby([unit_col, year_col])\n",
    "    \n",
    "    # Calculate the mean of the scores\n",
    "    averaged_scores = group[score].mean().reset_index()\n",
    "    \n",
    "    # Concatenate the data sources with specific formatting\n",
    "    def concatenate_sources(x, year):\n",
    "        formatted_sources = []\n",
    "        for entry in x:\n",
    "            parts = entry.split(\",\")\n",
    "            if len(parts) >= 3:\n",
    "                formatted_sources.append(f'TRPA, {parts[1].strip()}, {parts[-1].strip()}')  # Extract station code and year\n",
    "        if formatted_sources:\n",
    "            return '/ '.join(formatted_sources)\n",
    "        else:\n",
    "            return None  # Return None if all entries are invalid\n",
    "    \n",
    "    # Apply concatenate_sources to each group\n",
    "    concatenated_sources = group.apply(lambda grp: concatenate_sources(grp[source_col], grp[year_col])).reset_index(name=source_col)\n",
    "    \n",
    "    # Merge the averaged scores with concatenated sources\n",
    "    averaged_df = pd.merge(averaged_scores, concatenated_sources, on=[unit_col, year_col], how='left')\n",
    "    \n",
    "    return averaged_df\n",
    "\n",
    "\n",
    "# Drop duplicates based on 'Assessment_Unit_Name' and 'Year'\n",
    "dfbiotic = dfbiotic.drop_duplicates(subset=['Assessment_Unit_Name', 'Year', 'Biotic_Integrity_CSCI'])\n",
    "\n",
    "# Apply the function to dfbiotic\n",
    "averaged_biotic_df = average_biotic_scores(dfbiotic)\n",
    "\n",
    "# Apply the rating function to the averaged biotic integrity scores\n",
    "averaged_biotic_df['Biotic_Integrity_Rating'] = averaged_biotic_df['Biotic_Integrity_CSCI'].apply(categorize_csci)\n",
    "\n",
    "# Calculate the biotic score for each SEZ\n",
    "averaged_biotic_df['Biotic_Integrity_Score'] = averaged_biotic_df['Biotic_Integrity_Rating'].apply(score_indicator)\n",
    "\n",
    "averaged_biotic_df['Biotic_Integrity_Score']=averaged_biotic_df['Biotic_Integrity_Score'].astype(int)\n",
    "\n",
    "# Output the resulting DataFrame\n",
    "print(averaged_biotic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physical Habitat Data - IPI Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import IPI Data \n",
    "IPIfolder = \"F:\\Research and Analysis\\Fisheries\\Streams\\Bioassessment\"\n",
    "IPI22 = os.path.join(IPIfolder, \"2022\", \"IPI_22.csv\")\n",
    "IPI20 = os.path.join(IPIfolder, \"2020\", \"IPI_20.csv\")\n",
    "\n",
    "#Create IPI Dataframes\n",
    "IPI22df = pd.read_csv(IPI22)\n",
    "IPI20df = pd.read_csv(IPI20)\n",
    "\n",
    "IPI22df['IPIYear']= '2022'\n",
    "IPI20df['IPIYear']= '2020'\n",
    "\n",
    "#Merge dataframes into one \n",
    "concatIPI_df = pd.concat([IPI22df, IPI20df], axis=0, ignore_index=True)\n",
    "\n",
    "#Calculate Scores in IPI\n",
    "#Code for Grading IPI\n",
    "#Define Grade for TRPA's IPI Score - Used only for Stream HAbitat Condition\n",
    "def categorize_phab(IPI):\n",
    "     if   IPI >= 0.94:\n",
    "        return 'A'\n",
    "     elif 0.83 < IPI < 0.94:\n",
    "        return 'B'\n",
    "     elif 0.7 < IPI <= 0.83:\n",
    "        return 'C'\n",
    "     else:\n",
    "        return 'D'\n",
    "\n",
    "concatIPI_df['IPI_Rating']=concatIPI_df['IPI'].apply(categorize_phab)\n",
    "concatIPI_df['IPI_Score']= concatIPI_df['IPI_Rating'].apply(score_indicator)\n",
    "\n",
    "\n",
    "\n",
    "concatIPI_df.head()\n",
    "\n",
    "columns_to_keep = ['StationCode', 'IPI', 'IPI_Rating', 'IPI_Score', 'IPIYear']\n",
    "\n",
    "concatIPI_df = concatIPI_df[columns_to_keep]\n",
    "concatIPI_df['IPIYear']= concatIPI_df['IPIYear'].astype(str)\n",
    "concatIPI_df['IPI_DataSource']= 'TRPA, ' + concatIPI_df['StationCode'] +', ' + concatIPI_df['IPIYear']\n",
    "\n",
    "#Tune Up DATA Source Column to include year and station code\n",
    "\n",
    "print(concatIPI_df)\n",
    "\n",
    "#Join on station code(StationCode) to streamsdf(SITE_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP\n",
    "\n",
    "\n",
    "def get_fs_data_spatial(fs_url):\n",
    "    feature_layer = FeatureLayer(fs_url)\n",
    "    query_result = feature_layer.query().sdf\n",
    "    query_result.spatial.sr = 26910\n",
    "    return query_result\n",
    "\n",
    "# Get Stream Location data so we can do a spatial join on stream miles or riverine units\n",
    "stream_url = \"https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/8\"\n",
    "\n",
    "streamsdf = get_fs_data_spatial(stream_url)# Create DataFrame\n",
    "\n",
    "#Get SEZ spatially enabled dataframe\n",
    "SEZ_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/0\"\n",
    "\n",
    "dfSEZ = get_fs_data_spatial(SEZ_url)\n",
    "\n",
    "#spatial reference stuff\n",
    "streamsdf.spatial.sr = dfSEZ.spatial.sr\n",
    "\n",
    "#perform spatial join of sde.stream and sez units\n",
    "#thesdf = SEZsdf.spatial.join(streamsdf, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect location information of based on stationcode\n",
    "#  Join on ipi (StationCode) to streamsdf(SITE_NAME) so we have a location for the sites\n",
    "merged_df = pd.merge(concatIPI_df, streamsdf, left_on='StationCode', right_on='SITE_NAME', how='inner')\n",
    "\n",
    "#Clean up physical habitat dataframe\n",
    "# Keep only phab data and spatial data from Stream data do i need latitude and longitude.. i don't think so... (, 'LATITUDE', 'LONGITUDE')\n",
    "phab_columns = ['StationCode', 'IPI', 'IPI_Rating', 'IPI_Score', 'IPIYear', 'IPI_DataSource','SHAPE']\n",
    "\n",
    "Phabsdf = merged_df[phab_columns]\n",
    "\n",
    "#Spatial join to assessment units \n",
    "PHABSEZsdf= SEZscores_readydf.spatial.join(Phabsdf, how='inner')\n",
    "\n",
    "print(PHABSEZsdf)\n",
    "#Get most recent\n",
    "\n",
    "#Spatial join to final df\n",
    "\n",
    "#keep only riverine(perennial)? or do i keep feature type riverine ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option\n",
    "\n",
    "#Its either these indicators below.. more data is better\n",
    "RiverineIndicators = ['AOP_Score', 'Bank_Stability_Score', 'Biotic_Integrity_Score', 'Habitat_Frag_Score', 'Incision_Score', 'Headcuts_Score, IPI_Score']\n",
    "\n",
    "#Or this has nothing to do with Riverine indicators and is just CSCI and IPI Scores related to Assessment Unit because indicator such as bank stability and incision? are part of the IPI Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spatially enabled DataFrame (sdf) for target feature SEZ assessment units\n",
    "SEZsdf = pd.DataFrame.spatial.from_featureclass(SEZ_Master)\n",
    "#Spatial join to SEZsdf \n",
    "PHABSEZsdf= SEZsdf.spatial.join(Phabsdf, how='inner')\n",
    "\n",
    "print(PHABSEZsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep SEZ Baseline Data for assessment unit...will need to rethink if acreage changes.. or just manually change in sde\n",
    "keep_columns = ['SHAPE', 'SEZ_ID', 'Feature_Type', 'SEZ_Type', 'Ownership_Primary', 'Ownership_Secondary', 'Ownership_Secondary_2', 'Ownership_Secondary_3', 'Acres', 'Comments']\n",
    "\n",
    "dfSEZinfo=dfSEZ.loc[:,keep_columns].copy()\n",
    "\n",
    "dfSEZinfo['SEZ_ID']= dfSEZinfo['SEZ_ID'].astype(int)\n",
    "\n",
    "#In future we can filter for year if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to keep\n",
    "columns_to_keep = ['Assessment_Unit_Name', 'SEZ_Type', 'Feature_Type', 'SEZ_ID','SITE_NAME', 'COUNT_VALUE', 'YEAR_OF_COUNT', 'STATION_TYPE', 'LONGITUDE', 'LATITUDE', ]\n",
    "##Try this instead\n",
    "# Select only the desired columns\n",
    "bioticdf = thesdf.loc[:, columns_to_keep].copy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRPA conditional categories-- Scoring for Stream Habitat Condition for stream miles based on CSCI Score\n",
    "#Do i get the average of biotic and phab? aka csci and ipi and use this grading scale?\n",
    "#Define Grade for Bioassessment CSCI Condition Score???\n",
    "def categorize_cscicondition(biotic_integrity):\n",
    "     if pd.isna(biotic_integrity):\n",
    "        return np.nan\n",
    "     elif   biotic_integrity >= 0.92:\n",
    "        return 'excellent'\n",
    "     elif 0.8 <= biotic_integrity < 0.92:\n",
    "        return 'good'\n",
    "     elif biotic_integrity < 0.8:\n",
    "        return 'marginal'\n",
    "     else:\n",
    "        return 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT USE FOR NOW THIS MATCHES THE BIOTIC INTEGRITY SCORES with IPI, next code block will do a spatial join with the data\n",
    "#Rethink this.. I have the IPI Scores.. then I should just get the most recent... \n",
    "#Get scores for IPI-good to go\n",
    "#spatial join to sez so there is an Assessment Unit name.. or do I use biotic integrity..Do we have IPI score an dno biotic score? potentially\n",
    "#Use Process data function to get the most recent and assign SEZ ID\n",
    "#\n",
    "#prep biotic data so that we have a station code to match to so we have an sez assessment unit\n",
    "averaged_biotic_df['StationCode'] = averaged_biotic_df['Biotic_Integrity_Data_Source'].str.split(',').str[1].str.strip()\n",
    "\n",
    "sezipi_df = pd.merge(averaged_biotic_df, concatIPI_df, on='StationCode', how='outer')\n",
    "\n",
    "\n",
    "sezipi_df = sezipi_df.dropna(subset=['IPI'])\n",
    "\n",
    "## Function to get the most recent IPI score for each assessment unit\n",
    "def get_most_recent_scores(df):\n",
    "    return df.loc[df.groupby('Assessment_Unit_Name')['IPIYear'].idxmax()]\n",
    "\n",
    "# Convert IPIYear to an integer type\n",
    "sezipi_df['IPIYear'] = pd.to_numeric(sezipi_df['IPIYear'], errors='coerce')\n",
    "\n",
    "\n",
    "# Get the most recent IPI scores\n",
    "most_recent_IPI = get_most_recent_scores(sezipi_df)\n",
    "\n",
    "print(most_recent_IPI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column names of sezipi_df\n",
    "print(sezipi_df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep mergedriverine_df calculate stream  maybe match scores to biotic integrity since those will all get phab?\n",
    "#mergedriverine_df = StationCode so I can match data from IPI df to merged riverine\n",
    "#mergedmeadow_df = pd.merge(mergedmeadow_df, df, on='Assessment_Unit_Name', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code to make it a spatially enabled dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to conver to spatially enable geodataframe\n",
    "# Create a new DataFrame Final_df with columns based on the field mapping\n",
    "#df = df.rename(columns=field_mapping)\n",
    "\n",
    "# Convert DataFrame to Spatially Enabled DataFrame\n",
    "#sedf = GeoAccessor.from_xy(df, x_column='LONGITUDE', y_column='LATITUDE')\n",
    "\n",
    "# Convert the SEDF to a feature class without sanitizing columns\n",
    "#sedf.spatial.to_featureclass(location=os.path.join(workspace, 'PlanAreaNoise_23_Staging'), sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEZ Chart Wetland Restoration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Rating D",
         "type": "bar",
         "x": [
          "2019",
          "2023"
         ],
         "y": [
          17.241842998263564,
          19.2682808658033
         ]
        },
        {
         "name": "Rating C",
         "type": "bar",
         "x": [
          "2019",
          "2023"
         ],
         "y": [
          8.275057367924704,
          13.236539547118472
         ]
        },
        {
         "name": "Rating B",
         "type": "bar",
         "x": [
          "2019",
          "2023"
         ],
         "y": [
          22.77982124939392,
          28.03197575647972
         ]
        },
        {
         "name": "Rating A",
         "type": "bar",
         "x": [
          "2019",
          "2023"
         ],
         "y": [
          49.362499950362476,
          37.17711080562575
         ]
        }
       ],
       "layout": {
        "barmode": "stack",
        "legend": {
         "title": {
          "text": "Final Rating"
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Percentage of Acres by Final Rating for Each Year"
        },
        "xaxis": {
         "title": {
          "text": "Year"
         }
        },
        "yaxis": {
         "ticksuffix": "%",
         "title": {
          "text": "Percentage of Acres"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wetland Restoration\n",
    "def get_fs_data_spatial(fs_url):\n",
    "    feature_layer = FeatureLayer(fs_url)\n",
    "    query_result = feature_layer.query().sdf\n",
    "    query_result.spatial.sr = 26910\n",
    "    return query_result\n",
    "SEZ_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/0\"\n",
    "df = get_fs_data_spatial(SEZ_url)\n",
    "# Calculate total acres per year\n",
    "total_acres_per_year = df.groupby('Threshold_Year')['Acres'].sum().reset_index()\n",
    "total_acres_per_year.rename(columns={'Acres': 'Total_Acres'}, inplace=True)\n",
    "# Calculate acres per Final_Rating per year\n",
    "acres_per_rating = df.groupby(['Threshold_Year', 'Final_Rating'])['Acres'].sum().reset_index()\n",
    "# Merge total acres with acres per rating\n",
    "merged_df = pd.merge(acres_per_rating, total_acres_per_year, on='Threshold_Year')\n",
    "# Calculate percentage\n",
    "merged_df['Percentage'] = (merged_df['Acres'] / merged_df['Total_Acres']) * 100\n",
    "# Pivot the data\n",
    "pivot_df = merged_df.pivot(index='Threshold_Year', columns='Final_Rating', values='Percentage').reset_index()\n",
    "# Fill NaN with 0 (in case any ratings are missing for a year)\n",
    "pivot_df.fillna(0, inplace=True)\n",
    "import plotly.graph_objects as go\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "# List of ratings\n",
    "ratings = ['D', 'C', 'B', 'A']\n",
    "#colors = {'D': '#FF0000', 'C': '#FFA500', 'B': '#FFDB58', 'A': '#008000'}\n",
    "#Addbarfor each rating\n",
    "for rating in ratings:\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=pivot_df['Threshold_Year'],\n",
    "        y=pivot_df[rating],\n",
    "        name=f'Rating {rating}',\n",
    "        #marker_color=colors[rating]  # Assign color here\n",
    "    ))\n",
    "# Add a bar for each rating\n",
    "#for rating in ratings:\n",
    " #   fig.add_trace(go.Bar(\n",
    "  #      x=pivot_df['Threshold_Year'],\n",
    "   #     y=pivot_df[rating],\n",
    "    #    name=f'Rating {rating}'\n",
    "   # ))\n",
    "# Update layout to stack the bars\n",
    "fig.update_layout(\n",
    "    barmode='stack',\n",
    "    title='Percentage of Acres by Final Rating for Each Year',\n",
    "    xaxis_title='Year',\n",
    "    yaxis_title='Percentage of Acres',\n",
    "    yaxis=dict(ticksuffix='%'),\n",
    "    legend_title='Final Rating'\n",
    ")\n",
    "# Show the figure\n",
    "fig.show()\n",
    "#draftworkspace = r\"C:\\Users\\snewsome\\Documents\\GitHub\\Monitoring-1\\Chart\\Draft\"\n",
    "# save to HTML\n",
    "#fig.write_html(os.path.join(draftworkspace, \"SEZ_Scores24.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAtA clean up for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up data for Analysis\n",
    "# Get DAta from 2023 threshold\n",
    "# Wetland Restoration\n",
    "def get_fs_data_spatial(fs_url):\n",
    "    feature_layer = FeatureLayer(fs_url)\n",
    "    query_result = feature_layer.query().sdf\n",
    "    query_result.spatial.sr = 26910\n",
    "    return query_result\n",
    "SEZ_url = \"https://maps.trpa.org/server/rest/services/SEZ_Assessment_Unit/FeatureServer/0\"\n",
    "dfSEZ = get_fs_data_spatial(SEZ_url)\n",
    "\n",
    "#Keep data from 2023 Threshold only\n",
    "df = dfSEZ[dfSEZ['Threshold_Year'] == '2023']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data based on SEZ_Type and if Final Rating is Not Assessed\n",
    "#Maually remove data for sez that have a D rating but no final percent except hab frag and aquatic, biotic and invasive\n",
    "# Dictionary mapping SEZ Types to required indicators\n",
    "sez_type_indicators = {\n",
    "    'Riverine (Perennial)': ['AquaticOrganismPassage', 'Bank_Stability', 'Biotic_Integrity', 'Habitat_Fragmentation', 'Headcuts', 'Incision'],\n",
    "    'Riverine (Perennial) + Forested': ['AquaticOrganismPassage', 'Bank_Stability', 'Biotic_Integrity', 'Ditches', 'Habitat_Fragmentation', 'Headcuts', 'Incision'],\n",
    "    'Non-Channeled Meadow': ['Conifer_Encroachment', 'Ditches', 'Habitat_Fragmentation', 'Headcuts', 'Invasive', 'VegetationVigor'],\n",
    "    'Channeled Meadow': ['AquaticOrganismPassage', 'Bank_Stability', 'Biotic_Integrity', 'Conifer_Encroachment', 'Ditches', 'Habitat_Fragmentation', 'Headcuts', 'Incision', 'Invasive', 'VegetationVigor'],\n",
    "    'Forested': ['Bank_Stability', 'Ditches', 'Habitat_Fragmentation', 'Headcuts']\n",
    "}\n",
    "\n",
    "# Function to clean up indicators based on SEZ Type and 'Not Assessed' in Final_Rating\n",
    "def clean_indicators_dynamic(row, sez_type_dict, indicator_substrings):\n",
    "    # Check if Final_Rating is 'Not Assessed'\n",
    "    if row['Final_Rating'] == 'Not Assessed':\n",
    "        # Set all indicator columns to NaN\n",
    "        for indicator in indicator_substrings:\n",
    "            related_columns = [col for col in df.columns if indicator.replace(' ', '_').lower() in col.lower()]\n",
    "            row[related_columns] = np.nan\n",
    "    else:\n",
    "        # Clean based on SEZ Type\n",
    "        sez_type = row['SEZ_Type']  # Replace with your SEZ Type column name\n",
    "        required_indicators = sez_type_dict.get(sez_type, [])\n",
    "\n",
    "        # Loop through each required indicator and dynamically find columns related to it\n",
    "        for indicator in indicator_substrings:\n",
    "            if indicator not in required_indicators:\n",
    "                # Find columns that contain the indicator substring\n",
    "                related_columns = [col for col in df.columns if indicator.replace(' ', '_').lower() in col.lower()]\n",
    "                # Set those columns to NaN\n",
    "                row[related_columns] = np.nan\n",
    "\n",
    "    return row\n",
    "\n",
    "# List of indicator substrings to match with column names\n",
    "indicator_substrings = ['AquaticOrganismPassage', 'Bank_Stability', 'Biotic_Integrity', 'Conifer_Encroachment', 'Ditches', 'Habitat_Fragmentation', 'Headcuts', 'Incision', 'Invasive', 'VegetationVigor']\n",
    "\n",
    "# Apply the cleaning function row-wise\n",
    "df_cleaned = df.apply(lambda row: clean_indicators_dynamic(row, sez_type_indicators, indicator_substrings), axis=1)\n",
    "\n",
    "# List of columns to export (excluding 'geometry' or 'Shape')\n",
    "columns_to_export = [col for col in df_cleaned.columns if col != 'SHAPE']\n",
    "df_cleaned[columns_to_export].to_csv(r'C:\\Users\\snewsome\\Documents\\SEZ\\cleaned2023Threshold_SEZScores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use SEZ Type to clean data adn remove data for indicators that aren't included in the final score\n",
    "\n",
    "# Dictionary mapping SEZ Types to required indicators\n",
    "sez_type_indicators = {\n",
    "    'Riverine (Perennial)': ['AquaticOrganismPassage', 'Bank_Stability', 'Biotic_Integrity', 'Habitat_Fragmentation', 'Headcuts', 'Incision'],\n",
    "    'Riverine (Perennial) + Forested': ['AquaticOrganismPassage', 'Bank_Stability', 'Biotic_Integrity', 'Ditches', 'Habitat_Fragmentation', 'Headcuts', 'Incision'],\n",
    "    'Non-Channeled Meadow': ['Conifer_Encroachment', 'Ditches', 'Habitat_Fragmentation', 'Headcuts', 'Invasive', 'VegetationVigor'],\n",
    "    'Channeled Meadow': ['AquaticOrganismPassage', 'Bank_Stability', 'Biotic_Integrity', 'Conifer_Encroachment', 'Ditches', 'Habitat_Fragmentation', 'Headcuts', 'Incision', 'Invasive', 'VegetationVigor'],\n",
    "    'Forested': ['Bank_Stability', 'Ditches', 'Habitat_Fragmentation', 'Headcuts']\n",
    "}\n",
    "\n",
    "# Function to clean up indicators based on SEZ Type\n",
    "def clean_indicators_dynamic(row, sez_type_dict, indicator_substrings):\n",
    "    sez_type = row['SEZ_Type']  # Replace with your SEZ Type column name\n",
    "    required_indicators = sez_type_dict.get(sez_type, [])\n",
    "\n",
    "    # Loop through each required indicator and dynamically find columns related to it\n",
    "    for indicator in indicator_substrings:\n",
    "        if indicator not in required_indicators:\n",
    "            # Find columns that contain the indicator substring\n",
    "            related_columns = [col for col in df.columns if indicator.replace(' ', '_').lower() in col.lower()]\n",
    "            # Set those columns to NaN\n",
    "            row[related_columns] = np.nan\n",
    "    return row\n",
    "\n",
    "# List of indicator substrings to match with column names\n",
    "indicator_substrings = ['AquaticOrganismPassage', 'Bank_Stability', 'Biotic_Integrity', 'Conifer_Encroachment', 'Ditches', 'Habitat_Fragmentation', 'Headcuts', 'Incision', 'Invasive',  'VegetationVigor']\n",
    "\n",
    "# Apply the cleaning function row-wise\n",
    "df_cleaned = df.apply(lambda row: clean_indicators_dynamic(row, sez_type_indicators, indicator_substrings), axis=1)\n",
    "\n",
    "# List of columns to export (excluding 'geometry' or 'Shape')\n",
    "columns_to_export = [col for col in df_cleaned.columns if col != 'SHAPE']\n",
    "df_cleaned[columns_to_export].to_csv(r'C:\\Users\\snewsome\\Documents\\SEZ\\cleaned2023Threshold_SEZScores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis with Cleaned up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------\n",
    "# Analysis using cleaned up csvs for 2019 and 2023 data\n",
    "#-------------\n",
    "# Create Excel to compare indicators and what SEZ's changed the most based on final score\n",
    "\n",
    "# Load the CSV files\n",
    "Threshold23 = r'F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\cleaned2023Threshold_SEZScores.csv'\n",
    "Threshold19 = r'F:\\GIS\\PROJECTS\\ResearchAnalysis\\SEZ\\updatedThreshold19_SEZScores.csv'\n",
    "\n",
    "Threshold23df = pd.read_csv(Threshold23)\n",
    "Threshold19df = pd.read_csv(Threshold19)\n",
    "\n",
    "# List of all columns with 'Score' in their name for both datasets\n",
    "score_columns_2023 = [col for col in Threshold23df.columns if 'Score' in col]\n",
    "score_columns_2019 = [col for col in Threshold19df.columns if 'Score' in col]\n",
    "\n",
    "\n",
    "\n",
    "# Ensure both DataFrames have the same 'Assessment_Unit_Name' and 'SEZ_ID' columns, along with score columns\n",
    "Threshold23sezdata = Threshold23df[['Assessment_Unit_Name', 'SEZ_ID', 'SEZ_Type', 'Threshold_Year', 'Acres', 'Final_Percent', 'Final_Rating'] + score_columns_2023]\n",
    "Threshold19sezdata = Threshold19df[['Assessment_Unit_Name', 'SEZ_ID', 'SEZ_Type', 'Threshold_Year', 'Acres', 'Final_Percent', 'Final_Rating'] + score_columns_2019]\n",
    "\n",
    "#Calcluate what percent of total meadows that sez unit is\n",
    "total_acres = Threshold23sezdata['Acres'].sum()\n",
    "Threshold23sezdata['Percent_ofTotal_Acres']= (Threshold23sezdata['Acres'] / total_acres)*100\n",
    "\n",
    "#Calculate SEZ Condition Index\n",
    "Threshold23sezdata['SEZ_CI_Score']= Threshold23sezdata['Acres'] * Threshold23sezdata['Final_Percent']\n",
    "Threshold19sezdata['SEZ_CI_Score']= Threshold19sezdata['Acres'] * Threshold19sezdata['Final_Percent']\n",
    "\n",
    "# List of all columns with 'Score' in their name for both datasets\n",
    "score_columns_2023 = [col for col in Threshold23sezdata.columns if 'Score' in col]\n",
    "score_columns_2019 = [col for col in Threshold19sezdata.columns if 'Score' in col]\n",
    "\n",
    "# Merge the datasets on 'Assessment_Unit_Name' and 'SEZ_ID'\n",
    "merged_df = pd.merge(\n",
    "    Threshold23sezdata,\n",
    "    Threshold19sezdata,\n",
    "    on=['Assessment_Unit_Name', 'SEZ_ID'],\n",
    "    suffixes=('_2023', '_2019')\n",
    ")\n",
    "\n",
    "\n",
    "# Create a new DataFrame to store the changes\n",
    "changes_df = Threshold23sezdata[['Assessment_Unit_Name', 'SEZ_ID', 'SEZ_Type','Acres','Percent_ofTotal_Acres']].copy()\n",
    "# Calculate score changes per indicator\n",
    "for score_2023, score_2019 in zip(score_columns_2023, score_columns_2019):\n",
    "    score_2023_col = f'{score_2023}_2023'\n",
    "    score_2019_col = f'{score_2019}_2019'\n",
    "    changes_df[f'{score_2023}_Change'] = merged_df[score_2023_col] - merged_df[score_2019_col]\n",
    "\n",
    "# Calculate change in Final Percent\n",
    "changes_df['Final_Percent_Change'] = merged_df['Final_Percent_2023'] - merged_df['Final_Percent_2019']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Filter out rows where no change occurred\n",
    "#changes_df = changes_df[(changes_df.filter(like='_Change') != 0).any(axis=1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize acres based on Final Percent Change-difference in scores between 2023 and 2019 threshold eval\n",
    "# Create a new DataFrame to store the changes\n",
    "summary_df = changes_df[['Assessment_Unit_Name', 'SEZ_ID', 'SEZ_Type','Acres','Percent_ofTotal_Acres','Final_Percent_Change','SEZ_CI_Score_Change']].copy()\n",
    "summary_df['Change_Category'] = changes_df['Final_Percent_Change'].apply(lambda x: 'Improved' if x > 0 else ('Declined' if x < 0 else 'No Change'))\n",
    "\n",
    "# Calculate total acres for each change category\n",
    "total_acres_improved = summary_df.loc[summary_df['Change_Category'] == 'Improved', 'Acres'].sum()\n",
    "total_acres_declined = summary_df.loc[summary_df['Change_Category'] == 'Declined', 'Acres'].sum()\n",
    "total_acres_nochange = summary_df.loc[summary_df['Change_Category'] == 'No Change', 'Acres'].sum()\n",
    "\n",
    "# Count acres for each category\n",
    "#category_counts = summary_df.groupby('Change_Category')['Acres'].sum().reset_index()\n",
    "\n",
    "# Ensure lists are correctly created\n",
    "indicator_columns = [col for col in merged_df.columns if '_Change' in col]\n",
    "sum_changes = [merged_df[col].sum() for col in indicator_columns]\n",
    "\n",
    "# Count total number of SEZ units with negative change for each indicator\n",
    "negative_changes_count = {col: (changes_df[col] < 0).sum() for col in indicator_columns}\n",
    "\n",
    "\n",
    "# Create the summary DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    'Indicator': indicator_columns,\n",
    "    'Sum_Change': sum_changes,\n",
    "    'Total_Acres_Improved': [total_acres_improved],\n",
    "    'Total_Acres_Declined': [total_acres_declined],\n",
    "\n",
    "    'SEZs_with_Negative_Change': negative_changes_count.values()\n",
    "})\n",
    "\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize acres based on Final Percent Change - difference in scores between 2023 and 2019 threshold eval\n",
    "summary_df = changes_df[['Assessment_Unit_Name', 'SEZ_ID', 'SEZ_Type', 'Acres', 'Percent_ofTotal_Acres', 'Final_Percent_Change', 'SEZ_CI_Score_Change']].copy()\n",
    "summary_df['Change_Category'] = changes_df['Final_Percent_Change'].apply(lambda x: 'Improved' if x > 0 else ('Declined' if x < 0 else 'No Change'))\n",
    "\n",
    "# Count acres for each change category\n",
    "total_acres_improved = summary_df.loc[summary_df['Change_Category'] == 'Improved', 'Acres'].sum()\n",
    "total_acres_declined = summary_df.loc[summary_df['Change_Category'] == 'Declined', 'Acres'].sum()\n",
    "total_acres_nochange = summary_df.loc[summary_df['Change_Category'] == 'No Change', 'Acres'].sum()\n",
    "\n",
    "# Identify indicator columns and calculate sum of changes\n",
    "indicator_columns = [col for col in changes_df.columns if '_Change' in col]\n",
    "sum_changes = [changes_df[col].sum() for col in indicator_columns]\n",
    "\n",
    "# Count total number of SEZ units with negative change for each indicator\n",
    "negative_changes_count = [ (changes_df[col] < 0).sum() for col in indicator_columns]\n",
    "\n",
    "# Create the summary DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    'Indicator': indicator_columns,\n",
    "    'Sum_Change': sum_changes,\n",
    "    'SEZs_with_Negative_Change': negative_changes_count\n",
    "})\n",
    "\n",
    "# Add Total_Acres_Improved and Total_Acres_Declined as separate single values, repeating for consistency\n",
    "summary_df['Total_Acres_Improved'] = total_acres_improved\n",
    "summary_df['Total_Acres_Declined'] = total_acres_declined\n",
    "summary_df['Total_Acres_NoChange'] = total_acres_nochange\n",
    "summary_df['Total Acres'] = total_acres\n",
    "# Display the summary DataFrame\n",
    "print(summary_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
