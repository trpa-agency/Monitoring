{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureSet, GeoAccessor, GeoSeriesAccessor, FeatureLayer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "gis = GIS()\n",
    "# # Set Pandas display options to show all rows and columns\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# in memory output file path\n",
    "wk_memory = \"memory\" + \"\\\\\"\n",
    "\n",
    "# set workspace and sde connections \n",
    "working_folder = r\"F:/Research and Analysis/Fisheries/Streams/Bioassessment/California Stream Condition Index/California Stream Condition Index\"\n",
    "workspace      = \"C:\\GIS\\Scratch.gdb\"\n",
    "arcpy.env.workspace = \"C:\\GIS\\Scratch.gdb\"\n",
    "\n",
    "# network path to connection files\n",
    "filePath = r\"C:\\\\GIS\\\\DB_CONNECT\"\n",
    "\n",
    "# database file path \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "\n",
    "# local variables\n",
    "fdata = os.path.join(sdeBase, \"sde.SDE.Monitoring\")\n",
    "## Final feature class to append to in Enterprise Geodatabase\n",
    "sdeStreams = os.path.join(sdeBase, \"sde.SDE.Monitoring\\sde.SDE.Stream\")\n",
    "## orginal CSVs that come from preprocessing or R tools created by State\n",
    "originalcsv22 = os.path.join(working_folder,\"2022_CSCI\",\"19-20NV-22allcore.csv\")\n",
    "locationcsv22 = os.path.join(working_folder, \"2022_CSCI\",\"Stations19_22.csv\")\n",
    "originalcsv20 = os.path.join(working_folder,\"2020_CSCI\",\"core.csv\")\n",
    "locationcsv20 = os.path.join(working_folder,\"2020_CSCI\",\"Stations_20.csv\")\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(originalcsv22):\n",
    "    print(f\"Error: File not found at {originalcsv22}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do I need this? -->\n",
    "df = get_fs_data('https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assign Station type and lat long and LTinfo website to Trend Sites\n",
    "\n",
    "\n",
    "#Calculate Rating for CSCI value\n",
    "#Define a function to categorize values based on ranges\n",
    "def categorize_value(value):\n",
    "    if 0 <= value < 0.6:\n",
    "        return 'poor'\n",
    "    elif 0.6 <= value < 0.8:\n",
    "        return 'marginal'\n",
    "    elif 0.8 <= value < 1.0:\n",
    "        return 'good'\n",
    "    else:\n",
    "        return 'excellent'\n",
    "    \n",
    "def get_fs_data(service_url):\n",
    "    \n",
    "    feature_layer = FeatureLayer(service_url)\n",
    "    query_result = feature_layer.query()\n",
    "    # Convert the query result to a list of dictionaries\n",
    "    feature_list = query_result.features\n",
    "\n",
    "    # Create a pandas DataFrame from the list of dictionaries\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dictionary Usring Rest Service data\n",
    "# setup\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureLayer\n",
    "\n",
    "# Connect to TRPA Enterprise GIS Portal *if it's a service only shared with org\n",
    "# portal_user = \"TRPA_PORTAL_ADMIN\"\n",
    "# portal_pwd = str(os.environ.get('Password'))\n",
    "# portal_url = \"https://maps.trpa.org/portal/\"\n",
    "\n",
    "# setup connection to GIS server this can be GIS() with a public service\n",
    "gis = GIS()\n",
    "\n",
    "\n",
    "# get Stream data as a Spatially Enabled Dataframe\n",
    "service_url = 'https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/8'\n",
    "feature_layer = FeatureLayer(service_url)\n",
    "query_result = feature_layer.query()\n",
    "\n",
    "# Convert the query result to a Spatially Enabled Dataframe\n",
    "sdfStreamHab = query_result.sdf\n",
    "\n",
    "sdfStreamHab.info()\n",
    "columnstokeep = ['SITE_NAME','STATION_TYPE', 'LATITUDE', 'LONGITUDE', 'LTINFO']\n",
    "sdfStreamHab = sdfStreamHab.loc[:, columnstokeep]\n",
    "unique_values = sdfStreamHab.drop_duplicates()\n",
    "\n",
    "# Select specific columns for look up\n",
    "selected_columns = ['STATION_TYPE', 'LATITUDE', 'LONGITUDE', 'LTINFO']\n",
    "\n",
    "# Convert selected columns to dictionary\n",
    "lookup_dict = unique_values.set_index('SITE_NAME')[selected_columns].to_dict(orient='index')\n",
    "\n",
    "# Display the dictionary\n",
    "print(lookup_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd \\Research and Analysis\\Fisheries\\Streams\\Bioassessment\\California Stream Condition Index\\California Stream Condition Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform CSCI Scores to Point feature class in Enterprise Geodatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snewsome\\AppData\\Local\\Temp\\ipykernel_34732\\4006546074.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfCSCI = dfCSCI.append(pd.read_csv(originalcsv20), ignore_index=True)\n",
      "C:\\Users\\snewsome\\AppData\\Local\\Temp\\ipykernel_34732\\4006546074.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dflocations = dflocations.append(pd.read_csv(locationcsv20), ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrames from CSV files\n",
    "dfCSCI = pd.read_csv(originalcsv22)\n",
    "dflocations = pd.read_csv(locationcsv22)\n",
    "\n",
    "# Append DataFrames from additional CSV files\n",
    "dfCSCI = dfCSCI.append(pd.read_csv(originalcsv20), ignore_index=True)\n",
    "dflocations = dflocations.append(pd.read_csv(locationcsv20), ignore_index=True)\n",
    "\n",
    "# merge CSCI scores and location data\n",
    "RawData_df = pd.merge(dfCSCI, dflocations, how='inner', on='StationCode')\n",
    "\n",
    "try:\n",
    "    dfCSCI = pd.read_csv(originalcsv22)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please check the file path.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"The CSV file is empty.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error parsing the CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get year from sample id\n",
    "RawData_df['Year']=RawData_df.SampleID.str.split(\"_\").str[-1]\n",
    "\n",
    "#Calculate Station Type \n",
    "RawData_df['STATION_TYPE'] = RawData_df['StationCode'].map(lambda x: lookup_dict[x]['STATION_TYPE'] if x in lookup_dict else 'Status')\n",
    "\n",
    "#Calculate LATITUDE\n",
    "RawData_df['LATITUDE'] = RawData_df['StationCode'].map(lambda x: lookup_dict[x]['LATITUDE']if x in lookup_dict else None)\n",
    "RawData_df['LATITUDE'] = RawData_df['LATITUDE'].fillna(RawData_df['New_Lat'])\n",
    "#Calculate LONGITUDE\n",
    "RawData_df['LONGITUDE'] = RawData_df['StationCode'].map(lambda x: lookup_dict[x]['LONGITUDE']if x in lookup_dict else None)\n",
    "RawData_df['LONGITUDE'] = RawData_df['LONGITUDE'].fillna(RawData_df['New_Long'])\n",
    "#Caculate LTINFO\n",
    "RawData_df['LTINFO'] = RawData_df['StationCode'].map(lambda x: lookup_dict[x]['LTINFO'] if x in lookup_dict else None)\n",
    "\n",
    "# Apply the categorization function to create the new field\n",
    "RawData_df['Rating'] = RawData_df['CSCI'].apply(categorize_value)\n",
    "\n",
    "\n",
    "Field_Mapping={\n",
    "    'StationCode': 'SITE_NAME',\n",
    "    'Year': 'YEAR_OF_COUNT',\n",
    "    'LATITUDE': 'LATITUDE',\n",
    "    'LONGITUDE': 'LONGITUDE',\n",
    "    'CSCI': 'COUNT_VALUE',\n",
    "    'STATION_TYPE': 'STATION_TYPE',\n",
    "    'LTINFO': 'LTINFO'\n",
    "    \n",
    "}\n",
    "# rename feilds based on field mappings\n",
    "df_final = RawData_df.rename(columns=Field_Mapping).drop(columns=[col for col in RawData_df.columns if col not in Field_Mapping])\n",
    "\n",
    "# establish duration field\n",
    "def assign_duration(stationtype):\n",
    "    if stationtype == 'Status' :\n",
    "        return 'One-time'\n",
    "    else:\n",
    "        return 'Long-term'\n",
    "df_final['DURATION']= df_final['STATION_TYPE'].apply(assign_duration)\n",
    "\n",
    "# station code is site name, site name is station code.\n",
    "df_final['STATION_CODE']=df_final['SITE_NAME']\n",
    "\n",
    "# export to csv\n",
    "df_final.to_csv(os.path.join(working_folder,\"StreamCSCI_proccesed.csv\"), index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Thursday, December 14, 2023 3:21:57 PM\",\"Succeeded at Thursday, December 14, 2023 3:21:57 PM (Elapsed Time: 0.40 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'C:\\\\GIS\\\\Scratch.gdb\\\\NewStream_CSCI_Projected'>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert CSV to point feature class\n",
    "arcpy.management.XYTableToPoint(os.path.join(working_folder,\"StreamCSCI_proccesed.csv\"), \n",
    "                                \"NewCSCI_points\", \n",
    "                                \"LONGITUDE\", \"LATITUDE\")\n",
    "\n",
    "# project to UTM Zone 10N\n",
    "arcpy.Project_management(\"NewCSCI_points\", \"NewStream_CSCI_Projected\", 26910)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputfc= \"NewStream_CSCI_Projected\"\n",
    "\n",
    "# disconnect all users\n",
    "print(\"\\nDisconnecting all users...\")\n",
    "arcpy.DisconnectUser(sdeBase, \"ALL\")\n",
    " \n",
    "# unregister the sde feature class as versioned\n",
    "print (\"\\nUnregistering feature dataset as versioned...\")\n",
    "arcpy.UnregisterAsVersioned_management(fdata,\"NO_KEEP_EDIT\",\"COMPRESS_DEFAULT\")\n",
    "print (\"\\nFinished unregistering feature dataset as versioned.\")\n",
    "\n",
    "arcpy.management.Append(inputfc, sdeStreams,\"NO_TEST\")\n",
    "\n",
    "# disconnect all users\n",
    "print(\"\\nDisconnecting all users...\")\n",
    "arcpy.DisconnectUser(sdeBase, \"ALL\")\n",
    "\n",
    "# register SDE feature class as versioned\n",
    "arcpy.RegisterAsVersioned_management(fdata, \"NO_EDITS_TO_BASE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import raw tabular data from csv\n",
    "csci_data = pd.read_csv('Raw_Data\\csci_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot to new format\n",
    "grouping_columns = ['StationCode','STREAM_NAME', 'COUNTY']\n",
    "csci_data_flat = csci_data.pivot(index=grouping_columns, columns='SAMPLEDATE',values='CSCI')\n",
    "csci_data_flat = csci_data_flat.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to create stream csci sampling stations\n",
    "stream_csci_input_layer = \"import\"\n",
    "unique_stream_samples = \"\"\n",
    "\n",
    "stations=[]\n",
    "# Create a search cursor to iterate through the original feature layer\n",
    "with arcpy.da.SearchCursor(stream_csci_input_layer, ['SITE_NAME']) as cursor:\n",
    "    for row in cursor:\n",
    "        value = row[0]\n",
    "        if value not in stations:\n",
    "            stations.append(value)\n",
    "\n",
    "# Create an insert cursor for the output feature layer\n",
    "with arcpy.da.InsertCursor(output_feature_layer, [unique_column]) as cursor:\n",
    "    for value in unique_values:\n",
    "        cursor.insertRow((value,))\n",
    "\n",
    "# Clean up\n",
    "del cursor\n",
    "\n",
    "# Optionally, save the output feature layer to a file\n",
    "arcpy.CopyFeatures_management(output_feature_layer, \"C:/Path/To/Your/Output/FeatureClassFile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New feature class with the sampling station\n",
    "\n",
    "merged_df = pd.merge(sdfCensus, tdc_flat, on='TRPAID', how='inner')\n",
    "columns_drop=['GlobalID', 'YEAR', 'created_date', 'created_user', 'last_edited_date', 'last_edited_user', 'Shape.STArea()', 'Shape.STLength()']\n",
    "merged_df = merged_df.drop(columns=columns_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data from SDE.Index and SDE.Monitoring to get one place for all stream data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "\"F:\\GIS\\DB_CONNECT\\Vector.sde\\SDE.Index\\SDE.StreamHabitat_CSCI\" does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\arcgis\\features\\geo\\_io\\fileops.py\u001b[0m in \u001b[0;36mfrom_featureclass\u001b[1;34m(filename, **kwargs)\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mdesc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marcpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m             \u001b[0marea_field\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"areaFieldName\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Object: Error in accessing describe",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9404\\3008187392.py\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# Create a DataFrame from SDE.StreamHabitat_CSCI\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0msdfCSCIfromIndex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_featureclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCSCIfromIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# Define columns for the new DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\arcgis\\features\\geo\\_accessor.py\u001b[0m in \u001b[0;36mfrom_featureclass\u001b[1;34m(location, **kwargs)\u001b[0m\n\u001b[0;32m   3160\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3161\u001b[0m         \"\"\"\n\u001b[1;32m-> 3162\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfrom_featureclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3164\u001b[0m     \u001b[1;31m# ----------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\arcgis\\features\\geo\\_io\\fileops.py\u001b[0m in \u001b[0;36mfrom_featureclass\u001b[1;34m(filename, **kwargs)\u001b[0m\n\u001b[0;32m    738\u001b[0m             \u001b[0mlength_field\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"lengthFieldName\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# for older versions of arcpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m             \u001b[0mdesc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marcpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m             \u001b[0mdesc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"fields\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shapeType\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshapeType\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m             \u001b[0marea_field\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"areaFieldName\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\__init__.py\u001b[0m in \u001b[0;36mDescribe\u001b[1;34m(value, data_type)\u001b[0m\n\u001b[0;32m   1333\u001b[0m        \u001b[1;32mclass\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mIn\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mcase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mclarify\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m        which dataset you want to describe.\"\"\"\n\u001b[1;32m-> 1335\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mCreateObject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\geoprocessing\\_base.py\u001b[0m in \u001b[0;36mdescribe\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0marcpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marcobjects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marcobjectconversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvertArcObjectToPythonObject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         return convertArcObjectToPythonObject(\n\u001b[1;32m--> 392\u001b[1;33m                     self._gp.Describe(*gp_fixargs(args, True)))\n\u001b[0m\u001b[0;32m    393\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreateObject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[1;34m\"\"\"GP function CreateObject\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: \"F:\\GIS\\DB_CONNECT\\Vector.sde\\SDE.Index\\SDE.StreamHabitat_CSCI\" does not exist"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from arcgis.features import FeatureLayer\n",
    "from arcgis import GIS\n",
    "import arcpy\n",
    "\n",
    "# Set up the GIS connection\n",
    "gis = GIS()\n",
    "\n",
    "# set workspace and sde connections \n",
    "working_folder = r\"F:\\Research and Analysis\\Fisheries\\Streams\\Bioassessment\\California Stream Condition Index\\California Stream Condition Index\"\n",
    "workspace      = \"C:\\GIS\\Scratch.gdb\"\n",
    "arcpy.env.workspace = \"C:\\GIS\\Scratch.gdb\"\n",
    "\n",
    "# network path to connection files\n",
    "filePath = r\"F:\\GIS\\DB_CONNECT\"\n",
    "\n",
    "# database file path \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "\n",
    "arcpy.env.workspace = workspace\n",
    "\n",
    "\n",
    "# Paths to feature classes\n",
    "CSCIfromIndex = os.path.join(sdeBase, \"SDE.Index\", \"SDE.StreamHabitat_CSCI\")\n",
    "\n",
    "\n",
    "# Create a DataFrame from SDE.StreamHabitat_CSCI\n",
    "sdfCSCIfromIndex = pd.DataFrame.spatial.from_featureclass(CSCIfromIndex)\n",
    "\n",
    "# Define columns for the new DataFrame\n",
    "columns_for_new_df = ['StationCod', 'F2000_score', 'F2007_score', 'F2008_score', 'F2009_score', 'F2010_score',\n",
    "                       'F2011_score', 'F2012_score', 'F2013_score', 'F2014_score', 'F2015_score', 'F2016_score',\n",
    "                       'F2017_score', 'F2018_score', 'F2019_score', 'LATITUDE', 'LONGITUDE']\n",
    "\n",
    "# Create a new DataFrame using the defined columns\n",
    "wide_df = pd.DataFrame(sdfCSCIfromIndex, columns=columns_for_new_df)\n",
    "\n",
    "# Melt the DataFrame to convert from wide to long format\n",
    "long_df = pd.melt(wide_df, id_vars=['StationCod', 'LATITUDE', 'LONGITUDE'], var_name='Year', value_name='CSCI')\n",
    "\n",
    "long_df = long_df[(long_df['CSCI'] != 0) & (~long_df['CSCI'].isnull())]\n",
    "\n",
    "# Extract the year from the 'Year' column\n",
    "long_df['Year'] = long_df['Year'].str.extract(r'(\\d{4})')\n",
    "\n",
    "print(\"Wide DataFrame:\")\n",
    "print(wide_df)\n",
    "\n",
    "print(\"\\nLong DataFrame:\")\n",
    "print(long_df)\n",
    "\n",
    "# Use a lookup dictionary to filter out any data that is already present in SDE.Streams\n",
    "# Create a dictionary using Rest Service data\n",
    "service_url = 'https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/8'\n",
    "feature_layer = FeatureLayer(service_url)\n",
    "query_result = feature_layer.query()\n",
    "\n",
    "# Convert the query result to a Spatially Enabled DataFrame\n",
    "sdfStreamHab = query_result.sdf\n",
    "\n",
    "# Select specific columns for lookup\n",
    "columns_to_keep = ['SITE_NAME', 'STATION_TYPE', 'LATITUDE', 'LONGITUDE', 'LTINFO']\n",
    "sdfStreamHab = sdfStreamHab.loc[:, columns_to_keep]\n",
    "\n",
    "# Remove duplicates\n",
    "unique_values = sdfStreamHab.drop_duplicates()\n",
    "\n",
    "# Convert selected columns to a dictionary\n",
    "lookup_dict = unique_values.set_index('SITE_NAME').to_dict(orient='index')\n",
    "\n",
    "# Filter out rows where 'StationCod' is in the lookup dictionary\n",
    "filtered_df = long_df[~long_df['StationCod'].isin(lookup_dict)]\n",
    "\n",
    "\n",
    "\n",
    "# Add new fields and fill them with values\n",
    "filtered_df.loc[:, 'Duration'] = 'One-time'\n",
    "filtered_df.loc[:, 'STATION_TYPE'] = 'Status'\n",
    "filtered_df.loc[:, 'LTINFO'] = 'Null'\n",
    "filtered_df.loc[:, 'STATION_CODE'] = sdfStreamHab['SITE_NAME']\n",
    "\n",
    "\n",
    "print(\"Filtered DataFrame:\")\n",
    "print(filtered_df)\n",
    "\n",
    "# Field Mapping\n",
    "field_mapping = {\n",
    "    'StationCod': 'SITE_NAME',\n",
    "    'STATION_CODE': 'STATION_CODE',\n",
    "    'Year': 'YEAR_OF_COUNT',\n",
    "    'LATITUDE': 'LATITUDE',\n",
    "    'LONGITUDE': 'LONGITUDE',\n",
    "    'CSCI': 'COUNT_VALUE',\n",
    "    'STATION_TYPE': 'STATION_TYPE',\n",
    "    'LTINFO': 'LTINFO'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "ready_df = filtered_df.rename(columns=field_mapping).drop(columns=[col for col in filtered_df.columns if col not in field_mapping])\n",
    "\n",
    "# Export to CSV\n",
    "ready_df.to_csv(os.path.join(working_folder, \"OldCSCIdata.csv\"), index=False)\n",
    "\n",
    "# Convert CSV to point feature class\n",
    "arcpy.management.XYTableToPoint(csv_path, \"oldStream_CSCI_Points\", \"LONGITUDE\", \"LATITUDE\")\n",
    "\n",
    "# Project to UTM Zone 10N\n",
    "arcpy.Project_management(\"oldStream_CSCI_Points\", \"old_CSCI_Projected\", arcpy.SpatialReference(26910))\n",
    "\n",
    "print(\"Conversion and projection completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'F:\\\\Research and Analysis\\\\Fisheries\\\\Streams\\\\Bioassessment\\\\California Stream Condition Index\\\\California Stream Condition Index\\\\OldCSCIdata.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9404\\773635724.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Convert DataFrame to CSV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcsv_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworking_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"OldCSCIdata.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mready_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Convert CSV to point feature class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3770\u001b[0m         )\n\u001b[0;32m   3771\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3772\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3773\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3774\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1184\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         )\n\u001b[1;32m-> 1186\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    238\u001b[0m         \"\"\"\n\u001b[0;32m    239\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    241\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'F:\\\\Research and Analysis\\\\Fisheries\\\\Streams\\\\Bioassessment\\\\California Stream Condition Index\\\\California Stream Condition Index\\\\OldCSCIdata.csv'"
     ]
    }
   ],
   "source": [
    "#convert CSV to feature class\n",
    "# Convert DataFrame to CSV\n",
    "csv_path = os.path.join(working_folder, \"OldCSCIdata.csv\")\n",
    "ready_df.to_csv(csv_path, index=False) \n",
    "\n",
    "# Convert CSV to point feature class\n",
    "arcpy.management.XYTableToPoint(csv_path, \"NewStream_CSCI_Points\", \"LONGITUDE\", \"LATITUDE\")\n",
    "\n",
    "# Project to UTM Zone 10N\n",
    "arcpy.Project_management(\"NewStream_CSCI_Points\", \"NewStream_CSCI_Projected\", arcpy.SpatialReference(26910))\n",
    "\n",
    "print(\"Conversion and projection completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis import GIS\n",
    "\n",
    "# Set up the GIS connection\n",
    "gis = GIS()\n",
    "\n",
    "# File paths and workspace\n",
    "workspace = r\"C:\\GIS\\Scratch.gdb\"\n",
    "arcpy.env.workspace = workspace\n",
    "\n",
    "# CSV file path\n",
    "csv_path = r\"F:\\Research and Analysis\\Fisheries\\Streams\\Bioassessment\\California Stream Condition Index\\California Stream Condition Index\\OldCSCIdata.csv\"\n",
    "\n",
    "# Read CSV into a Pandas DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert the Pandas DataFrame to a Spatially Enabled DataFrame\n",
    "sdf = pd.DataFrame.spatial.from_df(df)\n",
    "\n",
    "# SDEindexdata feature class name (change it as needed)\n",
    "SDEindexdata_fc = \"OldCSCIdata\"\n",
    "\n",
    "# Save the Spatially Enabled DataFrame to a feature class in the file geodatabase\n",
    "sdf.spatial.to_featureclass(location=os.path.join(workspace, SDEindexdata_fc))\n",
    "\n",
    "print(f\"CSV data imported to feature class '{SDEindexcscidata_fc}' in '{workspace}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
