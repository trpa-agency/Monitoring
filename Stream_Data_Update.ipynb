{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureSet, GeoAccessor, GeoSeriesAccessor, FeatureLayer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "gis = GIS()\n",
    "# # Set Pandas display options to show all rows and columns\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# in memory output file path\n",
    "wk_memory = \"memory\" + \"\\\\\"\n",
    "\n",
    "# set workspace and sde connections \n",
    "working_folder = r\"F:/Research and Analysis/Fisheries/Streams/Bioassessment/California Stream Condition Index/California Stream Condition Index\"\n",
    "workspace      = \"C:\\GIS\\Scratch.gdb\"\n",
    "arcpy.env.workspace = \"C:\\GIS\\Scratch.gdb\"\n",
    "\n",
    "# network path to connection files\n",
    "filePath = r\"C:\\\\GIS\\\\DB_CONNECT\"\n",
    "\n",
    "# database file path \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "\n",
    "# local variables\n",
    "fdata = os.path.join(sdeBase, \"sde.SDE.Monitoring\")\n",
    "## Final feature class to append to in Enterprise Geodatabase\n",
    "sdeStreams = os.path.join(sdeBase, \"sde.SDE.Monitoring\\sde.SDE.Stream\")\n",
    "## orginal CSVs that come from preprocessing or R tools created by State\n",
    "originalcsv22 = os.path.join(working_folder,\"2022_CSCI\",\"19-20NV-22allcore.csv\")\n",
    "locationcsv22 = os.path.join(working_folder, \"2022_CSCI\",\"Stations19_22.csv\")\n",
    "originalcsv20 = os.path.join(working_folder,\"2020_CSCI\",\"core.csv\")\n",
    "locationcsv20 = os.path.join(working_folder,\"2020_CSCI\",\"Stations_20.csv\")\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(originalcsv22):\n",
    "    print(f\"Error: File not found at {originalcsv22}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do I need this? -->\n",
    "df = get_fs_data('https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assign Station type and lat long and LTinfo website to Trend Sites\n",
    "\n",
    "\n",
    "#Calculate Rating for CSCI value\n",
    "#Define a function to categorize values based on ranges\n",
    "def categorize_value(value):\n",
    "    if 0 <= value < 0.6:\n",
    "        return 'poor'\n",
    "    elif 0.6 <= value < 0.8:\n",
    "        return 'marginal'\n",
    "    elif 0.8 <= value < 1.0:\n",
    "        return 'good'\n",
    "    else:\n",
    "        return 'excellent'\n",
    "    \n",
    "def get_fs_data(service_url):\n",
    "    \n",
    "    feature_layer = FeatureLayer(service_url)\n",
    "    query_result = feature_layer.query()\n",
    "    # Convert the query result to a list of dictionaries\n",
    "    feature_list = query_result.features\n",
    "\n",
    "    # Create a pandas DataFrame from the list of dictionaries\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dictionary Usring Rest Service data\n",
    "# setup\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureLayer\n",
    "\n",
    "# Connect to TRPA Enterprise GIS Portal *if it's a service only shared with org\n",
    "# portal_user = \"TRPA_PORTAL_ADMIN\"\n",
    "# portal_pwd = str(os.environ.get('Password'))\n",
    "# portal_url = \"https://maps.trpa.org/portal/\"\n",
    "\n",
    "# setup connection to GIS server this can be GIS() with a public service\n",
    "gis = GIS()\n",
    "\n",
    "\n",
    "# get parcel master as a Spatially Enabled Dataframe\n",
    "service_url = 'https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/8'\n",
    "feature_layer = FeatureLayer(service_url)\n",
    "query_result = feature_layer.query()\n",
    "\n",
    "# Convert the query result to a Spatially Enabled Dataframe\n",
    "sdfStreamHab = query_result.sdf\n",
    "\n",
    "sdfStreamHab.info()\n",
    "columnstokeep = ['SITE_NAME','STATION_TYPE', 'LATITUDE', 'LONGITUDE', 'LTINFO']\n",
    "sdfStreamHab = sdfStreamHab.loc[:, columnstokeep]\n",
    "unique_values = sdfStreamHab.drop_duplicates()\n",
    "\n",
    "# Select specific columns for look up\n",
    "selected_columns = ['STATION_TYPE', 'LATITUDE', 'LONGITUDE', 'LTINFO']\n",
    "\n",
    "# Convert selected columns to dictionary\n",
    "lookup_dict = unique_values.set_index('SITE_NAME')[selected_columns].to_dict(orient='index')\n",
    "\n",
    "# Display the dictionary\n",
    "print(lookup_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd \\Research and Analysis\\Fisheries\\Streams\\Bioassessment\\California Stream Condition Index\\California Stream Condition Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform CSCI Scores to Point feature class in Enterprise Geodatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snewsome\\AppData\\Local\\Temp\\ipykernel_34732\\4006546074.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfCSCI = dfCSCI.append(pd.read_csv(originalcsv20), ignore_index=True)\n",
      "C:\\Users\\snewsome\\AppData\\Local\\Temp\\ipykernel_34732\\4006546074.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dflocations = dflocations.append(pd.read_csv(locationcsv20), ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrames from CSV files\n",
    "dfCSCI = pd.read_csv(originalcsv22)\n",
    "dflocations = pd.read_csv(locationcsv22)\n",
    "\n",
    "# Append DataFrames from additional CSV files\n",
    "dfCSCI = dfCSCI.append(pd.read_csv(originalcsv20), ignore_index=True)\n",
    "dflocations = dflocations.append(pd.read_csv(locationcsv20), ignore_index=True)\n",
    "\n",
    "# merge CSCI scores and location data\n",
    "RawData_df = pd.merge(dfCSCI, dflocations, how='inner', on='StationCode')\n",
    "\n",
    "try:\n",
    "    dfCSCI = pd.read_csv(originalcsv22)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please check the file path.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"The CSV file is empty.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error parsing the CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get year from sample id\n",
    "RawData_df['Year']=RawData_df.SampleID.str.split(\"_\").str[-1]\n",
    "\n",
    "#Calculate Station Type \n",
    "RawData_df['STATION_TYPE'] = RawData_df['StationCode'].map(lambda x: lookup_dict[x]['STATION_TYPE'] if x in lookup_dict else 'Status')\n",
    "\n",
    "#Calculate LATITUDE\n",
    "RawData_df['LATITUDE'] = RawData_df['StationCode'].map(lambda x: lookup_dict[x]['LATITUDE']if x in lookup_dict else None)\n",
    "RawData_df['LATITUDE'] = RawData_df['LATITUDE'].fillna(RawData_df['New_Lat'])\n",
    "#Calculate LONGITUDE\n",
    "RawData_df['LONGITUDE'] = RawData_df['StationCode'].map(lambda x: lookup_dict[x]['LONGITUDE']if x in lookup_dict else None)\n",
    "RawData_df['LONGITUDE'] = RawData_df['LONGITUDE'].fillna(RawData_df['New_Long'])\n",
    "#Caculate LTINFO\n",
    "RawData_df['LTINFO'] = RawData_df['StationCode'].map(lambda x: lookup_dict[x]['LTINFO'] if x in lookup_dict else None)\n",
    "\n",
    "# Apply the categorization function to create the new field\n",
    "RawData_df['Rating'] = RawData_df['CSCI'].apply(categorize_value)\n",
    "\n",
    "\n",
    "Field_Mapping={\n",
    "    'StationCode': 'SITE_NAME',\n",
    "    'Year': 'YEAR_OF_COUNT',\n",
    "    'LATITUDE': 'LATITUDE',\n",
    "    'LONGITUDE': 'LONGITUDE',\n",
    "    'CSCI': 'COUNT_VALUE',\n",
    "    'STATION_TYPE': 'STATION_TYPE',\n",
    "    'LTINFO': 'LTINFO'\n",
    "    \n",
    "}\n",
    "# rename feilds based on field mappings\n",
    "df_final = RawData_df.rename(columns=Field_Mapping).drop(columns=[col for col in RawData_df.columns if col not in Field_Mapping])\n",
    "\n",
    "# establish duration field\n",
    "def assign_duration(stationtype):\n",
    "    if stationtype == 'Status' :\n",
    "        return 'One-time'\n",
    "    else:\n",
    "        return 'Long-term'\n",
    "df_final['DURATION']= df_final['STATION_TYPE'].apply(assign_duration)\n",
    "\n",
    "# station code is site name, site name is station code.\n",
    "df_final['STATION_CODE']=df_final['SITE_NAME']\n",
    "\n",
    "# export to csv\n",
    "df_final.to_csv(os.path.join(working_folder,\"StreamCSCI_proccesed.csv\"), index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Thursday, December 14, 2023 3:21:57 PM\",\"Succeeded at Thursday, December 14, 2023 3:21:57 PM (Elapsed Time: 0.40 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'C:\\\\GIS\\\\Scratch.gdb\\\\NewStream_CSCI_Projected'>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert CSV to point feature class\n",
    "arcpy.management.XYTableToPoint(os.path.join(working_folder,\"StreamCSCI_proccesed.csv\"), \n",
    "                                \"NewCSCI_points\", \n",
    "                                \"LONGITUDE\", \"LATITUDE\")\n",
    "\n",
    "# project to UTM Zone 10N\n",
    "arcpy.Project_management(\"NewCSCI_points\", \"NewStream_CSCI_Projected\", 26910)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputfc= \"NewStream_CSCI_Projected\"\n",
    "\n",
    "# disconnect all users\n",
    "print(\"\\nDisconnecting all users...\")\n",
    "arcpy.DisconnectUser(sdeBase, \"ALL\")\n",
    " \n",
    "# unregister the sde feature class as versioned\n",
    "print (\"\\nUnregistering feature dataset as versioned...\")\n",
    "arcpy.UnregisterAsVersioned_management(fdata,\"NO_KEEP_EDIT\",\"COMPRESS_DEFAULT\")\n",
    "print (\"\\nFinished unregistering feature dataset as versioned.\")\n",
    "\n",
    "arcpy.management.Append(inputfc, sdeStreams,\"NO_TEST\")\n",
    "\n",
    "# disconnect all users\n",
    "print(\"\\nDisconnecting all users...\")\n",
    "arcpy.DisconnectUser(sdeBase, \"ALL\")\n",
    "\n",
    "# register SDE feature class as versioned\n",
    "arcpy.RegisterAsVersioned_management(fdata, \"NO_EDITS_TO_BASE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import raw tabular data from csv\n",
    "csci_data = pd.read_csv('Raw_Data\\csci_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot to new format\n",
    "grouping_columns = ['StationCode','STREAM_NAME', 'COUNTY']\n",
    "csci_data_flat = csci_data.pivot(index=grouping_columns, columns='SAMPLEDATE',values='CSCI')\n",
    "csci_data_flat = csci_data_flat.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to create stream csci sampling stations\n",
    "stream_csci_input_layer = \"import\"\n",
    "unique_stream_samples = \"\"\n",
    "\n",
    "stations=[]\n",
    "# Create a search cursor to iterate through the original feature layer\n",
    "with arcpy.da.SearchCursor(stream_csci_input_layer, ['SITE_NAME']) as cursor:\n",
    "    for row in cursor:\n",
    "        value = row[0]\n",
    "        if value not in stations:\n",
    "            stations.append(value)\n",
    "\n",
    "# Create an insert cursor for the output feature layer\n",
    "with arcpy.da.InsertCursor(output_feature_layer, [unique_column]) as cursor:\n",
    "    for value in unique_values:\n",
    "        cursor.insertRow((value,))\n",
    "\n",
    "# Clean up\n",
    "del cursor\n",
    "\n",
    "# Optionally, save the output feature layer to a file\n",
    "arcpy.CopyFeatures_management(output_feature_layer, \"C:/Path/To/Your/Output/FeatureClassFile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New feature class with the sampling station\n",
    "\n",
    "merged_df = pd.merge(sdfCensus, tdc_flat, on='TRPAID', how='inner')\n",
    "columns_drop=['GlobalID', 'YEAR', 'created_date', 'created_user', 'last_edited_date', 'last_edited_user', 'Shape.STArea()', 'Shape.STLength()']\n",
    "merged_df = merged_df.drop(columns=columns_drop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
