{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureSet, GeoAccessor, GeoSeriesAccessor, FeatureLayer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "gis = GIS()\n",
    "# # Set Pandas display options to show all rows and columns\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# in memory output file path\n",
    "wk_memory = \"memory\" + \"\\\\\"\n",
    "\n",
    "# set workspace and sde connections \n",
    "working_folder = r\"F:/Research and Analysis/Fisheries/Streams/Bioassessment/California Stream Condition Index/California Stream Condition Index\"\n",
    "workspace      = \"C:\\GIS\\Scratch.gdb\"\n",
    "arcpy.env.workspace = \"C:\\GIS\\Scratch.gdb\"\n",
    "\n",
    "# network path to connection files\n",
    "filePath = r\"C:\\\\GIS\\\\DB_CONNECT\"\n",
    "\n",
    "# database file path \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "\n",
    "# local variables\n",
    "fdata = os.path.join(sdeBase, \"sde.SDE.Monitoring\")\n",
    "## Final feature class to append to in Enterprise Geodatabase\n",
    "sdeStreams = os.path.join(sdeBase, \"sde.SDE.Monitoring\\sde.SDE.Stream\")\n",
    "## orginal CSVs that come from preprocessing or R tools created by State\n",
    "originalcsv22 = os.path.join(working_folder,\"2022_CSCI\",\"19-20NV-22allcore.csv\")\n",
    "locationcsv22 = os.path.join(working_folder, \"2022_CSCI\",\"Stations19_22.csv\")\n",
    "originalcsv20 = os.path.join(working_folder,\"2020_CSCI\",\"core.csv\")\n",
    "locationcsv20 = os.path.join(working_folder,\"2020_CSCI\",\"Stations_20.csv\")\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(originalcsv22):\n",
    "    print(f\"Error: File not found at {originalcsv22}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do I need this? -->\n",
    "df = get_fs_data('https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assign Station type and lat long and LTinfo website to Trend Sites\n",
    "\n",
    "\n",
    "#Calculate Rating for CSCI value\n",
    "#Define a function to categorize values based on ranges\n",
    "def categorize_value(value):\n",
    "    if 0 <= value < 0.6:\n",
    "        return 'poor'\n",
    "    elif 0.6 <= value < 0.8:\n",
    "        return 'marginal'\n",
    "    elif 0.8 <= value < 1.0:\n",
    "        return 'good'\n",
    "    else:\n",
    "        return 'excellent'\n",
    "    \n",
    "def get_fs_data(service_url):\n",
    "    \n",
    "    feature_layer = FeatureLayer(service_url)\n",
    "    query_result = feature_layer.query()\n",
    "    # Convert the query result to a list of dictionaries\n",
    "    feature_list = query_result.features\n",
    "\n",
    "    # Create a pandas DataFrame from the list of dictionaries\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dictionary Usring Rest Service data\n",
    "# setup\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureLayer\n",
    "\n",
    "# Connect to TRPA Enterprise GIS Portal *if it's a service only shared with org\n",
    "# portal_user = \"TRPA_PORTAL_ADMIN\"\n",
    "# portal_pwd = str(os.environ.get('Password'))\n",
    "# portal_url = \"https://maps.trpa.org/portal/\"\n",
    "\n",
    "# setup connection to GIS server this can be GIS() with a public service\n",
    "gis = GIS()\n",
    "\n",
    "\n",
    "# get Stream data as a Spatially Enabled Dataframe\n",
    "service_url = 'https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/8'\n",
    "feature_layer = FeatureLayer(service_url)\n",
    "query_result = feature_layer.query()\n",
    "\n",
    "# Convert the query result to a Spatially Enabled Dataframe\n",
    "sdfStreamHab = query_result.sdf\n",
    "\n",
    "sdfStreamHab.info()\n",
    "columnstokeep = ['SITE_NAME','STATION_TYPE', 'LATITUDE', 'LONGITUDE', 'LTINFO']\n",
    "sdfStreamHab = sdfStreamHab.loc[:, columnstokeep]\n",
    "unique_values = sdfStreamHab.drop_duplicates()\n",
    "\n",
    "# Select specific columns for look up\n",
    "selected_columns = ['STATION_TYPE', 'LATITUDE', 'LONGITUDE', 'LTINFO']\n",
    "\n",
    "# Convert selected columns to dictionary\n",
    "lookup_dict = unique_values.set_index('SITE_NAME')[selected_columns].to_dict(orient='index')\n",
    "\n",
    "# Display the dictionary\n",
    "print(lookup_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd \\Research and Analysis\\Fisheries\\Streams\\Bioassessment\\California Stream Condition Index\\California Stream Condition Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform CSCI Scores to Point feature class in Enterprise Geodatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snewsome\\AppData\\Local\\Temp\\ipykernel_34732\\4006546074.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfCSCI = dfCSCI.append(pd.read_csv(originalcsv20), ignore_index=True)\n",
      "C:\\Users\\snewsome\\AppData\\Local\\Temp\\ipykernel_34732\\4006546074.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dflocations = dflocations.append(pd.read_csv(locationcsv20), ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrames from CSV files\n",
    "dfCSCI = pd.read_csv(originalcsv22)\n",
    "dflocations = pd.read_csv(locationcsv22)\n",
    "\n",
    "# Append DataFrames from additional CSV files\n",
    "dfCSCI = dfCSCI.append(pd.read_csv(originalcsv20), ignore_index=True)\n",
    "dflocations = dflocations.append(pd.read_csv(locationcsv20), ignore_index=True)\n",
    "\n",
    "# merge CSCI scores and location data\n",
    "RawData_df = pd.merge(dfCSCI, dflocations, how='inner', on='StationCode')\n",
    "\n",
    "try:\n",
    "    dfCSCI = pd.read_csv(originalcsv22)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please check the file path.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"The CSV file is empty.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error parsing the CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get year from sample id\n",
    "RawData_df['Year']=RawData_df.SampleID.str.split(\"_\").str[-1]\n",
    "\n",
    "#Calculate Station Type \n",
    "RawData_df['STATION_TYPE'] = RawData_df['StationCode'].map(lambda x: lookup_dict[x]['STATION_TYPE'] if x in lookup_dict else 'Status')\n",
    "\n",
    "#Calculate LATITUDE\n",
    "RawData_df['LATITUDE'] = RawData_df['StationCode'].map(lambda x: lookup_dict[x]['LATITUDE']if x in lookup_dict else None)\n",
    "RawData_df['LATITUDE'] = RawData_df['LATITUDE'].fillna(RawData_df['New_Lat'])\n",
    "#Calculate LONGITUDE\n",
    "RawData_df['LONGITUDE'] = RawData_df['StationCode'].map(lambda x: lookup_dict[x]['LONGITUDE']if x in lookup_dict else None)\n",
    "RawData_df['LONGITUDE'] = RawData_df['LONGITUDE'].fillna(RawData_df['New_Long'])\n",
    "#Caculate LTINFO\n",
    "RawData_df['LTINFO'] = RawData_df['StationCode'].map(lambda x: lookup_dict[x]['LTINFO'] if x in lookup_dict else None)\n",
    "\n",
    "# Apply the categorization function to create the new field\n",
    "RawData_df['Rating'] = RawData_df['CSCI'].apply(categorize_value)\n",
    "\n",
    "\n",
    "Field_Mapping={\n",
    "    'StationCode': 'SITE_NAME',\n",
    "    'Year': 'YEAR_OF_COUNT',\n",
    "    'LATITUDE': 'LATITUDE',\n",
    "    'LONGITUDE': 'LONGITUDE',\n",
    "    'CSCI': 'COUNT_VALUE',\n",
    "    'STATION_TYPE': 'STATION_TYPE',\n",
    "    'LTINFO': 'LTINFO'\n",
    "    \n",
    "}\n",
    "# rename feilds based on field mappings\n",
    "df_final = RawData_df.rename(columns=Field_Mapping).drop(columns=[col for col in RawData_df.columns if col not in Field_Mapping])\n",
    "\n",
    "# establish duration field\n",
    "def assign_duration(stationtype):\n",
    "    if stationtype == 'Status' :\n",
    "        return 'One-time'\n",
    "    else:\n",
    "        return 'Long-term'\n",
    "df_final['DURATION']= df_final['STATION_TYPE'].apply(assign_duration)\n",
    "\n",
    "# station code is site name, site name is station code.\n",
    "df_final['STATION_CODE']=df_final['SITE_NAME']\n",
    "\n",
    "# export to csv\n",
    "df_final.to_csv(os.path.join(working_folder,\"StreamCSCI_proccesed.csv\"), index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Thursday, December 14, 2023 3:21:57 PM\",\"Succeeded at Thursday, December 14, 2023 3:21:57 PM (Elapsed Time: 0.40 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'C:\\\\GIS\\\\Scratch.gdb\\\\NewStream_CSCI_Projected'>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert CSV to point feature class\n",
    "arcpy.management.XYTableToPoint(os.path.join(working_folder,\"StreamCSCI_proccesed.csv\"), \n",
    "                                \"NewCSCI_points\", \n",
    "                                \"LONGITUDE\", \"LATITUDE\")\n",
    "\n",
    "# project to UTM Zone 10N\n",
    "arcpy.Project_management(\"NewCSCI_points\", \"NewStream_CSCI_Projected\", 26910)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputfc= \"NewStream_CSCI_Projected\"\n",
    "\n",
    "# disconnect all users\n",
    "print(\"\\nDisconnecting all users...\")\n",
    "arcpy.DisconnectUser(sdeBase, \"ALL\")\n",
    " \n",
    "# unregister the sde feature class as versioned\n",
    "print (\"\\nUnregistering feature dataset as versioned...\")\n",
    "arcpy.UnregisterAsVersioned_management(fdata,\"NO_KEEP_EDIT\",\"COMPRESS_DEFAULT\")\n",
    "print (\"\\nFinished unregistering feature dataset as versioned.\")\n",
    "\n",
    "arcpy.management.Append(inputfc, sdeStreams,\"NO_TEST\")\n",
    "\n",
    "# disconnect all users\n",
    "print(\"\\nDisconnecting all users...\")\n",
    "arcpy.DisconnectUser(sdeBase, \"ALL\")\n",
    "\n",
    "# register SDE feature class as versioned\n",
    "arcpy.RegisterAsVersioned_management(fdata, \"NO_EDITS_TO_BASE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import raw tabular data from csv\n",
    "csci_data = pd.read_csv('Raw_Data\\csci_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot to new format\n",
    "grouping_columns = ['StationCode','STREAM_NAME', 'COUNTY']\n",
    "csci_data_flat = csci_data.pivot(index=grouping_columns, columns='SAMPLEDATE',values='CSCI')\n",
    "csci_data_flat = csci_data_flat.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to create stream csci sampling stations\n",
    "stream_csci_input_layer = \"import\"\n",
    "unique_stream_samples = \"\"\n",
    "\n",
    "stations=[]\n",
    "# Create a search cursor to iterate through the original feature layer\n",
    "with arcpy.da.SearchCursor(stream_csci_input_layer, ['SITE_NAME']) as cursor:\n",
    "    for row in cursor:\n",
    "        value = row[0]\n",
    "        if value not in stations:\n",
    "            stations.append(value)\n",
    "\n",
    "# Create an insert cursor for the output feature layer\n",
    "with arcpy.da.InsertCursor(output_feature_layer, [unique_column]) as cursor:\n",
    "    for value in unique_values:\n",
    "        cursor.insertRow((value,))\n",
    "\n",
    "# Clean up\n",
    "del cursor\n",
    "\n",
    "# Optionally, save the output feature layer to a file\n",
    "arcpy.CopyFeatures_management(output_feature_layer, \"C:/Path/To/Your/Output/FeatureClassFile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New feature class with the sampling station\n",
    "\n",
    "merged_df = pd.merge(sdfCensus, tdc_flat, on='TRPAID', how='inner')\n",
    "columns_drop=['GlobalID', 'YEAR', 'created_date', 'created_user', 'last_edited_date', 'last_edited_user', 'Shape.STArea()', 'Shape.STLength()']\n",
    "merged_df = merged_df.drop(columns=columns_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data from SDE.Index and SDE.Monitoring to get one place for all stream data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame:\n",
      "     StationCod   LATITUDE   LONGITUDE  Year      CSCI  Duration STATION_TYPE  \\\n",
      "0     634001CLD  38.892139 -119.901891  2000            One-time       Status   \n",
      "1     634001GNL  39.053013 -120.115527  2000            One-time       Status   \n",
      "2     634001HVL  38.920892 -119.970389  2000            One-time       Status   \n",
      "3     634001LKF   39.18347 -120.119374  2000            One-time       Status   \n",
      "4     634001MEI  38.733374 -120.005478  2000            One-time       Status   \n",
      "...         ...        ...         ...   ...       ...       ...          ...   \n",
      "2118  634S19751  39.197029  -120.09652  2019  1.108738  One-time       Status   \n",
      "2119  634S19800  39.106619 -120.192954  2019  1.116173  One-time       Status   \n",
      "2120  634SAXOFS   38.87088  -119.98142  2019  1.166551  One-time       Status   \n",
      "2167  634UTR004   38.87912  -120.00146  2019  0.934572  One-time       Status   \n",
      "2168  634UTR005   38.86235  -120.02308  2019  1.026955  One-time       Status   \n",
      "\n",
      "     LTINFO STATION_CODE  \n",
      "0      Null    634001CLD  \n",
      "1      Null    634001GNL  \n",
      "2      Null    634001HVL  \n",
      "3      Null    634001LKF  \n",
      "4      Null    634001MEI  \n",
      "...     ...          ...  \n",
      "2118   Null    634S19751  \n",
      "2119   Null    634S19800  \n",
      "2120   Null    634SAXOFS  \n",
      "2167   Null    634UTR004  \n",
      "2168   Null    634UTR005  \n",
      "\n",
      "[413 rows x 9 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Wednesday, February 7, 2024 12:37:38 PM\",\"Succeeded at Wednesday, February 7, 2024 12:38:16 PM (Elapsed Time: 37.79 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'F:\\\\Research and Analysis\\\\Workspace\\\\Sarah\\\\Data Management 2023\\\\Scratch.gdb\\\\old_CSCI_Projected'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from arcgis.features import FeatureLayer\n",
    "from arcgis import GIS\n",
    "import arcpy\n",
    "from arcgis.features import GeoAccessor\n",
    "import os\n",
    "\n",
    "# Set up the GIS connection\n",
    "gis = GIS()\n",
    "\n",
    "# set workspace and sde connections \n",
    "working_folder = r\"F:\\Research and Analysis\\Fisheries\\Streams\\Bioassessment\\California Stream Condition Index\\California Stream Condition Index\"\n",
    "workspace      = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb\"\n",
    "arcpy.env.workspace = r\"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\"\n",
    "\n",
    "# network path to connection files\n",
    "filePath = r\"F:\\GIS\\DB_CONNECT\"\n",
    "# network path to connection files\n",
    "#filePath = r'F:\\Research and Analysis\\Workspace\\Sarah'\n",
    "\n",
    "# database file path \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "# database file path \n",
    "#sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "#sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "\n",
    "arcpy.env.workspace = workspace\n",
    "\n",
    "\n",
    "# Paths to feature classes\n",
    "CSCIfromIndex = os.path.join(sdeBase, \"SDE.Index\", \"SDE.StreamHabitat_CSCI\")\n",
    "\n",
    "\n",
    "# Create a DataFrame from SDE.StreamHabitat_CSCI\n",
    "sdfCSCIfromIndex = pd.DataFrame.spatial.from_featureclass(CSCIfromIndex)\n",
    "\n",
    "# Define columns for the new DataFrame\n",
    "columns_for_new_df = ['StationCod', 'F2000_score', 'F2007_score', 'F2008_score', 'F2009_score', 'F2010_score',\n",
    "                       'F2011_score', 'F2012_score', 'F2013_score', 'F2014_score', 'F2015_score', 'F2016_score',\n",
    "                       'F2017_score', 'F2018_score', 'F2019_score', 'LATITUDE', 'LONGITUDE']\n",
    "\n",
    "# Create a new DataFrame using the defined columns\n",
    "wide_df = pd.DataFrame(sdfCSCIfromIndex, columns=columns_for_new_df)\n",
    "\n",
    "# Melt the DataFrame to convert from wide to long format\n",
    "long_df = pd.melt(wide_df, id_vars=['StationCod', 'LATITUDE', 'LONGITUDE'], var_name='Year', value_name='CSCI')\n",
    "\n",
    "long_df = long_df[(long_df['CSCI'] != 0) & (~long_df['CSCI'].isnull())]\n",
    "\n",
    "# Extract the year from the 'Year' column\n",
    "long_df['Year'] = long_df['Year'].str.extract(r'(\\d{4})')\n",
    "\n",
    "#print(\"Wide DataFrame:\")\n",
    "#print(wide_df)\n",
    "\n",
    "#print(\"\\nLong DataFrame:\")\n",
    "#print(long_df)\n",
    "\n",
    "# Use a lookup dictionary to filter out any data that is already present in SDE.Streams\n",
    "# Create a dictionary using Rest Service data\n",
    "service_url = 'https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/8'\n",
    "feature_layer = FeatureLayer(service_url)\n",
    "query_result = feature_layer.query()\n",
    "\n",
    "# Convert the query result to a Spatially Enabled DataFrame\n",
    "sdfStreamHab = query_result.sdf\n",
    "\n",
    "# Select specific columns for lookup\n",
    "columns_to_keep = ['SITE_NAME', 'STATION_TYPE', 'LATITUDE', 'LONGITUDE', 'LTINFO']\n",
    "sdfStreamHab = sdfStreamHab.loc[:, columns_to_keep]\n",
    "\n",
    "# Remove duplicates\n",
    "unique_values = sdfStreamHab.drop_duplicates()\n",
    "\n",
    "# Convert selected columns to a dictionary\n",
    "lookup_dict = unique_values.set_index('SITE_NAME').to_dict(orient='index')\n",
    "\n",
    "# Filter out rows where 'StationCod' is in the lookup dictionary\n",
    "filtered_df = long_df[~long_df['StationCod'].isin(lookup_dict)]\n",
    "\n",
    "\n",
    "\n",
    "# Add new fields and fill them with values\n",
    "filtered_df = filtered_df.copy()\n",
    "filtered_df.loc[:, 'Duration'] = 'One-time'\n",
    "filtered_df.loc[:, 'STATION_TYPE'] = 'Status'\n",
    "filtered_df.loc[:, 'LTINFO'] = 'Null'\n",
    "filtered_df['STATION_CODE'] = filtered_df['StationCod']\n",
    "#filtered_df.loc[:, 'STATION_CODE'] = sdfStreamHab['SITE_NAME']\n",
    "# Drop rows with null values in the COUNT_VALUE column\n",
    "filtered_df.dropna(subset=['CSCI'], inplace=True)\n",
    "\n",
    "print(\"Filtered DataFrame:\")\n",
    "print(filtered_df)\n",
    "\n",
    "# Field Mapping\n",
    "field_mapping = {\n",
    "    'StationCod': 'SITE_NAME',\n",
    "    'STATION_CODE': 'STATION_CODE',\n",
    "    'Year': 'YEAR_OF_COUNT',\n",
    "    'LATITUDE': 'LATITUDE',\n",
    "    'LONGITUDE': 'LONGITUDE',\n",
    "    'CSCI': 'COUNT_VALUE',\n",
    "    'STATION_TYPE': 'STATION_TYPE',\n",
    "    'LTINFO': 'LTINFO'\n",
    "}\n",
    "\n",
    "# Rename fields based on field mappings\n",
    "ready_df = filtered_df.rename(columns=field_mapping).drop(columns=[col for col in filtered_df.columns if col not in field_mapping])\n",
    "\n",
    "# Drop rows with null values in the COUNT_VALUE column\n",
    "#ready_df.dropna(subset=['COUNT_VALUE'], inplace=True)\n",
    "\n",
    "# Convert DataFrame to Spatially Enabled DataFrame\n",
    "3#sedf = GeoAccessor.from_xy(ready_df, x_column='LONGITUDE', y_column='LATITUDE')\n",
    "\n",
    "# Convert the SEDF to a feature class without sanitizing columns\n",
    "#sedf.spatial.to_featureclass(location=os.path.join(workspace, 'CSCIfromIndex_Stagingg'), sanitize_columns=False)\n",
    "\n",
    "# Specify the input feature class (CSCIfromIndex_Staging) and the output projected feature class name\n",
    "#input_feature_class = os.path.join(workspace, 'CSCIfromIndex_Stagingg')\n",
    "#output_projected_feature_class = os.path.join(workspace, 'CSCIfromIndex_Projected_ready')\n",
    "\n",
    "# Specify the output spatial reference system (SR) using either a well-known ID (WKID) or a path to a .prj file\n",
    "#output_spatial_reference = arcpy.SpatialReference(26910)  \n",
    "\n",
    "# Project the feature class\n",
    "#arcpy.Project_management(input_feature_class, output_projected_feature_class, output_spatial_reference)\n",
    "\n",
    "#print(\"Projection completed.\")\n",
    "\n",
    "\n",
    "# Export to CSV\n",
    "ready_df.to_csv(os.path.join(working_folder, \"OldCSCIdata.csv\"), index=False)\n",
    "\n",
    "csv_path = r\"F:\\Research and Analysis\\Fisheries\\Streams\\Bioassessment\\California Stream Condition Index\\California Stream Condition Index\\OldCSCIdata.csv\" \n",
    "\n",
    "# Convert CSV to point feature class\n",
    "arcpy.management.XYTableToPoint(csv_path, \"oldStream_CSCI_Points\", \"LONGITUDE\", \"LATITUDE\")\n",
    "\n",
    "# Project to UTM Zone 10N\n",
    "arcpy.Project_management(\"oldStream_CSCI_Points\", \"old_CSCI_Projected\", arcpy.SpatialReference(26910))\n",
    "\n",
    "#print(\"Conversion and projection completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<StringArray>\n",
      "['634001CLD', '634001GNL', '634001HVL', '634001LKF', '634001MEI', '634001SAX',\n",
      " '634001TAY', '634001TRT', '634002SAX', '634002TRT', '634BLW001', '634MCK001',\n",
      " '634S10104', '634S10114', '634S11160', '634S11162', '634S12224', '634S13242',\n",
      " '634S14251', '634S14257', '634S14296', '634S16265', '634S16291', '634S16297',\n",
      " '634S16302', '634S16303', '634S16307', '634S16309', '634S16310', '634S16311',\n",
      " '634S16313', '634S16317', '634S16318', '634S16351', '634S16358', '634S16360',\n",
      " '634S16362', '634S16364', '634S16368', '634S16380', '634S17322', '634S17323',\n",
      " '634S17325', '634S17329', '634S17334', '634S17335', '634S17341', '634S17342',\n",
      " '634S17343', '634S17344', '634S17345', '634S17346', '634S17384', '634S17387',\n",
      " '634S17390', '634S17392', '634S17400', '634S17408', '634S17416', '634S18347',\n",
      " '634S18353', '634S18355', '634S18357', '634S18359', '634S18363', '634S18369',\n",
      " '634S18370', '634S18372', '634S18373', '634S18374', '634S18377', '634S18379',\n",
      " '634S18420', '634S18424', '634S18428', '634S18444', '634S18447', '634S18449',\n",
      " '634S18459', '634S19217', '634S19410', '634S19498', '634S19505', '634S19557',\n",
      " '634S19562', '634S19573', '634S19605', '634S19606', '634S19658', '634S19662',\n",
      " '634S19714', '634S19738', '634S19751', '634S19800', '634SAXOFS', '634TRT002',\n",
      " '634TRT003', '634UTR004', '634UTR005', '634UTR006']\n",
      "Length: 100, dtype: string\n"
     ]
    }
   ],
   "source": [
    "print(filtered_df['StationCod'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'F:\\\\Research and Analysis\\\\Fisheries\\\\Streams\\\\Bioassessment\\\\California Stream Condition Index\\\\California Stream Condition Index\\\\OldCSCIdata.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9404\\773635724.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Convert DataFrame to CSV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcsv_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworking_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"OldCSCIdata.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mready_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Convert CSV to point feature class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3770\u001b[0m         )\n\u001b[0;32m   3771\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3772\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3773\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3774\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1184\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         )\n\u001b[1;32m-> 1186\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    238\u001b[0m         \"\"\"\n\u001b[0;32m    239\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    241\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'F:\\\\Research and Analysis\\\\Fisheries\\\\Streams\\\\Bioassessment\\\\California Stream Condition Index\\\\California Stream Condition Index\\\\OldCSCIdata.csv'"
     ]
    }
   ],
   "source": [
    "#convert CSV to feature class\n",
    "# Convert DataFrame to CSV\n",
    "#csv_path = os.path.join(working_folder, \"OldCSCIdata.csv\")\n",
    "#ready_df.to_csv(csv_path, index=False) \n",
    "\n",
    "# Convert CSV to point feature class\n",
    "#arcpy.management.XYTableToPoint(csv_path, \"NewStream_CSCI_Points\", \"LONGITUDE\", \"LATITUDE\")\n",
    "\n",
    "# Project to UTM Zone 10N\n",
    "#arcpy.Project_management(\"NewStream_CSCI_Points\", \"NewStream_CSCI_Projected\", arcpy.SpatialReference(26910))\n",
    "\n",
    "#print(\"Conversion and projection completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis import GIS\n",
    "\n",
    "# Set up the GIS connection\n",
    "gis = GIS()\n",
    "\n",
    "# File paths and workspace\n",
    "workspace = r\"C:\\GIS\\Scratch.gdb\"\n",
    "arcpy.env.workspace = workspace\n",
    "\n",
    "# CSV file path\n",
    "csv_path = r\"F:\\Research and Analysis\\Fisheries\\Streams\\Bioassessment\\California Stream Condition Index\\California Stream Condition Index\\OldCSCIdata.csv\"\n",
    "\n",
    "# Read CSV into a Pandas DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert the Pandas DataFrame to a Spatially Enabled DataFrame\n",
    "sdf = pd.DataFrame.spatial.from_df(df)\n",
    "\n",
    "# SDEindexdata feature class name (change it as needed)\n",
    "SDEindexdata_fc = \"OldCSCIdata\"\n",
    "\n",
    "# Save the Spatially Enabled DataFrame to a feature class in the file geodatabase\n",
    "sdf.spatial.to_featureclass(location=os.path.join(workspace, SDEindexdata_fc))\n",
    "\n",
    "print(f\"CSV data imported to feature class '{SDEindexcscidata_fc}' in '{workspace}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
